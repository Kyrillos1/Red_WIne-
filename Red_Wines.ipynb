{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b0d534c"
      },
      "source": [
        "## Name: Kerolous Ashraf Gmail\n",
        "## ID: 2002015"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "n3m505D1K3Xx"
      },
      "outputs": [],
      "source": [
        "#Packages\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "MLK-RWh1LLg2",
        "outputId": "b5a4164b-8336-4fb2-aa59-75f6b173bc1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has 1599 Rows and 12 Features\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
              "0            7.4              0.70         0.00             1.9      0.076   \n",
              "1            7.8              0.88         0.00             2.6      0.098   \n",
              "2            7.8              0.76         0.04             2.3      0.092   \n",
              "3           11.2              0.28         0.56             1.9      0.075   \n",
              "4            7.4              0.70         0.00             1.9      0.076   \n",
              "\n",
              "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
              "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
              "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
              "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
              "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
              "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
              "\n",
              "   alcohol  quality  \n",
              "0      9.4        5  \n",
              "1      9.8        5  \n",
              "2      9.8        5  \n",
              "3      9.8        6  \n",
              "4      9.4        5  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1c127714-7342-42ed-86ff-5f7e01b71d37\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.9978</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0.098</td>\n",
              "      <td>25.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.9968</td>\n",
              "      <td>3.20</td>\n",
              "      <td>0.68</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.04</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.092</td>\n",
              "      <td>15.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.9970</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.65</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.2</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.075</td>\n",
              "      <td>17.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.9980</td>\n",
              "      <td>3.16</td>\n",
              "      <td>0.58</td>\n",
              "      <td>9.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.9978</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1c127714-7342-42ed-86ff-5f7e01b71d37')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1c127714-7342-42ed-86ff-5f7e01b71d37 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1c127714-7342-42ed-86ff-5f7e01b71d37');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "df = pd.read_csv('winequality-red.csv', sep=';')\n",
        "print(f\"Data has {df.shape[0]} Rows and {df.shape[1]} Features\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDLV9DbSLUqH",
        "outputId": "4e1361d0-96ea-4f17-822b-a5723a835b9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1599 entries, 0 to 1598\n",
            "Data columns (total 12 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   fixed acidity         1599 non-null   float64\n",
            " 1   volatile acidity      1599 non-null   float64\n",
            " 2   citric acid           1599 non-null   float64\n",
            " 3   residual sugar        1599 non-null   float64\n",
            " 4   chlorides             1599 non-null   float64\n",
            " 5   free sulfur dioxide   1599 non-null   float64\n",
            " 6   total sulfur dioxide  1599 non-null   float64\n",
            " 7   density               1599 non-null   float64\n",
            " 8   pH                    1599 non-null   float64\n",
            " 9   sulphates             1599 non-null   float64\n",
            " 10  alcohol               1599 non-null   float64\n",
            " 11  quality               1599 non-null   int64  \n",
            "dtypes: float64(11), int64(1)\n",
            "memory usage: 150.0 KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "Ktbx6e1wMmIP",
        "outputId": "9e54bef2-f077-4956-c532-551046e9c8cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
              "count    1599.000000       1599.000000  1599.000000     1599.000000   \n",
              "mean        8.319637          0.527821     0.270976        2.538806   \n",
              "std         1.741096          0.179060     0.194801        1.409928   \n",
              "min         4.600000          0.120000     0.000000        0.900000   \n",
              "25%         7.100000          0.390000     0.090000        1.900000   \n",
              "50%         7.900000          0.520000     0.260000        2.200000   \n",
              "75%         9.200000          0.640000     0.420000        2.600000   \n",
              "max        15.900000          1.580000     1.000000       15.500000   \n",
              "\n",
              "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
              "count  1599.000000          1599.000000           1599.000000  1599.000000   \n",
              "mean      0.087467            15.874922             46.467792     0.996747   \n",
              "std       0.047065            10.460157             32.895324     0.001887   \n",
              "min       0.012000             1.000000              6.000000     0.990070   \n",
              "25%       0.070000             7.000000             22.000000     0.995600   \n",
              "50%       0.079000            14.000000             38.000000     0.996750   \n",
              "75%       0.090000            21.000000             62.000000     0.997835   \n",
              "max       0.611000            72.000000            289.000000     1.003690   \n",
              "\n",
              "                pH    sulphates      alcohol      quality  \n",
              "count  1599.000000  1599.000000  1599.000000  1599.000000  \n",
              "mean      3.311113     0.658149    10.422983     5.636023  \n",
              "std       0.154386     0.169507     1.065668     0.807569  \n",
              "min       2.740000     0.330000     8.400000     3.000000  \n",
              "25%       3.210000     0.550000     9.500000     5.000000  \n",
              "50%       3.310000     0.620000    10.200000     6.000000  \n",
              "75%       3.400000     0.730000    11.100000     6.000000  \n",
              "max       4.010000     2.000000    14.900000     8.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2a1d763c-b8a9-4e61-9d96-b27daf5566bf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1599.000000</td>\n",
              "      <td>1599.000000</td>\n",
              "      <td>1599.000000</td>\n",
              "      <td>1599.000000</td>\n",
              "      <td>1599.000000</td>\n",
              "      <td>1599.000000</td>\n",
              "      <td>1599.000000</td>\n",
              "      <td>1599.000000</td>\n",
              "      <td>1599.000000</td>\n",
              "      <td>1599.000000</td>\n",
              "      <td>1599.000000</td>\n",
              "      <td>1599.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>8.319637</td>\n",
              "      <td>0.527821</td>\n",
              "      <td>0.270976</td>\n",
              "      <td>2.538806</td>\n",
              "      <td>0.087467</td>\n",
              "      <td>15.874922</td>\n",
              "      <td>46.467792</td>\n",
              "      <td>0.996747</td>\n",
              "      <td>3.311113</td>\n",
              "      <td>0.658149</td>\n",
              "      <td>10.422983</td>\n",
              "      <td>5.636023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.741096</td>\n",
              "      <td>0.179060</td>\n",
              "      <td>0.194801</td>\n",
              "      <td>1.409928</td>\n",
              "      <td>0.047065</td>\n",
              "      <td>10.460157</td>\n",
              "      <td>32.895324</td>\n",
              "      <td>0.001887</td>\n",
              "      <td>0.154386</td>\n",
              "      <td>0.169507</td>\n",
              "      <td>1.065668</td>\n",
              "      <td>0.807569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>4.600000</td>\n",
              "      <td>0.120000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.012000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.990070</td>\n",
              "      <td>2.740000</td>\n",
              "      <td>0.330000</td>\n",
              "      <td>8.400000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>7.100000</td>\n",
              "      <td>0.390000</td>\n",
              "      <td>0.090000</td>\n",
              "      <td>1.900000</td>\n",
              "      <td>0.070000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.995600</td>\n",
              "      <td>3.210000</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>9.500000</td>\n",
              "      <td>5.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>7.900000</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>0.260000</td>\n",
              "      <td>2.200000</td>\n",
              "      <td>0.079000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>0.996750</td>\n",
              "      <td>3.310000</td>\n",
              "      <td>0.620000</td>\n",
              "      <td>10.200000</td>\n",
              "      <td>6.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>9.200000</td>\n",
              "      <td>0.640000</td>\n",
              "      <td>0.420000</td>\n",
              "      <td>2.600000</td>\n",
              "      <td>0.090000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>62.000000</td>\n",
              "      <td>0.997835</td>\n",
              "      <td>3.400000</td>\n",
              "      <td>0.730000</td>\n",
              "      <td>11.100000</td>\n",
              "      <td>6.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>15.900000</td>\n",
              "      <td>1.580000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>15.500000</td>\n",
              "      <td>0.611000</td>\n",
              "      <td>72.000000</td>\n",
              "      <td>289.000000</td>\n",
              "      <td>1.003690</td>\n",
              "      <td>4.010000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>14.900000</td>\n",
              "      <td>8.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a1d763c-b8a9-4e61-9d96-b27daf5566bf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2a1d763c-b8a9-4e61-9d96-b27daf5566bf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2a1d763c-b8a9-4e61-9d96-b27daf5566bf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "XAr3ndDpNl2G",
        "outputId": "b4e5bed8-6344-454a-9cd1-7ba5157269a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Quality Value Counts  Quality Value Counts Norm\n",
              "5                   681                  42.589118\n",
              "6                   638                  39.899937\n",
              "7                   199                  12.445278\n",
              "4                    53                   3.314572\n",
              "8                    18                   1.125704\n",
              "3                    10                   0.625391"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-68281b7c-b6e5-4883-97d8-a22c9487abb9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Quality Value Counts</th>\n",
              "      <th>Quality Value Counts Norm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>681</td>\n",
              "      <td>42.589118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>638</td>\n",
              "      <td>39.899937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>199</td>\n",
              "      <td>12.445278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>53</td>\n",
              "      <td>3.314572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>18</td>\n",
              "      <td>1.125704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>0.625391</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-68281b7c-b6e5-4883-97d8-a22c9487abb9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-68281b7c-b6e5-4883-97d8-a22c9487abb9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-68281b7c-b6e5-4883-97d8-a22c9487abb9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "pd.concat(\n",
        "    {\n",
        "        'Quality Value Counts': df['quality'].value_counts(),\n",
        "        'Quality Value Counts Norm': df['quality'].value_counts(normalize=True)*100\n",
        "    },axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "X8Nw2MU7Mo5e",
        "outputId": "761e4353-7278-43ac-fcc7-6fbeb1a91e82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicate Rows :  240\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Num  Normalized\n",
              "5  104   43.333333\n",
              "6  103   42.916667\n",
              "7   32   13.333333\n",
              "8    1    0.416667"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dfee3498-e1a8-4c07-9314-23d005afd2e0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Num</th>\n",
              "      <th>Normalized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>104</td>\n",
              "      <td>43.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>103</td>\n",
              "      <td>42.916667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>32</td>\n",
              "      <td>13.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>0.416667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dfee3498-e1a8-4c07-9314-23d005afd2e0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dfee3498-e1a8-4c07-9314-23d005afd2e0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dfee3498-e1a8-4c07-9314-23d005afd2e0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "duplicate = df[df.duplicated()]\n",
        "print(\"Duplicate Rows : \",len(duplicate))\n",
        "pd.concat(\n",
        "    {\n",
        "        'Num': duplicate['quality'].value_counts(),\n",
        "        'Normalized': duplicate['quality'].value_counts(normalize=True)*100\n",
        "    },axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "gxyDhGB-Mzdq"
      },
      "outputs": [],
      "source": [
        "# df = df.drop_duplicates(subset=None, keep=\"first\", inplace=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "id": "s3tf9teuR7Kg",
        "outputId": "a82b316b-7677-46c9-af20-c597655327d5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1584x1152 with 12 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "column_names=df.columns.values\n",
        "\n",
        "number_of_column=len(column_names)\n",
        "\n",
        "fig, axarr=plt.subplots(4,3, figsize=(22,16))\n",
        "counter=0\n",
        "for i in range(4):\n",
        "  for j in range(3):\n",
        "    sns.barplot(x='quality', y=column_names[counter],data=df, ax=axarr[i][j])\n",
        "    counter+=1\n",
        "    if counter==(number_of_column-1,):\n",
        "      break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q2tU5O0YTubw",
        "outputId": "4b6b5756-96ed-4eb8-ab36-af02dff0642c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x720 with 12 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABKIAAAJNCAYAAAACpt3nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzde5ycdX33/9dnTnvenHbJOSyHBIxQQQMUqeIt0CJV0VYrequ09RZptbXFu623dx+Wm7betL2Lv7biAZV6QDxUrY2CRQpUBEpMCCeTcEhCyDnZ7CZ7nt3Zmc/vj2tmM5nMHjPneT8fj31k5rqumfnuI99r53t9rs/38zV3R0REREREREREpNhC5W6AiIiIiIiIiIjUBwWiRERERERERESkJBSIEhERERERERGRklAgSkRERERERERESkKBKBERERERERERKQkFokREREREREREpCQi5W5AOXV0dHhXV1e5myF16oknnjji7p3l+Gz1fSmncvZ9UP+X8lHfl3qlvi/1TP1f6tVUfb+uA1FdXV1s2rSp3M2QOmVmL5frs9X3pZzK2fdB/V/KR31f6pX6vtQz9X+pV1P1fU3NExERERERERGRklAgSkRERERERERESkKBKBERERERERERKQkFokREREREREREpCQUiBIRERERERERkZJQIEpEREREREREREpCgSgRERERERGREjCzq83seTPbbmYfz7O/wcy+nd6/wcy60tv/u5k9lfWTMrMLSt1+kUJQIEpERERERESkyMwsDNwOvAlYC7zbzNbmHPYB4Ki7nw18GvgbAHf/hrtf4O4XAO8DXnL3p0rXepHCUSBKREREREREpPguBra7+053HwO+BVybc8y1wFfTj78LXGFmlnPMu9OvFalKCkSJiIiIiIiIFN9yYE/W873pbXmPcfdxoA9YlHPMu4BvFqmNIkUXKXcDZPbu3rD7hOfvuWRVmVoiUrl0nkip5fa5bOp/Uuuy+7/6u9S7zPmgc0GKwcwuAYbd/RdTHHMDcAPAqlWl64caf8tMKSNKREREREREpPj2ASuznq9Ib8t7jJlFgHlAT9b+65gmG8rd73D3de6+rrOz85QbLVJoCkSJiIiIiIiIFN9GYLWZnWFmMYKg0vqcY9YD16cfvwN40N0dwMxCwG+h+lBS5TQ1T0RERERERKTI3H3czD4C3AeEgTvdfYuZ3QJscvf1wJeBr5vZdqCXIFiV8Xpgj7vvLHXbRQpJgSgRERERERGREnD3e4F7c7Z9MutxHHjnJK/9T+CXi9k+kVLQ1DwRERERERERESkJBaJERERERERERKQkFIgSEREREREREZGSUCBKRERERERERERKQoEoEREREREREREpCQWiRERERERERESkJBSIEhERERERERGRkqi4QJSZXW1mz5vZdjP7eJ79N5rZs2b2lJk9YmZr09u7zGwkvf0pM/t86VsvIiIiIiIiIiKTiZS7AdnMLAzcDlwF7AU2mtl6d9+addjd7v759PFvBW4Drk7v2+HuF5SyzSIiIiIiIiIiMjOVlhF1MbDd3Xe6+xjwLeDa7APcvT/raQvgJWyfiIiISNHNIEN8lZk9ZGZPmtkzZnZNOdopIiIiMluVFohaDuzJer43ve0EZvZhM9sB/C3wh1m7zkgPyH5qZq8rblNFRERECi8rQ/xNwFrg3ZlSBFn+HPiOu18IXAd8trStFBEREZmbSgtEzYi73+7uZwF/RjAQAzgArEoPyG4C7jaz9tzXmtkNZrbJzDZ1d3eXrtEiIiIiMzNthjhBRnhmnDMP2F/C9omIiIjMWaUFovYBK7Oer0hvm8y3gLcBuPuou/ekHz8B7ADW5L7A3e9w93Xuvq6zs7NgDRcREREpkJlkiN8MvNfM9gL3An9QmqaJiIiInJpKC0RtBFab2RlmFiNINV+ffYCZrc56+uvAi+ntnelUdszsTGA1sLMkrRYRkYIzs0Yz+7mZPW1mW8zs/+Q5psHMvp2uo7PBzLpK31KRsng38BV3XwFcA3zdzE4a1ykTXERERCpNRa2a5+7jZvYR4D4gDNzp7lvM7BZgk7uvBz5iZlcCCeAocH365a8HbjGzBJACbnT33tL/FiIiUiCjwBvdfdDMosAjZvZjd38865gPAEfd/Wwzuw74G+Bd5WisSAHNJEP8A6RXDXb3/zKzRqADOJx9kLvfAdwBsG7dupIs8HL3ht0Tj99zyapSfKSIiIhUkYoKRAG4+70EKebZ2z6Z9fijk7zue8D3its6EREpFXd3YDD9NJr+yb2QvpZgihLAd4HPmJmlXytSrSYyxAkCUNcB78k5ZjdwBfAVM3sF0Ago5UlEREQqXqVNzRMREZlgZmEze4ogy+N+d9+Qc8hELR13Hwf6gEWlbaVIYaX7ciZDfBvB6nhbzOwWM3tr+rCPAR80s6eBbwK/rQCsiIiIVIOKy4gSERHJcPckcIGZzQf+1czOc/dfzPZ9zOwG4AaAVas0VUgq3wwyxLcCl5W6XSKlYmYrga8BiwmyYe9w938ws5uBD3I8A/AT6fNFRESqhDKiRCqYma00s4fMbGu6WPNJU1PN7A1m1mdmT6V/PpnvvUSqmbsfAx4iXRMny0QtHTOLECxj35Pn9VoxVUSkuowDH3P3tcAvAx82s7XpfZ929wvSPwpCiYhUGWVEiVS2zCBss5m1AU+Y2f3pO+HZfububy5D+0SKxsw6gYS7HzOzJuAqgmLk2dYTLFrxX8A7gAc1PUlEpPq5+wHgQPrxgJltI5iOLSIiVU4ZUSIVzN0PuPvm9OMBglohGoRJvVgKPGRmzxAUb77f3X+UUyfny8AiM9sO3AR8vExtFRGRIjGzLuBCIFMn8CNm9oyZ3WlmC8rWMBERmRNlRIlUiTyDsGyXpgvW7gf+p7tvKWHTRIrC3Z8h6PO527Pr5MSBd5ayXSIiUjpm1kqwMvYfuXu/mX0O+EuCulF/Cfw98Lt5XqfagCIiFUoZUSJVIHcQlrN7M3C6u78K+CfgB5O8xw1mtsnMNnV3a4VvERERqWxmFiUY/3zD3b8P4O6H3D3p7ingi8DF+V6r2oAiIpVLgSiRCpdvEJbN3fvdfTD9+F4gamYdeY7TgExERESqgpkZwfTrbe5+W9b2pVmHvR2Y9UqqIiJSXpqaJ1LBJhuE5RyzBDjk7m5mFxMEmE9aNUxERESkilwGvA941syeSm/7BPBuM7uAYGreLuBD5WmeiIjMlQJRIpVtskHYKgB3/zzBSmG/Z2bjwAhwnVYNExERkWrm7o8AlmfXvaVui0ghmdnVwD8AYeBL7n5rzv4G4GvAawhuLr/L3Xel9/0S8AWgHUgBF6XrZYpUFQWiRCrYFIOw7GM+A3ymNC0SEREREZG5MLMwcDtwFbAX2Ghm6919a9ZhHwCOuvvZZnYd8DfAu8wsAtwFvM/dnzazRUCixL+CSEGoRpSIiIiIiIhI8V0MbHf3ne4+BnwLuDbnmGuBr6Yffxe4Il2u41eBZ9z9aQB373H3ZInaLVJQCkSJiIiIiIiIFN9yYE/W873pbXmPcfdxoA9YBKwB3MzuM7PNZvanJWivSFFoap6IiIiIiIhIZYsAvwJcBAwDD5jZE+7+QO6BZnYDcAPAqlWrStpIkZlQRpSIiIiIiIhI8e0DVmY9X5HelveYdF2oeQRFy/cCD7v7EXcfJijc/+p8H+Lud7j7Ondf19nZWeBfQeTUKRAlIiIiIiIiUnwbgdVmdoaZxYDrgPU5x6wHrk8/fgfwYHpF7PuA882sOR2guhzYikgV0tQ8ERERERERkSJz93Ez+whBUCkM3OnuW8zsFmCTu68Hvgx83cy2A70EwSrc/aiZ3UYQzHLgXne/pyy/iMgpUiBKREREREREpATc/V6CaXXZ2z6Z9TgOvHOS194F3FXUBoqUgKbmiYiIiIiIiIhISSgQJSIiIiIiIiIiJaFAlIiIiIiIiIiIlIQCUSIiIiIiIiIiUhIVF4gys6vN7Hkz225mH8+z/0Yze9bMnjKzR8xsbda+/5V+3fNm9mulbbmIiIiIiIiIiEylogJRZhYGbgfeBKwF3p0daEq7293Pd/cLgL8Fbku/di3B0pavBK4GPpt+PxERERERERERqQAVFYgCLga2u/tOdx8DvgVcm32Au/dnPW0BPP34WuBb7j7q7i8B29PvJyIiIiIiIiIiFSBS7gbkWA7syXq+F7gk9yAz+zBwExAD3pj12sdzXru8OM0UEREREREREZHZqrSMqBlx99vd/Szgz4A/n81rzewGM9tkZpu6u7uL00ARERERERERETlJpQWi9gErs56vSG+bzLeAt83mte5+h7uvc/d1nZ2dp9jc8uoZHOWBbYc40DdS7qaIiIiIiIiIiEyr0gJRG4HVZnaGmcUIio+vzz7AzFZnPf114MX04/XAdWbWYGZnAKuBn5egzWUxMpbks/+5gweeO8xvfvYxBkfHy90kEREREREREZEpVVQgyt3HgY8A9wHbgO+4+xYzu8XM3po+7CNmtsXMniKoE3V9+rVbgO8AW4F/Bz7s7smS/xIl8uiOI4wkkrz5l5ayvy/O3RteLneTREREpM6Njac4OjxW7maIiIhIBau0YuW4+73AvTnbPpn1+KNTvPavgb8uXusqx7P7+jirs4XXntXBkcFRvrFhNx983ZmYWbmbJiIiInXqrsdfZnv3ILe89ZVEwhV1v1NEREQqhEYIVah3aIzugVHOXdIOwFtetYyXe4Z54dBgmVsmIiIi9Wo8lWJ7dzAWebl3uMytERERkUqlQFQV2nE4GOStXtwKwFVrF2MG9289WM5miYiISB17ued48Gn7Yd0cExERkfwUiKpCe4+N0BgN0dnaAMBpbY2cs7iNDS/1AnD3ht0TPyIi1cjMVprZQ2a2NV0X8KRp2Wb2BjPrM7On0j+fzPdeIlIaR4eC2lBN0TD7j2lFXxEREcmv4mpEyfT2Hxth2fymE+pBXXzGQr77xF7Gk6kytkykMgyOjvPtjbuJhkNcc/5SGqPhcjdJZm8c+Ji7bzazNuAJM7vf3bfmHPczd39zGdonIjn64wkAVi5som8kUebWiIiISKVSRlSVGRtPcbA/zvL5TSdsv6hrIcNjSbYe6C9Ty0Qqx//+12d5em8fm14+ygPbDpW7OTIH7n7A3TenHw8QrKS6vLytEpGp9I+M0xwLs6A5Rv/IeLmbIyIiIhVKgagqs6tniGTKWTqv8YTtr1oxH4At+xWIkvr24qEB1j+9n8vXdHLhyvn8fFcv8USy3M2SU2BmXcCFwIY8uy81s6fN7Mdm9sqSNkxETtAfT9DeGGVeU5SRRJKEsrRFREQkD03NqzI706vRdKTrQ2WsWNBEW0OErfv7ecXS9nI0TaQgcmubveeSVbN6/Td/vodIyPiVszs4PDDKk3uOTZw3Un3MrBX4HvBH7p4bad8MnO7ug2Z2DfADYPUk73MDcAPAqlWz61MiMjP98QTtTRHaGqPBc03Pkzr1cs8Qi3LG6iIicpwyoqrMju4hgIlC5RmhkPGKpe2amid1bTyZYv3T+7ji3MW0NERYubCJWDjEi1q9qSqZWZQgCPUNd/9+7n5373f3wfTje4GomXXkey93v8Pd17n7us7OzqK2W6Re9Y+M094Ypb0puM/ZH9f0PKk/fcMJvvDwTj517zb6hhWMFRHJR4GoKrOze4j2xggNeYovr13WzrYD/aTcy9AykfL7+Uu9HBkc420XLgMgEgpxRkcLO48MlbllMlsWrMbwZWCbu982yTFL0sdhZhcTfKf1lK6VIpKRcmdodJz2pijtyoiSOrYva8XI5w7qBrGISD4KRFWZl44MnjQtL2Pt0naGx5L0ppdPFqk3/77lII3REJevOW1i2/IFTRwZGGVkTHWiqsxlwPuAN5rZU+mfa8zsRjO7MX3MO4BfmNnTwD8C17krEi9SDvGxJA40RcO0NQYZUYOjyoiS+nOw/3ggqntwtIwtERGpXKoRVWV2947Qtag57761y4LaUAf64pMGq0RqVSrl3LflIJev6aQpdjxjcNm8RpzgruSFqxaUr4EyK+7+CGDTHPMZ4DOlaZGITGUkvShEUzRMYzSMZW0TqScH+44Hn7oHFIgSEclHGVFVJJ5IcmRwlAUtsbz7zz6tlUjIONA3kne/SC17au8xDvWPcvV5S07YvnReE4Dqp4mIFFE8EayQ1xQLEzKjMRpmWJmoUocO9o1gBHdSDisQJXmY2dVm9ryZbTezj+fZ32Bm307v35BePRgz6zKzkaxM8c+Xuu0ihaJAVBXZezQIMC1ojubd3xgNc1ZnKwf74qVslhSRma00s4fMbKuZbTGzj+Y5xszsH9NfVs+Y2avL0dZy+/dfHCQSMt547uITts9vjtIQCfHcgYEytUxEpPZlsp8a0zUsm2JhRsY0NU/mbrIxkJktNLP7zezF9L8Vle58oC9OW2OE9qaoMqLkJGYWBm4H3gSsBd5tZmtzDvsAcNTdzwY+DfxN1r4d7n5B+udGRKqUpuZVkb1HhwGY35Q/IwqCrKjHd6pWbw0ZBz7m7pvNrA14wszud/etWce8iWDJ+tXAJcDn0v9Wjbs37D6l16dSzo+e3s/r13Qyr+nEQK2Z0dnWwK4eFSwXESmW44Go4B5nUzSsqXlyqvKOgYDfBh5w91vT2SQfB/6sjO08wcH+OO3psYgyoiSPi4Ht7r4TwMy+BVwLZI/trwVuTj/+LvCZzOIsIrVCGVFVJLMKx2RT8wDO7Gzh6PAY46lUqZolReTuB9x9c/rxALANWJ5z2LXA1zzwODDfzJaWuKlltXn3Ufb3xXnLq/L/2h2tDezsViBKRKRY4lk1ogCaY2EtEiGnZIox0LXAV9OHfRV4W3lamF/P4BitDRFaGyLKiJJ8lgN7sp7v5eSx/cQx7j4O9AGL0vvOMLMnzeynZva6YjdWpFgUiKoie4+OEAnZxGo0+ZzZ2ULKoXdQK+fVmvT88AuBDTm7ZvKFVpVSM1wA7UfPHKAhEuLKVyzOu39Ra4z9fSMTF0oiIlJYuYGopphqREnh5IyBFrv7gfSug0D+L/8yGRwdpzEaViBKiuEAsMrdLwRuAu42s/Z8B5rZDWa2ycw2dXd3l7SRIjOhQFQVOdQXZ3F7I6EpMjPP7GgF4IiWi60pZtYKfA/4I3efU9XtavpCSrnzjQ0vc/P6Ldz2k+enPDaRTPGjZw7wxnNPo60xf/20jpYG3GF373AxmisiUnDTFbNNH/NbWfVz7i51G7ONJJIYEItoap4U1lRjIHd3IO9dq3KNewZHx2mIhGiKhhkcTZTsc6Vq7ANWZj1fkd6W9xgziwDzgB53H3X3HgB3fwLYAazJ9yHufoe7r3P3dZ2dnQX+FUROnQJRVeTQQJzT2humPObMzhZAy8XWEjOLEgzAvuHu389zyEy+0KrqC2nTrqNs2d9Pa0OEf3xwOw8+d2jSY+/bcpAjg6O8c92KSY9Z1BpMZ911RNPzRKTyzaSYrZmtBv4XcJm7vxL4o5I3NMvIWJLGaJhMGZOm9NS8VGpmma0i+UwyBjqUKUGQ/vdwvteWa9wzGA8yohqiIeKJFONJlcuQE2wEVpvZGWYWA64D1uccsx64Pv34HcCD7u5m1pn+fsDMziSoD7uzRO0WKSgFoqrIof5RFrc1TnlMW2OUtsYI3ZqaVxPShQm/DGxz99smOWw98P706nm/DPRlpaxXpQ0v9bB8fhM3/eoazups4a/v2UYyz8XM3Rt28//ue56FLTH2H5t8tcj5zUEg6oBWlBSR6jBRzNbdx4BMMdtsHwRud/ejAO6e92K8VOKJJE2x8MTz5mgYBwa1cp7M0RRjoOyL9OuBfyt12yYTTyQZS6ZoiIRoiATnw9CoMgPluHTNp48A9xHUPfuOu28xs1vM7K3pw74MLDKz7QRT8DJZsa8HnjGzpwiKmN/o7r2l/Q1ECkOr5lWRw/1xLjtr0bTHdbQ2aGpe7bgMeB/wbPpLB+ATwCoAd/88cC9wDbAdGAZ+pwztLJiewVEO9MW55vylREIh/viqNXzk7ie5b8tBrjn/xGLkB/pG2NUzzJvOWzLllNWWWJhYJMT+dMF/EZEKl6/2X+5qqGsAzOxRIAzc7O7/XprmnSyeSE3UhwImglJ9wwnaJ5k2LTKNycZAtwLfMbMPAC8Dv1Wm9p1kcDQIvDZEw0RDwbhkYDTBvGadA3Kcu99LMH7P3vbJrMdx4J15Xvc9ggzBmpG7cvZ7LllVppZIqSkQVSVGxpL0x8c5rf3kjKjcE7izrYFn9/bhMyz0LJXL3R8BplyuNV0f4cOlaVHxPX9oAIC1S4Pai286bylndrzA7Q9t503nLZmY9gHw4HOHiUVCvOb0BVO+p5mxbF7jxMqTIiI1IEIwLeMNBFOyHzaz8939WPZBZnYDcAPAqlXFG+DHx5MT9aGAiWyQgbgyomRuphkDXVHKtszUYLq/N0ZCRMLB+ZAJTomIyHGamlclDg8EU4oW5wlE5epsbWAkkWRIq9VIFXq5Z5h5TVEWtgTT6cIh48Y3nMWW/f385/PHi40+8XIvW/b38/rVnTTHpo+pL5vfpKl5IlItZlL7by+w3t0T7v4S8AJBYOoEpaqTMzYeTEfKaIgGj4c0NU/qSCbo1BgN05g+H4YUiJI68+j2I9zzzH6tVi1TUiCqShzqD6bandY2dbFygI50YeYeTc+TKrS7d5hVC5tP2Pb2C5ezfH4T//jgi6RSTiKZ4pYfbaOtMcKvnN0xo/eNJ1JsPzx4UgahiEgFmkkx2x8QZENhZh0EU/XKVrR2NCcQ1ZjOiFI2iNST/niwSl5QIyo4H5QVKPVkT+8w9zx7gEd39PAf2yZfbEikogJR0y1VbGY3pZcpfsbMHjCz07P2Jc3sqfRP7mCt6s0mI2pRSxCs6lHBcqky/SMJ+kYSJwWiouEQH71iNU/uPsYn1/+Cm77zNE/vOcavn7/0hKkgU5nXFKV/JJG36LmISCWZYTHb+4AeM9sKPAT8SWZZ73IYG08RixyvEZX52zyoi3CpI5n+3hAN0xBVMFbqz+M7e2iIhDh/+Tw27OylbzhR7iZJhaqYGlFZSxVfRZBuvtHM1rv71qzDngTWufuwmf0e8LfAu9L7Rtz9gpI2uoQyGVGL26fPiJrfEiVk0DOkjCipLgf7g4DrsvlNJ+1757oVPLnnGHc9HmQ0feyqNSxqnf58yJjfHMU5frdSRKSSzaCYrROspnRTiZuW10lT8zQtSerQxNS8SIhwuli5zgGpJzuPDHHOkjZ+5ewOnt3Xx31bDvJbF62c/oVSdyomEEXWUsUAZpZZqngiEOXuD2Ud/zjw3pK2sIwO98eJRULMa5p+1Y1IKMT85hg9Q8qIkupyMF3DaUmezD8z4//+xvn8zmVdNMfCrFjQPKtpdvPT547uzIiIFFYq5YwlUydkqDYqG0TqUPaqeZHMqnnKCpQ60T0wSt9IghXzm1g+v4mFLTH+XYEomUQlTc3Lt1Tx8imO/wDw46znjWa2ycweN7O3FaOB5XR4YJTF7Q0nrBg2lUUtMU3Nk6pzsD/OvKboxLLf+axZ3MaKBc2T7p9MZunkYyMKRImIFNJwuiBtdkbUxNQ8BaKkjgxkrZqnc0DqzbP7gkVbly9oxsxYfVorG3b2kEimytwyqUSVFIiaMTN7L7AO+Luszae7+zrgPcD/Z2ZnTfLaG9IBq03d3d35DqlIh/rjnNY2fX2ojEWtMXqGRgky90Wqw8G+eN5sqEKY3xQU8e8bVoBWRKSQhtMX2tkZUSEzYuGQpiVJXRkaHScSMiLhECEzmmNh1UmTurF1fz8Ay+YFY/mzOlsZGkvyzN5j5WyWVKhKCkTNZKlizOxK4H8Db3X3iSJI7r4v/e9O4D+BC/N9SKmWMS60Q/3xGdWHyljU0kA8keKopiFJlUi50zM0SucMVoaci1gkRFM0rIwoEZECGxoLMqJi4ROHlQ2RkLJBpK6MJJInZHW3NkQYGtM5IPVhd+8wbQ2RiUL9Z3a2YAaPbi/bOhpSwSopEDXtUsVmdiHwBYIg1OGs7QvMrCH9uAO4jKzaUrXgcP/orDOiAF46MlSsJokUVN9IgkTS6ZhFAfLZmt8c5ZiCsyIiBZXJemqInDitOhYJMTiaLEeTRMoinkhN1EcDaGmI6ByQurGnd4QFLbGJ582xCK9c1s6j24+UsVVSqSomEDXDpYr/DmgF/sXMnjKzTKDqFcAmM3uaYAnjW3NW26tq8USSgdHxWWWKLGoJjn25R4EoqQ5HBoIEx4622DRHzl17Y5QBrZonIlJQw5mMqMiJw8rGaJhB/c2VOhJPJGnKCkQ1RsPEEwpESX3Yc3SYBc0nLqx12VkdPLn7GCNjOg/kRJW0at5Mliq+cpLXPQacX9zWlU9m9buO1plfoC9oiWLALmVESZXoHgwCUZ05GVHZK+O955JVU77HdKvotTVG2H9sZI4tFBGRfI5nRJ0YiIpFQgwpG0TqyMjYiYGo5pgCUVIfEskU+4+NsPq01hO2v/bsDr7w8E427url9WuqpyyOFF/FZETJ5HrSF+gLW2aeERUJhZjfHGVXz3CxmiVSUD2DYzREQrQ2FC8+3tYYYXB0nGRKRfxFRAolUwPnpIwo1YiSOjOSSNIYPX4eNEXDExmDIrXswLE4KYcFzScmTlzUtYBo2Hh0h6bnyYkUiKoCmYyohS2zm7K0qLVBU/OkahwdHmNhSwwzK9pntDVGcY4Hd0VE5NQNp7OecjOiGqJhBaKkrsQTyRNqRDVGw5qSJHXhQF8w42BeztS85liEC1ct4DEVLJccCkRVgd7BIBC1aLaBqJYYLx0Zwl3ZH1L5eofGTrqLUmhtjUG21eEBBaJERAplsoyoYGqeAlFSP+I5q+Y1aWqe1InM2LqtMXrSvted3cGz+/o4ohvBkkWBqCrwH9sOAfDgc4enrYGTbVFrA/3xca0SJhXP3Tk6PHZSgcNCy3w5Hh6IF/VzRETqyUSx8rCm5kl9G8kpVt4UDTGiQJTUge50IKo9T4mN/3buaQA8/EJ3SdsklU2BqCowNJokbHZSyvt0MhlUuzQ9Tyrc4Og4iaSfsORrMWQyorqVESUiUjAjY6v9tIAAACAASURBVElCBuHQiVOrY5Ewo+MpEslUmVomUlrxROqEqXlN0bACUVIXDg+MEg3bCRmBGWuXttPR2sBDzysQJccpEFUFhsbGaWkIz7p2zqJWBaKkOhxNZ+3Ntg7abLWl79Ic7lcgqhqY2Uoze8jMtprZFjP7aJ5jzMz+0cy2m9kzZvbqcrRVpJ7FE0mi4dBJ45RM0eavPrarDK0SKb2R3BpRMdWIkvrQPTBKZ2tD3uvVUMi4fE0nP3uxWwsGyQQFoqrA0Og4LXNYSWxhc4yQwa4jWjlPKltvuiB/sWtERcIhmqJh1YiqHuPAx9x9LfDLwIfNbG3OMW8CVqd/bgA+V9omishIOhCVK5PJPZpQRpTUh/hY7tS8ICswpYtvqXGHB+J0tk2+wvsbzunk2HCCp/YcLWGrpJIpEFUF5hqIioRDLJvfpIwoqXhHh0sTiIJgep5qRFUHdz/g7pvTjweAbcDynMOuBb7mgceB+Wa2tMRNFalrQSDq5LvgsUhwQT46rkCU1IcgI+r45VVzeppSfFxZUVLbugdG6WxrnHT/5ed0EouE+OHTB0rYKqlkCkRVgaGxJC155tvORNeiFnb1KCNKKtvRoTHaGiInrbhUDO2NUWVEVSEz6wIuBDbk7FoO7Ml6vpeTg1UiUkTxSTKiGjMZUboIlzqQSKYYT/lJGVFwvKC/SK06MjhKZ9vkN5TbG6Nc9YrF/PDp/aobKIACUVVhrhlRAKcvauZlZURJhesdGit6ofKM1saIakRVGTNrBb4H/JG798/xPW4ws01mtqm7W8UyRQppZGyaqXnKiJI6EE8XJc8u1pypF6U6UVLLgtWvE9PWen37hcvpGRrT6nkCwNyiG1Iyo+NJRsdTcw5EndHRwrHhBMeGx5hfgmlPInNxdHiMVQubpz3u7g27T/mz2hojbN3fj7vPegEAKT0zixIEob7h7t/Pc8g+YGXW8xXpbSdw9zuAOwDWrVunYh0iBTRpjaiopuZJ/cisjnfCqnmZqXlaOU9q2ODoOMmUM79p6mvNy8/pZGFLjO9t3ssVr1hcotZJpSpaRpSZfd/Mft3MlHV1CjJFnFtic82IagHQ9LwK8Bu/8Rvcc889pFIakGdLppy+kURJ6kMBtDVGGUum6BtJlOTzJDCX/m9BpPDLwDZ3v22Sw9YD70+vnvfLQJ+7qwCBVIx6+Ns/kkgRi5wc2D9erFwX4VL71waZovyNeabmjegcqFn18Dd+OsfSq1/Pa45OeVw0HOIdr1nBfVsOsadX16b1rphfBJ8F3gO8aGa3mtk5RfysmtUzGASiWhvmViPqjI4gy0TT88rv93//97n77rtZvXo1H//4xwEmX1qijhweiJPy6b+8CqWtMZL+XE3PK6Xs/g8sn+F3wmXA+4A3mtlT6Z9rzOxGM7sxfcy9wE5gO/BF4PeL0X6RuZpj368q8Umn5ikjSk5Q09cGmWBTvhpRmppXu3LH988//3y5m1RymUDU/Kbpx/K/c1kXIYMvP/JSsZslFa5ogSh3/w93/+/Aq4FdwH+Y2WNm9jvpqRYyAxMZUXOcmrdiQTNm8NIRBaLK7corr+Qb3/gGmzdvpqurC+AcnROw/1iwgt1MvrwKIROIOqJAVEll939gjBl8J7j7I+5u7v5L7n5B+uded/+8u38+fYy7+4fd/Sx3P9/dN5Xw15rU5peP8p1NezjQN1LupkiZzaXvV5vJpubFVKxcstT6tUEm2NQUO34uNMaUEVXrcsf3V155Ja997Wv553/+ZxKJ/Nn3Zna1mT1vZtvN7ON59jeY2bfT+zekF2zJ3r/KzAbN7H8W5ZeapWMjwfXqTMrALJ3XxLUXLOdbG3dPXOdKfSpqaqyZLQJ+G/gfwJPAPxB8+dxfzM+tJT1DwcXyXKfmNUbDLJvXxMuamlcRenp6+MpXvsKXvvQlgGF0TkxcqM+bZl55obQ1BGPd7kEFokot0/+BDmr4O2H/sRG+t3kvT+05xlcf26VpSVLzfX+yQFQ4ZETDNjFlSaSWrw0makRFTs6IUo2o2pY9vr/wwgv56Ec/yubNm7nqqqtOOtbMwsDtwJuAtcC7zWxtzmEfAI66+9nAp4G/ydl/G/Djgv8ic5TJiFoww9kNN15+JqPjKb7w0x3FbJZUuGLWiPpX4GdAM/AWd3+ru3/b3f8AaC3W59aazNS8uWZEAXR1NCsjqgK8/e1v53Wvex3Dw8P88Ic/BNiucwIO9gUZUfNKnBHVrYyoksru/wR9v2a/Ex7ZfoSGaIj3X3o6/fFxnth9tNxNkjKqh74fH0sSC+df/CEWCWtqngC1f22QCTY1Zq2a9x9bDwHwwLbDZWmTFF/u+H79+vW8613v4p/+6Z8YHBzM95KLCb4Ldrr7GPAt4NqcY64Fvpp+/F3ginTdTMzsbcBLwJai/EJzcGxkZjWiMs4+rY23X7Ccrzy2i37VbK1bxVw174vufm/2BjNrcPdRd19XxM+tKb1DY4QMGqNzjxmevqiFHz+r2r3l9sEPfpBrrrnmhG06J4KpebFw6JT6+Gw0REI0REIKRJVYdv//xCc+kYDa7P8pd144NMC5S9o5d0k7qxY2s2Fnr1ZprGP10Pfj4/kzoiD4mzuWVCBKgBq/NojnqREVTU9P1TlQu/KN70dHR2loaGDTprzVApYDe7Ke7wUumewYdx83sz5gkZnFgT8DrgIqYloeQN9wkDgxm5vKf3TlGtY/vZ+Hnj/MtRcsL1bTpIIV88rvr/Js+68ifl5N6h0aoyUWOaULmK5FzRwdTtA3rIhzOf35n/95vs11f04c6BthXlO0ZBfpZkZnW4MCUSVWL/1/39ERhseSrFncBsAFK+fTPTjK9sN574pKHaj1vp9IpkgkncgUgShNT5W0mr42yGQ93Z/OggKIpc+LRNLL0iYpvnx/4y+99NJifdzNwKfdfdpBhZndYGabzGxTd3d3sdoDBFPzmmPhiQUqZmLVombeddFKNu7qVa2oOlXwjCgzW0IQxW0yswuBzNVlO0EqrsxCz9DYKU3LA+ha1ALArp4hXtU8vxDNklk4ePAg+/btY2RkhCeffBL3icFIG1D3I5MDffGSTcvL6GxrUI2oEpmk/zeb2Ruowe+EPUeDenxndgR/d9cua+eHT+/n339xkNXp4JTUh3rp+5kskMmn5oUYVTZIXTt48CAEfd5q+dpgPB1sys4OjE4EonQO1JrJxvf9/f2ZqdiT2QeszHq+Ir0t3zF7zSwCzAN6CDKn3mFmfwvMB1JmFnf3z+R+iLvfAdwBsG7duqJebxwbScxp0aE/eONqvr1xDw8+d4h3vGbl9C+QmlKMqXm/RlCEcAVBIbWMAeATRfi8mtY7NEZLw8yjy/l0dWQFolYqEFVq9913H1/5ylfYu3cvN910U/auxcDvlqlZFeNA3wgr5pd2HNrZ2sDuXhXwL4VJ+v8K4CZq8DvhQF+cllh4ohZZe2OUpfMbeWT7Ef7gitVlbp2UUr30/UyB5swUpFwNkRDDWrq+rt13330Q9H2jhq8NMtPvYlmBqHDICBkkVCet5kw2vm9ra+NTn/rUVC/dCKw2szMIAk7XAe/JOWY9cD1BxuA7gAc9iHS9LnOAmd0MDOYLQpXaseEx5s1gxbxcS+Y1cumZi3hk+xFev7qT09obi9A6qVQFD0S5+1eBr5rZb7r79wr9/vWmd2hs4oJmrlYtbMYMFSwvk+uvv57rr7+e733ve/zmb/7mxHYz2+7u3y9j08oukUxxeGCUVy6bV9LP7Wxr4ImXVUC6FPL1fzN7wd3fWuamFcWBvhGWzm86YarpWR2tbHipl3giSWP01G4sSPWol74fHwsusCerERULhzg6rtIA9ez666/nt3/7t18A/m8tXxtksp6iOdmB0XBIGVE1aLLx/XTSNZ8+AtwHhIE73X2Lmd0CbHL39cCXga+b2XaglyBYVbGODSdmvGJertev6eTnu3r5j22HeM8lpxe4ZVLJijE1773ufhfQZWY35e5399vyvEwmcWRwlCWnGB1ujIZZuaBZNUrK5K677uK9730vu3bt4rbbTuj+i83spno+Jw4PjOIO8xpLOzWvo7WB3uExEsnUpBdPUhiT9P/Fme+HWur/yZRzuH+US888cfGnMztb+dn2Izzx8lEuO7ujTK2TUquXvj+RETVpjagwY8oGqWt33XVX5mFNXxskkimMIAsqWywcYkw1omrOFON7gNxZECdIF+2/N2fbJ7Mex4F3TvX57n7zLJtcNMdGEqxZPLeFL1saIlx2dgcPPneYfcdGCtwyqWTFmJrXkv53Tr3RzK4G/oEgQvwld781Z/9NwP8AxoFu4Hfd/eX0vuuBTMW4v0pnZ1WtsfEUA/HxU56aB7BmcSsvHBooQKtktoaGgky0PEu4hgjqRE3JzO4E3gwcdvfz8ux/A/BvBEu5Anzf3W+Ze4tL50D6C2emy70WSmdbA+5BxuFipQEX1ST9f0Z9v9ocGx5jPOWc1t5wwvauRc2EQ8ZjO44UJBB194bdebe/55JVp/zeUjj10vdHZlIjalxT8+pZ5lxgDtcG+cZA6SlJHyS4DgD4RO5qfOWQSDrRcOikxVeiEWVE1aIpxvd15e4NuznYF2dBc2zS8cl0fuXsDv5rRw8/faGbP/m1cwrcQqlUxZia94X0v/9ntq81szBwO8GSlHuBjWa23t23Zh32JLDO3YfN7PeAvwXeZWYLgb8A1hEUgH4i/dqqnX9zNL0U5qkWKwdYvbiNn77QrQyQMvjQhz4EwF/8xV+csP3mm28+MMPz5CvAZ4CvTXHMz9z9zXNsYtns74sDs1vutRA624JAQffAqAJRRZav/8+i71eVnvSqLwtbTgxENUTDvGrFPB7b0VOOZkmZ1EvfHxmbLiMqxNh4Cncv2eqoUlk+9KEPceONN87p2oDJx0Cfdvf/d8qNK6BgjH1yH49pal5Nmmx8X2/cnZGxJM2xuSdONEbDvHrVfB7fGaygt7Bl9vWmpPoULSJhZn9rZu1mFjWzB8ys28zeO83LLga2u/tOdx8DvgVcm32Auz/k7pkqw48TFD+EoEj6/e7emw4+3Q9cXbjfqPR6BtOBqNipB6LOWdxGIunsUp2osvnTP/1T+vv7SSQSXHHFFQCvmsE5gbs/TDA/vOYc7EtnRJUxECWlkd3/gTUz/E6oKr0TgaiTB1DtjVGe3nOMf37kJe7esHviR2pfrff9+DRT82KRECk/XshZ6tdcrg2qaQw02c3eaNg0PbWG5Y7vOzs7s6ej1ryxZIqkO02nWAPz1acvIOnOPc/sL1DLpNIVMzXmV929nyCddhdwNvAn07xmObAn6/ne9LbJfAD48RxfW/EyFzWFyYgKsqFfOFTf6aPl9JOf/IT29nZ+9KMf0dXVBfALpj8nZupSM3vazH5sZq8s0HsW3f5jcVobIiUv4NzZqkBUqWX3f2CUmX0nVJXeoTEiIcu7wERXRwspR6s11qFa7/vTrZoXS28fGtX0PJnTtcFkPmJmz5jZnWa2oFANPBWZqXm5VKy8tuWO77dv387f/d3flbtZJZNZFfVUMqIAlrQ30tHawI9/cbAQzZIqUIwaUbnv/evAv7h7XyFTstN3UNYBl8/ydTcANwCsWlXZ9TR6hoKL5ELUiDqrs5WQwfOHBvh1lp7y+8nsjY+PA3DPPffwzne+kzvvvDNJYYLBm4HT3X3QzK4BfgCctE58Jfb9A30jLJ1X+qlxExlRgwpElUp2/weOFvo7oRL0pNPJQ3l+r9MXNmPArp4hVi8+9RJB3QOjPPxCUB7lV1Z3sLi9UbWjKlSt9/2JqXmh/L9TQyQYwwyNjmu6hRTq2uBzwF8SlOL4S+Dvgd/NPajU455EMkU0cvLvEw2HGBwdL/rnS3nkju/nzSvtStDlNjJNICp3bDLZmMTMOG9ZOz/bfoRjw2PMb9b3Ra0rZkbUj8zsOeA1wANm1gnEp3nNPmBl1vMV6W0nMLMrgf8NvNXdR2fzWne/w93Xufu6zs7OGf8y5ZDJiGotwNS8xmiY0xe18KIKlpfNm9/8Zs4991yeeOKJzNS8CNOfE9Ny9353H0w/vheImtlJFZErse8f7IuzpAyBqMZomLbGiDKiSii7/wMDM/xOqCrHhsdYMMnAqSEaZtn8Jnb1nHpGVM/gKJ//6Q6e2XeMZ/f18bmf7mDvUWVaVapa7/vx8ZllRGXumktdm8u1wUnc/ZC7J909BXyRoLRHvuNKOu4ZS6aIhvJkREVCmppaw3LH993d3TQ21k/90czf9qZClJJZ0kYy5Ty6XTU160HRAlHu/nHgtQSFxRPAEDn1nvLYCKw2szPMLAZcB6zPPsDMLgS+QBCEOpy16z7gV81sQTpF91fT26pW79AY4ZDReAqpjtm1SJqiYa2cV0a33norjz32GJs2bSIajQKkmP6cmJaZLbH0LUUzu5jgvK6Kv+D7++Ism9dUls/ubGtQIKqEsvs/wV3smXwnVJX+kQTtU9Q761rUzJ7eYcZP4YLE3fnXJ4N7LH/4xtX88VVraImF+ebPdzOa0IV+Jar1vp+5Gx6bolg5oIwQmeu1wUnMLDu1/+0EpQ7KLjGeyhuQjYVNU/NqWO74vqWlhX/7t38rd7NKJjM9u+kUp+YBrFjQTFtjZCLjW2pbMafmAZwLdJlZ9udMuvKXu4+b2UcIAkhh4E5332JmtwCb3H098HcEy7/+S/rae7e7v9Xde83sLwmCWQC3uHtVFDeczJHBMRY0R/NO85iLxe2NPPxiN/FEsuQ1eSTw3HPPsWvXrkwabyZgOtVqeJjZN4E3AB1mtpdgdcgogLt/HngH8HtmNg6MANe5uxftlyiQsfEURwZHWTq/PHeNOlsViCq1TP8HFhH0W5im/1eLsfEUQ2NJ2vPUh8o4fVELj+7oYd+xEU5f1DKnz3l0ew87jwzxll9ayqJ0rbPfWreSLzy8k4df7OaqtUvm9L5SXLXc96crVt4wkRGlQJQAs7w2mGQM9AYzu4AgsLsL+FDRWjsL4ymndbIaUSpWXtNyxvcAvP/97y9ji0onczPiVIuVA4RDxqVnLuLxl6rifrqcoqIFoszs68BZwFNA5jatM83AKz216N6cbZ/MenzlFK+9E7hzjk2uOL1DowWtp7B8fiPJlLPtQD8XrqqIuo515X3vex87duzgggsuIBwOAzQT1Dmb7px49zT7P0OwtHFVOdQfxx2WzmukHDcKO9sa2Lq/v/QfXKey+z9B37+IGXwnVIsj6Xpj7Y1TZER1BMGnXT3Dcw5EfeWxXbTEwlzUtXBi2+mLWjhv+Twe3dHDpWd10FqABS6kcGq9748kkkRCRniSGlHHi5UrEFXv5nJtMMkY6MuFb92pGxtPEQ2ffB7EwiESyYq/PyhzlGd8j5nVTSAqczOiMVqYiVavOX0BP9l6iCODo3Skb7hJbSrmaHUdsLYaMjMqVW+68G2hLJsfTIH6xb4+BaLKYNOmTWzdupVMYc7PfOYze9z9D8vcrLI51B+UhVjc3sj+Y6Uvl6KpeaWV3f8/97nP7XH3Pyh3mwop05/bmib/Wm1tiNDZ2sCuI0Ncvmb29UoO98d58LlDvG51J5Gcu+5XvuI0tuzr4+EXurnm/FNfkEKFzwun1vv+yFhqyjvhx4uVa+qo1Pa1QSKZyr9qXiRYNc/dJ8aAUjtyx/f1Jj6eJGSTT8+erVefHlyjPrn7GFetXVyQ95TKVMxi5b8ANEfgFPQMjU1MvSiEeU1RFrbE+MU+ZYGUw3nnncfBg1qSNONwOgi0uL1MU/PaGhgYHZ9IKZbiqvX+f6g/6M9tU2REAXR1NPNy7xCpOVyH3fPsAVIOF66cf9K+09oaedXK+fx8V+/E3UmpDLXe90cSySlrWcY0NU+Oq+lrg0TS8weiwiEcGNX0vJpU63/jpxNPpGiIhGcciMuuX5zvptf5y+cRCRmbdx8tdFOlwhQzI6oD2GpmPwcm0g7c/a1F/MyacfeG3Rw4Fue0tsIFosyM85bP49l9fQV7T5m5I0eOsHbtWi6++GIaGhoAzjaz9fV6TmQySArZx2ejMx3kPTI4ysqFzWVpQz3J7v+k+z7UznfC4YGgP09VIwqga1ELG3cd5VB/nKWzLNT/42cPcu6SNk6bJHh72VkdPLXnGE+8fJTLzj5p4Uwpk1rv+/FEcsqMqMxd8kFlREmNXxskkilieabmZabrjYypRmstyjO+B2D9+vVTvKp2jCaSBZuWB8HK1muXtbP5ZQWial0xA1E3F/G9a14y5YwkkrQUYCnMbOcta+eOh3cyOp6cSJeX0rj55ptPeP7DH/7wIPD3ZWlMBTg8MEokZJMud19snekA2OEBBaJKIbv//+QnP6m5vn+oP07IoGWa+kxd6dpQu44MzSoQ1TeS4IndR/n9N5w16THLFzRx+sJm/mtnD5eetahgC13Iqan1vj8yNnUgKho2DGVECVDD1waplDOe8pOmTcPxYOxIIokKY9Se3PF9vYknCn9N+epVC/j2xj2MJ1N5zympDUX7n3X3nxKsZBFNP94IbC7W59WazIBtuoua2Tpv+TzGU84LBwcL+r4yvcsvv5yuri4SiQSXX345wDB1fE4c7h+ls62B0CQFbostE4hSnajSyO7/wCA19p1wqH+U1obItMGf+c1R5jVF2dUzPKv3f2z7EZIpn7a21GvP7qB3aIwXDg3M6v2leGq9749MczfczIhFQqoRJTV9bRAfD/p3vjo50axAlNSe3PH9RRddxKtf/epyN6tk4uOpgmZEAVy4aj4jiSTPHdRYppYVc9W8DwI3AAsJVshYDnweuKJYn1lLMgO2QgeidnYPAfClR3ZyyRmLVHi2hL74xS9yxx130Nvby44dOwCiwA+o03Pi8EB80ilGxXb3ht30xxMA3PvsAa4+r2ZLVlSM7P6fVlPfCYcHRmlvmro+FAQX5V2Lmtl5ZIip6vXm1k34/ua9NEZDbDswMOnqZABrl7bT1hDh5y/1cu6S9pn/AlI0td73g0DU1HfDGyIhrZonNX1tEE8E9Z/yrZo3EYhSTcqalDu+37dvHzfeeCMPPPBAuZtWEqOJ5IzGP7Nx/vJ5AGw90M956cdSe4qZ6/Zh4DKgH8DdXwROK+Ln1ZShTEbUFAVA52JBc5SWWJg9vSMFfV+Z3u23386jjz5Ke/vExeEodXxOHO4fLVt9KICWWAQDBnVxVBK5/b/WvhMO98enLVSe0dXRwkB8nN6hsRkd7+68cGiAsztbpwxCAYRDxmu6FvD8wQGODc/s/aW4ar3vxxNJmqYZq8Qi4YlxjdS1mr02yGQ75V81L/i7rYUkalPu3/jVq1dz+PDhMreqdIKMqMJer56+qIWmaJhtB7TAVi0rZiBq1N0nRsFmFgFqcrnWYsjcOSx0RpSZsWpRC7t7hwr6vjK9hoYGYrGT6iHV7TlxeKCwxfhnKxwymhsiDMR1cVQKuf2/1r4TDvXHpy1UnjFRJ6pnZn+HDw2M0h8fZ83ithkdf1HXQgA2qdBnRaj1vj9dsXJQRpRMqNlrg0y2U75AVExT82pa7t/48fHxGa8gVwuCGlGFDSmEQ8aaJW08d0BT82pZMYuV/9TMPgE0mdlVwO8DPyzi59WUYgWiAFYtbGbbgX4NCkvs8ssv51Of+hQjIyPcf//9EKSlf7bMzSqLr/3XLo4OJzjUH8+7dGuptDVEGExP0ZPiyu7/QDvwL9TId8LoeJKjwwnaZhiI6mxroCkaZteRmdWJejFd72n1DANRC5pjrFncxsZdvfy3c6ZONijn+Vcv5tr3zexq4B+AMPAld791kuN+E/gucJG7bypYw2doZAaBqFgkxJCmJUkNXxvEp8qI0tS8mpY7vv/sZz/LW97ylnI3qyTcnfgMpmfPxSuWtHHfloO4e10F9upJMTOiPg50A88CHwLuBf68iJ9XU4bGkhjQXOCpeRAEogB2986uWK6cmltvvZXOzk7OP/98vvCFLwD0UafnxGA6C2mmU5mKpa0xwoACsiWR3f8JlvCume+Ew/1Bwfv2GfbnULpO1Ewzol44NMCS9kbmzaIGw8VnLGQgPs5zB5XWXm5z6ftmFgZuB94ErAXebWZr8xzXBnwU2FDods/UyFiKxmnGKg2RkFbNE6jha4OJQFTk5AtmZUTVttzx/TXXXMNf/dVflbtZJRFPpEg5RQlEnbukjaPDCQ5rUaGaVbSMKHdPmdkPgB+4e3exPqdWDY6O0xQLF2X57RULmgiZAlGlFgqFeNvb3sbb3vY2Ojs7MbMjPlW14hrWPxGIKmZS5vRaGyJaNa9Esvv/d7/73Z3u/sXpXmNmdwJvBg67+3l59r8B+DfgpfSm77v7LYVs90xkBkmz6c9dHS1sOziQnqI6edH+0fEku3qGee1Zi2bVpjWL25jXFOXnL/VOf7AU1Vz6PnAxsN3ddwKY2beAa4GtOcf9JfA3wJ8UtNGzMJOpebFISNOgpaavDSZqRIXy1YhSRlQtyx3f15OB9KyCQq+aB/CKpUHNra0H+llcpsWNpLgK3msscLOZHQGeB543s24z+2ShP6uWDY2OF2VaHgQpwsvmNykQVSLuzs0330xHRwfnnHMO55xzTuaLamm521YumS+uSsmIqtN4YEnk6//Aq2b4nfAV4OppjvmZu1+Q/il5EAqYKDo+m7/ZmTpRG3ZOHSja2T1EMuUzrg+VEQ4Z605fwIuHB9ndo7/15XCKfX85sCfr+d70tglm9mpgpbvfU7BGz5K7z2hqnmpE1bf0d+yyWr42mKgRladWTmYlPWVE1ZbJxve33FKWoUhZZG4sN0SKkREVBKJUJ6p2FWNq3h8TrIhxkbsvdPeFwCXAZWb2x0X4vJo0NJqkJVa8bJFVC5vZe3SYRDJVtM+QwKc//WkeffRRNm7cSG9vL729vWzYsAGgtV7Picyd8ZkWdy6W1sYoyZTTP6ILpGLJ1/+BbczgsuPSDAAAIABJREFUO8HdHwYqPqXnaHp1uuZZ/M1eNr+JpmiY/3x+6qSAFw4NEAuHOD09pXo21nUtJGTwzY2qA1UOp9L3p2NmIeA24GMzOPYGM9tkZpu6uwubhJJIOsmUT79qXjjEsLJB6tanP/1pgBZq+NogPh6MpzNBp2xRTc2rSZON7x999NFMn695xcyImtccZdm8Rq2cV8OKEYh6H/Bud89MlSCdWv5e4P1F+LyaNDQ2TktD4aPLGasWNpNIuk7uEvj617/ON7/5Tc4444yJbWeeeSYE04nq8pzojycwilOMfzba0p/fPRgvaztqWb7+D4xRuO+ES83saTP7sZm9sgDvN2tHMxlRs6jpFw4Z5yxp46HnD5NM5c/Ic3eeOzjA2ae1EslTAHc685qinLOknX/ZtIexcd10KLVT7Pv7gJVZz1ekt2W0AecB/2lmu4BfBtab2brcN3L3O9x9nbuvK/S0kcyF9XQrJsUiYYbGlH1ar77+9a8DvFTL1wbxKVbNi4QMyzpGasNk4/u77rqLr33ta2VsWekMpjNdGwuYEXX3ht0TP62NEV44pIyoWlWMQFTU3Y/kbkzPBS/vPJwqUsypeXC8YPlmLe9ddIlEgo6Ojny7xqnTc2IgPk5rY6QoNdBmozWdkaVCiMUzWf8v0HfCZuB0d38V8E/ADyY7sJhZIUeHE0TDRmyWyxefs6SN3qExntpzLO/+/X1x+kYSE3US5uLiroUcGRzjvi0H5/weMjen2Pc3AqvN7AwziwHXAeuz3qPP3Tvcvcvdu4DHgbeWetW8TIHm6TKiGiIh3JURUq8SiQQEY54T1NK1wcgUq+aZGdFwSP2/xkz2N76zszPT52teZoZDMYqVAyxua2Rn9xDjmsFTk4oRiBqb4z5JS6ackbEkrUUMRM1vjjGvKcoTu/NfAEnhxGKxqXbX5TkxEE/MeIWxYspkRB0ZrMv/hpIoZv939353H0w/vheImlneqG8xs0KODY8xvzk26+WF15zWRjhkPLDtUN792w70YwQBq7la/f+zd9/Rcd3Xoe+/v+kFM+iNAEiwS6wqVLGa5SZLsS05thzbcdyiRJZLcm9yk1z75cXJc+L3FKdc927JVZYd2Y5lW8WWZHWJYhVFiqQIgiQK0fv09nt/zAwEUSA4AGbmnJnZn7WwBAwGmM2lA+CcfXZprmJlnYdvP9Et1ShFtpxjX2udAD4BPEi6ne+nWutDSqnPKKVuzF+Uy5Odi5PLsHJ4+e65qCyVcB6UTTI5zlK9ardJIqrcLHRcn+OYRyl1vVLqqFKqSyn1yXk+71RK/STz+Z1Kqc7M45cqpfZn3p5XSv3h8v4Vy1PI1jyAJr+LWDLFKZlrXJYKkenYrpSar99LATLyPgcToRiaxbV5LMXKOo9URBXB888/j98/b0XDhUBFpvhnIolFraIvlOywdNmcVzhnOf4vVErNsMy/CUqpFmBIa62VUpeSvrkytpzvuRTjwRh1noVPOufjdli5pLOW3744xN++eeOrElmHB6ZZWe9Z1k0Ji1J89Nq1fOrnL/DYSyNcu7HpnF+jtebg6WkOnZ7C57Tx2o1NBb0xUq6We+xnkqv3nfHYvMOdtdbXLjHMZcleWLvtViLxs/85y7buhaLJdFOhqCjPP/88pI/9M68PyubaIFsdaJtnRhSAw6oIxyrylK9sne38XmtNJHL2kQ9KKSvwFeBNpBdR7FJK3au1nrsV9RZgQmu9Tin1HtLbUd8NHAR2aK0TSqlW4Hml1K8yNy+KruAVUX4nAMeGAqxtrCrIawjj5P3MUmtd2OxJBVjKBqalWFnn4YX+KQamwrRWuwv6WpUsmZz/DphSap/W+lXzPCrBdCRBe63xx5zLbsFqUZKIKqD5jv9cj32l1I+Ba4EGpVQf8I9k2ji01l8HbgY+qpRKAGHgPdqAsp/JUJwaz9ISq2/bvoK//8VBDvZPs7W9evbxsUCUgakIN2xpWXZ877yonS89fIwvPHyM125oXLByK5nS/HxvH/t6J/E5bQRjCV4cmOYTr1t/zvYr8UrLOfZLRTYR5XJYIXT2VpRsIioYk4qoSpRMJsvu2D9TOJ7EZlFnHTmQbs2T47+cnO38PgeXAl2ZOWkope4GbgLmJqJuAv4p8/49wJeVUkprPbc0yAUYWuo8HUmgYNGjCXLV6Msmoma4Pg/nQ8JcCnPUiGUZCxQvEQWw95S054niSSRThKKJ2WokIyml8DltkogyKa31e7XWrVpru9a6XWv9Ha311zNJKLTWX9Zab9Zab9daX661ftqIOMdDMeq8i6+IAnjrthU4bBbuPmOz3Z5TEyhgW3vNsuNz2Cx84vXr2dczyS/29Z/1eYlkih8/18O+3knecF4T//uG8/jzq9cwFY5z3wsDy45DlJ9IPNfWvPTng1FpTRLlKRJLzjsfKstutcy2soqK1wb0zvm4L/PYvM/JVDtNAfUASqnLlFKHgBeA24yqhoJ0a57DZinYzFenzUp7rZuXhgMF+f7CWJKIMqHZiqhFrAJfitYaF06bhb090p4nimc0kG499bnM0epT5bIxEpBElFi67Iyopah22/nDC9r42d4+xjLHYTCaYNfJcTa2+PLWwvruSzq4aGUNn/n1i/MmXmOJFD949hQvDkzz1m2tvOH8ZixKsarey+Vr6tnXOzH7t0mIrFwTUVIRJcpdJJ7Cfpa2PECGlYu80Vrv1FpvBi4BPqWUmre9tZBLWrJmIomCteVlrW+q4phszitLkogyofFg+iLB6yzsD7bNYmF7ew17ZE6UKKLhmXTfvBmGlQNSESWWRWvNRChOnXfpx/Otr11DPKn51weOAPCNx44TjCW5dkP+hqpbLYrP3byNUCzJx3+0l+CcodGToRjffPw4XcMB3nFhG1esfeW896vXN6I17Dk1nrd4RHnIzrw5V9umY+6MKCHKUDi+cEWUw6YILzBHTVSUfqBjzsftmcfmfY5SygZUc8YMTK31YSAAbJnvRQq5pCUrEEnM3mgolA3NPtmcV6bMUZIgXiG7wctT4IoogItW1fKdJ7uJxJMFz2gLATA0nU76mKki6sSobOMQSzMdSZBMaWqXWBEFsLaxiluvWcPXHj1O90iQPT0TXNBRw8p6bx4jhXVNPv7jXdv5H3fv461fepI/2tHBU12j7D41jtbw/tes4ryWVw9erXbbWdtUxfN9U2itF70dUJSvcM6teZmKKNmaJ8rUuRJRdquFiLTmibRdwHql1GrSCaf3AH98xnPuBT4IPEN6HuYjmcUsq4HezLDyVcB5wMmiRX6GmWi88BVRzb7ZzXkysLy8mKoiKodVltcopfYqpRJKqZvP+FxyzjrLe4sXdf6NB2O47VaslsKf7F+8qpZ4UvNC/1TBX0sIeLkiygwzogCqnHbGg1GSKVltLxZvMpS+cbCcRBTA31y3kY9cs4bB6Qg3X9TO2y84c1xEfrxt+wp+eMtlOG0W/vWBIzx9fJQ1DVV8/HXr5k1CZW1rq2Y8GOPwgJTHi5dlE1HnmiEmrXmi3EXiSWnNEznJzHT6BPAgcBj4qdb6kFLqM0qpGzNP+w5Qr5TqAv4ayF4XX0V6U95+4BfAx7TWo8X9F7ws3ZpX6IqodPJJ2vPKjzlKEsh5lWUP8CHgb+b5FmGt9QUFD7QIxoOxgg8qzzoxGgTgO0+c4NhQgD++bGVRXldUruHpKApMsw7e57KR0jAWjNLkK4st0qKIsnOTar12BqeW3uJptSg+9Qfn86k/OB+Au3b2nOMrlu6KdQ088D+vYSoc57/39S94Fz9rQ4sPgMePjbBpxdkTVqKyZCs8znUMSUWUKHfhWHLBzWEOSUSJObTW9wH3nfHYp+e8HwHeNc/X/QD4QcEDzNFMJFHwDod1TelE1EtDAa6ftwlRlCozVUTNrrLUWseA7CrLWVrrk1rrA0BZN4mOBaMFnw+VVeW0Ue91cGpcWpNEcQzPRPE4bUWp+MtFNiEmc6LEUkxmVtYvdVi5kard9pySUJCe6dbid/HY0cIMPBWlKXthfa7jyGax4LBaCEprkihToXNuzVPSmifKzkwkjstW2GtWj8NGR52bl6QiquyYKRGVyyrLhbgymwGeVUq9Pb+hFdd4MFbwjXlzrar30DMWRGtpTRKFNzwdwW+S+VDw8qwqSUSJpchWRNWVYCJqsVY3etnfOykDQ8WscDyJVamcbix4nFZCUhElylTkXDOibBZC8aSca4uyMl2E1jyA9U0+jg0FCv46orjMlIharlVa6x2kh719Xim1dr4nFWOV5XKNBYrXmgewss5LMJaU1dyiKIZnoqYZVA4vz6qSRJRYiok8zYgqBR21HsLxJEflrqTICMeS2G25Vbd6HTYCsjVPlKlQDq15yZQmnpRElCgP0USSWCKFswjLrtY3V9E9GpAbYWXGTImoXFZZnpXWuj/z327gUeDCszyv4KsslyOV0kyEYkVrzYN0RRQg7XmiKIZnIqYZVA5zWvMCkogSizcZimO1KFMlVwtlZV36b8X+3kmDIxFmca4qkLm8TishGVYuylQuW/OyzxOiHAQi6d/nrgUSsPmyoclHPKk5OSbXquXETImo2VWWSikH6VWWOW2/U0rVKqWcmfcbgCuBFxf+KnOaDMdJaYramtfoc+KyW+iRH25RYMmUZsRkFVEOm4Uqp43haUlEicUbD8WocduxmGTmWSHVeuzUex3s65FElEg718X3XB6HTWZEibIVjidxnGNrHqSTt0KUg5lsIqoIFVEbmtMLU2RzXnkxTSIql1WWSqlLlFJ9pLcIfEMpdSjz5ecDu5VSzwO/B24/Y9teyRjLVGUUc6OYRSlW1nnokYooUWBjwSgpjakqogBaql0MTkWMDkOUoMlQjFpv+bflASiluKCjhn09E0aHIkwiHEviWERFlGzNE+UomdLEEinsC1SG2DNJqrAkY0WZKGYial1TFUqlN+eJ8mGesgRyWmW5i3TL3plf9zSwteABFkF2Tk1VkStGVtZ5ePjwMNOROH6TJQlE+cge374iJlpz0VrtYmBaElFi8caDMWo9lfM788KVNTx8ZJipcJxqd+X8u8X80hVRuc+IGgvIDS9RfrLtdgslZaU1T5SbmWh6a7CzCK15boeVjloPx4alIqqcmKYiSqRl59QU+0J9ZZ0XDeyXlgtRQMOZRJSZtuZBJhE1GTY6DFGCJkPxihhUnnVBRy0AB/rkb4XIrKzP8SLE67QRlBlRogxlq5wWalPNDjKXRJQoF8WsiALY0Fwlm/PKjCSiTGa2YqTIVUkdtW4UsOeUtFyIwnm54s9clRQt1W5GAlHiso1DLFK6IqpyElHnt6bnNBwdlLuSYvGteSHZmifKUDYRlVNFlLTmiTJR7ETUuiYf3aMBOVcvI5KIMpmRQBSH1YLLXtz/NU67lZZqF3tl9ofpKKXuUEoNK6UOnuXzSin1RaVUl1LqgFLqomLHmKuXE63mqohaUe1CaxiS9jyxCFprJkNxarzmSqwWUn2Vk4YqJ0ckESXIDGjOtSLKIRVRYvHmOwdSStUppX6nlDqW+W+tkTFmq5wWqg50SCJKlJmZSLo1rxhb8yBdERVPak6NBYvyeqLwJBFlMqMzMRqqHChV/A1MK+s87OuZJJnS837+rp09r3gTRfNd4PoFPn8DsD7zdivwtSLEtCTD0xH8LlvOW5aKpaXaBSADy8WihGJJYskUdRVUEQXpqqgjg9NGhyFMIBxb3Na8SDxFQu5mi8X5Lq8+B/ok8LDWej3wcOZjw8wmohbcmqde8VwhSl22IspZtNa8bEW2tOeVC3NdDQpGAlEafU5DXntVvYdANMFLshrTVLTWjwPjCzzlJuD7Ou1ZoEYp1Vqc6BbHyON7IStq3ACclkSUWITxYAygolrzADY2+zg2FDjrTQtROUKxxKJa8wBCciEuFuEs50A3Ad/LvP894O1FDeoMoUyl34KteTIjSpSZQDSBy27BailO8cT65irsVsXB01NFeT1ReJKIMpnRmSgNVcZcqK+s8wIyJ6oEtQG9cz7uyzxmOsPTUZp8LqPDeJWXK6JkYLnI3WQoXZZe662wRFSLj2gixUkpj694i2rNyyxhkTlRIg+atdYDmfcHgWYjg4nEzz2sPPu5iCSiRJmYicSLOtPYabOyscXHC32SiCoXkogyGSMrRmo9dhp9TvZKIqosKaVuVUrtVkrtHhkZMSSG4RlzVkT5XXaqnDZOT0pFlMjdeChbEVU5M6IAzmvxAzKwvNLFkyniSb1gO9JcHke6IioQlTlRIn+01hqYtzyzWOc94Vi63VRmRIlKMh1JFH3m69a2Gg70TZL+sRelThJRJpJMacYMTEQppbh4ZS17ZGB5qekHOuZ83J557BW01t/UWu/QWu9obGwsWnBzXp+RmShNJkxEQboqSmZEicWYzCaiKqwian1zFRYFRwZkTlQly7YY5dqaV5WtiJKB5WL5hrIjCDL/HZ7vScU678mlNc8mM6JEmZmJJIq+5X1bezXTkQRffqRLZhaXAUlEmchEKEZKY1hrHsDFq2o5NRaa3W4mSsK9wAcy2/MuB6bmlKybRiCaIBxP0uQ3XyLqrp09KJC+c7EoExU6I8plt9LZ4JXNeRUuW9mxUBXIXB5HOhElFVEiD+4FPph5/4PALw2MZU5r3tmrAy1K4bJbpCJKlI2ZSByfs9gVUdUA9E/KKI1yIIkoE8kmf4xsXbpoVXoD7l6pijINpdSPgWeAjUqpPqXULUqp25RSt2Wech/QDXQB3wI+ZlCoCzLD8b2QaredqXDc6DBECRkPxVEqfexUmg1NPo4Ny+aaSpa9oF50RZTMiBKLMN85EHA78Cal1DHgjZmPDZNrdaDHYSMkiShRJgIGtOZtaPZhsyj6JyQRVQ6Ke/SIBY0G0hfqDVXO2SG4xbalzY/DamHvqQnevLnFkBjEK2mt33uOz2vg40UKZ8mGM4moJp+LU2Mhg6N5Nb/bTiCSIJZI5Tx8V1S2yVCMare9aBtjzGRDcxW/fXGQSDyJq0irm4W5ZC+oFxrQPJcnszUvKK15YhEWOAd6Q1EDWUAox+pAj8Mqx78oGzMGJKIcNgst1S76pCKqLMjVlomYoWLEabOytb1aNueJvBs2wfG9kFqPAw2clj9uIkfjwVjFteVlrW/2kdJwfESqoipVOJ6Zi5Pr1rxMa15QKqJEmcluj7SohW9KVDltBKU1VZSJYm/Ny1pV56F3PEQ8mSr6a4v8kkSUiZghEQXpOVEH+qeIJuRkUeTPyGxFlEkTUd70H9PeCfNVawlzmgzFK25jXtaGZh8Ax4YkEVWpZjeF5VgR5c1URMmwclFuwrHk7FbIhXgcVmnNE2UhmdIEY8nZlutiWtNYRSKl6R2X8/VSJ4koExkNRHHZLXhz+GNWKHft7CEcSxJLpNh1QqqiRP4Mz0RwWC2mnadTl6ls6R2XiiiRm0quiFrd4MVmURwbloHllWp2U5gMKxcVLhxL4s6hRdkrFVGiTGR/jxe7NQ/S5x8KOD4SLPpri/ySGVEmMjgdpdnvQp2jtLfQ1jVV4bBauO/gAFetbzA0FlE+nusex+Ow8uPneo0OZV5+tx2rUlIRZSJKqTuAtwLDWust83xeAV8A/gAIAR/SWu8tVnyToRibVviX9T1KdfWww2ahs8HLS1IRVbFyHdCcZbUovA4rMxG5EBflJRzPLRHlcVhlK7UoCzOR9Cxjv8tOIqUL+lpnnie57Fbaat10jwSA5nM+/48vW1nI8MQySEWUiQxNRWj2u4wOA7vVwsYWHw8eHCQh/bciT2aixR9quBgWpajx2OmRUl8z+S5w/QKfvwFYn3m7FfhaEWKaNVHBrXmQHlh+bEgqoipVOHbulfVn8rvtsxcwQpSLcCyJO4duBq/DJhWBoixkbygYdV6/trGK3okQsYRcp5YySUSZyOB0hBYTJKIAtrRVMxaM8dzJcaNDEWViJhKnyoChhotR63XQJ4ko09BaPw4s9EvoJuD7Ou1ZoEYp1VqM2CLxJOF4klpvZbbmAaxr8nFqPEQkLjNPKlF21s1itoz6XXamw3IhLspLzhVRTpkRJcrDy4koY87r1zR6ZWFKGTBveUKF0VqnE1HV5khEbWz24bZbuXf/aa5YK+15YvlmIglW1XuNDmNBtR6H/FErLW3A3F7PvsxjA4V+4YlQDKAkZkQVqv1vQ3MVWkPXcIAtbdUFeQ1hXottzQPwu21MS0WUKDPheG5Dm70OmRElykO2srXKoIqo1Q1e3HYrz/dNGvL6Ij+kIsokJkNxYomUKVrzIH2H823bW/nv/f1MheSkUSxPLJEiFEuaujUPoM5jZzwYkxPFMqSUulUptVsptXtkZGTZ3288WDqJqHy6a2fP7Ft2Y973nj5ZsrOuxNKFY0ksKj37KVc+l10SUaLsLGZYeTSRkrEXAqXU9Uqpo0qpLqXUJ+f5vFMp9ZPM53cqpTozj79JKbVHKfVC5r+vL3bsYOywcgCbxcLWtmoOD0zLJtYSJokokxicjgCYpjUP4MNXriYST/HtJ7uNDkWUuNFAejinz2n+1jxABpaXjn6gY87H7ZnHXkVr/U2t9Q6t9Y7GxsZlv/BkJkFfyTOi6qscWBQMy/DdihSKJfE4bItasOJ32aQ1T5SdcDy3GVGezHOC0p5X0ZRSVuArpOdcbgLeq5TadMbTbgEmtNbrgP8D/Gvm8VHgbVrrrcAHgR8UJ+pXmjZ4RhTAto5q4knNQ4eHDYtBLI8kokxiNhFV7TQ4kped3+rnbdtX8K0nuhmYkpX2YumyW2JMXxGVSUT1jEkiqkTcC3xApV0OTGmtC96WB3Na8yp4RpTNYqGhyslw5u+XqCy5XnzP5XdLRZQoP4upiAKkgkNcCnRprbu11jHgbtIzL+e6Cfhe5v17gDcopZTWep/W+nTm8UOAWylV9IvHuVvzjNJZ78XvsnHPnj7DYhDLI4kokxiaSp/Im6U1L+vv3ryRlIbPPXDU6FBECRsukURUts2qd0ISr2aglPox8AywUSnVp5S6RSl1m1LqtsxT7gO6gS7gW8DHihXbRIW25p2pye9iSCqiKlI4lsjp4nsuv8vOTCSB1oVd9y1EMeW6NW+2IioqFVEV7mzzLed9jtY6AUwB9Wc8553AXq110f8Iz0QS2CwK5yKWVeSbRSkuW1PP4y+NcLB/yrA4xNKZ+6qwgmQropp85kpEddR5uPXqNXz59134XXY2tviMDkmUoJcroszdxuRxWPG5bJwcDRodigC01u89x+c18PEihfMKE5nWvJoKbs0DaPY5OdQ/JSuUK1C6NW+xFVE2kilNKJacrQ4RotTlujXP65CKKJEfSqnNpNv1rlvgObcCtwKsXLkyr68fiCTwuRbXml0Il6+u59njY3z10S6++r6LDY1FLJ7pKqJyGN52jVJqr1IqoZS6+YzPfVApdSzz9sHiRb18Q9MRGqoci1qDXCx/8YZ1bGiu4hf7+ghLX7tYguGZdKI1l60yRlJKsaaxiu5R2ZwnFjYejOFz2bAvYmNYOWryu9DASECqoirNYlvz7trZw+GBGQBpzxNlI55MkUjpnBJRHqdURAkgt/mWs89RStmAamAs83E78AvgA1rr42d7kXzPxpxrJhI3xc1lt8PKB65Yxf0HB6UqqgSZ6gw6x+FtPcCHgLvO+No64B+By0j33v6jUqq20DHny+BUxHRteVlOm5V/f9d2AtEEv3mhKONXRJkZnonicVgXtV3JKGsbvHSPSEWUWNhkKFbxbXkATb70aAqZE1V5cp2LM5cr83wZWC7KRShzgzaXpGz2ZlxANvNWul3AeqXUaqWUA3gP6ZmXc91Lehg5wM3AI1prrZSqAX4DfFJr/VTRIj7DTKYiygxuvXot9V4nn/r5C7KRssSYKhFFDsPbtNYntdYHgDOPtDcDv9Naj2utJ4DfAdcXI+h8GJyOmmpj3pm2tddwzfpG9vZMcGRwGnjlGm9Z3S0WMjITNXSg4WKsafQyMBUhKCeKYgHjoXhFDyrPaqhyYlVKNudVoKW05rns6dNOqYgS5SISzz0Rla0gmZHjv6JlZj59AngQOAz8VGt9SCn1GaXUjZmnfQeoV0p1AX8NZLuEPgGsAz6tlNqfeWsq8j/BVImoao+df7pxEy/0T3HHUyeMDkcsgtkSUbkMbyvE1xpuaDpCc7V5E1EArz+viSafk18fGCCZkkGjInfDM1GqTPIH61zWNFYBcELmRIkFpCuiSiO5WkhWi6K+ysGQVERVnEg8iduxuN/r7tmKKLkQF+VhtiIqh+rA7IW7VEQJrfV9WusNWuu1WuvPZh77tNb63sz7Ea31u7TW67TWl2qtuzOP/4vW2qu1vmDO23Cx45+OxKlymucc6C1bW3nj+c38+29fYnBKzkdKhdkSUQWnlLpVKbVbKbV7ZGTE6HCA9MnceDBm6oooAJvVwnWbmhkPxni+b9LocEQJGZ2J4jP5fKisNY1eAI6PyJwocXYT0po3q9nvkoqoChSKJXHbF3camW3Nm4nIhbgoD9nqaU8OSdlsa54c/6LUBaIJ/Ca6wayU4vZ3bsXvsvHT3b3EpUWvJJgtEZXL8LZlfW0hB7ct1enJ9Kr4thq3wZGc2/mtflqrXfz+yLBURYmcaK0ZmYmapoT3XDrrvSiFzIkSCxoPxKiT1jwAmvxOJoIxWWZRYUKxRE4X33PNzoiS1iRRJrKJqFyWsbjsVhxWixz/ouSZqTUvq6HKyedu3sbgdITfvThkdDgiB2ZLROUyvO1sHgSuU0rVZoaUX5d5zPT6JtKJqPZa8yeilFJcu7GJsWCMl4ZmjA5HlIDxYIxYMoXfbZ4S3oW47Fbaatx0S2ueOItIPEkwlpREVEazL705r2tYqggrSSSeWtTWPJgzI0pa80SZCMbSiSivM7efBZ/LRkAqokQJ01oTiCZMsTXvTK8/r5nLVtfxZNeodDbqWglAAAAgAElEQVSUAFMlonIZ3qaUukQp1Qe8C/iGUupQ5mvHgX8mnczaBXwm85jp9WcqotrrPAZHkptNrX6qnDb2nJowOhRRAgYyvdqlMqz8rp09uO1W9pwclyH8Yl4ToRiAJKIymvzpzXlyc6JyJJIpYsnUorfm2SwW7FbFtFyIizIRiKYrQXOpiIJ0Ikpa80QpC8aSJFPadBVRWTdsaaXe6+CX+/tJpKRFz8xMdwRpre8D7jvjsU/PeX8X6ba7+b72DuCOggZYAH0TIWwWRXNmDbbZWS2KC1fW8FTXKDORuCkz4sI8skMDq0ukIgqg0efk5FiQlJb2U/FqYwFJRM1V701vzjsmFVF5pZS6HvgCYAW+rbW+/YzP/zXwZ0ACGAH+VGt9qhixhTObwha7NQ/SVadSESXKRSiarYjK7ZKqymWTrXmipE1mbsaZZU7mmTeNHTYLb93WyveeOcXTXWN84DWdxgQmzslUFVGVqm8iTGuNC5u1dP53XLyylpSG/b0ytFwsbGC69BJRLX4X8aRmIhgzOhRhQlIR9UpWi6LB5+CYVETljVLKCnwFuAHYBLxXKbXpjKftA3ZorbcB9wCfK1Z82XlgrkVWRGW/RmbkiHIRWEQi6q6dPYSiSWn9FyVtMpT+/V1t4s3BG1v8nNfi45Gjw4zLubxplU7mo4z1TYRprymNtrysJr+L9lo3B/qmjA5FmNzgVBirRVFl0hLe+bRUpzdYDsgKWDGP7EmNJKJe1uRz8dKwJKLy6FKgS2vdrbWOAXcDN819gtb691rrUObDZzlLtXghBBYxoPlMbruV6bC0JonyEMy05nlzrA502a1E49IuJErXVKaitcbkN5jfvLmFeCLFNx/vNjoUcRaSiDKBvokQsUSKu3b2zL6Vgs2tfvonw7MlmkLMZ3AqSrPPiUUpo0PJWZPPhQIGpyURJV5tNhFlkrJ0M2j2O+kdD89ukBLL1gb0zvm4L/PY2dwC3F/QiOaYvfheQiLKZbdIa5IoG8FYApfdknNXg9NmIRKXDaOidM0mokx+DtTsd7GtvZrvPX2S0UDU6HDEPCQRZbBoIsnQdJQar7mzyvPZvKIagBcHpg2ORJjZ4HSY5kyFUalw2CzUVzln51sJMdd4MIZFlVa7aaG1Vqe3vsrfg+JTSv0JsAP4t7N8/lal1G6l1O6RkZG8vObL7UhLbc2ThKUoD4FoYlGVgS67lUhCElGidM225pXAOdDrz2smmkjyrSekKsqMJBFlsIHJ9IWuWQa+LUaDz0mTz8mh03LhIc5uYCpCa4kloiDdnicVUWI+48EYtR4HFkvpVPkVWnttOhH1vMwNzJd+oGPOx+2Zx15BKfVG4O+BG7XW897y1Vp/U2u9Q2u9o7GxMS/BBZfRmifDykU5CUYTi6oMdNktROMpUilZhiJK02Q4XRVeY+IZUVmNPid/sLWVHz3bM1vJJcxDElEG65sIA6XxwzyfzSv8nBwNSjuGmJfWmsGpCC1+t9GhLFqL38V4MDZ751+IrPFgTOZDncHnstNW4+Z5mRuYL7uA9Uqp1UopB/Ae4N65T1BKXQh8g3QSariYwS1mQPOZ3HYrU+E4WraSijIQjCbwOHL/OXDbrWggEJNzC1GapkJxnDbLkpZVGOGj164lEE3ww2eLslRWLIIkogzWN5GeM1qKFVEA57f60cBLsi1JzGM6kiAUS5ZkRVQ25qODcmyLVxoLxqiVRNSrbGuvloqoPNFaJ4BPAA8Ch4Gfaq0PKaU+o5S6MfO0fwOqgP9SSu1XSt17lm+Xd8sZVu51WEmkNDOS5BdlIN2al/sFuTuTtJoKSXWGKE2ToXhJFVBsXlHNtRsbuePJE7MbX4U5SCLKYD3jIWwWhd9VOj/Qc62oceNz2jgsF+tiHtkZSy0lmIjKxvziaanwEK80EYxRL4moV9neUUPPeEhWJeeJ1vo+rfUGrfVarfVnM499Wmt9b+b9N2qtm7XWF2Teblz4O+ZPcBkVUZ7M10zIcSLKQO94mKlwPOdFQ57Mdr1JSUSJEjUZjlHjLq1zoI9du46xYIyf7u4995NF0UgiymDdI0FW1nuwluisEYtSbGzxcWxohlhC1tGKVxqYSreelmJFVI3bTpXTxj6p8BBnkNa8+W1vrwHgQJ/8zJS7bCLKs4TWjOyae0lYinIQTaRw2hZREZX5mcnO2RGi1EyG4lSXUEUUwKWr69ixqpZvPt5NPCnXq2YhiSiDdY8GWNNQZXQYy3J+q59oIsWuk+NGhyJMZigz7LvZX3qJKKUUHXUe9vXIRbV4WSqlmQhJImo+W9urUQr5makAgWiSKqdtSQP7s/N0JkJyIS5KXzSRxGnL/XIqWxE1IRVRokRNhePUlMDGvDN99Nq19E+GuXf/aaNDERmSiDJQMqU5ORZibaPX6FCWZW1jFTaL4qHDQ0aHIkymfyKMUqWZiAJYWefhxGhQWkjErKlwnJRGElHzqHLa2NTq59nuMaNDEQWW3hS2tEG12Xa+8aBciIvSF4knZ6uccuHOJKKmJBErStTpyTBjwRh37ezJuSXVDF5/XhPntfj42mPHZWulSUgiykCnJ8PEEinWlHgiymGzsLaxiocPD8sWHPEKvRNhWv0uHIu4W2gmK+s8AOzrnTA4EmEW45mLB0lEze+qdQ3s65kkJBuhylogtriV9XO93JoXzWdIogIppU4qpV7IDOvfXezXjydTxJMa5xISUTIjSpSqcDy5pLZsoyml+Oi1a+kaDkjxhEmU5tVhmTg+EgBgTaM5W/Oyme5cMt7ntfroGQ/N/ptE/iilrldKHVVKdSmlPjnP5z+klBrJnIjtV0r9mRFxzqdvIkR7JplTitpq3Fgtir2npNVIpGXn2kgian5XrGsglkyx66Qkb8tZIJJY0sY8SN+8clgtUhEl8uV1mWH9O4r9wjORdMLdbc/9cspmseCwWZgMy/EvSk8kniSe1LMJ1VLzlq2trKzz8NVHj88WTyzmelfklySiDNQ9EgRgdUNpV0QBbGz2AfDw4WGDIykvSikr8BXgBmAT8F6l1KZ5nvqTOZuTvl3UIBfQOx6mo7Z0E1EOm4XzW33s7ZGLapE2Fkgnomo9koiazyWdtTisFp7qGjU6FFFAwWgCr2NpiSilFLVeu7Q8i5I3nUkmuRZZHeKxW6UiSpSkqcwxX6qJKJvVwq3XrGF/7yTPyBgBw0kiykDdowH8LltZrAGv8TjY1OqXRFT+XQp0aa27tdYx4G7gJoNjykk0kWRoJkJHndvoUJblks469vZMEIknjQ5FmMBIIN1O1OhzGhyJOXkcNi5cWSOJqDIXiC69NQ/SidwxSUSJ5dPAb5VSe5RStxb7xacjS0tEuR1WJmVGlChB2QSqZ4k3Iszg5ovbafQ5+dqjx40OpeJJIspA3SNB1jRWodTit86Y0RvOb2L3qXG5y5lfbUDvnI/7Mo+d6Z1KqQNKqXuUUh3FCW1h/RNhtKakK6IAXruhkUg8xc4TshVSwMh0BKUoixsIhXLNhkYOnZ7m9GTY6FBEgQSiCfyupV+INPqcs0ldIZbhKq31RaSrxj+ulLpm7ieVUrcqpXYrpXaPjIzk/cWzrXmLTUR5HbbZeYNClJJsAnUxA/rNxmW3cstVq3ni2Cgv9E0ZHU5Fk0SUgdKJqNJvy8t6w/nNpDQ89lL+/9iLBf0K6NRabwN+B3xvvicV+oTsTL0T6YvQ9trSroi6fE09TpuFx47KcS1geCZKvdeJzSp/Ps/mLVtbAfj1AVmRXK6mw3H8y1jf3ehzMjojiSixPFrr/sx/h4FfkK4in/v5b2qtd2itdzQ2Nub99V9uzVvc34Mql41RScSKEjRZgq15882Aet9lK/G7bHz10S6Do6tsciZtkMlQjMHpCBsys5XKwba2ahqqnPzuRdlEkEf9wNwKp/bMY7O01mNa6+wZzbeBi+f7RoU+ITtT30QIgI4SHlYO6Tsnl62p59GXpO1UpBNRTdKWt6DOBi/b26v55X5JRJWjVEozs8yKqCafi5GZqGzaFUumlPIqpXzZ94HrgIPFjGHpFVFWRmekIkqUnuyMqFLcmjeXz2Xng1d08sChQQampHrbKJKIMsiLp6cB2LzCb3Ak+WOxKG7Y0sJDh4dm++bFsu0C1iulViulHMB7gHvnPkEp1TrnwxuBw0WM76x6x8PYrYpmv8voUJbt2g2NdI8E6R0PGR1KRTHjxsjhmQhNfklEncuNF7Rx6PQ0XcOySbXcBGIJtGbZFVGxZGr2okaIJWgGnlRKPQ88B/xGa/1AMQPInusutk2pymUnHE8SiiUKEZYQBTMVKr2KqLP5s6vW4HPapIDCQJKIMsihTCJqU2v5JKIA3nlxO9FEit8cGDA6lLKgtU4AnwAeJJ1g+qnW+pBS6jNKqRszT/tLpdShzMnYXwIfMibaV+qdCNFW48ZqKf0ZaK8/rwmA+w/KcV0sZt0YOTwtFVG5eNu2ViwK7n5OViGXm2w7kt+1vEQUpCsMhViKzBKX7Zm3zVrrzxY7hulIAkV6w+5iVGUG/UtVlCg1Y8EYVqVwLvKYN6Nqj52PXruOI4MznBoLGh1ORSr9o6hEvTgwTYvfRX1VeV3QbG+vZl1TFT/b02d0KGVDa32f1nqD1npt9kRLa/1prfW9mfc/lTkJ2661fp3W+oixEac93zuJzWp5RU92qeps8LK9o4af7+0/95NFvphuY2QypRkNRGnylX6VX6E1+V3cdEEbP9rZw7gssCgr0+F0FYffvZzWvPS5z4gkokQJmw7HcdotWBa5dCibiJKB/ZUrh4pvp1LqJ5nP71RKdWYer1dK/V4pFVBKfbnYcY8Fonid1rJZtPWhKzrxuWw8cGhQWsUNIIkogxw6PVVWbXlZSineeVE7u09NcGxoxuhwhIHGgzFqPeWzWeydF7VxZHCG//jt0bJIrpUA022MHAtGSWmkNe8szhwIurLOQySe5DtPdhsdmsijbDtSfiqiInmJSQgjTIfji54PBS8nosYkEVWRcqz4vgWY0FqvA/4P8K+ZxyPAPwB/U6RwX2EsGKNqGfMBzcbtsPL685o4NRbiJbluLTrTJaKWkSHuVEqF58wK+XqxY89VJJ7k+EiQTWWYiAL4ox3teBxWvvSIbCKoVOPBGKFYcvZioxy8ddsKrEqxr2fS6FDEy3LaGAn52Ro5PJ2+aJDWvNw0+11sbqvmzqdOyny1MjLbmreMGVHZ2YGDU3IhLkrXRCiG17H4i/LshfxoQKpFK1QuFd838fI5zT3AG5RSSmsd1Fo/STohVXRjgehsIrVc7FhVR53XwW9fHCIlVVFFZapE1DIzxADH58wKua0oQS/B0cEZkildlhVRAPVVTj7wmk5+deC0VEVVqOMj6QHFjWXUelrndXBeq4+9PRPEEimjw6kEedsYmXnusrdGZtuIGqU1L2d/sKUFi1L8758dkLL3MjGd2RS2nIqoKqeNared/klJUIrSNR6K41nC0GavM/01UhFYsXKp+J59TmZe7BRQX5ToFjAaWFry1cysFsWbzm9mYCrCC31TRodTUUyViGIZGeIixrhsB/rTB/nmFdUGR1I4t16zBrfdyu33H0Fr/aqWDVHejmc2ZZVTRRTAFWsbCMWS7OudMDqUSmC6jZHZiwapiMpdjcfBG89v5unjY3z8R3vl70AZeLkiankXI201bk5PyoW4KF2TodiStofZLBYaqpwMTsnxLwonH5Xgc2mtGQuWX0UUwNb2alqrXfzu8JDcbC4isyWilpshXq2U2qeUekwpdXWhg12qXSfGafI5aa91Gx1KwdR5Hfz1mzbw8JFh7nzqpNHhiCLrGg5gsyhqPEu/Y25GnfUe2mrcPNU1KuW7BWbGjZHZ1rxyS7AW2iWdtWxq9fPAoUG6M9WSonRlZ0Qt92JkRY2b/olwPkISwhDjwaVXh7RWuxiQRFSlOmfF99znKKVsQDUwtpgXyUcl+FyhWJJIPIW3xBNR8xVHWJTiuk3NjAdj3CMLt4rGbImo5RgAVmqtLwT+GrhLKfWq3rd8Z4cXS2vNzhNjtFS7+PFzvWV1Z/jMH+oPX7ma6zY185lfv8i+HqkgqSTHRwI0+pyL3iRjdkoprlrXwGggxkuD0nZaaGbbGDk8E6XabV/ScNpKppTi5ovbqfc6+fFzPUxlKmpEaZoOJ/A6rNisyzuFbK910z8ZlpZNUZISyRQzkcSSWvMgnYiSiqiKdc6K78zHH8y8fzPwiDb4l+VYZqZZOVZEAWxo9tFe6+Ybjx8nkZSqqGIwWyJqyRlirXVUaz0GoLXeAxwHNpz5AvnODi/WqbEQQ9NRVjd4i/7axWa1KL743gu5Ym09/7Wnj0eODMkJZ4XoyiSiytGWtmqq3Xae6Bo1OhRRZMMzEWnLWyKX3cr7Ll9JIqX50c5TcpJXwiZCMWq9y9+I2lbjJhBNMB1O5CEqIYprMpNQX04iamBKKgIrUY4V398B6pVSXaQLLGYXeCmlTgL/CXxIKdU3zzzlghgJpBOnpV4RdTZKKa5Z38ipsRD3Hxw0OpyKYLZE1JIzxEqpxsywc5RSa4D1gOl2Ru88ka6qXF1f/okoSF983PGhS7iwo4aHDg9z965e6b0tc5F4kr6JcFkNKp/LalFcsbaeE6NBnu+VDXqVZHgmSpO/PI/rYmjyubj54nb6JsL86sBpo8MRSzQejFGfh0RUdjxBj2xUFCVoIpiuDvEs8aK8pdrNdCRBMCqJ2EqUQ8V3RGv9Lq31Oq31pVrr7jlf26m1rtNaV2mt27XWLxYj5qHMeILlzgc0s00r/Kxt9PLVR49L8UQRmCoRtcwM8TXAAaXUftJDzG/TWo8X919wbju7x6n3Osq2WmQ+LruVmy9u5/rNLRzsn+JbT3TLXaAydmI0iNblPUfnks46XHYLX3/suNGhiCI6PRmmtbp8Z/sVw+YV1Vy7oZFdJye4+7nyaEuvNBOhGOF4clmjBe7a2cPhTHtz96jMDROlZyK0/IooQOZEiZIxNJ0+VpezMdXsLEpx22vXcnhgmkdfKv4In0pjqkQULD1DrLX+WWZWyAVa64u01r8y8t8xH601Tx8f49LVdZTYor9lU0pxzYZG3n/5KkYCUa77z8f5wkPHymY+lnjZsTLdmDeXy27lstX1Mny5gsQSKYZnorTVSCJqud64qZm1jV7++dcvyoyUEjSWp/Xd9V4HFgXHR4J5iEqI4hrPVkQt8WchWxHYNyEVgaI0DE1HcVgtS06+loqbLmhjRbWLr/1ebjYXmukSUeXshf4pBqcjvOH8ZqNDKbj5NhIAnNfq57bXriWlNd99+gQzERlaW24OnZ7CblVlnYgCuGJtPXarhW89YboOYFEAg1MRtIa2Mt52WiwWpfjDC9tJpDT//JuidBSIPJoIxfIyI8RutdBR5+G4JPNFCRoNpNuUljq4efep9BIfaU0VpWJoOkKT31n2xRQOm4U/v2YNz50cZ/dJ0zVXlRVJRBXRg4cGsVoUbzivyehQDNXid/GB13QSiCb4wbOnCMWkP76cHOyfYmOLD5ulvH+9+Fx2Lmiv4ae7+6RFrwL0TaYvFqQiKj/qvA4+/rp1/ObAAM92L2ojtTBQJJ4kFEvm7Y74mgYvx4clESVKz8hMFKWWnojyOW3YLIqeMUlEidIwNB2h2e8yOoyiePclHdR67Hz1UTm/L6TyvlI0md8eGuLSzrq8bJspdR11Ht69YyX9E2H++ifPk0rJQLhyoLXmYP80W9uqjQ6lKK5e30AqpXm6Sy6ky13/RHqunSSi8ufWa9bQ7Hfy7w8elaGgJWIilG5HykdrHsDGFj/HRwJEE8m8fD8himUkEKXO48BqWVp1iFKKOq9DKqJEyUgnosq72yHL47Dxp1eu5pEjwxzsnzI6nLIliagi6R4JcGw4wHWby78tL1ebVvi5YUsLDxwa5P889JLR4Yg86JsIMxWOs6WMElHztZhm1Vc52dJWzc4TY0xLm2lZ659MJ6JaayrjbmAxuOxW/uL169l9akKGgpaIsUAmEeXMT0XUtvZq4knN0czgciFKxfB0dNkjCCQRJUqF1prBqcqoiMqe83udNlx2C59/6JjRIZUtSUQV0NwL2H/5zWEUEI4lZUD3HFeua+DdOzr40iNd/HJ/v9HhiGV6IXPXYMuK8klEncs1GxqJJlLyc13mesZCrKh24bSV95DOYvujHR101Ln59wePSmVsCcgOaHbnqSIqWz17oE/uOIvSMhJYfiKq3uvg1FhIfvcJ05sMxQnGkrTXeowOpWhcditXrWvkocNDvCB/owpCElFFkNKaPacmWNdURY1H2vLmUkrxz2/fwqWr6/jbew7w3Inxsw46F+b3Qv8UNotiY4vP6FCKpq3GzbrGKu548oS0l5SxE2NBVtV7jQ6j7DhsFv7qjRs4dHqaBw4NGh2OOIfhmfSAZr8rP4mo9lo3dV4H+3om8/L9hCiWkenIshNRDT4n4XiS01PhPEUlRGH0ZcYTtFfYwpYr1tZT47HzeencKQhJRBVB13CAqXCci1fVGh2KKTlsFr7+JxfTXuvmlu/umm2BEaXnYP8UG5p9uOyVVTVyzYZGhmei/GKvVPWVq1NjITobJBFVCDdd0Ma6piq+8NAxqQwwuaHpCAB+tz0v308pxWvW1PNU16jMCRMlQ2vNSCBKk295bUrZrz8+EsxHWEIUTN9EuoW00hJRLruVP796DQ8fGZYNegUgiagi2HVyHI/DyqZWv9GhmFad18EPb7kMv9vOnU+dmD3ZFaUjkUyxr2eS7R01RodSdGsbvWxp8/ONx7tJyoV02ZkKxxkPxuisr5yS9GKyWhR/+Yb1HB2a4b6DA0aHIxYwOBWh2m3Hbs3f6eNV6xsYnI7QJdvzRImYCMWJJ/WyK6KyXy/HvjC7lyuiKu886MNXdtJa7eIf7z0k5/h5JomoApsIxnjx9DQXr6rFlscTt3K0osbNj/7sMqxKcedTJ2ZnUYjScOj0NIFogpTWFddSqZTitteu5cRokN9Ke1HZOTWWvlstFVGF85atraxvquLzDx2TEz0TG5yO0JLnYbWv3dAIwAMH5XenKA39eWpT8jqs1HjsdA3LsH5hbr0TIfwuG9V5qoYtJR6Hjb9/y/kcOj3Nj3aeMjqcsiKZkQJ76vgoSsEVaxuMDqUkdDZ4+fCVq4knNXc8dYIZ2URWMp7pHgNgTYVerN+wpZVV9R7+83cvEYnLrKhy8tJQ+m71uqYqgyMpL3NnAf5kVy87OuvoGg7wf/38BaNDE2cxNB2huTp/iai7dvbw6NER1jR4uWdvn7TniZKQbVNqq1leIkopxcZmH0dka6QwuVNjITrqKq8aKustW1u5al0Dt99/hC89fExmGeeJJKIKKBxLsvvkBNvbayoyg7xULdUuPnRFJzORON9/5hTBaMLokEQOnjk+RqPPic9Vmce61aL4zE1bODYc4F9+86LR4Yg8emloBofNwqoKPgkrhs0r/LT4XTx8ZFiqokxqcCpCi3957Ujz2dFZx6mxkFRFiZKQnWXakYc2pU0r/BwZmJHfecLUuoYDFX0zTinF527ehtWi+K89ffLzmieSiCqg506MEUumuGq9VEMtVkedh/dcspLTk2E+cddeEsmU0SGJBcSTKXadHK/Yaqis125o5NZr1vDDZ3v46qNdcne/TBwZnGF9U5W0VxeYRSlef14To4Eo9+zpNToccYZEMsVoIJr31jyAbe3VrG+q4v+9/zDTUgktTK5vIozPacPvXv72yE2tfsLxJCfHZGC5MKdwLEn/ZJi1jZWbiIL0CJnP/uFWesZDMs8yT+SsukAi8SRPd4+xvqmK1urK2jCQL+e3+rnxghX8/ugI//DLg3JRb2IH+iYJxZKsqfA/UgB/++aN3Lh9BZ974Cif/NkLBKSir+QdHZxmY7PP6DAqwuYVflbWefi3B49KQsJk+ibCpDQFac+wKMXt79zK6ckIH/n+HqbC8v9emFffRIi2WjdKqWV/r00r0ouMDvZPLft7CVEIx0dkPEHWjdtXcOXaep45PsaeU7JFb7kkEVUgP36uh5lIYnYIp1iay1bX8/HXreXHz/XyrSe6jQ5HnMWDh4awWxXrJBGF3Wrh8+++gGs3NPLT3b1cdfsjfOZXL0o/eYkanokwNB2dvVgQhaWU4m3bVzAWjPHFh44ZHY6Y48RoumJjdYEqX48OBnjHhW3sPDHGNZ/7Pf/8a2lxFubUMx5CQV7+nm9o9uG2W9nfO7n8wIQogGwiqtIrorKu39LKusYq/nv/6dllNmJpJBFVAJF4kq8+epzVDV6pEMmD//Wmjbxlayv/3/1HZCOZCWmt+c2BAa5c14DbYTU6HFOwWBTXbW7hI9eswWa1cMdTJ/jFvn5iCWkxLTX7e9IXBxd01BgcSeVoq3Hz7h0dfPfpk7LW3ESyiahCbo+8cGUtH7lmLTaL4o4nT/Cfvz0qsziEqcSTKU6MBmnKU4uq3Wphe0c1e09N5OX7CZFvh05P47BZWNNYmeM35t5IvmtnD1aL4j2XdFDjtvP9Z07NJurE4kkiqgB++OwpRmaivPH8ZqNDKQln/oCf6e5dvVy6uo62Gjcfv2uvlC+bzIG+Kfonw/zB1lajQzGdlfVe/uL167h6fQO7T45z59MnZKNeidnfO4nNotjSVm10KBXlb968EY/Dyv/+2QFJRJjEybEgPqeNeq+joK/TUefhL16/ngtX1vLFR7r4yA92y+9NYRonR4PEk5omX36G9t+1sweXzcoL/VOEYtLKL8zn+d5JNrX6scuczFkep40PXdGJRcGH7nyOkZmo0SGVJDmi8iwUS/D1x7q5Ym19wcrXK5HdauH9l6/C67DxZ9/bzeBUxOiQRMZ9BwewWRTXbarsxOvZEqp2q4UbtrTy7ks66B0PcedTJ+Rks4TsPjXB+a1+XHap9iumhion//z2Lew5NcHXHztudDprsIkAACAASURBVDiCdEVUZ4M3L3NxzsVhs/DOi9p427ZWHjo8zEd+sEeSUcIUjmWqNPNVEQXpdteUhp3dMnNGmEsqpTnYP8W2drkZd6b6KicfeE0nIzNRbvneLjm3XwJJROXZNx7rZjQQ5X9dt8HoUMqOz2Xn/a9ZxUwkzvu+/SzDM5KMMlo8meIXe/u5en0DNZ7C3iUvddvaa3jvpSvpmwjzsR/tJS6bIE0vEE2wr2eCK9fJ5tNiu2tnD4FIgi1t1fzHb4/y2d8clvlqBtJa80L/FJtaizcrTSnFa9Y28I4L23jspRFu+6Eko4Txjg7OoBQ0VuWnIgrS7a52q+L3R4fz9j2FyIejQzMEY0m2tct4gvl01Hn48nsv4mD/FB//0V4ZwbFIkojKo9OTYb7x+HHeuq2Vi1fVGR1OWWqtdnPnhy9lYCrCe78pySijPXhokOGZKH9y+SqjQykJm1dU8/YL23j06Ah/d88BUtJyZGrPHh8jntRcs0ESUUZQSvGOC9uo8zq467keJkMxo0OqWL3jYSZDcbZ1FP+u+I7OOm5/x1YePTrCx360l2hCklHCOHt7JtjY7MNhy98llN1qYW1jFQ+9OCStyMJUnjw2CsCV6+oNjsS83ripmX95+1Z+L3+jFk0SUXmiteb/+dUhtIZP3nCe0eGUta7hAO+7bBU94yHe+bWnOTI4bXRIFUlrzVd+nx7Kf+3GJqPDKRmXdNbxt2/eyC/29fPZ+w6jtZx0mtV9BwfwuWxcvKrW6FAqlstu5U8uW0UimeLOp08yHpRklBGe70sP7d9u0F3xlIabLljBI0eGef93npv3JtRoIMqvD5zmzqdO8L2nT/L4SyMEotIqIfInkUyx99QEOzrz/zfhgo4aTk9FeKprNO/fW4ileqJrlHVNVbRWu40OxdT++LKVfOamzTx0eIj3f/s5RgO5z4w616zkcmYzOoBy8cv9p3nw0BCfvOE82ms9RodT9lY3ePmzq9bws719vOOrT/Pv79ouw7KL7N7nT3N4YJqbL27nJ7t6jQ6npHzs2rWMzET5zpMnaKhy8tFr1xodkjhDOJbkwYODvHXbCpw2mQ9lpCa/i/e/ZhXffeokH7hjJ9//08uoK/DAbPFKTx8fw+uwsrHFZ1gMl62ux2mzcu/z/Vz/+Sf4ox0d9E2EGA3EmArHOTzw6ptSVotiY7OPf3zbJi5dXVeU+VaifB0eSLcpXdJZRzCa36qHTa1+6r0Ovvboca5e3yDHqjDceDDG012j/OlVq40OpSR84DWd1Hgc/N09z3P955/g7968kbdf2JbX6slyI4moPDg+EuDTvzzIRStr+POr1xgdTsXoqPPwq7+4io/8YA8f+9Fe3rKtlX9622Ya87TJRJzdWCDKv/zmMG01bllrvwQ/fq6XdU1VbGuv5l8fOILfbeN9l0l7o5n8155egrEkN+9oNzoUAaxpqOJ9l63k7l29vOvrT/PdD19KR53c9CmGZErzuxeHuPa8JsO3Jl3QUcMtV63m9vsP860nukmmNA6rhbZaN9dtamZtYxX1XgdJrRmajnJ0cJp9vZO8+5vPckFHDX/1pg1cIxf5YonuPziA1aK4cl0Dvz00lNfvbbNa+J9vXM8//PIQ33nyBLdctRqlFFprQrEkHodVjltRVL/c308ipXnHRW1Gh2JqZ1Yx/fyjV/IPvzzI3/3sAJ+97zDttW7qPA5Q6eHvzdUuxoMxJoIxxoIxZiIJ3HYrDVUOVtZ7uXJdPavqK2PhmekSUUqp64EvAFbg21rr28/4vBP4PnAxMAa8W2t9MvO5TwG3AEngL7XWDxY63pGZKB+68zkcNgtfeM+FWC3yR6KYmv0ufvqR1/CNx47zpUe6ePzoCB++spM/vWp12QzPXs7PRCHEEin+x937mQrF+chr12CRE6MlsSjFzRe3E4kn+ftfHOTJY6P81Zs2sKHZuIoDMzLi+A9EE3z198e5eFUtO6QtzzQ2tvj5/p9eyp9/fzdv+eIT/Pu7tnPd5hajwyoYs/zuf6prlNFA1DSbUTe2+Ljzw5cSjiX5/jMnqXLa5r1A97nsrGuq4k2bWtjbM8Hjx0b44B3PcUlnLf/ruo1cvkZmnpS6c/2M5FMimeLe509zxdp6GvI4qHyu9166kieOjfIvvznMnU+dBGB4JkI8qbFZFG01bi5ZXcdn/3CLVOqWOLNf70YTSb71eDcXrqzhvJbiLakoB5tW+Lnnttfw6Esj/Gr/aR4/NjK75MCiFA1VTuq8Duq8DtY0VnFqLEQoluD0VISDp6e574UBzmvxceMFK7hx+4qy7rQyVSJKKWUFvgK8CegDdiml7tVavzjnabcAE1rrdUqp9wD/CrxbKbUJeA+wGVgBPKSU2qC1LtjEsBdPT3PrD3YzFohx962X88Qx6esutmwWur7Kycdet5ajgzN88ZEuvv3kCa7f0sKN21dw+Zr6BVevxxIpBqci9E+G028TYUYCEU6NhajxOFhR7aKl2sWHryx+aepyfiYKEc9EMMbHfrSXZ7rH+LebtxFPynyj5bBZLLz/8k5monG+8NAx7j84yKZWP5d01rK6wUtHnYf2Wg9ttW6qnKb6dV0URhz/Wms+/cuDDM1E+Mr7LpI70CZz2Zp6fv0XV3PbD/dw6w/28JatrfztmzfS2VBedw/N8rs/ldJ86ZFjNPudXL/FXEk/t8OKz2U/5/McNguXr6lnR2ctu09O8OjRYd7zzWe5al0Df/Wm9Vy0snben/NvPd7NyEyUre3VxJMpqt12mnwu2mvd1EprqOFy/BnJm//a00ffRJj/+y3nF+LbA/DT3X1cvb6RKqeNrpEAVqVY3eClymkjEE1wZHCGe/b08eSxUf78mjW8bXsrTT4XQPpCdjLCwFSY6XCCKpeNGredznov1Z5z/5yI4imF690vPnyM01MRPnfz9nx+24qhlOJ1G5t43camV1VM/fFlK1/x8dzPjwdjuB1W7nthgM89cJTPPXCUHatquenCNt6ytbXsxhKY7crmUqBLa90NoJS6G7gJmPuDeRPwT5n37wG+rNJnEDcBd2uto8AJpVRX5vs9k88AtdYcGw7ww2dP8ePneqj3Orn71svZ3lHDodMyNNtITT4XTT4X65t9PN01yn0vDPDzvf1YLYoNzT46at34XHaUgqlwnKODM0yG0iWRZ6ZT3HYr8WSKRGZ7iQJ+trePHavq2NFZyyWddTT7XcX4Zy35Z0LncQr2VDjOvfv7+dIjXUyG4vzHu7bzzovbK26oXiFYLYoat4O/uW4je3smmArH+a89fYRirzyncNutrGn00l7rpq3GQ32Vg3qvg2q3nXhKE40nCcWSBKIJwrEkXcMBvE4rNR4HH76yk9ZqdylWbBb1+B+ejvCZX7/Irw8M8D/fuF6GlJvUynoP//3xK/nm4+lK2PsPDvDG85t52/YVXLa6jqbi/G4uNMN/9yeSKW6//wi7Tk5w+zu2mqYCY6l/d2yWdELq4lW17Owe49kT47zza8/Q7HeyeUU1tR4H0USS05NhukeDTIbiZ/1e1W47qxu8rG7wMh2J01Dl5E8uW8WaRi/eJdw00Pr/Z+/O4+ys67v/v96zb8kkk42YhSAEFVEQI2q1VkUsWm+wFStqK7Ra1Mqtdel9433/yq3Y3tXaW7vIrUXQqhVB0dpIsUgVbjcIhJ2ELSQkJGSdJJPMvn1+f1zXJCfDOTNnMmedeT8fj/OY61zne67zOWeu9XN9l2B4NKn1Uozkd0TQNzRCz8AI/UMjtDXW0d5cT00ex4SIoHtgmBqp0pqH5bONTNvh/iF+smE3n/7RBs5e1cFvF7kWZm2NWLOqgzWrnj369ptOP4En9nTz6K5DfOamjXzmpo3MbapjNJiwY/6FbUnNi9WLk8dJi9pYPKeRRXMamd/SMOVzg4jgQO8QT+/vpX9oJKmxVSsa62poqq+lsa6GxvpamutrmdNUd6RJ74GeQR7ZeYj1Ww9w91P7uWfrAUYjmNfcwFtevJQ3vvAEXnri/Go8V5mqir3e7R8a4cu3P8lVtz3JO9as4NWrPWrwVE3n2mgs0fS2s5bzuuct5sHtB7n/6YP8xQ8f5tNrN/CbqxfyW6cuYs2qDp53wpxJm8tHBJ09g+zq6ueZg33sPtRP7+AIg8Oj1NfV0NZYx5ymOua1NNDR0sC8lno6WhtKtq+vtETUMiCz1+PtwMtzlYmIYUldwIJ0/p3j3ntcjVof3H6QXzyxj4GhEQaGRxkYHqV3cJjtB/p4cm83uw8NUFsj3vGyFXz0Dae6T6IKc8LcJn7vrOWcf8Zz2LS3m6f397LjYB8Pbu+iRhAkJ5F1NeKUxW3Ma2lgXnM981oamN9Sn7xWW0NEcKh/mGcO9rH9QB/9QyNcf/c2/vnXTx35nBUdzSxtb6a1sY6WhlpaGmppqK2hpkbUSLz/Nc/N60RvAtPZJqZcRa+rb4h/uXMrPQPD9A6O0NU3xOZ9PWzY0cXwaPCyVfO55uLTeHGZRk6ayVob6/jN1YsA+J0XLaV7YJiDvUPs7x3kYO8QB3sHOdA7yH3bDnLbo3sZHBnNuawaJaNMjbn2l1toqK1heUczJ3a0sKKjheZ0Xa2vrUHAwPAogyOjDA6P0j80wuH+YQ71D7FlXw99g8nFSwAtDXXMbapjTnOyrcxtSi5o2pvraairoVaitlb89gtP4ORFbdP9WUq2/o+OBu+4+k6e3t/Ln//28/hTdyBfkTJP8DpaG/nYuafyq037uOPJTn6yMemzZWl7Eys6WjhhbhOtjXU01tXQUFfD6GgwNDLK4Ejy9/HdhxkeCYZHR9O/R6fbGutoqKuhuaGWprpamuprjmwzDXU1NNbV0lA3Nl1zJIEgwfOWzOGcF0y7GVtJ9/33bN3PnZv3H9nWdx3qZ/1TB9h1qJ8/eMVK3vGyFcD0TrArRX1tDa9evYjPv/0MfvTAM3x73TYe3XmI3sER5rXUs2RuE286fSldvYMsmtPEvJbkfKF3cIRD/UMc6BlkXmsDT+3rYd3mTp7pSkbwGxu048QFLaxe3MbCtkbmtTSk60ZyQ2toNDjcP8Th/nT/3jNIZ/cAnT2DDAyPUisxt7mOjtYGFs1pZGFbkiwYa35YKzF2StE/PEL/ULK/7h8apW9omJ6BEXoHM/4OjtA7MEzv0Ajj05O1NWJ+S3JDY0Fb0kxkQWsDowEH+4bYd3iA3Yf62X6g78jxRoJ5zclvtGRuEyfMbWJBWwP1tTXU14qm+lreV7q+UvPZRia1r3uA763fnv6OyWPs3G9nVz/bD/QyGrBsXjNf/oPy1pKVkhurpy6Zw5oTO9i8t5v9vYPUSklysaWe9uYGmutrGRhOEo/7ugfY1z3Arq5+Htx+kP6hY88dagStDXWs6Ghh0ZxGlsxtpKO1kfra5Dy2rkYMjYzSPTBCZ88AT+3r4anOXrr6cidqx2tpqGV4JI6uRyRda7zwOXOpq6lhz+F+vnnHVq755RYWtjVw+rJ2lrY3U18rBodHeXTXYYbTm8ML2hoYGQ3am+uZ39LAvJb0oplk/Rz7/9RIBEFEcjE+GhABoxEEY/MinZc8D5JzgSApNzoadA+MsHHnIfqHRmiur0UivXivZ05TchE/9vwtL16ab9+FFXG9C/DNO55iZ1c/h/uH2La/j/u3HeBQ/zC/95JlfOatpx/vYm0C+R5HO1obeO3zFvNbpy5i16F+Hni6i/ufPshtj+0Fkn34yo4WlrY3MbepnuaG2iPn8Yf7h9jZ1c/Orn4Gh3NfL+TSUFfD/JZkG+tobWB+SwNzm+tprEv29Q11NTTU1lJXmxzfXrSs/cg1zFRUWiKq6CRdClyaPu2W9NjxLuuv08cEFnIcJ4QVzt8ptRVYN0mZP5345ZL2Tj3FdT/rb7IVuPGDRQju+FXT+lj2WJ/Ir1gp4ix5z+xTWf8v+2u47Pg+puz/4xKo+O+4lWPP0o9Tsb5nRa/7ZPnef5U+KlTFrY9bgZ/nfrni4p3AccX6J7lfqvR1f7xjvv9WYOEnCxhc4U1r3Xq4gIHkYeFTsC/bOfRW4J7SxpKPvH7bSjrnh+mt/19MH+NUyv7LcaQ2V0gcqVxx5Fz3Ky0RtQNYkfF8eTovW5ntkuqAdpJO3PJ5LxFxNXB1AWPOSdL6iFhTis8qFX+nkpvONnGMqaz7Ff6bHFEtcUL1xFphcZZl/T9eFfbbFcVs+I5QEd/T+/48ON7iqYJYJ91GprPfr4Lvf4xqireaYoWixFv0610o/HlPpfzfHMfMiaO84/A+293AakknSWog6Yxt7bgya4GL0+kLgZ+l/SGsBS6S1CjpJGA1cFeJ4jYrlulsE2bVzuu/zVZe980mls82YlaJfL1rRoXViErbwF4G3EIynOXXImKDpCuB9RGxFrgW+FbaOdt+ko2XtNx3STp6GwY+VMwR88xKYTrbhFm18/pvs5XXfbOJ5dpGyhyW2aR8vWuWkG+eFY+kS9NqkTOGv9PsUC2/SbXECdUTa7XEWYlmw283G74jzJ7vOV61fW/HWzzVFGsxVNv3r6Z4qylWqL54i6VSfgfHMXPicCLKzMzMzMzMzMxKotL6iDIzMzMzMzMzsxnKiagikfSUpIck3S9pfbnjOR6SviZpj6SHM+Z1SLpV0hPp3/nljHGqcnynT0nakf6v7pf05nLGWEqSzpP0mKRNki7P8nqjpBvS19dJWlX6KPOK8xJJezP+h+8rU5zPWr/GvS5J/5B+jwclnVXqGNM4JovztZK6Mn7PK0odYyWrlu1mOqplm5uOatleS2Gy/3elqfRzrGo7f5rN50bVtD/PI9aPSdqY7q9+KinnsOmlkO9+RdLbJIWkso78lU+8kn4//Y03SLqu1DGWS7mOEZJWSLot4zf/SDq/5PtTSbWS7pN0U/r8pHSfsCndRzQUO4b0c+dJulHSo5IekfTKMv0eH03/Jw9L+o6kpin/JhHhRxEewFPAwnLHMc3v8BrgLODhjHl/A1yeTl8OfK7ccRbgO30K+ES5YyvDb1ELPAk8F2gAHgBOG1fmT4GvpNMXATdUaJyXAF+qgN/0WevXuNffDPwYEPAKYF2Fxvla4KZy/56V+KiW7aYE37Eitrlpfs+q2F4r4f9daY9KP8eqtvOn2XpuVE378zxjfR3Qkk5/sJzHnnz3K8Ac4OfAncCaSo6XZIS6+4D56fPF5Yq30n6bIn72UuCsjHXlceC0cuxPgY8B15GeHwPfBS5Kp78CfLBEv8k3gPel0w3AvFL/HsAyYAvQnPFbXDLV38Q1oiyniPg5yUgNmS4g2QBI/761pEFNU47vNFudDWyKiM0RMQhcT/L/zZT5/74ROEeSShgj5BdnRchj/boA+GYk7gTmSVpamuiO8nYwLdWy3UxH1Wxz01Et22sJzIr/dylV2/nTLD4mVNP+fNJYI+K2iOhNn94JLC9xjJny3a98Bvgc0F/K4LLIJ94/Aa6KiAMAEbGnxDGWS9mOERGxMyLuTacPA4+QJEFKuj+VtBz4HeCa9LmA15PsE0oSQ/q57SQ3Dq4FiIjBiDhIeY4vdUCzpDqgBdjJFH8TJ6KKJ4CfSLpH0qXlDqaAlkTEznR6F7CknMEU0GVpVeavVVJ1+SJbBjyd8Xx7Oi9rmYgYBrqABSWJLksMqWxxArwt/R/eKGlFaUKbsny/SyV4paQHJP1Y0gvLHUwFqZbtZjpm0jY3HdW0vU5HNX7PajzHqsbzp5l+blRN+/OpbqfvJanRWS6TxqukufOKiPj3UgaWQz6/76nAqZJ+JelOSeeVLLryqohjRNos9iXAOkq/P/074L8Bo+nzBcDBdJ8ApftNTgL2Al9PmwleI6mVEv8eEbED+FtgG0kCqgu4hyn+Jk5EFc+rI+Is4E3AhyS9ptwBFVok9e5mwrCLXwZOBs4k2Zj+T3nDsePwI2BVRLwYuJWjdwXs+NwLnBgRZwD/CPywzPFY5fE2Z+VU1edYVXL+5HOjKiXpD4A1wOfLHUsukmqALwAfL3csU1BH0jzvtcA7ga9KmlfWiGYJSW3A94E/i4hDma8Ve38q6S3Anoi4p1ifMQV1JM2ovxwRLwF6SJriHVGK40t6Y+ICksTYc4BWYMqJWSeiiiTNFI5V2/xXkmqNM8HusaYJ6d+qr5YaEbsjYiQiRoGvMnP+V5PZAWTWYliezstaJq162Q50liS6LDGknhVnRHRGxED69BrgpSWKbary+c3LLiIORUR3On0zUC9pYZnDqhTVst1Mx0za5qajKrbXAqi671ml51hVdf40S86Nqml/ntd2KukNwP8Ezs/YR5fDZPHOAU4Hbpf0FEk/fGvL2GF5Pr/vdmBtRAxFxBaS/opWlyi+cirrMUJSPUkS6tsR8YN0din3p68Czk/X0+tJmp/9PUlz/bq0TKl+k+3A9ohYlz6/kSQxVerjyxuALRGxNyKGgB+Q/E5T+k2ciCoCSa2S5oxNA28Eso7KU4XWAhen0xcD/1bGWApiXJ8fv8vM+V9N5m5gdTrCQQNJJ5xrx5XJ/H9fCPwszbSX0qRxjvsfnk/ShrwSrQXeo8QrgK6MqrQVQ9IJY31gSDqb5FhRTYmUYqqW7WY6ZtI2Nx1Vsb0WQD7rdMWo4nOsqjp/miXnRtW0P89nv/wS4J9IklDlTnROGG9EdEXEwohYFRGrSPq0Oj8iyjUKZj7rwg9JakOR3pw7FdhcyiDLpGzHiPRc9FrgkYj4QsZLJdufRsQnI2J5up5eRLIPeDdwG8k+oegxZMSyC3ha0vPSWecAGyn98WUb8ApJLen/aCyOqf0mE/Vk7sdx9yT/XJIRBR4ANgD/s9wxHef3+A5Jdewhkgzse0naxP4UeAL4T6Cj3HEW4Dt9C3gIeJBkQ15a7jhL+Hu8meSOzpNj6ylwJcnJAEAT8D1gE3AX8NwKjfOv023tgXQn+PwKWr8+AHwgfV3AVen3eIgyjRCTR5yXZfyedwK/Ue51tZIe1bLdFPk7VsQ2N83vWBXba7n+35X6oArOsXKsWxV7/pQj3llxblRN+/M8Yv1PYDdwf/pYW8m/7biyt5d7H5vH7yuS5oQb023jonLGW+7fpkSf+2qSZmYPZqzXby7X/pSMUaXTY9Fd6b7he0BjiWI4E1if/iY/BOaX4/cAPg08SnKT4ltA41R/E6ULMjMzMzMzMzMzKyo3zTMzMzMzMzMzs5JwIsrMzMzMzMzMzErCiSgzMzMzMzMzMysJJ6LMzMzMzMzMzKwknIgyMzMzMzMzM7OScCJqBpD0YUmPSPq2pPMlXV6AZb5W0k0FWM6Vkt4w0fIzY5b0VkmnTfdzbfaStErSw3mUeVfG8zWS/iGdvkTSl4oYn7cJKzlJH5D0nnT6EknPmaBs1nW00HGMmz/pdms2VZKuybb/nO5+XlL39CIzKx1J/yzpwizzp7zflfQcSTfmeO12SWuON06bHTKvW8scx6ckfSKdfr6k+yXdJ+nkAi3/KUkL0+lfH+cyZvQ5U125A7CC+FPgDRGxPX2+tpzBZIqIK/Ios5ajMb8VuAnYWMy4bNZbBbwLuA4gItYD60vxwd4mrBwi4isZTy8BHgaeGV9OUm0+62iB4jDLmyQBiojRfN8TEe8rYkhllW6rI+WOw2YPSXUR8QzwrKSW2RSMv24Fjqxfw2WK6a3AjRHxl/m+YSrxRsRvHE9QM/2cyTWiqpykrwDPBX4s6aOZd/kk/VvGHfD3j2WeJb1R0h2S7pX0PUlt6fzzJD0q6V7g93J83ipJv0jfe6+k38h47b9LekjSA5I+m847chcm1/LHYk6XdT7w+TQrfXJadqzc6sznNjtI+qykD2U8/5SkTyjxeUkPp+vdO7K8N9f6+lngN9P17KPKUQNQ0iJJ35d0d/p41RQ+w9uElYWk90h6MF3vvpXOG9tuLgTWAN9O16nm9K7d59J16e3j1tGXSfp1uqy7JM0Z91ltkn6arvsPSbognzjS6Zemrz0AfAizcdL962OSvkmSPF0h6c/T/fGDkj6dlmuV9O/p+vTw2PFAGTU0JP2RpMcl3QW8KuMzjqktorS200Trdo5Yc8WQeVd8jaTb0+lFkm6VtEFJza2tGeV+KOme9LVLM2OT9H/SbeaV0/6BbUbKtu8FXpPuyzcre+2oJklfT9f1+yS9Lp1/iaS1kn4G/FQZNTHS48f1Smq3/CvQnLG8XNcan5W0MY3vb4v+Y1hF0bOvWz8l6VuSfgV8SznOu9P969fS85D7su2PJS2V9PP03OZhSb+Zzu/OKHOhpH8e9743A38GfFDSbRpX20jJudOn0unbJf2dpPXAR8YtZ4Gkn4zt0wFlvDZ2XJGyXLtI+ntJV6TTv51+jxrlcc4kqTZd5thx8f1T/b+Ui2tEVbmI+ICk84DXRcQ+SZdkvHwp8CtJW4CPA69IT3L+P5JMdI+k/w58TNLfAF8FXg9sAm7I8ZF7gHMjol/SauA7wBpJbwIuAF4eEb2SOjLfJKlpsuVHxK8lrQVuiogb0/d1STozIu4H/gj4+tR/JatyNwB/B1yVPv994LdJEjdnAmcAC4G7Jf183Huzrq/A5cAnIuItkDSLy/HZfw98MSJ+KWklcAvwgnw+w9uElYOkF5Ls438jPSYcs95FxI2SLiNZ/9en7wHojIiz0ufnpX8bSNbLd0TE3ZLmAn3jPrIf+N2IOJQeX+5M19nTJooj9XXgsoj4uaTPF+YXsBloNXBxRNwp6Y3p87NJTvLXSnoNsAh4JiJ+B0BSe+YCJC0FPg28FOgCbgPum+Rzs67bERE5yp83UQxZ/C/gZxHx1+k2996M1/44IvZLaiY5tn0/IjqBVmBdRHx8kmXbLJXjGPAFYCnwauD5JDWuxzev+xAQEfEiSc8HfiLp1PS1s4AXp+vkqoz3fBDojYgXSHoxcG8aQ65rjauA3wWeHxEhaV7BfwCraFmuWz9Fcr7w6ojok3Qd2c+7/yfJ/vKPl1MrHQAAIABJREFU0/XmLkn/GRE9GYt/F3BLRPyVpFqgJc+Ybk4TZN0R8bfj1vFsGiIiWxPU/wX8MiKulPQ7HLtPH5Pr2uWT6fQvgH8A3hwRo+n52Zhc50zvBboi4mWSGkmu/X8SEVsm++7l5hpRM1hE7AauIDnh+nhE7AdeQbLB/0rS/cDFwIkkB6YtEfFEepL1LzkWWw98VdJDwPfSZQG8Afh6RPSmn71/3PvyXf541wB/lO5Q3kHalMpmj4i4D1ispF+CM4ADEfE0yQnVdyJiJF3X/x/wsnFvz7W+5usNwJfSbWUtMFfpXb08PsPbhJXD64HvRcQ+yLre5ZLt5sPzgJ0RcXe6rENZqqEL+N+SHgT+E1gGLJksjvREcl5EjCWPv4VZdlsj4s50+o3p4z6Si97nkySmHgLOVVKz7zcjomvcMl4O3B4ReyNikNw32zLlWrdzmSyG8V4NXA8QEf8BHMh47cPpXe87gRXpdwQYAb6fR+w2e+Xa9/4wIkYjYiPZ1+NXk56HRMSjwFZgLBF1a45jyWsy3vMg8GA6P9e1RhdJgvdaSb8H9E7ni9qMsTYixm5y5TrvfiNweTr/dqAJWDluOXeTnB9/CnhRRBwuUry5jh+Z28O/c+w+fUzWa5f0WuFPgFuBL0XEk5lvmuSc6Y3Ae9LfZh2wgKPHjIrmGlEz34uATmCsY1qRHFDemVlI0pl5Lu+jwG6STG4NyQGlmL5PetcQuCe9I2izz/dI+iQ4gfwuIMZMd32tAV4RERO9z9uEzQQ9kxfJ6t0ktVFeGhFDkp4iOUE0K5TMdVPAX0fEP40vJOks4M3AX0r6aURcmefyh0lvzEqqARrS+VNatyPi8RwxHFn+RO/P+B6vJbkYe2Vam/b2jPf1h/uFsuMzkDGtnKWym+rxIeu1BoCks4FzSM7pLiNJnNnslrl+ZT3vVlI16G0R8ViuhaQ1hV4D/A7wz5K+EBHfBDJrseZzfpK5z872nuM9X5rM+Gv2fAn4rxFxS+FDKi7XiJrB0p39m4CXAJ+QdBLJ3bVXSTolLdOaVr19FFiloyMFPOvgkWonuUM+CvwhUJvOv5UkC92SLnd8M4x8l38YONIHSbojugX4Mm6CNJvdAFxEcuLyvXTeL4B3KGkbvYjkTsRd496Xa309Zj2bwE+A/zr2JEfC1tuEVZKfkfTztACyrneQ//r/GLBU0svSZc2RNP4GVjuwJ71Qfx3JXe9J44iIg8BBSa9OZ707j3jMbgH+WEf7m1kmabGSUSB7I+JfgM+TNCXKtA74LSV9eNQDb8947SmSJnuQ9MlXn07nWrezmiCGzOW/LeMtvyJpak7a5HB+xuceSJNQzyepXWKWr3yOAdn8gnQ/nF4XrCQ5Bkzk5yTNoZB0OvDidH7Wa410u22PiJtJbuKdkfe3stki13n3LcB/TRNSSHrJ+DdKOhHYHRFfJWk9MLYP3i3pBemNht/NI4bdJC0xFqRN3d6SZ+yZ28ObOLpPz5T12iWN/eMk1+xvkvTyzDdNcs50C0n/VvXpZ58qqTXPmMvKiagZKt1wvkrSz8AzJCv314B9JCMmfSetbn4HSVvtfpI+pf5dSYe1e3Is+v8CF6dVxp9PmhVOq5WvBdanVQM/kfmmKSz/euDPdezwmd8GRkl2TjYLRcQGkgvnHRGxM539ryTVwB8gOfH6bxGxa9xbs66v6ftGlHT699EJPvrDJP09PShpI/CBLGW8TVjFSLeVvwL+X7pOfiFLsX8GvqK0s/IJljVI0vzzH9Nl3cqz7wx+m2QbeQh4D0mCNd84/gi4Kt0+pnqH3mahiPgJSXPkO9J17kaSY8OLSE7m7yepMfqX4963E/gUyTnPr4BHMl7+KkmSaqwD8LHjRNZ1ewK5Yvg08PdKOrfNrM30aeCNSjrFfTuwiyRJ/B9AnaRHSAbWuBOzPOW5783m/wI16fp+A3BJRAxM8p4vA23punolcE8aw16yXGuQbKs3pfN+CXxsKt/NZoVc592fIblJ8KCkDenz8V4LPCDpPpJzl79P519OMvr0r4GdWd53jIgYIlmf7yI575ls3z/m0ySDAmwg6QtqW5Yyz7p2IUl8XUvSd+czJH0+XaOkL9lMuc6ZriEZWfve9HjyT1RJqzdFzj4XzSqDktEC2iPiL8odi1kl8DZhZlbd0huGIxExLOmVwJcjIt9uEszMzKpaVWTLbPZSMhzsybgNuRngbcLMbIZYCXw3bS4ySNJRrZmZ2azgGlFmZmZmZmZmZlYS7iPKzMzMzMzMzMxKwokoMzMzMzMzMzMrCSeizMzMzMzMzMysJJyIMjMzMzMzMzOzknAiyszMzMzMzMzMSsKJKDMzMzMzMzMzKwknoszMzMzMzMzMrCSciDIzMzMzMzMzs5JwIsrMzMzMzMzMzErCiSgzMzMzMzMzMysJJ6LMzMzMzMzMzKwknIgyMzMzMzMzM7OScCLKzMzMzMzMzMxKwokoMzMzMzMzMzMrCSeizMzMzMzMzMysJJyIMjMzMzMzMzOzknAiyszMzMzMzMzMSsKJKDMzMzMzMzMzK4m6cgdQTgsXLoxVq1aVOwybpe655559EbGoHJ/tdd/KqZzrPnj9t/Lxum+zldd9m828/ttsNdG6P6sTUatWrWL9+vXlDsNmKUlby/XZXvetnMq57oPXfysfr/s2W3ndt9nM67/NVhOt+26aZ2ZmZmZmZmZmJeFElJmZmZmZVQRJ50l6TNImSZfnKPP7kjZK2iDpulLHaGZm0zOrm+aZmZmZmVllkFQLXAWcC2wH7pa0NiI2ZpRZDXwSeFVEHJC0uDzRmpnZ8SpqjajJ7mhIapR0Q/r6Okmr0vnnSrpH0kPp39dnvOel6fxNkv5BktL5HZJulfRE+nd+Mb+bmZmZmZkV1NnApojYHBGDwPXABePK/AlwVUQcAIiIPSWO0czMpqloiaiMOxpvAk4D3inptHHF3gsciIhTgC8Cn0vn7wP+S0S8CLgY+FbGe75McgBanT7OS+dfDvw0IlYDP02fm5mZmZlZdVgGPJ3xfHs6L9OpwKmSfiXpTknnYWZmVaWYNaLyuaNxAfCNdPpG4BxJioj7IuKZdP4GoDmtPbUUmBsRd0ZEAN8E3pplWd/ImG9mZmZmZjNDHcnN6NcC7wS+Kmne+EKSLpW0XtL6vXv3ljhEMzObSDETUfnc0ThSJiKGgS5gwbgybwPujYiBtPz2HMtcEhE70+ldwJLpfgEzMzMzMyuZHcCKjOfL03mZtgNrI2IoIrYAj5Mkpo4REVdHxJqIWLNo0aKiBWxmZlNX0aPmSXohSXO990/lfWltqcixTN8dMTMzMzOrPHcDqyWdJKkBuAhYO67MD0lqQyFpIUlTvc2lDNLMzKanmImofO5oHCkjqQ5oBzrT58uBfwXeExFPZpRfnmOZu9Ome6R/s3Zc6LsjZmZmZmaVJ20hcRlwC/AI8N2I2CDpSknnp8VuATolbQRuA/48IjrLE7GZmR2PuiIu+8gdDZJk0UXAu8aVWUvSGfkdwIXAzyIi0nbe/w5cHhG/GiscETslHZL0CmAd8B7gH8ct67Pp338r2jfL4bp1245Mv+vlK0v98WZWQJnbM3ibtpnPxzArNa9zlk1E3AzcPG7eFRnTAXwsfcwY4887wNuFWaH5/L5yFK1GVJ53NK4FFkjaRHIwGRvp7jLgFOAKSfenj8Xpa38KXANsAp4EfpzO/yxwrqQngDekz83MzMzMzMzMrEIUs0ZUPnc0+oG3Z3nfXwJ/mWOZ64HTs8zvBM6ZZshmZmZmZmZmJSNpBcmI8EtI+jq+OiL+flyZ15K0+tmSzvpBRFxZyjjNCqWoiSgzMzMzMzMzm9Aw8PGIuFfSHOAeSbdGxMZx5X4REW8pQ3xmBVXRo+aZmZmZmZmZzWQRsTMi7k2nD5N0bbOsvFGZFY8TUWZmVpEkNUm6S9IDkjZI+nSWMpdI2pvRn+D7yhGrmZmZWSFIWgW8hGRwrvFemZ4X/VjSC0samFkBuWmemZlVqgHg9RHRLake+KWkH0fEnePK3RARl5UhPrOiklQLrAd2jG+KIamRpD+RlwKdwDsi4qmSB2lmZgUjqQ34PvBnEXFo3Mv3Aiem50VvBn4IrM6xnEuBSwFWrvTIcFZ5XCPKzMwqUiS606f16SPKGJJZqX2EpHlGNu8FDkTEKcAXgc+VLCozMyu49Kbb94FvR8QPxr8eEYfGzovSQcHqJS3MtqyIuDoi1kTEmkWLFhU1brPj4USUWYWRtELSbZI2ps2RPpLO/5SkHRlNkN6c8Z5PStok6TFJv12+6M0KS1KtpPuBPcCtEZGtmvrbJD0o6cZ01BmzqidpOfA7wDU5ilwAfCOdvhE4R5JKEZuZmRVWuv++FngkIr6Qo8wJY/t5SWeTXMt3li5Ks8Jx0zyzypN11Iz0tS9GxN9mFpZ0GnAR8ELgOcB/Sjo1IkZKGrVZEaTr8ZmS5gH/Kun0iHg4o8iPgO9ExICk95NcmL9+/HJcRd2q0N8B/w2Yk+P1ZcDTABExLKkLWADsK014ZmZWQK8C/hB4KL0BB/A/gJUAEfEV4ELgg5KGgT7goohwTXGrSk5EFcl167YdmX7Xy33RY/mLiJ3AznT6sKTJRs24ALg+IgaALZI2AWcDdxQ9WLMSiYiDkm4DzgMezpifeSfwGuBvcrz/auBqgDVr1vikzSqapLcAeyLiHkmvneaynIQ1M6twEfFLYMJarRHxJeBLpYnIrLjcNM+sgmUZNeOytAnS1yTNT+cduSue2o6He7UZQNKitCYUkpqBc4FHx5VZmvH0fHL3p2NWTV4FnC/pKeB64PWS/mVcmR3ACgBJdUA7WZpouJ8QMzMzqzRORJlVqCyjZnwZOBk4k6TG1P+Z4vIulbRe0vq9e/cWPF6zIlgK3CbpQeBukj6ibpJ0paTz0zIfTvtSewD4MHBJmWI1K5iI+GRELI+IVSRNr38WEX8wrtha4OJ0+sK0jGv7mZmZWcVz0zyzCpRt1IyI2J3x+leBm9KnR+6Kp5an847hpklWbSLiQZIagePnX5Ex/Ungk6WMy6xcJF0JrI+ItSSd2n4rbY69nyRhZWZmZlbxnIgyqzC5Rs2QtDTtPwrgdznaT85a4DpJXyDprHw1cFcJQzYzsyKJiNuB29PpzCRsP/D28kRlZmZmdvyciDKrPLlGzXinpDOBAJ4C3g8QERskfRfYSDLi3oc8Yl5xeTACMzMzMzPLlHmNAL5OmIgTUWYVZoJRM26e4D1/BfxV0YIyMzMzMzMzKwB3Vm5mZmZmZmZmZiXhRJSZmZmZmZmZmZWEE1FmZmZmZmZmZlYSTkSZmZmZmZmZmVlJuLNyM7M8PL77MJ+5aSONdbWcuWIeHa0N5Q7JzMzMzMys6jgRZWY2if6hEf7o63dzqG+IodFRHtl5iA/81snU1mQb3NDMzMzMzMxyKWrTPEnnSXpM0iZJl2d5vVHSDenr6yStSucvkHSbpG5JX8ooP0fS/RmPfZL+Ln3tEkl7M157XzG/m5nNHnds7mTHwT7++Y/P5m8uPIMdB/vY8ExXucMyMzMzMzOrOkWrESWpFrgKOBfYDtwtaW1EbMwo9l7gQEScIuki4HPAO4B+4C+A09MHABFxGDgz4zPuAX6QsbwbIuKyIn0lM5uFRkaDX2/ax/OWzOGxXYcZjWB+Sz13bdnPi5fPK3d4ZmZmZmZmVaWYNaLOBjZFxOaIGASuBy4YV+YC4Bvp9I3AOZIUET0R8UuShFRWkk4FFgO/KHzoZmaJJ/d20zM4wstWdQBQI/GSlfPZsq+H7oHhMkdnZmZmZmZWXYrZR9Qy4OmM59uBl+cqExHDkrqABcC+PJZ/EUkNqMiY9zZJrwEeBz4aEU9nf6uZWX4e2t5FU30Npy5pOzLv+SfM4WeP7mHTnsNct27bMeXf9fKVpQ7RzMzMzMysahS1j6giuwj4TsbzHwGrIuLFwK0crWl1DEmXSlovaf3evXtLEKaZVauI4PE9h1m9eA51tUd3l8+Z10xrQy1P7O4uY3RmZmZmZmbVp5iJqB3Aiozny9N5WctIqgPagc7JFizpDKAuIu4ZmxcRnRExkD69BnhptvdGxNURsSYi1ixatCjf72Jms9DuQwMc7h9m9eK2Y+bXSKxa2MrW/b1liszMzMzMzKw6FTMRdTewWtJJkhpIajCtHVdmLXBxOn0h8LNxTe1yeSfH1oZC0tKMp+cDjxxX1GZmqSf3JjWeThmXiAJYMb+F/T2D7ifKzMzMzMxsCorWR1Ta59NlwC1ALfC1iNgg6UpgfUSsBa4FviVpE7CfJFkFgKSngLlAg6S3Am/MGHHv94E3j/vID0s6HxhOl3VJsb6bmc0OTx/opb25nnktDc96bUVHCwDb9/fy/KVzSx2amZmZmZlZVSpmZ+VExM3AzePmXZEx3Q+8Pcd7V02w3OdmmfdJ4JPHG2sxRASSyh2GmR2n7Qf6WD6/Oetry+Y1I2BHV58TUWZmZmZmZnmq5s7KK9q6LZ186kcbuP2xPeUOxcyOQ/fAMPt7BlkxvyXr6w11NXS0NrCrq7/EkZmZmc1cks6T9JikTZIuz/L6JZL2Sro/fbyvHHGamdnxK2qNqNnsF0/sY2gkuGNzJ8Mjo8eMuGVmlW972hH5WBO8bE5ob2L3ISeizMzMCkFSLXAVcC6wHbhb0tqM7jnG3BARl5U8QDMzKwhnR4qgs3uA/T2DnLKojcP9w9yxedKBAM2swjx9oJcaJU3wcjlhbhOd3YMMDo+WMDIzM7MZ62xgU0RsjohB4HrggjLHZGZmBeZEVBFs2dcDwLmnLUHAfdsOljcgM5uypw/0sWRuEw11uXeTi+c2EcC+7oHSBWZmM56kJkl3SXpA0gZJn85Sxs2TbCZaBjyd8Xx7Om+8t0l6UNKNklaUJjQzMysUJ6KKYF/3ILU1Ytn8Zha2NfLgdieizKpJRPDMwb4Ja0MBLGxLRtNzIsrMCmwAeH1EnAGcCZwn6RVZyt0QEWemj2tKG6JZ2fwIWBURLwZuBb6RrZCkSyWtl7R+7969JQ3QzMwm5kRUEezvHWR+Sz01EsvnN/PA9q5yh2RmU7C3e4DewRFOaG+asNyC1kYAOnsGSxHWrJNnrZBGSTekndquk7Sq9JGaFVYkutOn9ekjyhiSWansADJrOC1P5x0REZ0RMXYH6BrgpdkWFBFXR8SaiFizaNGiogRrZmbHx4moIjjQM8j8lqSmxNJ5zew9nPQZZWbV4bFdhwFYMnfiRFRDXQ1zm+rodI2oYsmnVsh7gQMRcQrwReBzJY7RrCgk1Uq6H9gD3BoR67IUc/Mkm2nuBlZLOklSA3ARsDazgKSlGU/PBx4pYXxmZlYATkQVwf6eQTpak0TUorakxsSTe7sneouZVZB8E1EAC9oa2dftRHMx5Fkr5AKONsu4EThHkkoUolnRRMRIRJxJUiPkbEmnjyvi5kk240TEMHAZcAtJgum7EbFB0pWSzk+LfTitJfsA8GHgkvJEWzjbOnvZ2tlT7jDMzErGiagC6xscoW9o5Ggiak6aiNrjRJRZtXh012HaGutoa6ybtOyC1gbXiCqiPGqFHOnYNr2A6QIWlDZKs+KJiIPAbcB54+a7eZLNSBFxc0ScGhEnR8RfpfOuiIi16fQnI+KFEXFGRLwuIh4tb8TTMzQyyh9+bR3/9PPN/GrTvnKHY2ZWEk5EFVhX/xAA7c31AMxrqaehrobN+3yXw6xaPL77MCfkURsKYGFbIz2DI/QPjRQ5qtkpj1oheXGtEKsmkhZJmpdONwPnAo+OK+PmSWYzwM0P7WRrZy8Av35yHxHuDs7MZj4nogqsZ2AYgLampCZFjcRJC1pdI8qsSoyMBo/vPsySuY15lV/gkfNKIletEDI6tpVUB7QDnVne71ohVk2WArdJepCkz5xbI+KmmdI8aXhklP09g77gNgPu3NzJ3KY6LjxrOQd6h3h6f2+5QzIzK7rJ253YlHSPJaIajv60Jy5oYYtrRJlVhW37e+kfGs2rfyhI+ogC6OweZPn8lmKGNutIWgQMRcTBjFoh4zsjXwtcDNwBXAj8LHx1a1UuIh4EXpJl/hUZ058EPlnKuArlD65dx52b9/O2s5bx0hM7yh2OWVnds/UAZ504n+edMAeALZ29rFzQWuaozMyKyzWiCuxIjaiMvmWWz29h+4E+3/kzqwKP786/o3JI+ogC2NfjGlFFkE+tkGuBBZI2AR8DLi9TrGaWh2cO9nHn5v0ArN96oMzRmJXXof4hntjTzVkr59PaWMeC1ga2udNyM5sFXCOqwLoHhqkRNDXUHpm3oqOZvqEROnsGWdiWX3MfMyuPzXuTE8CxgQYmU19bw9ymOvZ75LyCy7NWSD/w9lLGZWbH79aNuwH4vbOW8YN7d3Cob4i5ab+aZrPNk3u6iYAXLJ3L3sMDrOxo4fHdh4kIPACsmc1krhFVYD0Dw7Q21FGTcfAYa66z/UBfucIyszxt3tvNojmNNNXXTl44Na+lgYN9Q0WMysxsZnhg+0GWzG3k3S8/EYAdB31uZLPXWCflJy1MrhWWz2+mZ3CEQ/3D5QzLzKzonIgqsO6BEVrHDfm+fH4zgDsfNKsCW/b1cNLCqfXNMK+lnoO9rhFlZjaZJ/d0c8ritiP94ezs6i9zRGbl81RnD9LRm9ZL2pNuAfYc8nZhZjObE1EF1jMwfEz/UHA0EeUaUWaVb/O+Hk5eNLVE1PyWBrr6hhh1P3BmZlldt24b375zK4/uOswpi9poa6yjo7WBXV0+N7LZa2tnL89pbz5SC3vJnCQRtduJqFlJ0gpJt0namI6K+pEsZSTpHyRtkvSgpLPKEavZdDkRVWDdA8O0NB7bpGdOUz3zWup5+oBrRJlVsoO9g+zvGTyuGlGjAYddld7MLKfD/cMMDI9y8uI2AJa2N7HLF9w2iz3V2cOJC46OuNvaWEdbYx27D3kAlFlqGPh4RJwGvAL4kKTTxpV5E7A6fVwKfLm0IZoVhhNRBdY3OEJLw7P7llmRjpxnZpVr876ko/LnLmyb0vvmNScj57l5nplZbnu7k4vrsX3sgtZGDvS4NqnNXjsO9B1pOTFm8dxG9hx2gnY2ioidEXFvOn0YeARYNq7YBcA3I3EnME/S0hKHajZtTkQVUETQPzSStZPj5fOb2e4aUWYVbUs6Yt5JU2yaN68lGfHpYK87LDczy6UrHdRhWXrh3dHawEgEhzzYg81CwyOj7Ose4IS5TcfMX9jaSGePb2zNdpJWkYwcvG7cS8uApzOeb+fZySqziudEVAENDI8SQHOWRNSKjqRG1Oio7/qZVarN+7qprRErO1omL5xhfotrRJmZTWYs4fT/HtvLdeu2Mb81SeLv977TZqHOnkFG42gH5WMWtDXQOzhC3+BImSKzcpPUBnwf+LOIOHScy7hU0npJ6/fu3VvYAM0KoKiJKEnnSXos7Uzt8iyvN0q6IX19XZr5RdKCtKO2bklfGvee29Nl3p8+Fk+0rFLqH0oOGNkSUcvnNzM4nNz5MJtIro4KJXVIulXSE+nf+el8d1pYIJv39rCyo4X62qntGhvqamhpqOWA7+qbmeV0qH+IpvoaGuqSfWxHmsQ/0ON9p80+u9IRI8c6KB+zoDXZLjp7fM0wG0mqJ0lCfTsifpClyA5gRcbz5em8Y0TE1RGxJiLWLFq0qDjBmk1D0RJRkmqBq0g6VDsNeGeWztbeCxyIiFOALwKfS+f3A38BfCLH4t8dEWemjz2TLKtk+tJEVLameSvSYVndYbnlIVdHhZcDP42I1cBP0+fgTgsLZsu+Hp47xY7Kx8xrqXeNKDOzCRzqG2ZuU/2R5+0t9Qg44H2nzUJjI+MtGdc0r6OtEcDN82YhSQKuBR6JiC/kKLYWeE96I/oVQFdE7CxZkGYFUswaUWcDmyJic0QMAteTdK6W6QLgG+n0jcA5khQRPRHxS5KEVL6yLuv4w5+6iRJRYx0RusNym8wEHRVmruPfAN6aTrvTwgIYHQ227OuZ8oh5Y+Y1N7iPKDOzCRzqH6K9+Wgiqq6mhrnNTuLb7LT7cFLjacncxmPmj9UU7HQritnoVcAfAq/PaP3zZkkfkPSBtMzNwGZgE/BV4E/LFKvZtNQVcdnZOlJ7ea4yETEsqQtYAOybZNlflzRCUm3xLyMiprGsgukfHAVyNc1La0Ttd40oy9+4jgqXZNzx2AUsSadzdVrouyNT8ExXHwPDozx30dRGzBszr6WeTXu6iQhKnAM3M6sKh/qGWDyuGdKcpjoO9w+XKSKz8tlzqJ/aGrGg7dhEVENdDe3N9XR2D3Ldum3HvPaul68sZYhWYmlFjAlPItPr3g+VJiKz4qnGzsrfHREvAn4zffzhVN5czI7bjvQR1fDsRFRzQy0L2xpcI8ryNlFHhelBaEo937vTwoltHhsx77ib5jUwODJ6ZFQoMzM7ajSCw/3DzG069h7onKZ6J6JsVtp7eICO1gZqa56dd+hobXDTPDOb0YqZiMqnI7UjZSTVAe1A50QLjYgd6d/DwHUkTQDzXlYxO2472jQv+8+6fH6L+4iyvOToqHD3WJO79O9Y/2jutLAAtuxLElEnLzq+RNRYc5Ndh6bSotisMu053M8tG3axtbOn3KHYDNE3OEIAbc9KRNVxqN8JfJt99vcMHumYfLwFTkSZ2QxXzETU3cBqSSdJagAuIulcLdNa4OJ0+kLgZ2lNj6wk1UlamE7XA28BHj6eZRXDRH1EQdJP1NP7XSPKJjZBR4WZ6/jFwL9lzHenhdP0VGcPLQ21LJrTOHnhLNrTi6udXU5EWXV7fPdhXvf523n/t+7htX97O//39k3lDslmgJ6BpNZTa8OzE1G9gyMMDo+WIyyzsjnYO8S8lvqsry1oa6RnYPhIawszs5mmaH1Epf00XQbcAtQCX4uIDZKuBNZHxFqSi+1vSdoE7CcuLfvWAAAgAElEQVRJVgEg6SlgLtAg6a3AG4GtwC1pEqoW+E+STtqYaFml0j80QmNdDTXj+ocZa999uH+YZw72MTIaWavhmqXGOip8SNL96bz/AXwW+K6k95JsC7+fvnYz8GaSTgt7gT8qbbjVbWz7vOPJTlZ2tBx3/05zx2pEORFlVexQ/xA33P00Jy5o4Yr/chrXrdvG3/zHYzx3YSvnne4xEOz49QwmF9QtjcferBsbRW9v9wDL5jWXPC6zctnfO8jqxdn7pRyrKdXZM+jtwsxmpGJ2Vk5E3ExykZw574qM6X7g7TneuyrHYl+ao3zOZZVK/9BoztpQkIyCMTwa7OzqO9J5udl4k3RUeE6W8u60sAD29wxyxop5x/3+OU3JMORORFk1++UT+xgYHuGqd5/FyYvaWHNiB5v39vC/b36UN7xgCXW11di1pFWCiWpEQdJxsy+4bTY52DvI/BxN8zrS+fudiDKzGcpnlAU0MDxCQ13un3TsYOPmeWaVJSI40DvIyo7jTxDX1oi2pjonoqxqDQ6PcvdT+zl9WTsnp6NHNtTV8NFzT2Xb/l7+Y8OuMkdo1axnME1ENT67s3KA3Yc8VL3NHsl5xxDzczTNO5KI6vZ2YWYzkxNRBTQ4PErTBImojiOJKHdYblZJugeGGRqJaSWiIOmwfKc7K7cq9fjuwwwMj/KyVR3HzD/n+YtZPKeRmx5w13OlIqlJ0l2SHpC0QdKns5RplHSDpE2S1klaVfpI89czkDTNax03svBYjai9h73vtNnjUP8wI6PB/JbsNaKa6mtpaahlf687LDezmcmJqAIaGB6lsS5307z25npqhEfOM6sw+9ORaaabiJrbVM+uLtd4tOr08DNdtDbUsmrBsSNH1tSIN51+Arc9tofetFaLFd0A8PqIOAM4EzgvHYwi03uBAxFxCvBF4HMljnFKegeHaayreVbzzrbGOgTsOeyaHzZ7HEwTTLkSUeCR88xsZnMiqoAma5pXWyOeM6+Zba4RZVZRxhJRKwpRI8pN86wKjY4Gm/Z087wT5mQdTON1z1/MwPAo9207WIboZp9IdKdP69PH+JGALwC+kU7fCJyj4x1toQR6Boaf1SwPoEairbGOPW6aZ7PI2HnH/NbsTfMgaUlxwIkoM5uhitpZ+WwzMDRK4wSJKEhqXLhpnlllGav6/qtN+7hry/7jXs7c5noO9w/nvOAyq1Qbdx6id3DkSN9Q4730xPnUCK795Ra2dh49hr3r5StLFeKsI6kWuAc4BbgqItaNK7IMeBqOjFTcBSwA9pU00Dz1Do48q1nemDlNdexx0zybRQ72DgET14jqaG3gwe1dDI+OUlfjugNmNrN4r1ZAA8OjNNZP/JOumN/CNndWblZRDvQMMbepjvppjgjW3pwkn3a5nyirMndu7gTguVkSUdet28aPHtjJCe1NPLWvp9ShzVoRMRIRZwLLgbMlnX48y5F0qaT1ktbv3bu3sEFOQe/gCM05E1H17qzcZpUDeTTN62htJDiatDIzm0mciCqQiGBgeGTCPqIAVi5oYV/3AH2DIyWKzMwms79n8MhgAtMxtzmpYu+R86zaPLSji7lNdbQ3524msnx+C8909RExvoWYFVNEHARuA84b99IOYAWApDqgHejM8v6rI2JNRKxZtGhRscPNqW9ohOb6iWpEORFls8eRpnmT1IjKLGtmNpM4EVUgA8OjjAaTNs1bPr8ZcIflZpXkQG9hElHt6TDk7idq+iStkHSbpI3pqGEfyVLmtZK6JN2fPq4oR6wzwUM7ulg2r3nCMkvbm+gfGuVgn+/OF5ukRZLmpdPNwLnAo+OKrQUuTqcvBH4WFZwl7Bscobkhe5PlOU31dPYMMDwyWuKozMrjYO8QtTU6MmpkNk5EmdlM5kRUgfQMJCMJ5dNHFOB+oswqxNDIKIf6hphf0BpRbn5bAMPAxyPiNOAVwIcknZal3C8i4sz0cWVpQ5wZugeG2bKvh+fMnyQRNbcJcI2/ElkK3CbpQeBu4NaIuEnSlZLOT8tcCyyQtAn4GHB5mWKd1Oho0D9JjagIX3Db7LG/d5B5zfXUZBkcYsycpjrqauTtwsxmJPemWyA9A0lTu8ma5p2YDou9xf1smFWEg71DBNAxQfX4fNXX1jC/pd59RBVAROwEdqbThyU9QtI588ayBjYDbXzmEBFMWiNqSXsTAnZ29fGCpXNLE9wsFREPAi/JMv+KjOl+4O2ljOt4HR4YJiBnH1Fjgzt09gyyOE14ms1kB3sHmdeSuyk0JCNKdrQ2OBFlZjOSa0QVSHdaI6phkhpRP35oJ831tdy6cXcpwjKzSYyd4BWiaR7ACe3NrjFSYJJWkVyUjx81DOCVkh6Q9GNJLyxpYDPEQzu6gMkTUY11tbQ317Ov2xdFNjVdaWfLLTlqRLWNJaK8bhkg6TxJj0naJClnTT9Jb5MUktaUMr5CONAzlNd5hxNRZjZTORFVID2DadO8SUbNk8TCtgb2dbtTTrNKsL+3sImope1N7iOqgCS1Ad8H/iwiDo17+V7gxIg4A/hH4IcTLKciRg6rRA/v6GLxnEbmNE18dx5gYVujj182ZV1pv2K5a0Ql8zt7vG7NdpJqgauANwGnAe/M1ixb0hzgI2S/QVHxDvQOMi+PmthjiagK7v7NzOy4OBFVIN1H+oiauGkejJ3I++6GWSU40DNIfa2O3JGfrhPam1wjqkAk1ZMkob4dET8Y/3pEHIqI7nT6ZqBe0sJsy6qUkcMq0UM7unjRsva8yi5Ib6T4osimYiwR1TRJjSifGxlwNrApIjZHxCBwPXBBlnKfAT4HVOUB90DvYF5dAnS0NjA4MnrkOsPMbKZwIqpAetM+oiZrmgewoK2Rrr4h+gZHih2WmU1if88g81sakHJ3GDoVJ8xtorNnkP4hb9/ToeQfci3wSER8IUeZE9JySDqb5Jj2rOHrLbeegWGe3NvN6XknohrpHxql18cvm4KDfUmCqSVHjaim+lrqakSna9tZ0hfg0xnPt6fzjpB0FrAiIv69lIEVSkRwoHeIea2T10L1yHlmNlO5s/IC6UsvOhtqJ09ELWxLDipb9vVw2nPc4atZOXX1DU3aYehUbO1MBiK45hdbuOz1pxRsubPQq4A/BB6SdH86738AKwEi4iskQ9Z/UNIw0AdcVMnD11eijTuTjspftKydPYePJgGuW7cta/mx49e+7oEjHUybTeS6ddtYtyXJD+caNa9GYr77wrE8SKoBvgBckkfZS4FLAVauXFncwKagb2iEweFR5jXnVyMKkkTU2IBHZmYzgc8iC6RvML/OyiFpmgdORJlVgoO9gyybl19tkHzMbU6SWmNNUez4RMQvgQmrqUXEl4AvlSaimenhtKPyFy1v56eP7Jm0/Py0KcnB3iFOXFDU0GwG6U9r0OVqmgewoLXBTfMMYAewIuP58nTemDnA6cDtaYXYE4C1ks6PiPWZC4qIq4GrAdasWVMxNykO9yfXDHObJ78Mm9/SgHCNKDObedw0r0DGakTV107evOdoIqq7qDGZ2cT6h0boGRwpaI2o9rTD50NORFkVeGhHF4vmNLJkblNe5dudaLXj0D88So0mPkda2NbozsoN4G5gtaSTJDUAFwFrx16MiK6IWBgRqyJiFXAn8KwkVCU73J/sP/MZIKK+toa5zfVORJnZjONEVIH0DY4CyQFjMg11NbQ317N5X0+xwzKzCTxzsA84enFdCK4RZdXk4R1dnD6FmrlN9bU01ddw0Ou3TUH/0AhN9bUT9sW3oK2BTteImvUiYhi4DLgFeAT4bkRskHSlpP+fvTsPk/OuDnz/PdW19VJdvaq7JVmWbclyZBtsI4zNEkgwxDAEkwkMJgM492FwfMFzk2HunWtmMsTDExhncgMTApjYgTEGzBIgQQEzhjgkYBvLFvImyZs2S2qpW71XdXdVdy3n/vG+1Wq3eqmuqrfW83kePe6ueuutX8O7nvec83tnZUdXGjE3IyqSZ3lzl5WtGmPqkJXmlchsKo3fJ/jybHjc3RbkqAWijKmoU5POZDvREmZEhQNNhPw+ppJ2o26q2+x8mkNnprn+0v51fa6jOcjUrN0UmfzNpbOrluUBdLeGrFm5ARZmQb1/yWufWGHZN5VjTKWUK82LhPMPRL0wFPdySMYYU3aWEVUiyflMXtlQOT1tIQtEGVNhuYyofBqGrkd7c8BK80zVe/Z0jKyS94x5OdHmgGX8mXVJpjKE1uih2d0WZGY+YzMKm7o3vRCIyu8hWFdrkPhcmvl01sthGWNMWVkgqkQSqUxejcpzetpCTM6mmLBUW2MqZnAygZBfw9D1iFogytSA/YMxwGlUvh7RloCV5pl1SabWzojKzchofaJMvTvbIyr/jCiAcctENcbUEU8DUSJyvYg8LyKHROS2Zd4Pici33ff3iMhW9/VuEfmZiEyLyOcXLd8iIj8SkedE5ICI3LHovd8XkRERedL99++8/NuWml13RpRzUjliDcuNqZhTkwkiYT9+X2kPhdGwZYyY6vfM4BQ9bUH682xUntPRHGB2PmNP503e5tIZwmtlRLU6E7lYLxxT73KleW15BqK6c4Eo66FmjKkjnvWIEpEm4AvAW4CTwOMisltVDy5a7EPAhKpuE5EbgT8D3gskgf+KMz3rZUtW/f+p6s/cmTQeFJG3qeqP3fe+raq3evU3rSaZyhDMY8a8nNzMeUdGZnjV+V1eDcsYs4pTU4mSNirPaW/2E0+mSWey+NcRoDamnPYPTnHZpuiqDaSXYzPnmfVKpjKE1gh4ducyouxm29S5eDKFALufPJVXb9muFjcQZdmCxpg64uUd0tXAIVU9oqrzwLeAG5YscwPwVffn7wJvFhFR1RlVfQgnILVAVWdV9Wfuz/PAPmCzh39D3hKp9WVEdbYE8fvEZs4zpoJOTSbpaCltfyhwekQpMGKNd02VSqYyvHhmmss2rq8sD84297dAlMmXU5qXX0bUqB03TZ2Lz6UJ+n15T3DUHHRmK7XSPGNMPfEyELUJOLHo95Pua8su407XOgV057NyEekAfht4cNHLvysiT4vId0XkvEIHXojZ+fX1iGryCVu6WjhmgShjKkJVGZxM0OFBRlQuY+T0VHKNJY2pjP2DU2SyyivW2R8Kzjb3n7SbIpMHVXVL89aYNW+hR5RtV6a+xZPpNXumLSYidLUGrWzVGFNXarJmRET8wDeBz6nqEfflfwC2quorgJ9yNtNq6WdvFpG9IrJ3ZGSkZGNKrLNHFEDQ7+OJ45Pct+d4ycZhjMnP2Mw88+nsQnZHKeUCUUMWiDJV6onjkwBcuaVz3Z9tD/sRLCPK5CeVUbIKoTVuvFvcrI8xy4gydS6eTK2ZIbhUV2vIylaNMXXFy0DUILA4K2mz+9qyy7jBpSgwlse67wJeVNX/mXtBVcdUNXf18jfAq5b7oKrepaq7VHVXb29vXn9IPpKpDIF19IgCp0/U2MwcWdWSjcMYk5/Tk06QKJfdUUrRsGVEmer2xIkJNnc20xsJrfuz/iYfbSG/BaJMXpLpDMCaN94iQrfdbJsGEE+mCa2RIbhUb1uQidl5myTCGFM3vAxEPQ5sF5EL3MbiNwK7lyyzG7jJ/fndwD+prh6VEZE/xQlY/dGS1wcW/fpO4Nkixr5u6y3NAycNPZVRm+bdmAoYnEwA0OFBRlRzsAm/TxiaSpR83caUwpPHJwvKhsqJttjMkCY/yZQbiMrjxrunLciolR+ZOjc9l153RlRvJERW4fi4tfQwxtQHz2bNU9W0iNwKPAA0AV9R1QMi8klgr6ruBr4MfE1EDgHjOMEqAETkGNAOBEXkXcBbgRjwX4DngH3uTD+fV9W/Af4vEXknkHbX9fte/W3LWW+zcjg7c96oPf0zpuxOuYEoL2bNExGizQHLiDJVaWgqyampJP/uvI6CS8MjIT8TsxaIMmubSzkZHKE8bry720IMx+y4aepbPJkmEl7fLVjunuHQmRm2bYh4MSxjjCkrzwJRAKp6P3D/ktc+sejnJPCeFT67dYXVLlv/pqofBz5e0EBLIFlUIMr6IRhTbqcmE4QDPlqC60uPz1d7c8B6RJmq9OSJCQCu3NLBs6fjBa0jEg5wfHy2lMMyi7gTrtwL9AEK3KWqf7lkmTcBPwCOui99X1U/Wc5x5mOhNC+PjKju1iDPno55PSRjKiqeTNHbtr6y6Nzyh0emvRiSMcaUnaeBqEaRymRJZXTdpXmRsJ9Ak1hjTmMq4NRUgo0dzUie0yevl2VEmWr1q5cmCDb52LmxveBAVFvYz+x8hlQmu+6HMCYvaeA/quo+EYkAvxKRn6rqwSXL/UJV31GB8eVtvRlRY9PzqKpnx2ZjKi2WTOe1PywWCjTRHvZbIMoYUzfs6rEEEm7/g/VejPtE6GkLWWmeMRUwOJlkU0ezZ+uPNgcYjiXJZm0yAlNd9hwd54otHetulrtYJOxHwaYT94iqnlbVfe7PcZy+l5sqO6rCLPSIymO6+u7WIPOZLPG5tNfDMqYi5tIZ5tPZvPaHpXojIQ6PWI8oY0x9sEBUCSTnc4Go9T+9624NWmmeMRVwejJBf3vYs/W3NwdIZ5XRGdu/TfWIJVPsH5zimgu7i1pPJOT0VjsTs+3bayKyFbgS2LPM29eKyFMi8mMRubSsA8tT0p3lK6/SvDZnFlObOc/Uq+mkE2QNr7OKApxA1JGRadaY18nUMBH5ioicEZH9K7z/JhGZEpEn3X+fWG45Y2pBXkdBEfm+iPwrEbHA1TJm3UBUsIDyhJ62EBOz86QyNh1rPfrX//pf86Mf/Yhs1v7/rSbpTJbR6Tn6o94FoqJh50Z9eMpu1MH2hWrx2JFxsgrXFhuIchvtjkxb+elaitn2RaQN+B7wR6q6tHnSPuB8VX0l8FfA36+wjptFZK+I7B0ZGVn3GIo152ZE5VuaB1jLgjphx/1zxXOBqAIyonraQsSTaUZs/6gJBW7/9wDXr7HML1T1Cvdf1fUFNCZf+UZOvgj8HvCiiNwhIjs8HFPNKbQ0D5yTSlbhhDV9rUsf+chHuO+++9i+fTu33XYbzz//fKWHZICxmXmyCn0eZkTlZuM7PZXw7DtqyeJ9Adhk55HKePTIGEG/jyu3dBS1nrZcICpuN0RrKXTbF5EAThDqG6r6/aXvq2pMVafdn+8HAiLSs8xyd6nqLlXd1dvbW9TfUohkKkPQ78OXR8+n7lYnI8paFtQHO+6fKxeIKqQ0ujeSmznP+kTVgkK2f1X9Oc7s78bUvbwiJ6r6j6r6b4GrgGPAP4rIIyLyf7gXSg0tF4hab7NyOJuGfmzMar7r0XXXXcc3vvEN9u3bx9atW7nuuut47Wtfy//6X/8LVpgBEpZPzRWR20VkcFE67tsXvfdxETkkIs+LyG95+1fVvtxsdl4GotqbnRv1IZuKHHj5vgDMY+eRivjlkTGu2tJR0NP4xdpCFojKVyHbvjidur8MPKuqn1lhmX53OUTkapxrujFP/ogiJNPZvMuQcrMJj1lJc12w4/654skUAOF1NisH2BBxrlleHLZAVC3wcPuv+pJsY/KR91FQRLqB3wf+HfAE8Jc4gamfejKyGpKYLy4jCuCINR+sW2NjY9xzzz38zd/8DVdeeSV/+Id/mDspXbzKx+5h+dTczy5Kx70fQER2AjcCl7qf+aKIFHeXWeeG3eCQlz2iWkPOrJg2c95ZuX0B6MHOI2U3NZvi4OkY1154TtLMugWafDQHmjhjgai8FLDtvw74APCbix8+iMgtInKLu8y7gf0i8hTwOeBGrcLmMXOpDKE8A59dbkbUuGVE1Q077r9crhF/IQ8D2sN+OloCPDe0tErXVCsPtv+8SrKh8mXZxqwl3x5Rfwf8AmgBfltV36mq31bVfw+0eTnAWpAookdUS7CJ5kATR0ctEFWPfud3foc3vOENzM7O8g//8A/s3r2b9773vfzVX/0VrLL/rTM19wbgW6o6p6pHgUPA1UUPvo7lAlF97SHPvsMnQl97eCH7qtEt3heAQ3YeKb89R8dQhWsu7CrJ+trCfsuIykMh276qPqSqoqqvWPzwQVW/pKpfcpf5vKpeqqqvVNVrVPWRMv5ZeVtPRlTQ76M97GfMZmOsC3bcP1cxPaJEhB19EZ4bipd6WMYDXmz/+ZZku+9XtCzbmLXkGzm5W1V3qup/V9XTACISAlDVXZ6Nrkac7RG1/lnzRITutqAFourUhz/8YQ4ePMjHP/5xBgYGAJibW7hxe7aAVd4qIk+7pXud7mubgBOLljlJjU7zXS7DsTmafLLQGNcrA9Gw9YhyLd4XgBTYeaTcHj0yTsjv44oi+0PlREIWiMpHo2/7yVQmr5vu+/Yc5749xwn6fTabcJ1o9G1/ObnSvFAB7TwALumP8MJQnGy26pIfzRJebP+1UpJtTD7yPQr+6TKv/bKUA6llCxlRBZ5UulqDnJiwZuX16I//+I/Pee3aa68tdHV3AhcBVwCngb9Yz4ctRfes4ViS3rYQTb71B4/Xoz/abBlRruX2Bew8UlZ7jo5x1ZbOgprkLicS9tvsTXlo9G1/LpXNuzQPoDXoZ8xK8+pCo2/7yykmIwpgR387M/MZBiftIVe1K2T7F5FvusvsEJGTIvKhWizJNiYf/tXeFJF+nMyKZhG5krPNldtxyvQMxc2aB9DVEuTAqRjpTBZ/gesw1WVoaIjBwUESiQRPPPEEuXNELBbLpeium6oO534WkbuBH7q/DgLnLVp0s/va0s/fBdwFsGvXroY+aQ3Fkp6W5eX0t4f4yVQSVUXymDGqHq2wL7SIyJtY4zwiIucB9wJ9gAJ3qepfLllGcHouvB2YBX5fVfeV/i+pbfFkimdPx/j3v7m9ZOuMhAO8MDzd0Nv3aorZ9utJMp3JuzQPnP561qy8ttm2v7J4MkU44Cv4QdglAxEAnhuKc15XQ/9PWbWK2f5V9X1rvP954PMlG6wxFbRqIAr4LZwG5ZuBxbO2xIH/7NGYak6xgajO1iCZrHJ6KmknlTrxwAMPcM8993Dy5Ek+9rGPLbweiUT49Kc/XdA6RWQgVxoL/A6Qm1FvN3CfiHwG2AhsBx4rePAN4ExsjvO7vd/X+qPNzKWzTM6m6HSb8DaaFfaFzcDHWPs8kgb+o6ruE5EI8CsR+amqHly0zNtwtvntwGtwMgdfU9I/og786qUJsgqv3lqa/lDgzJyXSGWYmc8szKJnzipy268bc6nsurI/2kJ+Do/YrGC1zLb9lU3PpYmEC58w7eI+NxB1OsZbdvaValimhGz7NyY/q145qupXga+KyO+q6vfKNKaaM+uW5vkL6BEF0Nni3KCeGJ+1QFSduOmmm7jpppv43ve+x+/+7u+u+/Nuau6bgB4ROQn8CfAmEbkCJzPkGPAHAKp6QES+AxzEuXH/qKpmSvKH1KmhWJKrLyjdDflKBqLOrHynp5ING4habl8QkRdU9Z1rfdYNvJ52f46LyLM4WbqLA1E3APe6qemPikjHkqCtAfYem6DJJ1xZov5Q4JTmAYzE5ywQtYxitv16kc5kmc9kCa1jqvrWkJ/x2XkyWfW8fNp4w7b9lcWSaSJFHC/bQn7O62rmueGzDcvv23P8nOV+7zVbCv4OUxzb/o3Jz1qlee9X1a8DW0XkY0vfV9XPLPOxhpNMZWgONOErsDQhN12x9YmqH1//+td5//vfz7Fjx/jMZ87dTRZnSS1nhdTcL6+y/KeAT613nI0omcowlUjR7waJvJT7jqFYgp0b2z3/vmq0wr7Qlzun5HseEZGtwJXAniVvrdSs3wJRizx2bJxLN7bTWsKAUe6p/plYkgt6Wku23npRqm2/lk3npqpfR1+ytlATqjA5O+/5hBLGG7btryyeTC8E8Qu1c6Cdg6diJRqRKTXb/o3Jz1pHwtyVZUNOsZqvxHyG5mDhzV+jzQF8AifGrfFgvZiZcWZBnJ628oJqMxxzmodviHh/g7M4I6pRrbAv+IBIvusQkTbge8AfqWpBV98icjNwM8CWLY31pHguneGpE5O8/5rzS7retlxGlDUsX1Yptv1ad7Yx8/oyogDGZiwQVats219ZPJkqqjQP4BWbO3jgwDBTsymiLcWty5Sebf/G5Get0ry/dv/738oznNpz357jHDgVK2oa1Saf0N4c4BcvjrCxo9nSaevAH/zBHwDwJ3/yJxUeiVlqOObcNJcjI6q3LYRPaOiZ85bbF26//fbT+Z5XRCSAE4T6hqp+f5lFrFn/Gp49HWcunWXX+Z0lXW+uvGQkboGo5RS77deDXCBqPTM15so8R6fnFvrhmNpi2/7KppNp+tuLu/545WanxPrpwUnesL23FMMyJWTbvzH5yesRlYj8DxFpF5GAiDwoIiMi8n6vB1cr5jNZAuuYEWY5nS1BJmZTJRqRqRb/6T/9J2KxGKlUije/+c309vby9a9/vdLDami5jKi+Ii8E8+Fv8rEhEm7ojKicxfsCcHE+5xF3RrwvA8+uksq+G/igOK4Bpqw/1MvtH5wC4MjIDPftOf6yf8VoDjYRaBILRK2hkG2/XsSTznXNepqVL2RETc97MiZTPo287a8knkwX3VPv8s1RAJ4+OVWKIRmP2PZvzOryjZ681S2HeAdOk+RtwP/j1aBqTSqdJVjgjHk5XS1BJmbsoqve/OQnP6G9vZ0f/vCHbN26lUOHDvHnf/7nlR5WQ/vxM06M4hcvjBZ9I56P/mi4oTOichbvC8Ac+Z1HXgd8APhNEXnS/fd2EblFRG5xl7kfOAIcAu4GPuLNX1C7DpyK0RxooqPEJRw+EXraQpyxQNSqCtz268JCj6h1lOa1LQSibLuqdY287a+kFKV50eYAF/S08tSJyRKNynjBtn9jVpdvSD633L8C/lZVp6TAxtz1KJXJEihwxrycztYg8bk0qUy2RKMy1SCddi7Cf/SjH/Ge97yHaDRa4RGZWD6zxQwAACAASURBVDJNoEnWdWNUjIFomBcWzW7TqBbvC8BEPucRVX0IWHUhd7a8j5ZmlPXp4KkpBqJhvDhv90ZClhG1hkK2/Xqx0CNqHaV5zcEmfOL0iDK1rZG3/eVkssrMfKboZuUAr9gc5dEjY+v6jM2uV162/RuzunyPhD8UkeeABPB/ikgvYI/4XfOZLC1FNCsH6Gp1no5YVlR9ecc73sEll1xCc3Mzd955JyMjI4TD3peEmZXFkinawwFPbsqX0x8N8y8vjKCqZfvOarR4XwDidh4pj1Qmy7NDca7e2uXJ+nvbQlZ6uoZG3vZzpXmhdQT+fSJ0tQYZtdK8mtfI2/5ychmCpQlEdfCDJ08ttBsw1ce2f2NWl9eVgareBrwW2KWqKWAGuMHLgdUSJyOq+B5RABOzduFVT+644w4eeeQR9u7dSyAQoLW1lR/84AeVHlZDiyXSRafFr8dANMzsfIa4ewHaqBbvC4Bi55GyODwyzXw6y8YObwLgG9qtNG8tjbztxxdK89b3sK67NWSleXWg0G1fRK4XkedF5JCI3LbM+7eIyDNuufZDIrKz9KMvvVxgthSBqKu2OA3LHz82XvS6jDca+dhvTD7WcyS8BNgqIos/c+9qHxCR64G/BJqAv1HVO5a8H3LX8SpgDHivqh4TkW7gu8CrgXtU9dZFn3kVcA/QjNMb5A9VVUWkC/g2sBWnj9W/UdWJdfx9BUtltPhAVKsTiBq3huV157nnnuPYsWMLKboAH/zgBys4osYWS6bY3Nlctu/rjzrfNTSVpL2MAbBqlNsXgG7g3e7Lq55HTHEODMYAGIh6s833toUYm5kjncniL/I8WM8adduPJ9M0ieD3rS8bdEN7iGELcNaF9W77ItIEfAF4C3ASeFxEdqvqwUWL3aeqX3KXfyfwGeD60o++tHKlqpFwgMkir/cv3xSlLeTnl4fHuHSjtX2oVo167DcmH3kFokTka8BFwJNAxn1ZKf5E8iGcmtltInIj8GfAe3HSFv8rcJn7b7E7gQ8De3ACUdcDPwZuAx5U1Tvcpye3Af9vPn9fseZL0Kw8EvLj94mV5tWZD3zgAxw+fJgrrriCpibnibCIWCCqQlSVeDJFe7i9bN85EHUyUU5PJRt6KvLF+wLQgvOgYdXziCnegVMxwgEfvZGQJ+vf0B5G1ennU46ZKGtRI2/708k0oYBv3WXJfe1hXhwe9WhUplwK3PavBg6p6hEAEfkWThbJwv2DO4FSTqu7zqp3NhDlLzoQ5W/ycfUFXRaIqmKNfOw3Jh/5ZkTtAna6TWHzteaJxP39dvfn7wKfFxFR1RngIRHZtniFIjIAtKvqo+7v9wLvwglE3QC8yV30q8A/U6ZAVCmalYsInS1Bxi0QVVf27t3LwYMHG7o3UDWJJdOkMkp7CdLi89Xv3pwPTSXK9p3VaPG+cOedd55Q1X9f6TE1ghfPxNm+IYLPo2PQBjfAdSY2Z4GoFTTyth9PptZdlgfOcXNkeo5MVmlaZzaVqR4FbvubgBOLfj8JvGbpQiLyUeBjQBD4zZIM2GNnS/MCOG13i/Pai7r5p+fOMJVIEW1u7IzratTIx35j8pFvGs9+oH+d617uRLJppWVUNQ1M4aQurrbOkyuss09VT7s/DwF96xxvQVTVCUT5iy9J6GwNWI+oOnPZZZcxNDRU6WEYV66pZ3sZL9j6FgJRjV1mYvtCZRwZmeHC3lbP1r/B3b7PxK3/6koaeduPJ9OEC7g+6o+GyWSVUesTVdO83PZV9QuqehHOQ+c/Xm4ZEblZRPaKyN6RkRFPxrEepWxWDnDNhc4t05GR6ZKsz5RWIx/7jclHvkfCHuCgiDwGLFwVqOo7PRlVkdyeUctmb4nIzcDNAFu2FD9laSarZJWiS/PAaVh+fHy26PWY6jE6OsrOnTu5+uqrCYXOlsbs3r27gqNqXLlAVDmblQf9PnraQgzFGjsjavG+AGwTkd1QveeRevDVR45xajLB9Jx3JaELGVHWz2dFjbztx5NpQgVmRIGzDW/ubAFsmvlaVOC2Pwict+j3ze5rK/kWTtuOc6jqXcBdALt27ap4+V4sV5oXKk0gaudAO9HmAIdHprlyS2dJ1mlKp5GP/cbkI98j4e0FrDufE0lumZNuE/QoTtPy1da5eYV1DovIgKqedkv4ziy3glKflFIZZxXFNisH6GoNkkxlmZpNEW2xFNt6cPvtt1d6CGaRIXea+XKnsA9Eww0/xf3ifeEnP/nJEPAXFRtMgxidnkNxGop7pcdd93AsyX17ji+8bkGDsxp524/PFZ4RBTCVSLHZ7q9rVoHb/uPAdhG5AOca/0bg9xYvICLbVfVF99d/BbxIDXh5aV7xfD7hN3b08sCBYStjrUKNfOw3Jh95XR2o6r/gzEQXcH9+HNi3xscWTiQiEsQ5kSxNA9kN3OT+/G7gn1brQ+WW3sVE5Bpxmu58EPjBMuu6adHrnprPZIHSZUQBnJiwrKh68cY3vpGtW7eSSqV44xvfyKtf/WquuuqqSg+rYeWyNkqVFp+v/mh4IQjWqBbvC8A0+Z1HTBFGp51S7x4PA1FBv4+u1qBlRK2ikG1fRM4TkZ+JyEEROSAif7jMMiIin3OnuH9aRKru5FJoj6hcSXMsYTMJ17JCtn23VcetwAPAs8B3VPWAiHzSnSEP4FZ3v3gSp0/UTSusrqrEk2n8PiEcKN0Mo2+7fIBEKsORUSvPqzZ23WPM6vKdNe/DOOVsXTiz520CvgS8eaXPqGpaRHInkibgK7kTCbBXVXcDXwa+JiKHgHGcYFXuO48B7UBQRN4FvNWdce8jwD1AM06T8h+7H7kD+I6IfAh4Cfg3+fxtxUqlnUBUKXpEdbW6gajxWS7bZDNg1IO7776bu+66i/HxcQ4fPszg4CC33HILDz74YKWH1pCGY0maA00lyWBcj/72MI8dHS/rd1abxfuCa83ziCnOiBsc8jIQBU553pnYHGz09GtqVoHbfhr4j6q6T0QiwK9E5KdLZh5+G7Dd/fcanPKkc5o6V1I8mV4orVuP7tYggSZZKGUytanQ476q3o8zM/bi1z6x6OdzArPVLJctuu+lCYJ+H9987MQan8jfGy/uJej3sX9wiu0bGndm3mpk1z3GrC7fu7GPAq8DYgBuOuyGtT6kqver6sWqepGqfsp97RNuEApVTarqe1R1m6penZthz31vq6p2qWqbqm7OXXyp6l5Vvcxd5625DCpVHVPVN6vqdlW9TlXLctd3NiOq+HRYy4iqP1/4whd4+OGHaW9vB2D79u2cObNs1agpg6GpJO3N5c2GAicjaiqRYna+cW+qlu4L+Z5HTOFGp+eINgcIluBByWp6IyFGrFn5igrZ9lX1tKruc3+O42SGLJ3w5QbgXnU8CnS4rQmqgqoyXWBpns8nbIiELSOqxtlx/+Xm0llCJT4ehwNNXNIf4cCpGJlsxdtgmUVs+zdmdfkeDedUdWE6N7efkx3tgJQbiCpFhkVzsIlwwMeJ8cZualxPQqEQwWBw4fd0Oo14NI26WdtwfI72MjYqzxmI5mbOa9yb9aX7gp1HvDc6Pedpf6icDZGwleatothtX0S2AlcCe5a8lc/sxBWTSGXIZLWg0jyAvvYQU0kLRNUyO+6/XDKVKXh/WM0rNnUwO5/h2dOxkq/bFM62f2NWl2/05F9E5D8DzSLyFuBvgX/wbli1YyEjqkRPOLpabea8evLGN76RT3/60yQSCX7605/ynve8h9/+7d+u9LAa1vBUsiKBqH4LRL1sX8Apu7bziIdUlZH4HD2R4NoLF6mvPcRIfI7syi0eG1ox276ItAHfA/5IVQu6y6zUFPZxt6wuVGA/nP5omFiicbNI64Ed91/Oq0DUJQMROloCPHJ4tfmeTLnZ9m/M6vK9OrgNGAGeAf4Ap277j70aVC1JpUs3ax5AV0uQExaIqht33HEHvb29XH755fz1X/81b3/72/nTP/3TSg+rIWWyysj0HJEKlOZtjDYDcHKycbMdF+8LQA92HvHU2Mw8c+ms5/2hwOkRlc4qs/MZz7+rFhW67YtIACcI9Q1V/f4yi+Q1zb2q3qWqu1R1V29vbyF/QkFyM4QVnhEVJmYZUTXNjvsvN5fOFlSquhafCNde2M2xsRlONfB1RrWx7d+Y1eU7a14W+HvgI6r6blW9e7XZ7RpJKUvzALpaQ5yYmLU67zrh8/l417vexRe/+EW++93v8uEPf9hK8ypkbHqOTFYrU5rXEUYEBica9wJx8b4AHLHziLdOuttarveglza4M5zFLWiwrEK2fXdm4C8Dz6rqZ1ZYbDfwQXf2vGuAKXd24aqQazRe6I33QDTMfDpLMmUBzlplx/2XS6YyhDzIiALYdX4XwSYf//xC+bIezeps+zdmdateHbgXN7eLyCjwPPC8iIyIyCdW+1wj8aI0L5VRhmKNW8JTD1SV22+/nZ6eHnbs2MGOHTvo7e3lk5/8ZKWH1rCGY04Pm0oEokL+JvoiYQYb8EnlcvsC8Eo7j3gr91S8o8X77X1DxMm6itsMZy9T5Lb/OuADwG+KyJPuv7eLyC0icou7zP3AEeAQcDfOrMJVYzoXiCrwxrvfzSSdsoblNceO+8tLprKECyxVXUtzsIk3bO9h/+AUR0dnPPkOkx/b/o3Jz1pHw/+AczH0ancGuy6cqYFfJyL/wfPR1YCzGVGlyXLpanWeXr80ZieRWvbZz36Whx9+mMcff5zx8XHGx8fZs2cPDz/8MJ/97GcrPbyGNOwGdysxax7Aps5mTjbgjJjL7Qs4M4DZecRDuey7juYyZERFLCNqOcVs+6r6kKqKqr5CVa9w/92vql9S1S+5y6iqftSdRfhyVd1bhj8rb2d7RBUWiDqv0wlEjc/Mr7GkqTZ23D+XqjKXzhD2e5MRBfCG7b1EmwP88OlTzKeznn2PWZ1t/8bkZ61A1AeA96nq0dwLqnoEeD/wQS8HVitS6VKX5jk3DdYnqrZ97Wtf45vf/CYXXHDBwmsXXnghX//617n33nsrOLLGlcsyrERGFMDmzuaFcqlGsty+AMxj5xFPDU4mCPl9nj19z7lvz3F+9vwZwDKilmr0bX+hR1SBGePnd7cCFoiqRY2+7S9nPpMlq4VnCOYj6PfxjlcMcHoqyX/7hwOefY9ZnW3/xuRnrauDgKqOLn1RVUeAytzNVZn5TBYB/L7SZERFmwP4fcJLYxaIqmWpVIqenp5zXu/t7SWVsqyBShiOJfEJtIUrkxG1ubOZoakk6UxjPaVcaV+w84i3BicTdLQEytKTLtDkBLxiFoh6mUbf9uNFluZ1tgQI+X0WiKpBjb7tL2cu5Zz7C51FMl+Xbozy69t7+cae43zhZ4ewlkTlZ9u/MflZ645stbO/XRkAqYwS8PtKdrHf5BM2dTZz3DKialowuHI5zGrvGe8MTSXpjYTwVahZ/ObOFtJZZTg+x6aO5oqMoRLW2N7tPOKRU5OJspTl5URCASvNW6LRt/34XBqRwntoighdrUELRNWgRt/2l5Nruu9laV7OWy/to6MlwJ8/8Dw/f2GEm167ld+8ZIPn32sctv0bk5+1AlGvFJHYMq8LEPZgPDVnPpMtWVlezpauFgtE1binnnqK9vb2c15XVZJJa0RfCUOxJP3tlTtsbXb7nZwcn22oQNQK+8KVIhLHziOeGZxMsKMvUrbvizT7rTRviUbf9uPJFG1Bf1HB/67W4MJEE6Z2NPq2v5yk28rD63JpAJ8I//O9V7Brayd3/vNhPvKNfYQDPrZviPCWnX30tIU8H0Mjs+3fmPysGohSVe/D9jUulc4SLFGj8pwtXS386JmqmYHZFCCTsemmq81wLMlWt+dIJeSCTycnErymYqMov+X2BRF5QlV3VWA4DWFmLs3kbIqOlvJlRLWHAzbJxhKNvu3Hk+miS6G7W4M8NxQna+VFNaXRt/3lLGREedgjajGfT/jgtVv5t685n0cOj/LTg8N8+/ETvDAc5/dfu3WhB5spPdv+jcmP92H5OudFRtT53S1MzqZsymJjSmhoKkl/tHIPoja6gajBycZrWG7K69Rkbsa88rWiiIScjCjrR2Jy4skUkSIDUV2tITJZJWbXQ6bGzbkZUaEylOYt1uQT3rC9l0/ecBl/+ObttIX8fO3Rl5ieswxWY0xlWSCqSKlMtuD+ByvJPaU4NmpPl40phcR8hlgyTV8FS/PCgSY2REKcnLCy23yJyFdE5IyI7F/h/TeJyJSIPOn++0S5x1iNTuYCUS1lDESF/aSzSiJl2aDGMT2XJlLkLKW5mYStT5SpdWczoip369XREuT915zPXCrL/94/VLFxGGMMWCCqaKmMljwj6qJeJxB1ZHS6pOs1plENxZy+XJXsEQVOn6iTE5YRtQ73ANevscwvVPUK998nyzCmqreQEVXG0ryo+12WyWty4sk0baFiM6IsEGXqQ7lL81bS1x7m2ou6eeL4hO1XxpiKskBUkebTWYIlb1beSpNPOHzGMqKMKYWhKTcQVcHSPHBmzrNAVP5U9efAeKXHUWsGJxL4fVJ0WdR65MoAp2YtEGUc8WS66G0w2hzAJzBmN8ymxiVTTmleqasoCvH67T34fMLDh0crPRRjTAOr/NGwxqUyWQIlblYe9PvY0tViGVHGlMjfPXESgMePVTamsamzmdNTCTJZ66NTQteKyFMi8mMRubTSg6kGpyYT9EfDRc1Wtl5Rtwxw0jKijMvpEVVYad59e45z357jNPmErtYgo9M2c56pbcl0hpDfV9bj8krawwEu3djOk8cnFzK1jDGm3Mr3uLROpTxoVg5wYU8rR0YsI8qYUoglnKac0SL7lazXfXuOv+z3zZ3NpDLKmXiSgWhzWcdSp/YB56vqtIi8Hfh7YPtyC4rIzcDNAFu2bCnfCCtgcDKxMEtjubSF/DT5hMlZy1wxjngyTXsJsvL62sMMxywQZWrbXCpb8bK8xV51fidPn5zin58/w/WXDVR6OMaYBmQZUUWaT5e+WTnAhb2tHBmdscwJY0pgKpki5PcRqvBF4ObOFgArzysRVY2p6rT78/1AQER6Vlj2LlXdpaq7ent7yzrOchucSLCps7yBKJ8I0eaAZUQZwLk2mktni+4RBU4gamx6zjI3TE1LppyMqGpxYU8bzYEmHjgwXOmhGGMaVPUcEWuUF83KAS7qbWM+nV1oOmuMKVwskaK9zNlQy3ny+CQA33n8RIVHUh9EpF/EqXMQkatxzmljlR1VZaUzWYZiybJnRIHTJ2rSekQZnLI8oCR9yvrawyhw6Iy1KzC1K5nOVFVGVJNP+LWBCA8+O0wqk630cIwrj9mCRUQ+JyKHRORpEbmq3GM0plQsEFUEVfWuNK+3DYDDI3bhZUyxYokU7c2Vr0TucPvoTNjNel5E5JvAL4EdInJSRD4kIreIyC3uIu8G9ovIU8DngBtVtaHTSIdiSbJKZQJRLQErzTOAU5YHFNwjarG+9hAALwzHi16XMZWSTGVorqJAFMDOgSixZJpHjzT085tqcw+rzxb8NpwWBNtx2g3cWYYxGeOJyt+Z1bC5dBbFmxkwLuxtBeBv957k1KQz49fvvaa++5qYs0TkK8A7gDOqepn7WhfwbWArcAz4N6o64WaE/CXwdmAW+H1V3VeJcVerWDLNhT2tlR4GgSYf7WE/4zPW7yQfqvq+Nd7/PPD5Mg2nJuTOFxs7msteAtrREiSeTFtJuWF6zglEtYX9zE0XF5zsbg3R5BOet0CUqWHJVJaetup6/r+9zynP+8mBYd6wvb5L1muFqv5cRLaussgNwL3uQ7dHRaRDRAZU9XRZBmhMCXl6RBSR60XkeTd98LZl3g+JyLfd9/cs3vFE5OPu68+LyG+5r+0QkScX/YuJyB+5790uIoOL3nu7l38bQGLe6VdQ6lnzALpbg0SbA4zE7Ya1Qd3DuU9EbgMeVNXtwIPu72BPR1aVzSrxZIr25sqX5gH0tIUYLfLGzJiVDE7OApS9RxQ4pXmKk4FoGlushKV5TT5hQyTE80MWiDK1K5mqrtI8cB6OvW5bNz9/caTSQzH52wQs7u9w0n3NmJrjWSBKRJqAL+DcJO8E3iciO5cs9iFgQlW3AZ8F/sz97E7gRuBSnJvxL4pIk6o+r6pXqOoVwKtwsj/+btH6Ppt7321c66mE2zgz6EFpnoiwoy/CcCxZ8nWb6qeqPwfGl7x8A/BV9+evAu9a9Pq96ngU6BARmwLFNTozR1apskCUBZiNNxYyoiowK2PULT21huUmV5pXqt58fe1hXrBAlKlRquoEovzVFYgCeP22Hl4am+XE+Gylh2JKTERuFpG9IrJ3ZMSCjab6eFmadzVwSFWPAIjIt3BumA8uWuYG4Hb35+8Cn3fLjG4AvqWqc8BRETnkru+Xiz77ZuCwqr7k4d+wqtlcRpRHs2Bc3N/G04OTqCpuP17T2PoWpd4OAX3uzys9HbE0XWB4ygn6REvwZL4UutuCzM5nmJydp6MlWOnhmDpzciJBd2uQ5mD5b3g6mp3t2fpEmemFHlGlOe72tYd58sQkU7OphYCnMbUilVGyCs2B8pXm3bfneF7LvX67M9Hsw4dGufFqawFSAwaB8xb9vtl97RyqehdwF8CuXbusZt5UHS+PiPmkDi4so6ppYArozvOzNwLfXPLare4MAl8Rkc7ihr+2pIcZUQA7+ttJprJM2dNls4RbG76uk0qjPhkZcrMKqykjCuDo6EyFR2Lq0eBkgo0VaFQO1ozfnJWbNa8tVJpA1MaOMAD7T02VZH3GlFMy7dwvhKqsNA+cWbo3REI8dGi00kMx+dkNfNCdPe8aYMr6Q5laVV1d8/IkIkHgncDfLnr5TuAi4AqcTJC/WOGzJbsZX8iI8igQdUl/BDh7I20a3nCu5M797xn39byejqjqXaq6S1V39fY2TlPKhUBUiUpEimWBKOOlU5OJisyYB865sC3kZ8IyooqWxxTebxKRqUV9MT9R7jGuppSz5sHZWSCfOjlZkvUZU065B9fV1iMKnFYgr9/WwyOHx8jaRBMVl8dswfcDR4BDwN3ARyo0VGOK5mWtSj43x7llToqIH4gCY3l89m3APlUdzr2w+GcRuRv44XKDKmWa4uy8c6EV9KBZOcDFfU4gajg2xyX9nnyFqS27gZuAO9z//mDR67e65a+vwZ6OvMzwVBKfOLM3VYPO1gA+sUCUKT1VZXAiwRsvrlyguactyJj1QCuFe3BmhLx3lWV+oarvKM9w1ic+lybk95VsVuGWoJ/zu1t45qRlRJnak0xlgdKU5uVbcrcer9vWw/efGOS5oTg7N7aXfP0mf3nMFqzAR8s0HGM85WVG1OPAdhG5wM1guhHnhnmx3I01wLuBf3J3sN3Aje6sehfgzAb22KLPvY8lZXlLmjP/DrDsU8RSSnjcIyraHCDaHLCG5Q1ouSciOAGot4jIi8B17u9gT0dWNRRL0hby46uSPmt+n4+OliBHLBBlSmxyNkUilWFoKunJzUo+um1WyJJYYcKKmhFPpkvWHyrn8k1RnrZAlKlB1ZwRBU4gCpw+UcYYUy6epQioalpEbgUeAJqAr6jqARH5JLBXVXcDXwa+5jYjH8cJVuEu9x2cxuZp4KOqmgEQkVbgLcAfLPnK/yEiV+D0zTm2zPsllyvN86pHFEBfe4ihKQtENZpVnoi8eZll7enIKk5PJYhWSX+onJ62IMcsEGVKbHAyAZzt1VQJPW0hfjU3QTyZKllZllnRtSLyFHAK+L9V9UClB5Tjxf//r9zcwQ+fPs3o9NxCibMxtSAXiKrGHlEA/dEw2za08dChUT786xdWejjGeGZwMsE/PjvMG7b3EKrCWSwbjae1Kqp6P062xuLXPrHo5yTwnhU++yngU8u8PoPT0Hzp6x8odrzrNZtrVu5RRhRAf3uYw2fGyFjdtjEFGZxIVN3sdD1tIZ48MUk2q/h81ZGpZWrfQiCquXLbe3er890vjc1y2aZoxcbRAPYB56vqtIi8Hfh7nOzxc4jIzcDNAFu2lGdWLE8yojY729MzJ6f4jUs2lHTdxngpV5oX9vB+oViv39bDtx4/zlw6Yzfopm59+kfP8k/PneH0ZIIPXLu10sNpeNV7RKwBiYUeUV5mRIXJqDJqPTeMWbdsVjk1maxohshy+iJhZucznJxIVHoopo4MTlRHRhRgpaceU9WYqk67P98PBESkZ4Vlyz5RxdHRGWbm0iUtEb1sUxQRrDyvAYjI9SLyvIgcEpHblnn/YyJy0J0p+0EROb8S48xXLiOquUozogBee1E3yVSWJ47bhACmPo1Nz/HAgSEAXhieZj6drfCIjAWiijDrcY8ocAJRgPWJMqYAI9NzzGeydFZZRlR/1NmvnxuKVXgkpp6cmkwQaBJagpW72eluc/Y1Kz31loj0iziN70TkapzrubHKjuqsZKr0WRVtIT8X9bbxtM2cV9dEpAn4As7ERDuB94nIziWLPQHsUtVXAN8F/kd5R7k+yXQGwdsKimJdc1E3PoFHrE+UqVNPn5winVXeeHEvGVWOj89WekgNr3qPiDUgMZ/B7xNPmyBviITwydkp6I0x+TtZBRkiy9nQHkIEnhuKV3oopo4MTiboaA4iFWzMH2jyEW0OWCCqSHlM4f1uYL/bI+pzwI1uv8CqkEhlaPYgIPqKzVGeHpyiiv5UU3pXA4dU9YiqzgPfAm5YvICq/kxVc3eRj+LMrl21kqksoYCvosfmtbSHA7zyvA4eskCUqVMHTjnZtNdc6ARdj43ZdUqlVcd85jVqdj7j+dMNf5OP7rYQw9aw3Jh1O9u8uboyokL+Js7varGMKFNSg5OJqgi69rTZrJDFymMK788Dny/TcNYtMZ/xpAzpFZuifH/fIEOxJAPR5pKv31SFTcCJRb+fBF6zyvIfAn7s6YiKlEx5sz+U2uu39fDFfz5sk02YunTgVIzzu1uIRVkCnwAAIABJREFUNgfobAlyxoMkj1Qmy2h8jp6ITaiRD8uIKsLsfMaT/lD37Tm+8A+chuWWEWXM+uV65nRW2ax5ADv6I5YRZUrqVJUEorrbQhy1QFTDSqYypLPqTUbUeR2A9YkyDhF5P7AL+PMV3r9ZRPaKyN6RkZHyDm6RZCpDuAYCUa+9qIdMVtlzZLzSQzGm5J4birNzoB1w+lmOTs+XdP3pTJb3/vUv+cw/vmBlf3myQFQREqm0p/2hcjZGw0zMpphKpDz/LmPqyeDkLNHmQFVOmXxJfzvHRmdIuL3mjClGMpVhdHq+KrL/NkRCTCVSnjxtNNUv5l6reJEBsnOgHb9PrE9UfRsEzlv0+2b3tZcRkeuA/wK8U1WXndGnEo36l5NMZWtiJrqrzu8gHPBZeZ6pO5mscnJilq09rYCTuT02M0e2hGXeT56YZJ/b7P9fXqhc4LuWWCCqCF5lRC3V76afP3fayniMWY/BiQSbOqqzfOPXBiJkFV48Y1lRpnincmWoVZD9lyuZOmDnrIaUe2jmRUZUONDExX0Ry4iqb48D20XkAhEJAjcCuxcvICJXAn+NE4Q6U4ExrouTEVX9t1whfxNXX9DNwxaIMnVmOJYklVE2dzrXJ91tIVIZJZ5Ml+w7Hjo0ighccV4Hh87EyWStl+Faqv+oWMXK0SMKYKDDmWHroF3UG7Mug5MJNnVWZyDqkn4nPfhZ269NCVRTP7QBd1bIg6ds225EUx5mRIHTsPwZa1het1Q1DdwKPAA8C3xHVQ+IyCdF5J3uYn8OtAF/KyJPisjuFVZXFZLp2ugRBfC6i7p58cy0zdZt6soJt1TuvM4WwCnNAxidXjaZsiCPHBrj8k1Rtm9oI5VRRkq47nplgagiJMqUERUJ+WkN+e2i3ph1UNWqzoja0tVCe9jPkyfsyb4pXK6f4N/tcypXqiEjKhxoYktXiz08aVBeB6Iu3xxlcja1MCuqqT+qer+qXqyqF6nqp9zXPqGqu92fr1PVPlW9wv33ztXXWFnJVKYqWwQs53XbegB45LBlRZn6ccI9X5zX5QSiOt1+mlOzpWl7o6o8ezrGKzd3LDwAH7Rz1JosEFWE2fny9IgSETZGw3ZRb8w6TM6mmJnPLKThVhufT7hySyf7Xpqo9FBMHZiYnccn0F4FgShwevk8aw9PGpJXpXm5oOvlm6IA7B+cetnELsZUo2xWmUtla6I0D5xjd2dLgIdeHKv0UIwpmZMTs4jARrfKKHetNJUsTSBqJD5HfC7Ntg1t9LSFCPp9Cy0TzMpq46hYpcqVEQXQHw3z4vA0qUy2LN9nTK3LzViRe/pRbe7bcxx/k/DCcJxYiU6EpnGNz8wTbQ7Q5JNKDwWAnRvbOTo2w8xc6fovmNrgdUbUxX0R/D7hmUHLJjXVb2Y+jeLd/lBqPp/wum09/MsLI2Stx42pE4MTCTZEQguTBgSafLQEm0o2EdihkWkALuptwydCT6vTDN2szgJRRZhNZQj6y3PRPxBtZj6T5bC7oRtjVndkNHdSaK3wSFZ2flcrCjx1wmaAMsUZn5mns7Xy/aFydg60owrPDVlWVKPJXdh7NV19rmG5BaJMLcg1Qw7XwKx5OW/+tQ2MTs/xtO1jpk4Mx+fobw+/7LVoc2BhltdiHR6ZAeCiDc49R1drkLHp+ZKsu55ZIKoI5Zo1D6z5qzHrdXRkBp9Ub0YUwObOZgT4lZXnmSJNzKboqoJG5Tk7NzrN+PcP2jmr0UwlUoT8Pk+z8y7fFOXAqZg1LDdVbyEw68Eskl75jR0baPIJDz47XOmhGFMSZ2JJNiwTiCpVRtTRkRmaA00Lwa6u1hCTsymydo5alQWiCpTJKvPpbFl6RIHT3T/k99kMW8bk6fDoDOd1tSyk4VajcKCJvvYw+45bRpQp3Hw6y/Rcmq4qyogaiIbZEAlZkLUBTSVSnpYh3bfnOIlUhvGZ+ZLdRBjjFa9LVb1w/zNDnNfZwnd/ddL6sJm6MBxL0tceetlr7SUMRA1OzjoPl8V5ANPVGiSjaueoNVggqkCz806qbbkyopp8wo7+iDUsNyZPR0dmuKCnesvycs7vbmHfSxPW/80UbHzWSf+uptI8EeHVF3Tx+LFxy1ppMLFEquSNypfKzYY6aM1gTZWbdGflaqmhjCiAXxuIcHoqycSslReZ2jaXzjAxm6Iv8vKMqPZwgNn5DMlUpujvODWZZOOiWbpzDwbHZ2z/WY0FogqUmHc22mCZMqLACXo9cXySbzz6Utm+05halM0qR0dnuLCnrdJDWdO2DW1Mz6V5wrKiziEiXxGRMyKyf4X3RUQ+JyKHRORpEbmq3GOsBhPuhU41leYBvOaCLk5PJTlpUxg3lKlEyrP+UDn90TA+wWYlMlVvKuEcn70OzpbazgGnvPqA9YkyNe5MzGka3rekNC8S8gMwVoJg0anJxMsCUZ0t7qx8s6XPiMplKdZDtqIFogo0mwtElSkjCmCgo5nZ+QyxpM1CZMxqhuNJEqkMF1Rxo/Kci3rbaPIJP39hpNJDqUb3ANev8v7bgO3uv5uBO8swpqqTe+JWTaV5AK/e2gXAY0fHKzwSU05el+aBM+PRhkjYMqJM1VvIiKqh0jyA7rYQmzqarWG5qXln4kkANiwpzWsLO4Go0Xhxs9slUxnGZubZ3Hk2EBUJO4EomxV7dRaIKlAuEBUoZyDKjeSenrILL2NWc9SdveLCGijNCweauPK8Dn7+ogWillLVnwOrRTFuAO5Vx6NAh4gMlGd01WN8dp6g31d1pR87+iK0h/08fswCUY1kcjZVlm1xY0czg5NJK/00VW0qkcIn5a2gKJVXbI5yciLB2PTcOVkYtZ6JYRrHsJsRtWFJaV6bmxE1Ol1cICr3QGRjx9n1B/0+wgGfJY+sofaOilUikXJ7RJXxxNIfzQWikmX7TmNq0eFRNxBVAxlRAL9+cS/PDE5ZLfn6bQJOLPr9pPtaQ5mYmaerJbjQJLNa+HzCq7d2sccyohpKOTKiADZ1hJmZS9uFvqlqk4kUzUF/1R2f83H5piiAZUWZmjbmBpp6IytkRBUZiMqViA9Em1/2ens4QNwyolZlgagCzcyVvzQvHGiiqzVogShj1nBkZJrmQNM5jQmr1a9f3Isq/MsLZyo9lLolIjeLyF4R2TsyUl/ZZ+Mz81VXlpdz7UXdHB2dsRKqBpFMZZhLZ8vSDyfXsNz6RJlqNjVbnsCsFzpagmztbuGJ4xOWeWhq1sj0PCJn+zblnM2IKu4hcK4HVX/7uc3QYzZr3qo8jaKIyPUi8rzbSPa2Zd4Pici33ff3iMjWRe993H39eRH5rUWvHxORZ0TkSRHZu+j1LhH5qYi86P6308u/bbYCzcrBmRL7tF10GbOq507Hubg/gs9XG08g9w9O0R72c/fPj1q6+/oMAuct+n2z+9o5VPUuVd2lqrt6e3vLMrhyyKpWdSDqDdud/63/4oHnbdtuALmLbq+blQP0R5sRbOY8U90mE/NVVza9Hru2djE6Pc/RsZlKD8WYgoxNz9HZEsS/JHkk0OQj5PcxUmSPqJEVMq4iYb9l7K7BsyiKiDQBX8BpJrsTeJ+I7Fyy2IeACVXdBnwW+DP3szuBG4FLcRrVftFdX85vqOoVqrpr0Wu3AQ+q6nbgQfd3zyyU5pUxIwqcQNT4zDwzc7ZhG7McVeXZoRg7ByKVHkrefCLs3NjOi2fizKezlR5OLdkNfNCdPe8aYEpVT1d6UOU0OZsindVzLoCqxcV9bUTCfl48M13poZgymHIDUeXIiAr6ffRGQgzarIymik1WeUbUWr2fLtsYJRzw8biVWJsaNTY9T0/b8g/r2kL+okvzRuJztASbaHUzrHLam53SvGzWsglX4mUU5WrgkKoeUdV54Fs4jWUXuwH4qvvzd4E3i1NEfQPwLVWdU9WjwCF3fatZvK6vAu8qwd+wooVm5WXPiGpGgeeG4mX9XmNqxempJJOzqYWph2vFpRujpDLKC8O2b+eIyDeBXwI7ROSkiHxIRG4RkVvcRe4HjuCcI+4GPlKhoVbMSG42mCoNRIkI2ze0cejMNFkr7ah7C4GoMt14b+po5pRN4GKq2FSiPM37vRL0+7jivE4OnIrZQ3BTk0an5+huXf4aqS1cfCDqTHxu2WuwSNhPVp0JZczyvIyi5NNEdmEZVU0DU0D3Gp9V4Cci8isRuXnRMn2LnoQPAX2l+CNWkpgvf48ocGaJAXjm5GRZv9eYWnHgVAyAX6uxQNTW7lZagk3sP2VNQXNU9X2qOqCqAVXdrKpfVtUvqeqX3PdVVT+qqhep6uWqunetddabM25KeW9bdQaiALZtiJBIZayXzzqIyFdE5IyI7F/hfRGRz7ktDJ4WkavKPcbllDsQtbGjmXgyvTA9tzHVZmo2VZYMQS9dfUEX6ayy12ZANTVobGaenhUe1rWF/MWX5sWTy2alt4ednlTDMTs/raQWm5W/XlWvwin5+6iI/PrSBdTpqLfso9dSNaytVI+oaHOAaHOAx1+aKOv3GlMrnjwxgd8nXLoxWumhrEuTT9g50M7zQ3Hm0plKD8fUiJH4HK3BJlqWpIRXk20b2gA4ZOV563EPTmuClbwN2O7+uxm4swxjWtOYO/Pn0hIFr+QezuUeQBhTTVKZLPG5dFWX5uWjvz3Mhb2tPHp0nIyVGZka42RErVaaV1zG0kh8bvlAVLMTiMo1Mzfn8jKKkk8T2YVlRMQPRIGx1T6rqrn/ngH+jrMle8MiMuCuawBYdvqpUjWsnZ3PEGgSmirQDPmCnlYeOzpuM1gYs4wnjk/yawPtNfkE8tKNUebSWR56cbTSQzE1wrkAqu7ZIdtCfjZGw9Ynah1U9efAaukHNwD3ulmBjwIduWugSprIBaLKdPwdiDrb/kELRJkqFCtjzzSvve6iHqYSKQ5Y1rapIclUhngyvXKPqLCfqUSqqP6sI/G5ZbPS28POAxnLiFqZl4Gox4HtInKBiARxmo/vXrLMbuAm9+d3A//kZjPtBm50Z9W7AOeJ32Mi0ioiEQARaQXeCuxfZl03AT/w6O8CIDFfuSccW7tbGYnP8dLYbEW+35hqlckqT52Y5MotHZUeSkEu2tBKOODjx/uHKj0UUwNUdcXeBNVm24YIx8dmmbYeI6WST/uDshufnSfo95UtWzwcaKKrNWg3x6Yq5UpVa7lHVM6O/ghdrUF+eXis0kMxJm/j7sORnhXaF7S52btjM4VlLSVTGWLJNBvaz30g2LYQiLKMqJV4dqXg9ny6FXgAeBb4jqoeEJFPisg73cW+DHSLyCHgY7gz3anqAeA7wEHgfwMfVdUMTt+nh0TkKeAx4Eeq+r/ddd0BvEVEXgSuc3/3zOx8hpZgZUohtna3APDYMjNYrDTrhTGNYP/gFDPzGXZt7ar0UAri9/m4pL+dnx4cJpWx2fPM6sZm5kmkMlU7Y95i2/vayKiy54jdxJRbqVoS5GNiZp6uliDOvDPlsTEaZv+gZUSZ6jO50DOtekun8+UT4doLu3lpfNZmqjQ1Y8wtu+teIRAVcQNRo/HCyvNGVunT6ff5aA02MWw9DFfk6SMrVb1fVS92G8l+yn3tE6q62/05qarvUdVtqnq1qh5Z9NlPuZ/boao/dl87oqqvdP9dmlun+96Yqr5ZVber6nWq6mlHvdlUpmJPOHojIbpag+yxqVSNeZmHDzslbdde2F3hkRTuso3tTCVS7Dli+7dZ3WG31K0WAlHnd7UQaBJ+YWWnpZJP+wOgdC0J8jE+M0/nCr04vLKxo5nj47PEkqmyfq8xa5marZ+MKIBXnd9J0O/jkcN2HDe1ITcjXvdKpXm5QFSBM+eNuJ9b6TqsvTnAGSvNW1Hth+grJDGfqVjNt4hw9dYufnl4FFUt65NHY6rZQy+OsqMvwk8PDld6KAXbtiFCOODjJweHeP32nkoPx1SxF9xAVDWW5i3NyvU3+bigp5Wfv+htRk4D2Q3cKiLfAl4DTC2aObhixmfmV2wK65Vcw/KDp2JcU8MPIUz9mUw4WRa13qw8Jxxo4qotnTx+dJzrL+uv9HCMWVMuwLTSzMJt7sx2I4UGouKrB6IiYb+V5q2iFmfNqwrTyfRCFLUSrtvZx6mpJE+dtL4IxgDEkikeOzrOmy7x9om/14J+H7++vZefHBi2CQnMqg4MTtESbCLqzsxS7bZtiHBkZIaTE9bfcC0i8k3gl8AOETkpIh8SkVtE5BZ3kfuBI8Ah4G7gIxUa6stMzKbKnhGVa1i+f9Cuh0x1mZytn2blOa+9sJuM6rLtQYypNrmZXL3KiDrjBqJWeiAYCQUWglXmXBaIKtD0XJpIuHKBqLfs7CPQJOx+8lTFxmBMNfnn50dIZ5W37uyr9FCK9tZL+xmKJXnGbqzMKvafmmJjR3PNZMVu39AGYLNC5kFV36eqA6oaUNXNqvplVf2Sqn7JfV9V9aNuC4PLVXVvpccMMDY9R1dLeQOjkXCAvvaQzZxnqk6uWXm4TjKiAHoiIXb0Rfj/2TvvOLnKev+/v1N2Znvvu8luOgmkkAahI01AsKACSlG8ilfFcr0qVy8/9V7vFevFgoiIgEqNNJFigCRAgHTSs8kmu9nee5vdmXl+f8yZZbPZMrNlzszs83695rVnTtvPOfOcc57zfb5la1kLLrfHbDkazZg0dbqItVtHzescY7MQF2MdzCUVLI2dLkQgbZQBmASnjaYuF16vHlgeCW2ImiBdLjfxJnpEJcfauXxJDk/tqKRT50XQzFCGJud//r0ashIdLC9MNVvWpGnr7keAX2w4YrYUTZjS7/ZSUtdJXvKplVrClaxEB9lJDp0nKkoZ8Hjp6HOTFh/6UNEleckc0IYoTZjR1jNAosOG1RIZgwWBsm5uOl0uN//Ya3o0sEYzJs3d/WQkju2lm54QM/EcUZ0u0uNjsFlHNqkkOm24vWqwcIHmZLQhaoJ0u8wNzQP4/Plz6HS5+dVrR03VodGYTWffABtLGvjImflR0eGLc9goyojnUK1+sdKMzJH6TgY8ajA/TiQgIpw3P5O3Spvw6NHBqMMfhpQWH/pQ0SV5SZQ2dtE3oD00NOFDS3c/aaOEBEUy87ISyEx08Kct5TqFgCasaepykT7O4EhGgmNSHlEZo+SfgvdD/3R43shoQ9QE6QoDQ9TSghRuXDuLP7xZxv1vHNNuf5oZy56qdjxexXVnFpgtZcpYnJtEfYeL8qZus6VowpADNb6wzUgyRAGcNz+D9t4Bnc8nCmkxcnGEOkcU+AxRHq/icF1nyP+3RjMazd2ukCfvDwUiwrq56eyrbmfniVaz5UQVInKFiJSISKmIfGeE5beKSKOIvGd8PmeGzkihqaufjHGMwenxjkl4RPWRlTS6Z3qiPxm6NkSNiDZETYABjxeX22u6IQrgrqsXc8WSHP7nxcPc8Id3aZ7ghaSJHESkXET2GQ+gHca8NBHZICJHjb+RH58WIEopdp1opSA1lu3lradU64pUFucmAfDqocitAKiZPvZXd5DgsI2alyBcOXeerxLkm7p6XtThN0TtrmgL+f9ekpcMvG+g1WjCgabO/jG9JSKZFYWpJDlt/OntcrOlRA0iYgV+C3wQWAzcICKLR1j1CaXUcuPzQEhFRhjNXWN7LAFkJMTQNAmPqNEq8gEk+j2iuvomtP9oRxuiJkC3yw1gao4oP067ld99+kzu/tgZHKzp4LebSqnv0I19BnCR8QBaZXz/DvCaUmo+8JrxfUZQ295HXUcfZ86KLttbanwMWYkONh/RL+yaU9lX3c7ivCQsEZKo3E96goPT85N4Q+eJijpae3wd+fhRksJOJwWpsSTH2nWeKE1Y0dztIj1KDVExNgvXr5nFy/vrqGnrNVtOtLAGKFVKHVdK9QOPA9earCli8XoVzd39o1bM85OR4KClO/iE4kopGrtcZI5SMQ98ycpBe0SNhjZETYDOPp8hKsHEqnlDERE+uXoW/7jjPOwWC49tq8Dt9ZotSxNargUeNqYfBj5sopaQsrOiFatFWFaQYraUKWdBdiJbj7fQ0+82W4omjOgb8HCwpoPlhZHZ5s+dl8muE62Dgzqa6MDvERXnCH2FMBFhcW4SB3TIpyZM8HgVLd39ZEZhjig/N589G6UUf373hNlSooV8oHLI9ypj3nA+JiJ7RWS9iBSGRlrk0d47gMerxvWISk+IwaveH0wJlLaeAQY8iqwxDFEOmwWn3aINUaOgDVEToNt4KQyH0LyhzEqP4yMr8mnodLH1eIvZcjTThwL+KSI7ReTzxrxspZS/fEkdkG2OtNDi8Sr2VrZxWk4isTHRUx7Zz/zsBPo9Xu5+6fBgdUCNZl91O/0eL6tmR6YX4Nlz03F7lSkhXJrpw2+IMsMjCnx5og7XdeL26IE4jfm09vTjVUStRxRAQWocly3O4bFtFfT260IBIeLvQJFSaimwgfcHoU9BRD4vIjtEZEdj48zzrm/u9hl/xrsG/cubu4MzRDUa6XDG8ogSETITHdoQNQraEDUBuvrC0xAFsCg3iTkZ8bxxpFFXj4lezlVKnYkvhvxLInL+0IXKV8LkFP/SaHwgHW3opLvfw4ooC8vzU5Qej90qHGnoMluKJozYUe5LDrsyAg1Rj26t4HhDFwJsLWs2W45mCmnp7sdpt5hWufT0/GRcbi/HGnWBB435+KtwRWuOKD+fOaeItp4Bnn2v2mwp0UA1MNTDqcCYN4hSqlkp5bdqPACsHG1nSqn7lVKrlFKrMjMzp1xsuNPYaVyD4+TS9CczDzZhud+4NJYhCiAzwTFotNKcjDZETYCuMMoRNRIXLMyk0+Xm5f11ZkvRTANKqWrjbwPwDL6Y8noRyQUw/jaMsF3UPZB2V7QRF2NlfnaC2VKmBbvVwpyMBI7W60pQmvfZUd7CnMz4iB1pd9it5KXEsrVMe+5GE609/cSZ5A0FPo8oQFdkjAICqBx2vojsEhG3iFxnhsbx8L/UjpefJtJZU5zGablJPLSlHN84qGYSbAfmi0ixiMQA1wPPD13B39c3uAY4FEJ9EYXfIypjHEOR31gcbMLygA1R2iNqVLQhagL4DVGJJuaI8ofpjBSqMzczgbT4GB7bpsN4og0RiReRRP80cBmwH9+D6hZjtVuA58xRGDo6+wY4VNvB0oJkbJbovZXNz06gqat/MOxFM7PxehU7K1pZPTvNbCmTojgjnvcq27TnbhTR0t1PvIkh0nMyE3DaLTpheYQTYOWwCuBW4NHQqgscvyFqvNLxkY6I8Jlziiip7+SdY9rLdTIopdzAl4FX8BmYnlRKHRCRH4rINcZqd4jIARHZA9yB7zrQjIDfKzF9XI8oIzQvSK+lhk5fcbCxckSBNkSNRfS+vU0j4VQ1byQsIqyencrWshaON+qQnigjG3jLeABtA/6hlHoZ+DFwqYgcBS4xvkc1bx1twu1VnJEfmQmbA2VBViIAR7RXlAY43tRFW88AK4siLyxvKMUZ8fS7veyt0t4r0UJDh4tEp92U//3o1gqe2F5JZoKDAzW6TUU441YOU0qVK6X2AmGbEKxphoTmAVyzLI/0+Bge3FJutpSIRyn1olJqgVJqrlLqR8a8u5RSzxvTdyqlliillimlLlJKHTZXcfjS1OXCIpASN7YhKiXWjkXeN1wFSmOnC6fdMm6qnswEJ609A/S7w/Z2ZRraEDUBOsMsR9RI3lFnzk7FZhEe3145xpaaSMPomC0zPkuGPKSalVIfUErNV0pdopSK+piXjSUNOO0WZqXFmS1lWklPiCE1zs5RnSdKA7xjFKKob++L6OT1s9N91+3W43oEPVqo7+wz1VMcIC8lloM1HUGX4daEFYFWDgtrmrtc2CxCkknG2VDitFu5ce0sXjtcz4lmnaNNEx40dLjISHCMm7fQYhHS4h0TyhGVmehAZOz9+0P3/KGCmvfRhqgJ0NE7gAgkhokhaiQSnXYuWpTF07uqGdAVZDRRhlKKjSWNzM9KNC0xbqgQEeZlJVDW1IVHv1zNeLYcbSIlzk7aOK7m4U5cjI1FOYlsK496m/mMwOX20NYzYJpHlJ+85Fg6XW4qW3tM1aEJD8ws0tLU5SI9IQZLlPdR/Hz6rNlYRXjgzTKzpWg0gC90LispMI/EjISY4HNEdbnIDMDj0W+I0uF5p6INUROgo89NosMW9g+XT6wqpKnLxeaS6KiQptH4OVDTQWOni4XZiWZLCQlzMxPoG/BS09ZrthSNiXi8irePNTE3M2HcEbhIYG1xGjtPtOrBkiigocPXwU4y2yMqNRZAh3xGNuNWDgsUM4u0NHf1kx4f/WF5frKTnHxydSGPbqvgUK3O06Yxn4bOwAxF4AuhDdYjqqHDRVaic9z1tCFqdLQhagK09w6QHBf+rrYXLswkIyGGJ3fo8DxNdLGpxFcUMFqr5Q1nbqbvOEt1zrcZzYGadjr63MzLjI52v6Y4nZ5+j04uHQX4k7YmxZrbN8pJcuK0W9hd0WaqDs2kGLdyWCTQ1OUat1pXtPHvly8kyWnjruf26/BYjek0dgZmKAJfGoxgQ+cau1zjVswDbYgaC22ImgAdvQMREfP91I4qFuUk8eqh+qCtvBpNOLOxpJGlBcmmh4GEiniHjbxkJ6UzLE9UACW8bxWRRhF5z/h8zgydoeKt0iYA5mTGm6xkalhd7Eu4rvNERT71hkeU2TmirBZhaX4KuytbTdWhmTiBVA4TkdUiUgV8HPi9iBwwT/HINHX1kxHhIdTBkhIXw51Xnsb28lbu3VRqthzNDMbjVTR1uQIOzctM8FW2UyowA6o/HD0QQ5S/cqY2RJ2KNkRNgPbeAZJNHvULlJWzU/EqeHb3hLyaNZqwo7W7n90VrVy4MMtsKSFlblYCFS099PS7zZYSEgIs4Q3whFJqufF5IKQiQ8yW0iYW5SRGjQH21YMNZCTE8Ix+PkU89R0+j6hwaJsrZqVwoLoDl9tjthQmG4/kAAAgAElEQVTNBAmgcth2pVSBUipeKZWulFpiruKTUUrR3D3zPKIAPr6ygGuX5/GLDUd444hODaIxh+ZuF15FQIYi8IWW9g146egNrI/tr7AXyP4dNivJsXYatVPIKWhD1ATo6IsMjyjwXVgFqbGs31kVsJVXowln3jjaiFfBRQtDm+/BbOZlJuDxKraXz5iR/nFLeM8k+gY8bC9vZd3cDLOlTCnFGfGUN3frRPwRTkOnC7tViIuxmi2FFbNS6Pd4dcinxjS6XG76BrykzzCPKPAVWPnfj57BguxEbv/LTnboghQaE/DnLcwK1BCV7AvhqzfCzMfdf2dw+89MdGiPqBGYVkNUAGEVDhF5wli+VUSKhiy705hfIiKXG/MKRWSjiBwUkQMi8tUh639fRKqHhGhcOV3H1dHrJik2fCvmDWfl7FQO13Wyr1on79RENo9ureChLeXExVhn3EtGUUY8NouwxQjPmgEEWsL7YyKyV0TWi0jhCMujgp0nWul3ezl3frrZUqaUovR4+ga8lNR1mi1FMwnqO/rISnRiCYMk+itm+UI+dZ4ojVn4PQRzkgPLTxNNPLq1gmd31/CRFfnExVj51ANb2Vulr0VNaPF7H2UGmCMqJ8m3Xl17YIaoWqN4UKDXuD/0T3My02aICjCs4jagVSk1D/glcLex7WJ8yQmXAFcA9xr7cwP/ppRaDJwFfGnYPn85JETjxek6tkgKzQNYmp+Cw2bhr+9WmC1Fo5kUXqUoqe9kQXZiWLzwhBK71cKs9DjeOjpjDFGB8HegSCm1FNgAPDzaimaW8Z4K3jzahM0irCmOLkNUcYYv39XWMp0nKpJp6Ag8F8d0k53kJD8llmd3V/Po1goe3ar7PprQUmu8zPpfbmciiU47t507h7gYK5/8/bv8YsORwetRX5Oa6aYxWI8o4/nlNyKPh/8az02ODWj9zESHDs0bgen0iAokrOJa3n9xWA98QHw1qa8FHldKuZRSZUApsEYpVauU2gWglOrEl8RwpBHyaaPf7aV3wBMxoXkAsTFWPraygGfeq9bWWE1EU93aS0+/h4XZiWZLMYV5mQkcrO2geWY8zMYt4a2UalZK+U/GA8DK0XZmZhnvqWBTSQOrilJJcESON24gpMTFkBJnZ1uZDt8YTiQl6/d5RIWHIQp8nuDlTd06JYHGFIJ9SY1UhhqWRjIwJcf6jFE2q/Cnt8pmSt9FEwb4K7kGkyMKAjdE1XX04bBZSI0LzB6QleigvqNPP5OGMZ2GqEDCKgbXMapktAPpgWxrhPGtALYOmf1lI0TjQRFJnfwhnEpH3wBgfoniYMlOdNLv9vLtv+01W4pGM2FK6jsRYH5WdJSvD5Z5xnG/fWxGeI+MW8JbRHKHfL0G3+BE1FHX3sfhuk5SYmOiciS5OD2ebWUtuoM2hEhL1l/f0TfYkQ8Hzp6bTqfLrUegNabgD+/JTg4f46xZpMXH8NlzivEoxYNbymZMwRWNuTR0ukhy2nDaA8tb6LT7Eor7K8COR217H7nJTiTA6IzclFj6Bry09QwEtP5MISKTlYtIAvA34GtKKX+imN8Bc4HlQC3w81G2nVR4RnuvrwFFUmge+CzCi3ISefd4M30DupKMJjI5Ut9JYVoccVHmFRIoeSmxJDltMyJPVCAlvIE7jHyBe4A7gFvNUTu9bD7SAMCCKPUELM6Ip7m7n2ON3WZLCSciJll/34CHjj53WBmizprjC2Eta9JtShN6atv7yEiIwWEzP3l/OJCd5OTms4vo6HXz5I5KvEqN602l0UwGX7h4cM+knCQndYGG5rX1BpUDLj/Ft261kVtK42M6DVHjhlUMXUdEbEAy0DzWtiJix2eE+qtS6mn/CkqpeqWURynlBf6ArxN3CpMNz2jt9pVrTI3AShjnzs+gp9/D33ZVmS1Fowmaxk4XVa29LMyJzpfxQLCIsG5uBm8ebZoR3iMBlPC+Uym1RCm1TCl1kVLqsLmKp4eNhxtJctoGcxhEG0U6T9RIREyy/mCrE003j26t4O3SJpKcNo5r46bGBOrag3tJnQnMSovjqqW5HKnvmhGDaRpzaexyBf1Myk520hBEjqhgQm/zUnzr1mhD1ElMpyFq3LAK4/stxvR1wOvK93b1PHC9UVWvGJgPbDPyR/0ROKSU+sXQHQ0L0fgIsH/KjwhoNgxRkViStTg9noLUWH6/+TgDHq/ZcjSaoHjjiM+Dcabmh/JzzvwMqtt6qWjpMVuKJgQMeLxsKW1iQXZiwC7gkUZ6fAyZiQ6dJyp4AkrWP92J+mvag6seFApEhDmZCRzXeaI0JlDT1kdOUnTnh5oIa4vTOC0nkVcP1et8UZpppaGzL+D8UH6yEx0BeUR5vYr6Dl9oXqBoQ9TITJshKsCwij8C6SJSCnwD+I6x7QHgSeAg8DLwJaWUBzgHuAm4eEhyziuNff1ERPaJyF7gIuDr03FckewRJSJcvDCLipYentk13DlNowlvNhysJ8lpC6uXHTM4d14GAG/pEcUZwc4TrXS63FEblge+Z9Oa4jS2Htd5ooYwZcn6pztRf1Wrr2NdmBo35fueDHMy4ul2uanXRVo0IUQpRWVrD4Vp2hA1HBHhmuX5WER4bk+Nvt9rpgWllC80L0hDVE6yk8ZOFx7v2O2yqcuF26uCMkSlx8cQY7NQ0x6Yx9VMYVoTrSilXgReHDbvriHTfcDHR9n2R8CPhs17CxhxSFgpddNk9QZCJHtEASzMSaQgNZb/fekQfW4PN59dZLYkjWZcevs9bDrSwPLCFCxR6hUSKEXpceSnxLKltIlPrZ1tthzNNLOppBGbRQYT1UcrZxWn8Y+9tVS19lKYFl4GDZMY9CrHZ4C6Hrhx6AoikquUqjW+mpasv7KlBxHITQmvQYL5hvH2SF2nyUo0M4mW7n56+j3M0vexEUmOtXPp4mxe2FvL0YauqB5k0ZhDS3c/Lrd30AspULKSnHiVz9A0Vs5Df1XMnCBC80SE/JTYSeWIauvp51evlXKssYuLFmYRY4vIVN8nEflHEGJauvuJi7EGnIU/3BARLjktm9aeAXadaDNbjkYTEJuPNNI34GVJXrLZUkznsW2V5CQ72Xi4cdxRG03ks6mkgVVFqRH7zAmUNcW+5NLvHtd5oiCykvVXtfaSnegMu8TMybF2cpOdHNaGKE0I8YfNh5uHYDixpjiNtPgYXt5fh1d7RWmmmJo2n6EoWENUjmF8qh8nPM9viArGI8qnxznh0DyPV3Hzg9t4cEsZm4808ti2iqjwKNSGqCBp6e4nNS4yvaH8zM9KYFZaHK8frtdlVDURwSsH6kiJs1OUHm+2lLBgXmYCvQMeDtZ0jL+yJmL53aZjHK7rJCU2sp85gbC9vIVYu5XHt1WOv/IMIVKS9VeFcRjSwuxEKlq6adclszUhotIfqqo9okbFZrFw6eJs6jr62FfVbrYcTZTh9zrKn6Ahqm6c8LlaIy9i0Iao5NgJG6Ke3FHJ3qp27rl+OVeekUtJfScl9ZE/yKINUUHS0t1PekJkvxSICB88PYeOPjf3bjxmthyNZkz6Bjy8eqieS07LxmqZ2WF5fuZk+gxyOk9UdHPE6GTMhNAFiwjFGfEca+yKilG+mURVay8FYer9sTAnEa+CzUenPkm7RjMSlYZHVEFqeBpnw4Uz8pPJTHSw+UijvudrppQJG6IMw9J4xqK69j5ibBbSgkzTk5cSS0Oni353cAXDlFI88OZxlhUkc82yPM6ek05qnJ03jkT+O4A2RAVJS3d/0A0vHJmdHs/ywhTuf+M4ZU26vLEmfHlhby2dfW6uW1lgtpSwIdFpJyfJyaaSBrOlaKaRAzXtpMXHkJ0UXMLNSGVRTiJtvQMc0J5+EUPfgIea9l5mp4enIaowLY64GCuvHqw3W4pmhlDW1E12koN4x7Sm4Y14LCJcsCCTuo6+qPDs0IQPNW29xNqtpMTZg9ouIyGGWLt10Ktx1P2395GT5Ay6knF+SixKjR/6N5zt5a0ca+zmprOLEBGsFmFNURrlzd00RXgxDm2ICpLmLhdpER6a5+eKJTk47Ra+/sR7DHiCs85qNNPJo1srBj9/3XqCuZnxrC1OM1tWWLEkL4lt5S00BPlA00QGHX0DHGvoZkluUtCdnUjltNwkBHh5f53ZUjQBcqK5B6WgOCM8w6YtIizJS+K1Q/X09nvMlqOZAZQ2dDE3M7qLS0wVywpSSImzs6lEe0Vppo7q1l5yU4I3FIkIhWmxg3neRqOiuXtCgy/+nFXBJix/eX8dMVYLV5yeMzhvxexUBNhbHdmhrdoQFQRer6Kh0xU15eOTYu38z0fP4L3KNn6x4YjZcjSaU6hp62V3RRufWjt7xryMB8oZBckoBf/YVzv+ypqI4/VDDXiUYklektlSQka8w0ZxRjwvH9CGqEjheGMXQFi/eC8tSKG738Prh7UHqWZ6UUpxrFEbogLFahHOm5dBRUsP5c1jv/xrNIFyoqWH2RPM0TYrLW4wvHY0ypt7JmiI8tkPqsfxuBqKUooNh+pYNy+dhCFelklOO4VpcRys1YaoGUNTtwu3V0WNIQrg6qV53LCmkN9tOsbj2yrMlqPRnMTGkgbiYqx87EwdljecrEQni3ISeWGvNkRFIy/vryPJaaNghiW8XZKXRGlDF6UNOlQjEjhuhPaHq0cU+LRlJjr4+54as6VoopzGLhedfW7mZobv9RBurCpKIz7GyuYj2lCseZ+hkRGPbg38/VQpxYnmboom+EwqNAxRo3notfX00947MKHiSXkpsYgwrsfVUI7Ud1HZ0suli7NPWbY4N4matr6gPazCCW2ICgJ/Fn1/Vv1o4YfXns4FCzL5j2f2sX5nldlyNBrAd6M+UNPB58+fQ3KQcd4zhQ8ty2PnidZxR280kUVrdz+vH25gSX4ylhnmCbgkPxmrRXhqh34WRQLHGrtIctp47r2aoF4WQolFhKvOyOX1kgbae3X1PM30UVpveAhmaY+oQLFbLZwzL4Mj9V26ErBm0jR2uejp90y4yvastDi6+z00d/ePuNyfV3ki+3farRSkxg4O4ATChoM+D/FLThvZEAWwIYK9yLUhKghqDUNUbnJ0VcKwWy3c+6kzKc6I55tP7eELj+wY1RI8Eeu0RhMsXqV4aV8tCQ4b/3LeHLPlhC0fWZGP1SI88k652VI0U8jTu6vp93hZPXvm5UVLctq5fEk2T+yopNvlNluOZhwO13aSHQGDc9etLKDf7dWDbZppxV9owf+CqAmMtcXpOGwW7tusK3lrJkd5k29gdqIFNOYYYbXHGrpG3n9z9+T2n5EwGNIeCBsO1rOsMGXE52xGooPMRAf/jOBiHNoQFQT+LPfRFJrnNyo9914Nt6wrYllBMq8crOdjv3ubh7aUa4OTxhTeOtrEiZYerliSoyvPjEFeSixXnZHL49sq6dIv7VGBUorHtlWwYlZKVD1rguFfzptDW88AD75Vpgc/wph+t5ejDZ3kRkA7PT0/mZWzU/nzO+V4vTopsmZ62F/TTl6yk/SEmVHpdKqIjbGypjiNF/bWUKFzRWkmQfkkPJYA5hvejEdHMUQdre/CZhFmT3D/xRnxlDV1B5Scv76jjz1V7Vx6Wtao6yzOTWJrWQvtPZHp7asNUUFQ296H3Sqkx0dH1bzh2CwWPr6qkIsXZbG7oo3fbiyN6LhTTWTyXmUbGw7WsyQviRWzUsyWE/bcdm4xnS63zvEWJWwra6G0oYsbVs8yW4pprJiVyuVLsvnNxlJdFTKMOdbYxYBHRYyX+M1nz6a8uYc3jjaaLUUTpeyvbmdxXrLZMiKSc+ZmYLNY+N3mUrOlaCKYI/WdxNgsFKRO7LmUm+wkPsZK6SiGqJK6TuZmJhBjm5gJZW5mPD39nsEoq7HYYHg6Xbo4Z9R1TstNwuNVbIrQHGvaEBUE1a295CQ7sViiN2eHRYRLTsvms+cW0zfg4d6NpXxr/R6dg0YTEipbevjcwztIirXxkeX5ulLeODy6tYIDNR3MyYjntxtLaR0lpl0TGSiluOe1o2QkxPChZXlmyzGV/7r2dBIcNh56p5w6bYwKS/z5XCLFc++Dp+eSlejg3o3HdKl4zZTT3jPA8aZulhZoQ9RESIq1c8OaQp7cUcWxIEKXNJqhlNR3Mj8rAZt1YiYOEWFeVsKohqjDdZ0syEmcsL6FOUnGfsbPh/bqoXpmpcWxIHv0nHMFqbFkJDgGjVaRhjZEBUFZUzfFGTMjAeHczAS+dskC1s1N59ndNVz0s038+1N7aOpymS1NE6U0dPRxy5+24XJ7uOXsIuJ0SF7AXL00j44+Nz95pcRsKZpJ8PL+Ot4+1sy/XjiP2Bir2XJMJSvJycOfXcOAR3HvxlK2lDbh0SFVYcXOilYSHTYyEyMjDCnGZuErF89jW3lLxHbaNeHLjhMtKAVrimdebr+p4isfmE+s3cpPXj5sthRNhHK4rpNFOZPL0bYwJ5GDtR2nDFh09g1Q3dbLokkYok7L9W17oHpsQ1SXy83bpc1cujh7zEF5nwNJFptLGul3eyesyyy0ISpAlFKUNXUzJ4xLFE81sTFWrlqax+ZvXcinz5rN83tq+OWGIzyxvUJXntFMKTVtvdzwh3epa+/jj7esJmtIUj6dI2Z8cpKd3LquiMe3V/DaIf2CFYmUNXVz5zP7WJKXxM1nzzZbTlhwen4yd1w8j7mZCfxjXy0fv+/tweq1GvPZWd7KitmpEVbZUchOcvDNp/bQrAfWNFPItrIWYqwWlhfqlAITJSPBwe0XzOGVA/W8flj3ZTTB0dLdT2Ona1KGIoBlhSm0dPdT1Xpyepq9Ve0ALMmbuKEr0WmnKD2Og7VjG6JeO1RPv8fLZYtPrZY3nEtOy6bT5WZrWfOEdZmFNkQFSGOniy6XmzmZM8cQ5Sc3OZbvX7OEN799EefOz+BgbQe/33xssISlRjMR/Maln7x8mA//dgsNHS4e+swaPZo4Qb552UKW5CXx1cff42h9p9lyNAGilOKVA3V8/L63EeDeT505YZfyaCTRaefms2fziVUFlNR18pF7t1BSp9u32bT3DHCkoZNVs1PNlhIUVotw3cpCevo9fPah7drLWzNlbD7SyPJZKTjtM9ubdbL8y/lzWJSTyLfW76NFpxvQBMF7la0ALMmfnEfUsgKfMXl3ZdtJ83eeaEXEl8dyMizJSx40ao3GC3tryU5ysLpo/Heic+dn4LRbItLTV/d2A+S4YXQpnkEeUcPJSnTywdNz+fz5c+n3eLn+/nd0IlnNhFFKsaW0id9vPo7damH9F9dpI9QkiI2xcv9Nq3Dardzwh60BxZ9rzOORd8r5+hPvsfZ/XuMLf95JWnwMT92+bsKVWKIZEWF5YSqfPbeYbpeb6+57m99u1AltzeTN0kaUgnVz082WEhBDvWrzU2K5Yc0sDtV1cvHPNvHdZ/bx/57bz4NvlZmsUhOpVLb0cLiuMyDvBc3YOGxWfvnJ5XT0DvDFv+ykb8BjtiRNhLDzRCtWi0zaK3FRTiJOu4Wd5S2n7H9BViLJsfZJ7X/l7FSq23pHLQjW2TfA5pJGrjwjN6C81E67lfPmZ7LhYH3EVYXVhqgAOWS40C3Inpy7XzSQnxLrq9TV5+YL+iGhmQB17X08/E45/9hXy4KcRF74yrksnKQr7Uzn0a0VbCpp5NNrZ+Fye/jEfe/oML0ww/8yfNdz+/m/V4/yzO5qrBbhF59YxgtfOY95WQk6FJXRw3Fzk2P5zDnFDHi8PPR2ecSWK44GNh5uJNZu5XCEeqedlpvEC185l/MXZPL0rmoefucE//XCQT567xYe2lI2qqfUTL82NSPz4r5aAC7Vhqgp4bTcJH768aVsK2/h83/eqdOBaAJiR3krS/KSiIuZXI5Zm9XC2XPS2VjSOJgnqrffw9ayZs6aM/kB87Pm+AZwth4fOZRuw0FfWN7VSwMvWnP10lxq2/t4+1hkhedpQ1SA7K5oIy/ZSXZSZFSHmW5yk2P58PJ8dle0cdMft5ktRxMhKKX4284qLvvlZsqaurlmWR6fXjuL1PgYs6VFDVlJTr5w/lwK0+K47eEdfOdve2ns1OEn4YBXKV7aV8sj75zAbhVuOXs2X75oHh89s2DCpYBnGtlJTj69djYtXf188a87IzI5Z6Tjcnt47XA9C3MSIyw/1MnsKG9l3dwMvn3FIm47t5iLFmXRO+Dl+38/yNr/eY1bHtzG+p2+Cl46Ub5mNJRSPL69klWzU7VH6xRy7fJ87v7oUraUNnHlPW/y2qF6Xe0yyvF6Fa8erOfn/yzhzaONQYVmdrnc7K5oY00AoWyBcPFp2VS09AxWcNx8pJG+AS+XL8mZ9L4X5SSSEmfnraNNIy7/264q8lNiOXNW4J5dly/JITXOzmPbImugRJelCpDdla0sD6JBRBOjjf6dnp/MBQsy2Xykkad2VPLxVYUhVqaJJOra+/jes/t49VADq4tSOX9+JukJkVFtKdJIi4/hb19cx8//WcKftpTz9K5qLl2SzQXzMzk9P5k5mfE6j0WIcXu8PLWjkj1V7awtTuPKM3KxG7mgtIdFcMzJTOCjZ+bz1M4q/uOZffz0uqVjVpXRTC0bDzfS1jPA8uXR0SeKsVmYm5nA3ExfVeT6jj7eq2xjT1Ubm480AuC0W1iSl8zczATOyE82U64mzNhwsJ6ypm7u+MA8s6VEHZ9YXUhZUzdP7qjktod3kJfiZNXsNH547RJS4t4fwBzpGXrj2lmhlKqZJAdrOrjz6b3sqWrHIuBVvkrCFy7M5OJF43savnmkkX6Pl0umyCvxssXZ/OD5Azy6tZK7PrSYJ7ZXkBYfMyUpRCwW4ZLTsnnlQB0utweH7f3++NH6TraUNvPvly8Mql/jtFv52JkFPPR2OY2droipZqsNUQFQ1dpDZUsvt5xdZLaUsOPSxdlUtfbwvWf3c1puEqfrDppmGHXtffz53XL++FYZSsF/Xr2YW9cV8cT2SrOlRTVOu5XvXrWYG9fO5uG3y/n7nhr+sdcXPiBASpydzEQH583PZG5mAvOyEnivso34GOvgw0935KYGl9vDHY/tZk9VO5ctzubChVlmS4p4VsxKJS8llnteO8qstDju+MB8syXNGB56u4ycJOeg4SbayE5ycvmSHC5dnE1dex9FGfEcru1gY0kDT+6o5J1jTVy4MJPCtDizpWpMpm/Aw90vH2ZORjwfCiKMRhM4hWlxfO2SBeyqaOWdY808v6eGF/fVsmJWCufMy2BpQTJtPf0kx9r1gESE8vrher70193EO2z8/OPL+NCyPB548zivHqpnY0kjx5u6ufKMnDEHr1/YW0tKnH3KCmhkJzn50LI8HttWgd0mbCxp5N8vXzhlxWSuWprL+p1VvHqwgauW5g7Ov+e1o8TFWLlhTfD97+vXzOKBt8r4y7sn+PqlCwLebrghN5R9f22ICoCX9tUBcNniybvjRRsWET65ehYPbSnj1j9t4/HPn828rOjsnGpGZugNzO310tDhorrVl4Svd8DDwdoOPF7FGfnJXL4kh1i79RQjlPYImT6KM+JZkJ3INy5dQGOXi7r2Pho7XTR2uWjsdPHIO+UMeN53d4+1W5mVFseC7AQuWJhJfkqseeKjgJ5+N1/4807ePNrE1UtzWTc3w2xJUcPXLplPZUsPv9hwhJ5+D9+6fGFAiT01E+ftY028e7yF/7x6MdYoP9cWEfJSYrluZQEA/3HlaXzn6X08v6eaK+95kx999Aw+tDRXv/zOUJRS/PCFgxxr7Oahz6zW1U6nEatFWF2UxuqiNGraetlX3U5pQxf3vHoUf+/FabeQlxxLXkos87MTGPB4B72ONeHLX949wV3P7WdxXhIP3rqarERfCpyUuBiuW1nIvKxEnt5VxYfv3cIfb1k9Yq7m2vZeXj5Qx2fPKZrS6/DbVyxi6/Fmfr/5OEsLkrl1XdGU7fv8+ZkUZ8Tzm42lXHF6DlaLsKW0iRf21vLli+aRNoGUJfOyErh8STZ/fKuMT581OyK8oqbVECUiVwD3AFbgAaXUj4ctdwCPACuBZuCTSqlyY9mdwG2AB7hDKfXKWPsUkWLgcSAd2AncpJSadN1Pr1exfmcVp+cnMStdj36NRILDxl8+t5ZP/P5dbvjDu9z7qTMDKjepmVrGu96mA6UU1a29HKrr4Gh9J7XtfbiNXBpOu4UzZ6XyxQvm8vFVBWwpjawEetGGiJCV6Bx8yPvxKkV7z8CgYaq+o4/jTd2U1Hfywr5a1s1N57qVBVyxJJfYmNCH803mOWI27T0D3PbwdnZVtPKT65bi9uj8FlOJiPCT65bijLFy3+Zj7Ktu438+ckbU5GkJt7bf0TfAd5/Zz6y0OG5cM4tndldP178KK4YOlCwvTGFWWhyvHqrnjsd288KeGm6/cC4rClMGDVJKKbpcbtp7B3hqRxXxDhvxMVY+ddZssw4h4gi3tj+c3n4PP3zhAI9tq+T2C+ae5OWqB9aml7wUn7Hp8iU+j7S69j7qOvqoa++jpr2Xd48381ZpE8/sruaqM3K5fvUsTs9PiiiDcbi3f5i8F82Ax8v/vniYB7eUcfGiLH59wwriHaeaJZYXppAeH8P6XVV87N63+fWNK07xKr/7pcNYBG6e4silnGQnL331fA7VdbC8MGVKU1pYLcLXL13AHY/t5nvP7uO8+Zl895l9zMmM50sXTTzM91tXLOKD97zJnU/v5f6bVgU8OKeUotPlpqffQ2VLD1lJjpNCBqeLaTNEiYgV+C1wKVAFbBeR55VSB4esdhvQqpSaJyLXA3cDnxSRxcD1wBIgD3hVRPw+ZqPt827gl0qpx0XkPmPfv5vscazfWUVJfSf3XL98sruKauZkJvDYv6zlXx7ZwfX3v8vNZ8/mixfOPeWlVzM9BHi9TQkut4d3jjXz6qF6Xj3YQF1HH4LPffrsOenkp8aSnxJLWnzM4J8CFJIAAB97SURBVINfG6FCT6CdYYsIqfExpMbHnDTS1NTlYk9VG7tOtPL10j18L2Y/V56Ry8dWFrCmKC0knieTeY5Mu7gxUErxVmkT316/l8YuF7++4UyuWpqrX1CmGP/5XJKbxIeX5/PS/lou+tkmrlqax4eW5rK2OJ3kuFPLLLs9Xh555wQiEGO1jGogMNNdPdzaflOXiy/+ZSeVLT389XNrTTFKhwtp8TGsv/1sHnirjHtePco/D9YTF2MlNS6Gnn43HX3uU5KbJzps7Kxo5aozcjlvfuaEihO0dPfT3OUixmYhNzl22gsc9A14ONHcgwhkJDgmNEI/EcKt7Q+lqcvFs7ureeDNMuo6+rhgQSYFqbH63j5FBHsenXYrRRnxFGW8P/gw4PFytL6TTpebv+2q4q9bK1icm8T1awq5dln+iM+EcCKc279SirYhA5d2q5DotAftHbu3qo27njvAe5Vt3LquiO9dddqYnkyFaXE896Vz+NzDO/jsQ9u5fs0sbju3mLS4GO5/8zjPvlfDHR+YPy3h0slx9sEqd1PNNcvy2F/dzv1vHOexbZXMTo/jT7euntTzdW5mAnd+cBE/+PtBvvvsPn5wzemjPisaOvvYUtrEUzsqKW3sorPPDcCvXjuK3SosL0zhrDnpnL8gkxWFKdPi9TmdHlFrgFKl1HEAEXkcuBYYeiFdC3zfmF4P/EZ8b67XAo8rpVxAmYiUGvtjpH2KyCHgYuBGY52Hjf1OyBCllKK6rZd/7K3l5/88wpqiNB37PQ7+h8fNZxdRUtfJI++c4JF3TrBubjpritI4PT+ZwrRY0uMdxDts2K0SNqMTXq/PCtzZN0CXy01Hr5u2nn7aewdo7x2go3eAPrcXm0WwWQSH3UqCw0aCw0a8w0ai0/fXPy/BaSPObg11iEgg11vAeL2KPreH3n4PrT0DVLR0c6S+i3eONbO9vIWefg9xMVbOn59JgsPGgpxEEkYYydBELhkJDj6wKJuLFmZR3txNe88AL+6r5amdVaTHx7BuXgarZqcyJzOewtQ4kmLtJDhsU/1yNOHniJpEeR2lFP0eL/1u36d3wEOXy01Xn9v315j2KIVFBKsIIj6jXkVLDxtLGthb1U5xRjzrb1/HssLoSOocrogIa4rTWJSTyFulTWwqaeDve2oASI6147BZsFqEvgEPPf0eXEMq7dkswr2bjpGb7CQ72UlOkhObRejp97Cnqm3wN/cqxV/ePUFyrJ28lFjyU5zkp8YOjs7nJcdOtXHGtLbv9ir63V5auvspbejiLaOj2uf28n/XL2ftNHXKI4knd1SR5LTz75cvJNFp41BtJ7srWnHYY3HarMTGWIm1W7HbLHT1uals7eG1Qw08vauaJKeNK07P4eJFWSzITiQtPoZ4hw2vUrjcXpq7+qlt66WsuZuj9V0cqe/kSH0XTV3vVz+1iC8MY7GRm3NxXhKFqXGkJ8QQa7eO27/yehVepfAoRV+/l9qOXqpaeimp7+RgbQfbjrfQ1OViaEPKSHBwWm4ii/OSWJybxLysBDITHaTGxWCzTGmfzpS2Dz4jtcu477f09FPX3kd1Wy8HazrYXdnGvqo2vArWFqdxz/XLOdbYPZl/p5kG7FYLi/OSuXHtLNp7B3h+Tw1PbK/grucO8KN/HOLceRmcUZBMTpKT9AQHAriN68E7rPnYrRYcNgsOmxWH3YLT+OuwWXDarYPLpvidJuTt39/n6Rvw4hrw0NHnpra9l5q2Xipbeilv7uZEcw8nmrvpMIwVfqwipMbb2XCwjtnp8RRnxDM7PY7sJCcJDhs2q9DtctPc1c+h2g5eOVDPO8ebSY+P4dc3rOBDywJ7v85LieWp28/m7pcP89i2ipOMlp9YVcBXIzRP5H9ceRofWZFPU5eLtcXpU9KHvnVdEY2dLu7ddIwtpc18aFkuC7ITibFaaOpysb+6g92VrRyp91UEjIuxMjczgdnpcSQ4bJw5O5VjDV28W9bCvZuO8evXS0l02DhnXgarilKZl5VAYVocSU47iU7bpDzFpvPNMR8YmgimClg72jpKKbeItOMLrcsH3h22bb4xPdI+04E2pZR7hPWD5vdvHOfHLx0G4IIFmdxz/XKddyJAnHYrywpTyE+NZeeJVqpae/j5hiMjrmsRsFktWIfcvNWQbs/Q2+kpd9aTlo2/zfB788nLRjkYAwFsVsHrBU+A93gRiI+xses/Lw1VWfZArrdxqWrt4ZJfbKZvYOSS6POyErhuZQEXLczi7LnpOO1WPRIY5VhEmJORwI1rZ/GDa5ew4WA9m0saebO0afBlfyg2i2C1CL+6YcVUlLmdzHNk5Lq4Y3Dd795mb1U7/Z6R238giMBpOUn894dP57qVBTy9q5oDNR0T3p8mcJJi7Vx5Ri4P3rqaHSda2F/dzmuHGvB4FUrB4rwk4hxW4uw2Suo68CrodrnJSHRQ197HwZoOo0S4r2MmIiQ6bGQmOrCIkJXooLWnn7ePNVHf0ccwpxdEwG6x8OEVefzkumWTPZyQtv3fbTrG/716hH6P95Rnot0qXLwoi29etpDt5a36nj8Ep93KgEcxLyth3PyY160sYItx33xxXx1P7qgad/8xNgtZiQ5mp8WxuiiV5Fg7bo+iqcuFzSpsLWvh2fdOvQ9bxBf6YbUIFhE8gy/anOKtNZxZaXFkJjo4oyCZLCPHSEfvAHEOG4dqO3jwrbKT8grC+20/KdbGju9dOu5xjUNI2/6R+k6u+c1b9Lu9p1zTfmLtVpYWJPPli+fzwdNzOC03CUAbosKc5Fg7N501m5vOms3+6nae3FHJltImXjvcMKX/xyLgsFn54y2rWDdv0nkgQ9r+L/vlZkobukZt+1aLUJAay+z0eFbMSmF2ejzZSQ7ePNI0aLBt7nJR1+Fia5lvoHosijPi+calC/jMOUUkOoPzTot32PjhtafzxQvnsqmkka4+NyuLUjlz1tQkKDcL//1kqhARvnXFIlYXpfH7N45x3+bjJ933U+LsrChM4cMr8jlvXiZ7qtqwDHkX/8SqwsHp9t4B3jnWxOYjjWwuaeTlA3Wn/D+rRbjprNl8/5olQWudcS4MIvJ54PPG1y4RKRlr/UeAR24bcVEGE7jgTUTrnV5G1Ov44ZjbhDRhRLBtfyROAK8B/3Xy7Kj4rcIc0zV/Ksj1r/jRmJpDnixlKtp/oJQDLwE3Tc3uTP/tp4CQHsNobfXpye024GP4qfEZhYhs+6XA/ROXEK5tOCza5QicpOvoNGgZixOjLwrofMl/jrooIts+xnEfBp4E/m2qxE2ccLuewk0PQManTNB0zn+PuiiDyG3/HJ86SZwANgFfHX/Vk9pVsH3QYJiGfYdM+0T2fwLYAzw0+v6CvqZ/YHxGYdS2P52GqGqgcMj3AmPeSOtUiYgNSMaXdG2sbUea3wykiIjN8Ioa6X8BoJS6n0n1p3yIyA6l1KrJ7idUaL3TSxjoHfd6m6q2P5wwOPagiDS9MKM1T+Y5chLT1f6nm0j87Yejj2FCRE3bD9ffX+sKjhDqCqu2H26/h9YzPuGmydBTFODqYdX+zSLcfsNgiGTtEFr90xkztB2YLyLFIhKDL/n488PWeR64xZi+DnjdiG99HrheRBxGNbz5wLbR9mlss9HYB8Y+n5vGY9Nowo1ArjeNJtKYzHNEo4lkdNvXzFR029fMZHT718wYps0jyohZ/TLwCr7ykw8qpQ6IyA+BHUqp54E/An82kpG34LvYMNZ7El9iNjfwJaWUB2CkfRr/8tvA4yLy38BuY98azYxgtOvNZFkazaSYzHNEo4lkdNvXzFR029fMZHT718wkRBtQJ4aIfN5weYwItN7pJdL0TiWRduyRphe05plMNJxHfQwzm3A9d1pXcISrrukm3I5b6xmfcNMUbnoigUg+Z5GsHUKrXxuiNBqNRqPRaDQajUaj0Wg0ISEkdeU1Go1Go9FoNBqNRqPRaDQabYgKEhG5QkRKRKRURL5jth4AEXlQRBpEZP+QeWkiskFEjhp/U435IiK/MvTvFZEzTdBbKCIbReSgiBwQka9GgGaniGwTkT2G5h8Y84tFZKuh7QkjsSBGov0njPlbRaQo1JonwnjtW0Rmi8hrxu+wSUQKhiy7W0T2G59PDpk/4jkKc80PiUiZiLxnfJZPod5Trtdhy0dt7yJyi3F9HBWRW4bMXyki+4xtfiUiMlV6p1HzJuN385/jrKnUHEmMdn8Ztk7Y3lMC1H+riDQO+b0/Z4bW8RARq4jsFpEXRlgWtr9BOBDAvTjkbWAy9y6TdV0oIu1DztVdIdI1Yv9s2DqmnLNQIyJfN87BfhF5TEScJmgIuH9vop6fishhoy08IyIpodIzmqYhy/5NRJSIZJitR0S+YpynAyLyk1DpiUTGeg6HOyKSIiLrjd/6kIicbbamQDHlnqeU0p8AP/iSxh0D5gAxwB5gcRjoOh84E9g/ZN5PgO8Y098B7jamrwReAgQ4C9hqgt5c4ExjOhE4AiwOc80CJBjTdmCroeVJ4Hpj/n3AF43pfwXuM6avB54wu50EcIzjtm/gKeAWY/pi4M/G9FXABnwFEOLxVf1IMpaNeI7CXPNDwHXTdJ5PuV6HLR+xvQNpwHHjb6oxnWos22asK8a2H4wAzZuAVWa3+3D4jHZ/GbZO2N5TAtR/K/Abs7UGcCzfAB4FXhhhWdj+BmZ/ArwXh7wNTPTeFQa6LhypDYZA14j9s3A4ZyE+D/lAGRBrfH8SuNUEHQH3703UcxlgM6bvDqWe0TQZ8wvxJfw+AWSYfI4uAl4FHMb3rFC3pUj6jPUcDvcP8DDwOWM6BkgxW1OAuk2552mPqOBYA5QqpY4rpfqBx4FrTdaEUuoNfFUThnItvosB4++Hh8x/RPl4F0gRkdzQKPWhlKpVSu0ypjuBQ/gugHDWrJRSXcZXu/FR+Awb60fR7D+W9cAHRKbWS2UaCKR9LwZeN6Y3Dlm+GHhDKeVWSnUDe4ErjGMe7RyFpeYp1DYio1yvQxmtvV8ObFBKtSilWvEZ0a4wliUppd5VvqfHI0ztOZ5yzVOpLRoY4/4ylLC9pwSoP+wRn7fkVcADo6wStr9BGBBJ/aOhmNK/CECXKYzRPxuK6X2yEGEDYkXEBsQBNaEWEGT/3hQ9Sql/KqXcxtd3gYJTNgyxJoNfAt8ixM+iUfR8EfixUsplrNMQSk2RRADP4bBFRJLxGSL/CKCU6ldKtZmrKihCfs/ThqjgyAcqh3yv4tQHdLiQrZSqNabrgGxjOqyOwQhtWIFvBD2sNRuuou8BDfheqI8BbUMewEN1DWo2lrcD6aFVHDSBnOc9wEeN6Y8AiSKSbsy/QkTiDBfoi/CNRqUz+jkKV81+fmS4mv9SRBxTqHk8RjumseZXjTA/lASr2c+fjLCT/5zpL/TD7y9Kqa3DVgnre0oA+gE+ZlxT60WkcITlZvN/+F5cvKMsD+vfwGQCfU6HWxsIi/7FKJwtvnDXl0RkSaj/+bD+2VDC+ZxNCUqpauBnQAVQC7Qrpf5prqpBRusrhwOfxectZyoici1QrZTaY7YWgwXAeeIL6d4sIqvNFhTGjPccDmeKgUZ8fdvdIvKAiMSbLSoQzLrnaUPUDMDwkgi70WkRSQD+BnxNKdUxdFk4alZKeZRSy/GN9qwBFpksyQy+CVwgIruBC4BqwGPcrF4E3gYeA94BPKapPJmJaL4T3++7Gl9Y2bdDLXoG8Cml1BnAecbnJpP1mMrw+4uInG62pmAIQP/fgSKl1FJ8hvyHh+/DTETkaqBBKbXTbC1RTFi3gTBjFzBbKbUM+DXwbCj/+Vj9s5mAkXfpWnwvlnlAvIh82lxVpxJOfWUR+S7gBv5qso444D+AkORVCxAbvr7kWcC/A0/O9MG3kYiC57ANX1jm75RSK4BufOGzYY9Z9zxtiAqOak72mCgw5oUj9X5XaeOv3w00LI5BROz4Ojl/VUo9bcwOa81+DDfLjcDZ+FzSbSPoGtRsLE8GmkMsNVjGPc9KqRql1EeNG+x3jXltxt8fKaWWK6UuxZc74gi+Yx7tHIWrZn94gjLcqP+Ez/AYKkY7prHmF4wwP5QEq9k/+uIP/3iU0J7jsGXI/WV4CGNE3FNG06+UavaHJeBzuV8Zam3jcA5wjYiU4wsru1hE/jJsnYj4DUwikHtxOLaBsOpf+FFKdfjDXZVSLwL2UCVcHqV/NpSwPGdTzCVAmVKqUSk1ADwNrDNZk5/R+sqmISK3AlfjG2Ay2zA2F9/L9B7jfl4A7BKRHBM1VQFPG/3Kbfi8fUKWQD2CCOQ5HM5UAVVDPMLX4zNMRQKm3PO0ISo4tgPzxVcFLAZfstLnTdY0Gs8D/gpVtwDPDZl/s/g4C5/rXe1IO5gujFGAPwKHlFK/GLIonDVnilEJRERigUvx5U7YCFw3imb/sVwHvB4GD+fxGLd9i0iGiPjvG3cCDxrzrUa4GyKyFFgK/NM45tHOUVhqNr77O3mCL//CiJWNponR2vsrwGUikmqMXFwGvGIs6xCRswy9NzO153jKNYuIzf9SZbz0XE1oz3FYMcr95fCw1cL2nhKIfjk5h8w1+O6fYYNS6k6lVIFSqgjffeR1pdTw0cCw/Q3CgEDuxeHYBkzvX4yEiOT4PSZEZA2+/vq0Gz3H6J8NJSzP2RRTAZwlvtB9AT5AeLRXGL2vbAoicgW+UKprlFI9ZmoBUErtU0plKaWKjPt5Fb4E/HUmynoWX/oHRGQBviTWTSbqCUsCfA6HLUYbqxSRhcasDwAHTZQUDObc81QYZGqPpA++aiFH8OUH+q7ZegxNj+GL5xzAd8O9DV/eiteAo/gqNaQZ6wrwW0P/PkyoWgWci8+VeC/wnvG5Msw1LwV2G5r3A3cZ8+fgq1hWiq86m78ihtP4Xmosn2N2OwnwOE9p38AP8XUwwPfyddRY54Fhx3vQ+LwLLB+yzxHPUZhrft1oa/uBv2BUBJsivSNdr7cDt4/X3vHlXyg1Pp8ZMn+VofUY8BtApvgcT6lmfFUKdxrX0wHgHsBqdvs36zPG/WVoOw7be0qA+v/X+K334DNOLzJb9xjHcyFGtZ5I+Q3C4RPAvTjkbWAy9y6TdX15yLl6F1gXIl2j9c9MP2eh/gA/wGdQ3w/8mSnuu0yinYzYVzZRTym+nGH+9nKf2edo2PJyQls1b6RzFIOvL7kfX9jtxaFuS5H2waTKoVOgezmww7iHPotRKToSPmbc88T4xxqNRqPRaDQajUaj0Wg0Gs20okPzNBqNRqPRaDQajUaj0Wg0IUEbojQajUaj0Wg0Go1Go9FoNCFBG6I0Go1Go9FoNBqNRqPRaDQhQRuiNBqNRqPRaDQajUaj0Wg0IUEbojQajUaj0Wg0Go1Go9FoNCFBG6JMRkRSRORfA1ivSERuDHC9/VOg6/si8k1jepGIvCciu0Vk7mT3beyzXEQyjOm3J7iP20Xk5hHmT8k50EQ/Q9v5FO3vReOaDui61mgiDRHZJCKrhnzX91tNRCIit4rIbya7zgjbfE1E4ianTqOZfob2xYPc7iERuS6I9fVzQhPxDG3HIrJKRH5lTF8oIuvMVReZaEOU+aQAgbywFgHjGqKmiQ8D65VSK5RSxwLZQERsge5cKTWhi1cpdZ9S6pGJbKvRTAdKqSuVUm0Efl1rNBqNJrr4GqANURqNRhOlKKV2KKXuML5eCGhD1ATQhijz+TEw1/A4+qn4+KmI7BeRfSLyySHrnWes93XDKvumiOwyPmNeACKSKyJvGNvvF5HzjPldQ9a5TkQeGrbdlfg6VV8UkY3DRzVE5Jsi8n1jepOI/J+I7AC+Omw/6SLyTxE5ICIPADJkWZfxd8RjF5F7ROQuY/py4zgsw7y2VorIHhHZA3xpyL6txj63i8heEfnC+D+JJpoRke+KyBEReQtYaMybKyIvi8hO47paZMx/SER+JSJvi8hx/wjgGNeTf3Rx+HX9iIh8eIiGv4rItSE/eI0mQIx7/WGjrR4SkfXay0MT7ohIvIj8w+gP7BeRT8rJHtirRGTTCNs9JCL3icgO4/lw9ZDFecbz4aiI/GTINr8z1j8gIj8w5t0B5AEbRWSjMe8yEXnH6Ks9JSIJxvwfi8hBo2/ys+k7KxoNiMizRh/ngIh8foTlNxttcY+I/NmYVyQirxvzXxORWUM2OX+EvtGI/XiNxmyG9v1F5DHj/XXQw1tEMkSk3Jge9x1bfF5QL4hIEXA78HWjz3+eiJSJiN1YL2nod83JBOy1opk2vgOcrpRaDiAiHwOWA8uADGC7iLxhrPdNpdTVxnpxwKVKqT4RmQ88Bqwa6R8Y3Ai8opT6kYhYCXC0Tin1oojcB3QppX5mXHBjEaOUGknH/wPeUkr9UESuAm4bYZ2PMvKx32lMvwn8CrhSKeUVkaHb/gn4slLqDRH56ZD5twHtSqnVIuIAtojIP5VSZeMduyb6EJGVwPX42pkN2AXsBO4HbldKHRWRtcC9wMXGZrnAucAi4HlgPeNfT8Ov6wuArwPPikgyvpGTW6btQDWaqWEhcJtSaouIPMj7Xn5/FZFeYzoG8JqiTqM5lSuAGqXUVQDG/fbuALctAtYAc/EZkuYZ85cDKwAXUCIiv1ZKVQLfVUq1GM+A10RkqVLqVyLyDeAipVSTYQD7HnCJUqpbRL4NfENEfgt8BFiklFIikjIlR6/RjM5njfYai69P/Tf/AhFZgq+drvv/7d1bjF1lGcbx/1M0lChUJUhioqAG4qnR1GhsFFIVTeohGgQNKpXRmGC0JqAlXnCj4oXB6AUmRAEtUEUoCMaixQZDqTV4SJE0VKqJEojAgLVUdFJT2seL95vpnuk+zK5z6vj8rtZeh73WStba6/2+9X7vbtfti9qiq4DrbV8v6ZNUDD7+Uq1bbNQrjo+YN31i/16eZJptbNsPd7aT2/7uAd4L3NH2+2PbB2bodBaVZEQtPG8DbrJ90PYosBV4U5f1ngtcI2knsBF4zYDv/R0wospeWm77mRk85k4395h/NrABwPadwN4u63Q9d9tjwKeBLcC3pw4PbAHcC2yPP+xu7Fj8bmCNpD8AvwFOBs44qjOLxeAs4HbbY7b/SQVPS6mOoY3tOvkOFWCNu8P2Idu7gFPbvKHuJ9tbgTMknQJcANxm+9mZPLGIWfCo7e1tegP1Gw3wMdtvaB2t75mfQ4voaifwLklfl3SW7X1DbHtL+63/M/AXqoENcLftfbb3A7uA09r8D0vaAdwPvJbucdhb2vzt7fnyibb9PmA/cJ2kc4Gx4U4zYmifV40auA94KZNj4XcAG23/HcD2P9r8lcAP2/SNHH4GQPfYaLptmIi51C3272fYNvZU1wIjbXqESpaILpIRdey6BBil3josoQKanlqm0NlUD+16Sd9s9ZXcsdrSaez3WSZ3YE7d5t/T+I6jsRzYQ6W8D0PAWtt3zfwhxSKxBHh6PHupi/90TAv63k/93AB8nHo7MjJg3YiFwAM+Rywotv8kaQXVQXqFpLuZHLf0i3N6Xe+dz4CDwHMkvRz4IvWybK+qrEG37xawxfYFRyyQ3gy8EzgP+ByHs3AjZpSkVcA5wErbYy1jYzoxfz9HxEYRx5hez4ah2thTtSzy09t9d5ztFOrvIRlR8+8Z4MSOz9uAj6hqG51CZRL9tst6y4DHbR8CLgSO67cTSacBo7avoXpqV7RFo5JeLWkJlSY+yCjwYlXNp+OB9w3aoLmXVmxd0mrghV3W6Xru7di/QKXGr25Dpya04tBPS5p4W9+x+C6qvtX4WN0zJT1vmscci8+9wAclnSDpROD91Jvov0o6HyZqHLy+35f0uZ/GTb1fAdZT9dZobxAjFrqXSVrZpj8K/Go+DyZiEEkvAcZsbwCupH6bHwbe2Fb5UJ/Nz1fVn3wl8Apgd591T6JevO2TdCqwumNZ5+//fcBbx4f5qWpYnamqE7XM9s+oRk/fZ07E/2gZsLd1Qr2KytTr9Evq+j8ZoGNo3q+pl2dQsfW2Afvp1YaJmE/dYn+Y/Gzo/BfIodrYdI/5b6CyCZMN1UcyouaZ7T2StqsKgP8cuIxKhX2Aeht3me0nJO0BDra02vVUDZvbJK0BNjM4E2kVsE7SAeBfwJo2/0vAJuAp4PfA8wcc7wFJX6EeLH8DHprmqX4ZuEnSg9SD7ZEu69zOlHOnOr62UPWxHpP0KSoDZWqq7wjwPUkGftEx/1qq7sMOVVGppzg8vj3+z9jeIelm6hp7khpiBxVgXS3pciol90dtnV5W0f1+Gt/PpPva9jrbo5L+SI0ZjzgW7AY+2+pD7QKu5nAAF7EQLQeulHQIOAB8BjiBGgL3VeCePts+QsU2J1E1A/dPqUU5wfYDku6nYqBHge0di78LbJb0mO23S7qIin+Ob8svpxouP5G0lMomufRoTjZimjYDF7cYZDfVQTrB9oOSvgZslXSQGm56EbAW+L6kdVT8PCib+4g4vrVhTp+5U4kYTp/Y/xvALari/Xd2bDJsG/unwK2qPyFaa3sb8APgCqq+VPQgO5n2ERGzTfUHAzuBFUPWLYmYc63hsMn26+b5UCJmXRtat8n2rfN9LBERMXtafdeJ4uKztI/zgA/YvnC29rEYJCMqImKWSToHuA74VjqhIiIiIiIWH0lXUcO182cuAyQjKiIiIiIiIiIi5kSKlUdERERERERExJxIR1RERERERERERMyJdERFRERERERERMScSEdURERERERERETMiXRERURERERERETEnEhHVEREREREREREzIn/AhgFcJ0vy+erAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "fig,ax = plt.subplots(ncols = 6, nrows = 2,figsize = (20,10))\n",
        "ax = ax.flatten()\n",
        "index = 0\n",
        "for col in df.columns:\n",
        "    sns.distplot(df[col], ax = ax[index])\n",
        "    index+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "BtPeHxCJYdBo",
        "outputId": "0e22b620-80a8-4ddc-dfe2-ef5a4ae4d0ce"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#heatmap correlation\n",
        "plt.figure(figsize = (10,6))\n",
        "sns.heatmap(df.corr(), vmax = 0.9, square = True)\n",
        "plt.title(\"Correlations\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modeling using ANN**"
      ],
      "metadata": {
        "id": "hkod0K0vkxIM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "OZfZow0uW1CX"
      },
      "outputs": [],
      "source": [
        "x = df.drop([\"quality\"], axis =1)\n",
        "y = df['quality']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "MS9OLS5rXMUS"
      },
      "outputs": [],
      "source": [
        "scaler  =StandardScaler()\n",
        "x = scaler.fit_transform(x)\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "QxGkrFtHf2AC"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "def build_model(n_hidden=1, n_neurons=30, learning_rate=0.01, input_shape=[11]):\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
        "  model.add(keras.layers.BatchNormalization()),\n",
        "  for layer in range(n_hidden):\n",
        "    model.add(keras.layers.Dense(n_neurons, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
        "  model.add(keras.layers.Dense(1,activation='linear'))\n",
        "  optimizer = keras.optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999)\n",
        "  model.compile(optimizer =\"adam\",\n",
        "                loss='mean_squared_error',\n",
        "                metrics=['mse', 'mae']\n",
        "                )\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WvTcai1hnTY",
        "outputId": "405f8260-504a-4188-c7f0-0e9dcf266e48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-85-651c14c6d32f>:1: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n"
          ]
        }
      ],
      "source": [
        "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjqZpM4DoizY",
        "outputId": "89806bce-1c70-417e-cbf0-d8a1642bb040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 14ms/step - loss: 8.6943 - mse: 8.6943 - mae: 2.4549 - val_loss: 4.4176 - val_mse: 4.4176 - val_mae: 1.6466\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 3.6367 - mse: 3.6367 - mae: 1.5044 - val_loss: 2.9531 - val_mse: 2.9531 - val_mae: 1.4074\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.4379 - mse: 2.4379 - mae: 1.2515 - val_loss: 2.6338 - val_mse: 2.6338 - val_mae: 1.2886\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.3043 - mse: 2.3043 - mae: 1.2041 - val_loss: 2.4460 - val_mse: 2.4460 - val_mae: 1.2413\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.0346 - mse: 2.0346 - mae: 1.1032 - val_loss: 2.2595 - val_mse: 2.2595 - val_mae: 1.1863\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.8767 - mse: 1.8767 - mae: 1.0727 - val_loss: 2.0865 - val_mse: 2.0865 - val_mae: 1.1460\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.8148 - mse: 1.8148 - mae: 1.0686 - val_loss: 1.9117 - val_mse: 1.9117 - val_mae: 1.0911\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.6245 - mse: 1.6245 - mae: 1.0074 - val_loss: 1.8428 - val_mse: 1.8428 - val_mae: 1.0758\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.5744 - mse: 1.5744 - mae: 0.9904 - val_loss: 1.7298 - val_mse: 1.7298 - val_mae: 1.0415\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.4707 - mse: 1.4707 - mae: 0.9577 - val_loss: 1.7041 - val_mse: 1.7041 - val_mae: 1.0655\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.4872 - mse: 1.4872 - mae: 0.9694 - val_loss: 1.5256 - val_mse: 1.5256 - val_mae: 0.9895\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.2359 - mse: 1.2359 - mae: 0.8841 - val_loss: 1.4813 - val_mse: 1.4813 - val_mae: 0.9656\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.2129 - mse: 1.2129 - mae: 0.8744 - val_loss: 1.4235 - val_mse: 1.4235 - val_mae: 0.9653\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1791 - mse: 1.1791 - mae: 0.8516 - val_loss: 1.3070 - val_mse: 1.3070 - val_mae: 0.9167\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0345 - mse: 1.0345 - mae: 0.8066 - val_loss: 1.2584 - val_mse: 1.2584 - val_mae: 0.8975\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0908 - mse: 1.0908 - mae: 0.8376 - val_loss: 1.1940 - val_mse: 1.1940 - val_mae: 0.8819\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0444 - mse: 1.0444 - mae: 0.8063 - val_loss: 1.1601 - val_mse: 1.1601 - val_mae: 0.8637\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.9583 - mse: 0.9583 - mae: 0.7833 - val_loss: 1.0918 - val_mse: 1.0918 - val_mae: 0.8296\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.9543 - mse: 0.9543 - mae: 0.7717 - val_loss: 1.0745 - val_mse: 1.0745 - val_mae: 0.8153\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.8839 - mse: 0.8839 - mae: 0.7375 - val_loss: 1.0196 - val_mse: 1.0196 - val_mae: 0.8012\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8370 - mse: 0.8370 - mae: 0.7239 - val_loss: 0.9556 - val_mse: 0.9556 - val_mae: 0.7784\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8479 - mse: 0.8479 - mae: 0.7219 - val_loss: 0.9707 - val_mse: 0.9707 - val_mae: 0.7927\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8779 - mse: 0.8779 - mae: 0.7406 - val_loss: 0.9099 - val_mse: 0.9099 - val_mae: 0.7482\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7486 - mse: 0.7486 - mae: 0.6731 - val_loss: 0.8768 - val_mse: 0.8768 - val_mae: 0.7341\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7470 - mse: 0.7470 - mae: 0.6865 - val_loss: 0.8596 - val_mse: 0.8596 - val_mae: 0.7406\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7030 - mse: 0.7030 - mae: 0.6511 - val_loss: 0.8255 - val_mse: 0.8255 - val_mae: 0.7223\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6472 - mse: 0.6472 - mae: 0.6256 - val_loss: 0.7998 - val_mse: 0.7998 - val_mae: 0.7140\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6279 - mse: 0.6279 - mae: 0.6175 - val_loss: 0.8109 - val_mse: 0.8109 - val_mae: 0.7153\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6153 - mse: 0.6153 - mae: 0.6062 - val_loss: 0.7639 - val_mse: 0.7639 - val_mae: 0.6789\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5716 - mse: 0.5716 - mae: 0.5862 - val_loss: 0.7500 - val_mse: 0.7500 - val_mae: 0.6912\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5797 - mse: 0.5797 - mae: 0.5909 - val_loss: 0.7310 - val_mse: 0.7310 - val_mae: 0.6713\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5879 - mse: 0.5879 - mae: 0.5991 - val_loss: 0.7371 - val_mse: 0.7371 - val_mae: 0.6697\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5659 - mse: 0.5659 - mae: 0.5852 - val_loss: 0.6757 - val_mse: 0.6757 - val_mae: 0.6492\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5598 - mse: 0.5598 - mae: 0.5812 - val_loss: 0.6864 - val_mse: 0.6864 - val_mae: 0.6389\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5162 - mse: 0.5162 - mae: 0.5627 - val_loss: 0.6521 - val_mse: 0.6521 - val_mae: 0.6282\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5108 - mse: 0.5108 - mae: 0.5619 - val_loss: 0.6146 - val_mse: 0.6146 - val_mae: 0.6166\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4651 - mse: 0.4651 - mae: 0.5368 - val_loss: 0.6177 - val_mse: 0.6177 - val_mae: 0.6234\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5085 - mse: 0.5085 - mae: 0.5506 - val_loss: 0.5872 - val_mse: 0.5872 - val_mae: 0.6024\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4891 - mse: 0.4891 - mae: 0.5471 - val_loss: 0.5794 - val_mse: 0.5794 - val_mae: 0.5954\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4531 - mse: 0.4531 - mae: 0.5144 - val_loss: 0.5602 - val_mse: 0.5602 - val_mae: 0.5881\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4273 - mse: 0.4273 - mae: 0.5021 - val_loss: 0.5707 - val_mse: 0.5707 - val_mae: 0.5733\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4950 - mse: 0.4950 - mae: 0.5324 - val_loss: 0.5367 - val_mse: 0.5367 - val_mae: 0.5723\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4354 - mse: 0.4354 - mae: 0.5016 - val_loss: 0.5153 - val_mse: 0.5153 - val_mae: 0.5650\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4096 - mse: 0.4096 - mae: 0.5028 - val_loss: 0.5839 - val_mse: 0.5839 - val_mae: 0.6103\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4226 - mse: 0.4226 - mae: 0.5041 - val_loss: 0.5206 - val_mse: 0.5206 - val_mae: 0.5618\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3929 - mse: 0.3929 - mae: 0.4913 - val_loss: 0.5324 - val_mse: 0.5324 - val_mae: 0.5614\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3887 - mse: 0.3887 - mae: 0.4726 - val_loss: 0.5484 - val_mse: 0.5484 - val_mae: 0.5713\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3757 - mse: 0.3757 - mae: 0.4708 - val_loss: 0.5235 - val_mse: 0.5235 - val_mae: 0.5638\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3651 - mse: 0.3651 - mae: 0.4635 - val_loss: 0.5149 - val_mse: 0.5149 - val_mae: 0.5560\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4251 - mse: 0.4251 - mae: 0.5055 - val_loss: 0.5081 - val_mse: 0.5081 - val_mae: 0.5550\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3785 - mse: 0.3785 - mae: 0.4817 - val_loss: 0.4964 - val_mse: 0.4964 - val_mae: 0.5486\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3939 - mse: 0.3939 - mae: 0.4909 - val_loss: 0.5523 - val_mse: 0.5523 - val_mae: 0.5802\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4116 - mse: 0.4116 - mae: 0.5023 - val_loss: 0.5450 - val_mse: 0.5450 - val_mae: 0.5664\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3877 - mse: 0.3877 - mae: 0.4738 - val_loss: 0.4972 - val_mse: 0.4972 - val_mae: 0.5445\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3571 - mse: 0.3571 - mae: 0.4645 - val_loss: 0.4926 - val_mse: 0.4926 - val_mae: 0.5427\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3330 - mse: 0.3330 - mae: 0.4435 - val_loss: 0.4865 - val_mse: 0.4865 - val_mae: 0.5404\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3614 - mse: 0.3614 - mae: 0.4589 - val_loss: 0.4891 - val_mse: 0.4891 - val_mae: 0.5415\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3480 - mse: 0.3480 - mae: 0.4522 - val_loss: 0.4809 - val_mse: 0.4809 - val_mae: 0.5372\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3624 - mse: 0.3624 - mae: 0.4665 - val_loss: 0.4953 - val_mse: 0.4953 - val_mae: 0.5432\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3339 - mse: 0.3339 - mae: 0.4539 - val_loss: 0.5017 - val_mse: 0.5017 - val_mae: 0.5463\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3365 - mse: 0.3365 - mae: 0.4357 - val_loss: 0.4958 - val_mse: 0.4958 - val_mae: 0.5357\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3317 - mse: 0.3317 - mae: 0.4471 - val_loss: 0.4811 - val_mse: 0.4811 - val_mae: 0.5294\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3474 - mse: 0.3474 - mae: 0.4539 - val_loss: 0.4985 - val_mse: 0.4985 - val_mae: 0.5468\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3150 - mse: 0.3150 - mae: 0.4350 - val_loss: 0.4792 - val_mse: 0.4792 - val_mae: 0.5384\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3481 - mse: 0.3481 - mae: 0.4543 - val_loss: 0.4852 - val_mse: 0.4852 - val_mae: 0.5485\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3233 - mse: 0.3233 - mae: 0.4342 - val_loss: 0.4705 - val_mse: 0.4705 - val_mae: 0.5317\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3615 - mse: 0.3615 - mae: 0.4589 - val_loss: 0.4742 - val_mse: 0.4742 - val_mae: 0.5299\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3392 - mse: 0.3392 - mae: 0.4495 - val_loss: 0.5069 - val_mse: 0.5069 - val_mae: 0.5423\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3057 - mse: 0.3057 - mae: 0.4239 - val_loss: 0.4628 - val_mse: 0.4628 - val_mae: 0.5232\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2917 - mse: 0.2917 - mae: 0.4173 - val_loss: 0.4789 - val_mse: 0.4789 - val_mae: 0.5322\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2903 - mse: 0.2903 - mae: 0.4203 - val_loss: 0.5306 - val_mse: 0.5306 - val_mae: 0.5556\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3045 - mse: 0.3045 - mae: 0.4304 - val_loss: 0.4957 - val_mse: 0.4957 - val_mae: 0.5456\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3351 - mse: 0.3351 - mae: 0.4544 - val_loss: 0.4721 - val_mse: 0.4721 - val_mae: 0.5321\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2742 - mse: 0.2742 - mae: 0.4099 - val_loss: 0.4921 - val_mse: 0.4921 - val_mae: 0.5406\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3130 - mse: 0.3130 - mae: 0.4352 - val_loss: 0.4613 - val_mse: 0.4613 - val_mae: 0.5170\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2945 - mse: 0.2945 - mae: 0.4205 - val_loss: 0.4828 - val_mse: 0.4828 - val_mae: 0.5309\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3185 - mse: 0.3185 - mae: 0.4202 - val_loss: 0.4500 - val_mse: 0.4500 - val_mae: 0.5166\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2805 - mse: 0.2805 - mae: 0.4028 - val_loss: 0.5004 - val_mse: 0.5004 - val_mae: 0.5412\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2956 - mse: 0.2956 - mae: 0.4292 - val_loss: 0.4560 - val_mse: 0.4560 - val_mae: 0.5177\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3028 - mse: 0.3028 - mae: 0.4216 - val_loss: 0.4581 - val_mse: 0.4581 - val_mae: 0.5207\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.2763 - mse: 0.2763 - mae: 0.4110 - val_loss: 0.4597 - val_mse: 0.4597 - val_mae: 0.5088\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.2706 - mse: 0.2706 - mae: 0.3929 - val_loss: 0.4698 - val_mse: 0.4698 - val_mae: 0.5156\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2790 - mse: 0.2790 - mae: 0.4052 - val_loss: 0.4789 - val_mse: 0.4789 - val_mae: 0.5305\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2748 - mse: 0.2748 - mae: 0.4053 - val_loss: 0.4585 - val_mse: 0.4585 - val_mae: 0.5229\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2826 - mse: 0.2826 - mae: 0.4088 - val_loss: 0.4629 - val_mse: 0.4629 - val_mae: 0.5230\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3573 - mse: 0.3573 - mae: 0.4572 - val_loss: 0.5022 - val_mse: 0.5022 - val_mae: 0.5352\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3001 - mse: 0.3001 - mae: 0.4187 - val_loss: 0.4765 - val_mse: 0.4765 - val_mae: 0.5322\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3279 - mse: 0.3279 - mae: 0.4424 - val_loss: 0.4678 - val_mse: 0.4678 - val_mae: 0.5289\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2830 - mse: 0.2830 - mae: 0.4127 - val_loss: 0.4767 - val_mse: 0.4767 - val_mae: 0.5249\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2832 - mse: 0.2832 - mae: 0.4085 - val_loss: 0.4573 - val_mse: 0.4573 - val_mae: 0.5118\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2579 - mse: 0.2579 - mae: 0.3932 - val_loss: 0.5304 - val_mse: 0.5304 - val_mae: 0.5690\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3043 - mse: 0.3043 - mae: 0.4270 - val_loss: 0.4652 - val_mse: 0.4652 - val_mae: 0.5282\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2596 - mse: 0.2596 - mae: 0.3969 - val_loss: 0.5055 - val_mse: 0.5055 - val_mae: 0.5530\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3062 - mse: 0.3062 - mae: 0.4292 - val_loss: 0.5072 - val_mse: 0.5072 - val_mae: 0.5508\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2547 - mse: 0.2547 - mae: 0.3937 - val_loss: 0.4805 - val_mse: 0.4805 - val_mae: 0.5172\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.2537 - mse: 0.2537 - mae: 0.3855 - val_loss: 0.4609 - val_mse: 0.4609 - val_mae: 0.5167\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2934 - mse: 0.2934 - mae: 0.4226 - val_loss: 0.4853 - val_mse: 0.4853 - val_mae: 0.5430\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2881 - mse: 0.2881 - mae: 0.4187 - val_loss: 0.4753 - val_mse: 0.4753 - val_mae: 0.5279\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2693 - mse: 0.2693 - mae: 0.3996 - val_loss: 0.4612 - val_mse: 0.4612 - val_mae: 0.5181\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2715 - mse: 0.2715 - mae: 0.3954 - val_loss: 0.4561 - val_mse: 0.4561 - val_mae: 0.5206\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2727 - mse: 0.2727 - mae: 0.4105 - val_loss: 0.4495 - val_mse: 0.4495 - val_mae: 0.5154\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3019 - mse: 0.3019 - mae: 0.4260 - val_loss: 0.4448 - val_mse: 0.4448 - val_mae: 0.5152\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2961 - mse: 0.2961 - mae: 0.4189 - val_loss: 0.4442 - val_mse: 0.4442 - val_mae: 0.5170\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2824 - mse: 0.2824 - mae: 0.4117 - val_loss: 0.4906 - val_mse: 0.4906 - val_mae: 0.5470\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2681 - mse: 0.2681 - mae: 0.4042 - val_loss: 0.4870 - val_mse: 0.4870 - val_mae: 0.5303\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2752 - mse: 0.2752 - mae: 0.4050 - val_loss: 0.4498 - val_mse: 0.4498 - val_mae: 0.5146\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2752 - mse: 0.2752 - mae: 0.4030 - val_loss: 0.4731 - val_mse: 0.4731 - val_mae: 0.5199\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2586 - mse: 0.2586 - mae: 0.3959 - val_loss: 0.4679 - val_mse: 0.4679 - val_mae: 0.5247\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3034 - mse: 0.3034 - mae: 0.4243 - val_loss: 0.5762 - val_mse: 0.5762 - val_mae: 0.5897\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2602 - mse: 0.2602 - mae: 0.3952 - val_loss: 0.4715 - val_mse: 0.4715 - val_mae: 0.5195\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2372 - mse: 0.2372 - mae: 0.3759 - val_loss: 0.4814 - val_mse: 0.4814 - val_mae: 0.5227\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2821 - mse: 0.2821 - mae: 0.4111 - val_loss: 0.4958 - val_mse: 0.4958 - val_mae: 0.5388\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2613 - mse: 0.2613 - mae: 0.3979 - val_loss: 0.4877 - val_mse: 0.4877 - val_mae: 0.5339\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2393 - mse: 0.2393 - mae: 0.3758 - val_loss: 0.4714 - val_mse: 0.4714 - val_mae: 0.5229\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2505 - mse: 0.2505 - mae: 0.3821 - val_loss: 0.4563 - val_mse: 0.4563 - val_mae: 0.5126\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2575 - mse: 0.2575 - mae: 0.3943 - val_loss: 0.4739 - val_mse: 0.4739 - val_mae: 0.5186\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2502 - mse: 0.2502 - mae: 0.3777 - val_loss: 0.4685 - val_mse: 0.4685 - val_mae: 0.5228\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2540 - mse: 0.2540 - mae: 0.3937 - val_loss: 0.4576 - val_mse: 0.4576 - val_mae: 0.5206\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2527 - mse: 0.2527 - mae: 0.3826 - val_loss: 0.5118 - val_mse: 0.5118 - val_mae: 0.5358\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2602 - mse: 0.2602 - mae: 0.3880 - val_loss: 0.4691 - val_mse: 0.4691 - val_mae: 0.5194\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2374 - mse: 0.2374 - mae: 0.3705 - val_loss: 0.4505 - val_mse: 0.4505 - val_mae: 0.5119\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2710 - mse: 0.2710 - mae: 0.3879 - val_loss: 0.4583 - val_mse: 0.4583 - val_mae: 0.5151\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2359 - mse: 0.2359 - mae: 0.3724 - val_loss: 0.4634 - val_mse: 0.4634 - val_mae: 0.5190\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2428 - mse: 0.2428 - mae: 0.3712 - val_loss: 0.4609 - val_mse: 0.4609 - val_mae: 0.5184\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2124 - mse: 0.2124 - mae: 0.3524 - val_loss: 0.4642 - val_mse: 0.4642 - val_mae: 0.5223\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2546 - mse: 0.2546 - mae: 0.3951 - val_loss: 0.4778 - val_mse: 0.4778 - val_mae: 0.5226\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2523 - mse: 0.2523 - mae: 0.3869 - val_loss: 0.4427 - val_mse: 0.4427 - val_mae: 0.5053\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.2465 - mse: 0.2465 - mae: 0.3866 - val_loss: 0.4586 - val_mse: 0.4586 - val_mae: 0.5153\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2789 - mse: 0.2789 - mae: 0.4025 - val_loss: 0.5434 - val_mse: 0.5434 - val_mae: 0.5648\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2610 - mse: 0.2610 - mae: 0.3992 - val_loss: 0.4459 - val_mse: 0.4459 - val_mae: 0.5052\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2454 - mse: 0.2454 - mae: 0.3805 - val_loss: 0.4511 - val_mse: 0.4511 - val_mae: 0.5236\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2443 - mse: 0.2443 - mae: 0.3827 - val_loss: 0.4319 - val_mse: 0.4319 - val_mae: 0.5021\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2299 - mse: 0.2299 - mae: 0.3716 - val_loss: 0.4491 - val_mse: 0.4491 - val_mae: 0.5065\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2316 - mse: 0.2316 - mae: 0.3734 - val_loss: 0.4773 - val_mse: 0.4773 - val_mae: 0.5218\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2535 - mse: 0.2535 - mae: 0.3847 - val_loss: 0.4620 - val_mse: 0.4620 - val_mae: 0.5115\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2418 - mse: 0.2418 - mae: 0.3837 - val_loss: 0.4352 - val_mse: 0.4352 - val_mae: 0.4916\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2458 - mse: 0.2458 - mae: 0.3793 - val_loss: 0.4457 - val_mse: 0.4457 - val_mae: 0.4990\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2148 - mse: 0.2148 - mae: 0.3570 - val_loss: 0.4681 - val_mse: 0.4681 - val_mae: 0.5065\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2516 - mse: 0.2516 - mae: 0.3791 - val_loss: 0.4385 - val_mse: 0.4385 - val_mae: 0.5003\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2282 - mse: 0.2282 - mae: 0.3706 - val_loss: 0.4821 - val_mse: 0.4821 - val_mae: 0.5160\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2450 - mse: 0.2450 - mae: 0.3831 - val_loss: 0.4329 - val_mse: 0.4329 - val_mae: 0.5001\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2403 - mse: 0.2403 - mae: 0.3800 - val_loss: 0.4558 - val_mse: 0.4558 - val_mae: 0.5187\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2351 - mse: 0.2351 - mae: 0.3750 - val_loss: 0.4785 - val_mse: 0.4785 - val_mae: 0.5259\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2102 - mse: 0.2102 - mae: 0.3539 - val_loss: 0.4603 - val_mse: 0.4603 - val_mae: 0.5181\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2336 - mse: 0.2336 - mae: 0.3710 - val_loss: 0.4506 - val_mse: 0.4506 - val_mae: 0.5107\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2159 - mse: 0.2159 - mae: 0.3566 - val_loss: 0.4641 - val_mse: 0.4641 - val_mae: 0.5231\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2150 - mse: 0.2150 - mae: 0.3630 - val_loss: 0.4474 - val_mse: 0.4474 - val_mae: 0.5196\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2269 - mse: 0.2269 - mae: 0.3691 - val_loss: 0.4724 - val_mse: 0.4724 - val_mae: 0.5228\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2139 - mse: 0.2139 - mae: 0.3583 - val_loss: 0.4517 - val_mse: 0.4517 - val_mae: 0.5034\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2158 - mse: 0.2158 - mae: 0.3635 - val_loss: 0.4638 - val_mse: 0.4638 - val_mae: 0.5254\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2391 - mse: 0.2391 - mae: 0.3790 - val_loss: 0.4470 - val_mse: 0.4470 - val_mae: 0.5084\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2046 - mse: 0.2046 - mae: 0.3527 - val_loss: 0.5213 - val_mse: 0.5213 - val_mae: 0.5501\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2595 - mse: 0.2595 - mae: 0.3997 - val_loss: 0.4700 - val_mse: 0.4700 - val_mae: 0.5124\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2277 - mse: 0.2277 - mae: 0.3711 - val_loss: 0.4409 - val_mse: 0.4409 - val_mae: 0.5018\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2101 - mse: 0.2101 - mae: 0.3584 - val_loss: 0.4600 - val_mse: 0.4600 - val_mae: 0.5118\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2213 - mse: 0.2213 - mae: 0.3714 - val_loss: 0.4536 - val_mse: 0.4536 - val_mae: 0.5109\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2352 - mse: 0.2352 - mae: 0.3741 - val_loss: 0.4739 - val_mse: 0.4739 - val_mae: 0.5188\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2540 - mse: 0.2540 - mae: 0.3853 - val_loss: 0.4820 - val_mse: 0.4820 - val_mae: 0.5238\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2326 - mse: 0.2326 - mae: 0.3746 - val_loss: 0.4543 - val_mse: 0.4543 - val_mae: 0.5115\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2208 - mse: 0.2208 - mae: 0.3683 - val_loss: 0.4703 - val_mse: 0.4703 - val_mae: 0.5160\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2395 - mse: 0.2395 - mae: 0.3813 - val_loss: 0.4584 - val_mse: 0.4584 - val_mae: 0.5230\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2033 - mse: 0.2033 - mae: 0.3525 - val_loss: 0.4830 - val_mse: 0.4830 - val_mae: 0.5137\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2271 - mse: 0.2271 - mae: 0.3711 - val_loss: 0.4308 - val_mse: 0.4308 - val_mae: 0.4970\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2139 - mse: 0.2139 - mae: 0.3540 - val_loss: 0.4596 - val_mse: 0.4596 - val_mae: 0.5192\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2066 - mse: 0.2066 - mae: 0.3486 - val_loss: 0.4260 - val_mse: 0.4260 - val_mae: 0.4984\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2233 - mse: 0.2233 - mae: 0.3687 - val_loss: 0.4795 - val_mse: 0.4795 - val_mae: 0.5316\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2322 - mse: 0.2322 - mae: 0.3729 - val_loss: 0.4712 - val_mse: 0.4712 - val_mae: 0.5208\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1978 - mse: 0.1978 - mae: 0.3475 - val_loss: 0.4959 - val_mse: 0.4959 - val_mae: 0.5295\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2166 - mse: 0.2166 - mae: 0.3580 - val_loss: 0.4593 - val_mse: 0.4593 - val_mae: 0.5214\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2171 - mse: 0.2171 - mae: 0.3584 - val_loss: 0.4841 - val_mse: 0.4841 - val_mae: 0.5264\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2184 - mse: 0.2184 - mae: 0.3615 - val_loss: 0.4844 - val_mse: 0.4844 - val_mae: 0.5249\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2180 - mse: 0.2180 - mae: 0.3664 - val_loss: 0.4710 - val_mse: 0.4710 - val_mae: 0.5240\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2103 - mse: 0.2103 - mae: 0.3536 - val_loss: 0.4677 - val_mse: 0.4677 - val_mae: 0.5173\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2276 - mse: 0.2276 - mae: 0.3691 - val_loss: 0.4659 - val_mse: 0.4659 - val_mae: 0.5188\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1964 - mse: 0.1964 - mae: 0.3465 - val_loss: 0.4902 - val_mse: 0.4902 - val_mae: 0.5360\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2121 - mse: 0.2121 - mae: 0.3540 - val_loss: 0.4583 - val_mse: 0.4583 - val_mae: 0.5216\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1855 - mse: 0.1855 - mae: 0.3318 - val_loss: 0.4625 - val_mse: 0.4625 - val_mae: 0.5150\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2010 - mse: 0.2010 - mae: 0.3505 - val_loss: 0.4791 - val_mse: 0.4791 - val_mae: 0.5226\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2188 - mse: 0.2188 - mae: 0.3546 - val_loss: 0.4565 - val_mse: 0.4565 - val_mae: 0.5159\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2215 - mse: 0.2215 - mae: 0.3590 - val_loss: 0.5088 - val_mse: 0.5088 - val_mae: 0.5626\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2535 - mse: 0.2535 - mae: 0.3872 - val_loss: 0.4595 - val_mse: 0.4595 - val_mae: 0.5267\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2203 - mse: 0.2203 - mae: 0.3546 - val_loss: 0.4564 - val_mse: 0.4564 - val_mae: 0.5207\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2337 - mse: 0.2337 - mae: 0.3714 - val_loss: 0.4554 - val_mse: 0.4554 - val_mae: 0.5186\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1958 - mse: 0.1958 - mae: 0.3369 - val_loss: 0.4883 - val_mse: 0.4883 - val_mae: 0.5251\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2023 - mse: 0.2023 - mae: 0.3439 - val_loss: 0.4638 - val_mse: 0.4638 - val_mae: 0.5200\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2118 - mse: 0.2118 - mae: 0.3595 - val_loss: 0.4855 - val_mse: 0.4855 - val_mae: 0.5300\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2126 - mse: 0.2126 - mae: 0.3534 - val_loss: 0.4806 - val_mse: 0.4806 - val_mae: 0.5206\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2111 - mse: 0.2111 - mae: 0.3551 - val_loss: 0.4776 - val_mse: 0.4776 - val_mae: 0.5203\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1963 - mse: 0.1963 - mae: 0.3474 - val_loss: 0.4794 - val_mse: 0.4794 - val_mae: 0.5376\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2121 - mse: 0.2121 - mae: 0.3632 - val_loss: 0.4928 - val_mse: 0.4928 - val_mae: 0.5339\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1875 - mse: 0.1875 - mae: 0.3389 - val_loss: 0.5075 - val_mse: 0.5075 - val_mae: 0.5326\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2106 - mse: 0.2106 - mae: 0.3503 - val_loss: 0.4629 - val_mse: 0.4629 - val_mae: 0.5164\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1770 - mse: 0.1770 - mae: 0.3218 - val_loss: 0.5326 - val_mse: 0.5326 - val_mae: 0.5420\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1882 - mse: 0.1882 - mae: 0.3375 - val_loss: 0.4595 - val_mse: 0.4595 - val_mae: 0.5182\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2031 - mse: 0.2031 - mae: 0.3463 - val_loss: 0.4888 - val_mse: 0.4888 - val_mae: 0.5292\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2060 - mse: 0.2060 - mae: 0.3482 - val_loss: 0.4506 - val_mse: 0.4506 - val_mae: 0.5168\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1962 - mse: 0.1962 - mae: 0.3423 - val_loss: 0.4566 - val_mse: 0.4566 - val_mae: 0.5147\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2063 - mse: 0.2063 - mae: 0.3484 - val_loss: 0.4658 - val_mse: 0.4658 - val_mae: 0.5271\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2056 - mse: 0.2056 - mae: 0.3488 - val_loss: 0.4818 - val_mse: 0.4818 - val_mae: 0.5206\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2124 - mse: 0.2124 - mae: 0.3645 - val_loss: 0.5379 - val_mse: 0.5379 - val_mae: 0.5569\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1983 - mse: 0.1983 - mae: 0.3406 - val_loss: 0.4763 - val_mse: 0.4763 - val_mae: 0.5199\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.2033 - mse: 0.2033 - mae: 0.3470 - val_loss: 0.4746 - val_mse: 0.4746 - val_mae: 0.5314\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2063 - mse: 0.2063 - mae: 0.3566 - val_loss: 0.4683 - val_mse: 0.4683 - val_mae: 0.5271\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2258 - mse: 0.2258 - mae: 0.3752 - val_loss: 0.4484 - val_mse: 0.4484 - val_mae: 0.5052\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2149 - mse: 0.2149 - mae: 0.3590 - val_loss: 0.4921 - val_mse: 0.4921 - val_mae: 0.5217\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2309 - mse: 0.2309 - mae: 0.3820 - val_loss: 0.4720 - val_mse: 0.4720 - val_mae: 0.5224\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1963 - mse: 0.1963 - mae: 0.3368 - val_loss: 0.4666 - val_mse: 0.4666 - val_mae: 0.5177\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1913 - mse: 0.1913 - mae: 0.3297 - val_loss: 0.4780 - val_mse: 0.4780 - val_mae: 0.5169\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.1684 - mse: 0.1684 - mae: 0.3178 - val_loss: 0.4718 - val_mse: 0.4718 - val_mae: 0.5219\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1950 - mse: 0.1950 - mae: 0.3412 - val_loss: 0.4871 - val_mse: 0.4871 - val_mae: 0.5322\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2031 - mse: 0.2031 - mae: 0.3477 - val_loss: 0.5345 - val_mse: 0.5345 - val_mae: 0.5588\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.2158 - mse: 0.2158 - mae: 0.3578 - val_loss: 0.4716 - val_mse: 0.4716 - val_mae: 0.5128\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1936 - mse: 0.1936 - mae: 0.3326 - val_loss: 0.4459 - val_mse: 0.4459 - val_mae: 0.5097\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.1716 - mse: 0.1716 - mae: 0.3166 - val_loss: 0.4517 - val_mse: 0.4517 - val_mae: 0.5107\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.1987 - mse: 0.1987 - mae: 0.3417 - val_loss: 0.4842 - val_mse: 0.4842 - val_mae: 0.5337\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1956 - mse: 0.1956 - mae: 0.3416 - val_loss: 0.5043 - val_mse: 0.5043 - val_mae: 0.5360\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1928 - mse: 0.1928 - mae: 0.3413 - val_loss: 0.4673 - val_mse: 0.4673 - val_mae: 0.5241\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1799 - mse: 0.1799 - mae: 0.3263 - val_loss: 0.4501 - val_mse: 0.4501 - val_mae: 0.5053\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1719 - mse: 0.1719 - mae: 0.3208 - val_loss: 0.4493 - val_mse: 0.4493 - val_mae: 0.5087\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2032 - mse: 0.2032 - mae: 0.3462 - val_loss: 0.4592 - val_mse: 0.4592 - val_mae: 0.5132\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2002 - mse: 0.2002 - mae: 0.3510 - val_loss: 0.4587 - val_mse: 0.4587 - val_mae: 0.5091\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2132 - mse: 0.2132 - mae: 0.3586 - val_loss: 0.4620 - val_mse: 0.4620 - val_mae: 0.5145\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2045 - mse: 0.2045 - mae: 0.3483 - val_loss: 0.4746 - val_mse: 0.4746 - val_mae: 0.5303\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2025 - mse: 0.2025 - mae: 0.3541 - val_loss: 0.5325 - val_mse: 0.5325 - val_mae: 0.5558\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2185 - mse: 0.2185 - mae: 0.3676 - val_loss: 0.4680 - val_mse: 0.4680 - val_mae: 0.5123\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2096 - mse: 0.2096 - mae: 0.3466 - val_loss: 0.4699 - val_mse: 0.4699 - val_mae: 0.5230\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1787 - mse: 0.1787 - mae: 0.3245 - val_loss: 0.4874 - val_mse: 0.4874 - val_mae: 0.5276\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1932 - mse: 0.1932 - mae: 0.3370 - val_loss: 0.4685 - val_mse: 0.4685 - val_mae: 0.5104\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1897 - mse: 0.1897 - mae: 0.3437 - val_loss: 0.4598 - val_mse: 0.4598 - val_mae: 0.5180\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2055 - mse: 0.2055 - mae: 0.3490 - val_loss: 0.4755 - val_mse: 0.4755 - val_mae: 0.5261\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1955 - mse: 0.1955 - mae: 0.3419 - val_loss: 0.5060 - val_mse: 0.5060 - val_mae: 0.5371\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2136 - mse: 0.2136 - mae: 0.3551 - val_loss: 0.4574 - val_mse: 0.4574 - val_mae: 0.5175\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2130 - mse: 0.2130 - mae: 0.3649 - val_loss: 0.5128 - val_mse: 0.5128 - val_mae: 0.5312\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1928 - mse: 0.1928 - mae: 0.3315 - val_loss: 0.4605 - val_mse: 0.4605 - val_mae: 0.5175\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1967 - mse: 0.1967 - mae: 0.3435 - val_loss: 0.4411 - val_mse: 0.4411 - val_mae: 0.4979\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1950 - mse: 0.1950 - mae: 0.3404 - val_loss: 0.5180 - val_mse: 0.5180 - val_mae: 0.5499\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2031 - mse: 0.2031 - mae: 0.3487 - val_loss: 0.4353 - val_mse: 0.4353 - val_mae: 0.4981\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1905 - mse: 0.1905 - mae: 0.3460 - val_loss: 0.4403 - val_mse: 0.4403 - val_mae: 0.5033\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1936 - mse: 0.1936 - mae: 0.3339 - val_loss: 0.5777 - val_mse: 0.5777 - val_mae: 0.5849\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2571 - mse: 0.2571 - mae: 0.3947 - val_loss: 0.4683 - val_mse: 0.4683 - val_mae: 0.5089\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1970 - mse: 0.1970 - mae: 0.3504 - val_loss: 0.4747 - val_mse: 0.4747 - val_mae: 0.5114\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1793 - mse: 0.1793 - mae: 0.3266 - val_loss: 0.4639 - val_mse: 0.4639 - val_mae: 0.5062\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1901 - mse: 0.1901 - mae: 0.3397 - val_loss: 0.4682 - val_mse: 0.4682 - val_mae: 0.5137\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1804 - mse: 0.1804 - mae: 0.3271 - val_loss: 0.4805 - val_mse: 0.4805 - val_mae: 0.5128\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1776 - mse: 0.1776 - mae: 0.3253 - val_loss: 0.4847 - val_mse: 0.4847 - val_mae: 0.5218\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1723 - mse: 0.1723 - mae: 0.3273 - val_loss: 0.5311 - val_mse: 0.5311 - val_mae: 0.5456\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2140 - mse: 0.2140 - mae: 0.3648 - val_loss: 0.4527 - val_mse: 0.4527 - val_mae: 0.5077\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1833 - mse: 0.1833 - mae: 0.3290 - val_loss: 0.4566 - val_mse: 0.4566 - val_mae: 0.5269\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2111 - mse: 0.2111 - mae: 0.3510 - val_loss: 0.5072 - val_mse: 0.5072 - val_mae: 0.5279\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1861 - mse: 0.1861 - mae: 0.3404 - val_loss: 0.4709 - val_mse: 0.4709 - val_mae: 0.5177\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1806 - mse: 0.1806 - mae: 0.3239 - val_loss: 0.4705 - val_mse: 0.4705 - val_mae: 0.5228\n",
            "Epoch 252/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1783 - mse: 0.1783 - mae: 0.3262 - val_loss: 0.4944 - val_mse: 0.4944 - val_mae: 0.5289\n",
            "Epoch 253/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1909 - mse: 0.1909 - mae: 0.3339 - val_loss: 0.4566 - val_mse: 0.4566 - val_mae: 0.5109\n",
            "Epoch 254/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1761 - mse: 0.1761 - mae: 0.3250 - val_loss: 0.4831 - val_mse: 0.4831 - val_mae: 0.5220\n",
            "Epoch 255/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1527 - mse: 0.1527 - mae: 0.2980 - val_loss: 0.4822 - val_mse: 0.4822 - val_mae: 0.5280\n",
            "Epoch 256/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1724 - mse: 0.1724 - mae: 0.3158 - val_loss: 0.4634 - val_mse: 0.4634 - val_mae: 0.5229\n",
            "Epoch 257/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2086 - mse: 0.2086 - mae: 0.3479 - val_loss: 0.4897 - val_mse: 0.4897 - val_mae: 0.5302\n",
            "Epoch 258/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1676 - mse: 0.1676 - mae: 0.3172 - val_loss: 0.4547 - val_mse: 0.4547 - val_mae: 0.5093\n",
            "Epoch 259/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1794 - mse: 0.1794 - mae: 0.3267 - val_loss: 0.5350 - val_mse: 0.5350 - val_mae: 0.5530\n",
            "Epoch 260/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1619 - mse: 0.1619 - mae: 0.3115 - val_loss: 0.4772 - val_mse: 0.4772 - val_mae: 0.5264\n",
            "Epoch 261/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1580 - mse: 0.1580 - mae: 0.3076 - val_loss: 0.4641 - val_mse: 0.4641 - val_mae: 0.5208\n",
            "Epoch 262/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2036 - mse: 0.2036 - mae: 0.3495 - val_loss: 0.4770 - val_mse: 0.4770 - val_mae: 0.5299\n",
            "Epoch 263/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1753 - mse: 0.1753 - mae: 0.3288 - val_loss: 0.5141 - val_mse: 0.5141 - val_mae: 0.5377\n",
            "Epoch 264/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1696 - mse: 0.1696 - mae: 0.3215 - val_loss: 0.4589 - val_mse: 0.4589 - val_mae: 0.5154\n",
            "Epoch 265/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1805 - mse: 0.1805 - mae: 0.3254 - val_loss: 0.4843 - val_mse: 0.4843 - val_mae: 0.5310\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.5053 - mse: 0.5053 - mae: 0.5378\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 14ms/step - loss: 14.9966 - mse: 14.9966 - mae: 3.4156 - val_loss: 3.2982 - val_mse: 3.2982 - val_mae: 1.4088\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 3.7020 - mse: 3.7020 - mae: 1.4549 - val_loss: 2.7245 - val_mse: 2.7245 - val_mae: 1.3040\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.3971 - mse: 2.3971 - mae: 1.2544 - val_loss: 2.4885 - val_mse: 2.4885 - val_mae: 1.2378\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.2162 - mse: 2.2162 - mae: 1.1844 - val_loss: 2.2880 - val_mse: 2.2880 - val_mae: 1.1890\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.0427 - mse: 2.0427 - mae: 1.1451 - val_loss: 2.1841 - val_mse: 2.1841 - val_mae: 1.1536\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.8835 - mse: 1.8835 - mae: 1.1062 - val_loss: 2.1070 - val_mse: 2.1070 - val_mae: 1.1208\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.8124 - mse: 1.8124 - mae: 1.0613 - val_loss: 2.0214 - val_mse: 2.0214 - val_mae: 1.0976\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.7218 - mse: 1.7218 - mae: 1.0433 - val_loss: 2.0124 - val_mse: 2.0124 - val_mae: 1.0849\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.5624 - mse: 1.5624 - mae: 0.9919 - val_loss: 1.8898 - val_mse: 1.8898 - val_mae: 1.0499\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.4613 - mse: 1.4613 - mae: 0.9656 - val_loss: 1.8032 - val_mse: 1.8032 - val_mae: 1.0239\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.5040 - mse: 1.5040 - mae: 0.9766 - val_loss: 1.8028 - val_mse: 1.8028 - val_mae: 1.0169\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.3477 - mse: 1.3477 - mae: 0.9304 - val_loss: 1.6708 - val_mse: 1.6708 - val_mae: 0.9832\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.3526 - mse: 1.3526 - mae: 0.9271 - val_loss: 1.6008 - val_mse: 1.6008 - val_mae: 0.9614\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.2079 - mse: 1.2079 - mae: 0.8781 - val_loss: 1.5238 - val_mse: 1.5238 - val_mae: 0.9373\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.1721 - mse: 1.1721 - mae: 0.8575 - val_loss: 1.5056 - val_mse: 1.5056 - val_mae: 0.9281\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.2087 - mse: 1.2087 - mae: 0.8696 - val_loss: 1.4836 - val_mse: 1.4836 - val_mae: 0.9174\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0771 - mse: 1.0771 - mae: 0.8173 - val_loss: 1.4064 - val_mse: 1.4064 - val_mae: 0.8942\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0040 - mse: 1.0040 - mae: 0.7902 - val_loss: 1.3672 - val_mse: 1.3672 - val_mae: 0.8834\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0103 - mse: 1.0103 - mae: 0.7982 - val_loss: 1.2817 - val_mse: 1.2817 - val_mae: 0.8703\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.9446 - mse: 0.9446 - mae: 0.7692 - val_loss: 1.2664 - val_mse: 1.2664 - val_mae: 0.8505\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.9459 - mse: 0.9459 - mae: 0.7718 - val_loss: 1.1856 - val_mse: 1.1856 - val_mae: 0.8308\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.9227 - mse: 0.9227 - mae: 0.7559 - val_loss: 1.1426 - val_mse: 1.1426 - val_mae: 0.8172\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8813 - mse: 0.8813 - mae: 0.7526 - val_loss: 1.1221 - val_mse: 1.1221 - val_mae: 0.7962\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8809 - mse: 0.8809 - mae: 0.7418 - val_loss: 1.1241 - val_mse: 1.1241 - val_mae: 0.8047\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7754 - mse: 0.7754 - mae: 0.7021 - val_loss: 1.0205 - val_mse: 1.0205 - val_mae: 0.7602\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7487 - mse: 0.7487 - mae: 0.6748 - val_loss: 1.0527 - val_mse: 1.0527 - val_mae: 0.7674\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7249 - mse: 0.7249 - mae: 0.6811 - val_loss: 1.0714 - val_mse: 1.0714 - val_mae: 0.7708\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7083 - mse: 0.7083 - mae: 0.6599 - val_loss: 0.9564 - val_mse: 0.9564 - val_mae: 0.7281\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6813 - mse: 0.6813 - mae: 0.6418 - val_loss: 0.9805 - val_mse: 0.9805 - val_mae: 0.7426\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6606 - mse: 0.6606 - mae: 0.6410 - val_loss: 0.8772 - val_mse: 0.8772 - val_mae: 0.7010\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6500 - mse: 0.6500 - mae: 0.6412 - val_loss: 0.8876 - val_mse: 0.8876 - val_mae: 0.6954\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6163 - mse: 0.6163 - mae: 0.6248 - val_loss: 0.8380 - val_mse: 0.8380 - val_mae: 0.6886\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5845 - mse: 0.5845 - mae: 0.6139 - val_loss: 0.8700 - val_mse: 0.8700 - val_mae: 0.6805\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6235 - mse: 0.6235 - mae: 0.6171 - val_loss: 0.7808 - val_mse: 0.7808 - val_mae: 0.6770\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5652 - mse: 0.5652 - mae: 0.5939 - val_loss: 0.7500 - val_mse: 0.7500 - val_mae: 0.6455\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5642 - mse: 0.5642 - mae: 0.6009 - val_loss: 0.7727 - val_mse: 0.7727 - val_mae: 0.6617\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5719 - mse: 0.5719 - mae: 0.5926 - val_loss: 0.7455 - val_mse: 0.7455 - val_mae: 0.6349\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5093 - mse: 0.5093 - mae: 0.5616 - val_loss: 0.7078 - val_mse: 0.7078 - val_mae: 0.6309\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5372 - mse: 0.5372 - mae: 0.5715 - val_loss: 0.6470 - val_mse: 0.6470 - val_mae: 0.6106\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4868 - mse: 0.4868 - mae: 0.5382 - val_loss: 0.6289 - val_mse: 0.6289 - val_mae: 0.5994\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4793 - mse: 0.4793 - mae: 0.5365 - val_loss: 0.6794 - val_mse: 0.6794 - val_mae: 0.6238\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4982 - mse: 0.4982 - mae: 0.5517 - val_loss: 0.6114 - val_mse: 0.6114 - val_mae: 0.6097\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4709 - mse: 0.4709 - mae: 0.5339 - val_loss: 0.6583 - val_mse: 0.6583 - val_mae: 0.6089\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4313 - mse: 0.4313 - mae: 0.5146 - val_loss: 0.6378 - val_mse: 0.6378 - val_mae: 0.5982\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4176 - mse: 0.4176 - mae: 0.5112 - val_loss: 0.7233 - val_mse: 0.7233 - val_mae: 0.6333\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4343 - mse: 0.4343 - mae: 0.5134 - val_loss: 0.5731 - val_mse: 0.5731 - val_mae: 0.5719\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4102 - mse: 0.4102 - mae: 0.4993 - val_loss: 0.5886 - val_mse: 0.5886 - val_mae: 0.5843\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4428 - mse: 0.4428 - mae: 0.5181 - val_loss: 0.5581 - val_mse: 0.5581 - val_mae: 0.5729\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4325 - mse: 0.4325 - mae: 0.5234 - val_loss: 0.5461 - val_mse: 0.5461 - val_mae: 0.5556\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4321 - mse: 0.4321 - mae: 0.5171 - val_loss: 0.5564 - val_mse: 0.5564 - val_mae: 0.5592\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4051 - mse: 0.4051 - mae: 0.4973 - val_loss: 0.5149 - val_mse: 0.5149 - val_mae: 0.5362\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3957 - mse: 0.3957 - mae: 0.4898 - val_loss: 0.5596 - val_mse: 0.5596 - val_mae: 0.5530\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3856 - mse: 0.3856 - mae: 0.4790 - val_loss: 0.5094 - val_mse: 0.5094 - val_mae: 0.5223\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3788 - mse: 0.3788 - mae: 0.4825 - val_loss: 0.5224 - val_mse: 0.5224 - val_mae: 0.5371\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3880 - mse: 0.3880 - mae: 0.4808 - val_loss: 0.6385 - val_mse: 0.6385 - val_mae: 0.5980\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4198 - mse: 0.4198 - mae: 0.5026 - val_loss: 0.5962 - val_mse: 0.5962 - val_mae: 0.5728\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3584 - mse: 0.3584 - mae: 0.4658 - val_loss: 0.4748 - val_mse: 0.4748 - val_mae: 0.5291\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3544 - mse: 0.3544 - mae: 0.4627 - val_loss: 0.5014 - val_mse: 0.5014 - val_mae: 0.5441\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3607 - mse: 0.3607 - mae: 0.4621 - val_loss: 0.5344 - val_mse: 0.5344 - val_mae: 0.5456\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3779 - mse: 0.3779 - mae: 0.4770 - val_loss: 0.5072 - val_mse: 0.5072 - val_mae: 0.5508\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3510 - mse: 0.3510 - mae: 0.4629 - val_loss: 0.5010 - val_mse: 0.5010 - val_mae: 0.5459\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3524 - mse: 0.3524 - mae: 0.4570 - val_loss: 0.4863 - val_mse: 0.4863 - val_mae: 0.5246\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3503 - mse: 0.3503 - mae: 0.4594 - val_loss: 0.4516 - val_mse: 0.4516 - val_mae: 0.5082\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3679 - mse: 0.3679 - mae: 0.4696 - val_loss: 0.5028 - val_mse: 0.5028 - val_mae: 0.5310\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3301 - mse: 0.3301 - mae: 0.4540 - val_loss: 0.4321 - val_mse: 0.4321 - val_mae: 0.5018\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3530 - mse: 0.3530 - mae: 0.4562 - val_loss: 0.5338 - val_mse: 0.5338 - val_mae: 0.5445\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3372 - mse: 0.3372 - mae: 0.4489 - val_loss: 0.4397 - val_mse: 0.4397 - val_mae: 0.5000\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3198 - mse: 0.3198 - mae: 0.4389 - val_loss: 0.4556 - val_mse: 0.4556 - val_mae: 0.5297\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3055 - mse: 0.3055 - mae: 0.4253 - val_loss: 0.4694 - val_mse: 0.4694 - val_mae: 0.5166\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3207 - mse: 0.3207 - mae: 0.4415 - val_loss: 0.4990 - val_mse: 0.4990 - val_mae: 0.5257\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3566 - mse: 0.3566 - mae: 0.4653 - val_loss: 0.4390 - val_mse: 0.4390 - val_mae: 0.5091\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3186 - mse: 0.3186 - mae: 0.4376 - val_loss: 0.4640 - val_mse: 0.4640 - val_mae: 0.5122\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3200 - mse: 0.3200 - mae: 0.4435 - val_loss: 0.4538 - val_mse: 0.4538 - val_mae: 0.5120\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3276 - mse: 0.3276 - mae: 0.4423 - val_loss: 0.4405 - val_mse: 0.4405 - val_mae: 0.5089\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3056 - mse: 0.3056 - mae: 0.4182 - val_loss: 0.4176 - val_mse: 0.4176 - val_mae: 0.4922\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3113 - mse: 0.3113 - mae: 0.4280 - val_loss: 0.4284 - val_mse: 0.4284 - val_mae: 0.4943\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3080 - mse: 0.3080 - mae: 0.4313 - val_loss: 0.4661 - val_mse: 0.4661 - val_mae: 0.5151\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2900 - mse: 0.2900 - mae: 0.4104 - val_loss: 0.4376 - val_mse: 0.4376 - val_mae: 0.4968\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3070 - mse: 0.3070 - mae: 0.4287 - val_loss: 0.4601 - val_mse: 0.4601 - val_mae: 0.5087\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2956 - mse: 0.2956 - mae: 0.4151 - val_loss: 0.5421 - val_mse: 0.5421 - val_mae: 0.5549\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3021 - mse: 0.3021 - mae: 0.4248 - val_loss: 0.4344 - val_mse: 0.4344 - val_mae: 0.5150\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2894 - mse: 0.2894 - mae: 0.4097 - val_loss: 0.4332 - val_mse: 0.4332 - val_mae: 0.4989\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2748 - mse: 0.2748 - mae: 0.4113 - val_loss: 0.4471 - val_mse: 0.4471 - val_mae: 0.5056\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3012 - mse: 0.3012 - mae: 0.4239 - val_loss: 0.4001 - val_mse: 0.4001 - val_mae: 0.4854\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2818 - mse: 0.2818 - mae: 0.4129 - val_loss: 0.4383 - val_mse: 0.4383 - val_mae: 0.4994\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2734 - mse: 0.2734 - mae: 0.4007 - val_loss: 0.4258 - val_mse: 0.4258 - val_mae: 0.4951\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2843 - mse: 0.2843 - mae: 0.4023 - val_loss: 0.4328 - val_mse: 0.4328 - val_mae: 0.5110\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2763 - mse: 0.2763 - mae: 0.4011 - val_loss: 0.4388 - val_mse: 0.4388 - val_mae: 0.5026\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2789 - mse: 0.2789 - mae: 0.4016 - val_loss: 0.3892 - val_mse: 0.3892 - val_mae: 0.4755\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2752 - mse: 0.2752 - mae: 0.4064 - val_loss: 0.4476 - val_mse: 0.4476 - val_mae: 0.5064\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2978 - mse: 0.2978 - mae: 0.4296 - val_loss: 0.4171 - val_mse: 0.4171 - val_mae: 0.4884\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2916 - mse: 0.2916 - mae: 0.4144 - val_loss: 0.4330 - val_mse: 0.4330 - val_mae: 0.5096\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2986 - mse: 0.2986 - mae: 0.4254 - val_loss: 0.4685 - val_mse: 0.4685 - val_mae: 0.5167\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3088 - mse: 0.3088 - mae: 0.4372 - val_loss: 0.4229 - val_mse: 0.4229 - val_mae: 0.5016\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3387 - mse: 0.3387 - mae: 0.4536 - val_loss: 0.4394 - val_mse: 0.4394 - val_mae: 0.4986\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2833 - mse: 0.2833 - mae: 0.4139 - val_loss: 0.5373 - val_mse: 0.5373 - val_mae: 0.5626\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2779 - mse: 0.2779 - mae: 0.4095 - val_loss: 0.4236 - val_mse: 0.4236 - val_mae: 0.4878\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2844 - mse: 0.2844 - mae: 0.4118 - val_loss: 0.4333 - val_mse: 0.4333 - val_mae: 0.5012\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2683 - mse: 0.2683 - mae: 0.4027 - val_loss: 0.4294 - val_mse: 0.4294 - val_mae: 0.5095\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2720 - mse: 0.2720 - mae: 0.4009 - val_loss: 0.5011 - val_mse: 0.5011 - val_mae: 0.5296\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3055 - mse: 0.3055 - mae: 0.4346 - val_loss: 0.4162 - val_mse: 0.4162 - val_mae: 0.5002\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3060 - mse: 0.3060 - mae: 0.4241 - val_loss: 0.4358 - val_mse: 0.4358 - val_mae: 0.4974\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2922 - mse: 0.2922 - mae: 0.4255 - val_loss: 0.4028 - val_mse: 0.4028 - val_mae: 0.4708\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2595 - mse: 0.2595 - mae: 0.3986 - val_loss: 0.4283 - val_mse: 0.4283 - val_mae: 0.4899\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2840 - mse: 0.2840 - mae: 0.4126 - val_loss: 0.4062 - val_mse: 0.4062 - val_mae: 0.4831\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2505 - mse: 0.2505 - mae: 0.3848 - val_loss: 0.4469 - val_mse: 0.4469 - val_mae: 0.5038\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2514 - mse: 0.2514 - mae: 0.3797 - val_loss: 0.4308 - val_mse: 0.4308 - val_mae: 0.5030\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2505 - mse: 0.2505 - mae: 0.3866 - val_loss: 0.3991 - val_mse: 0.3991 - val_mae: 0.4819\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2393 - mse: 0.2393 - mae: 0.3816 - val_loss: 0.3998 - val_mse: 0.3998 - val_mae: 0.4911\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2650 - mse: 0.2650 - mae: 0.4018 - val_loss: 0.4142 - val_mse: 0.4142 - val_mae: 0.4919\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.2895 - mse: 0.2895 - mae: 0.4113 - val_loss: 0.3878 - val_mse: 0.3878 - val_mae: 0.4798\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2932 - mse: 0.2932 - mae: 0.4161 - val_loss: 0.4203 - val_mse: 0.4203 - val_mae: 0.5004\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2619 - mse: 0.2619 - mae: 0.4018 - val_loss: 0.4116 - val_mse: 0.4116 - val_mae: 0.4859\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2503 - mse: 0.2503 - mae: 0.3801 - val_loss: 0.4676 - val_mse: 0.4676 - val_mae: 0.5084\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2751 - mse: 0.2751 - mae: 0.4103 - val_loss: 0.4010 - val_mse: 0.4010 - val_mae: 0.4852\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2715 - mse: 0.2715 - mae: 0.4071 - val_loss: 0.4772 - val_mse: 0.4772 - val_mae: 0.5236\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.2486 - mse: 0.2486 - mae: 0.3941 - val_loss: 0.4281 - val_mse: 0.4281 - val_mae: 0.4979\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.2324 - mse: 0.2324 - mae: 0.3719 - val_loss: 0.3950 - val_mse: 0.3950 - val_mae: 0.4794\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2387 - mse: 0.2387 - mae: 0.3764 - val_loss: 0.4202 - val_mse: 0.4202 - val_mae: 0.5003\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2655 - mse: 0.2655 - mae: 0.3968 - val_loss: 0.4043 - val_mse: 0.4043 - val_mae: 0.4813\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2601 - mse: 0.2601 - mae: 0.3958 - val_loss: 0.4284 - val_mse: 0.4284 - val_mae: 0.5016\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2515 - mse: 0.2515 - mae: 0.3895 - val_loss: 0.4083 - val_mse: 0.4083 - val_mae: 0.4890\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2751 - mse: 0.2751 - mae: 0.4054 - val_loss: 0.4810 - val_mse: 0.4810 - val_mae: 0.5335\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2621 - mse: 0.2621 - mae: 0.3970 - val_loss: 0.3942 - val_mse: 0.3942 - val_mae: 0.4802\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2458 - mse: 0.2458 - mae: 0.3721 - val_loss: 0.5337 - val_mse: 0.5337 - val_mae: 0.5572\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2522 - mse: 0.2522 - mae: 0.3901 - val_loss: 0.4672 - val_mse: 0.4672 - val_mae: 0.5170\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2649 - mse: 0.2649 - mae: 0.4021 - val_loss: 0.4155 - val_mse: 0.4155 - val_mae: 0.5031\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2416 - mse: 0.2416 - mae: 0.3827 - val_loss: 0.4057 - val_mse: 0.4057 - val_mae: 0.4758\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2448 - mse: 0.2448 - mae: 0.3798 - val_loss: 0.4067 - val_mse: 0.4067 - val_mae: 0.4869\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2385 - mse: 0.2385 - mae: 0.3720 - val_loss: 0.4722 - val_mse: 0.4722 - val_mae: 0.5200\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2799 - mse: 0.2799 - mae: 0.4113 - val_loss: 0.4529 - val_mse: 0.4529 - val_mae: 0.5376\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2820 - mse: 0.2820 - mae: 0.4116 - val_loss: 0.4491 - val_mse: 0.4491 - val_mae: 0.4957\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2445 - mse: 0.2445 - mae: 0.3799 - val_loss: 0.4601 - val_mse: 0.4601 - val_mae: 0.5075\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2831 - mse: 0.2831 - mae: 0.4128 - val_loss: 0.4211 - val_mse: 0.4211 - val_mae: 0.4975\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2324 - mse: 0.2324 - mae: 0.3732 - val_loss: 0.4469 - val_mse: 0.4469 - val_mae: 0.5186\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2436 - mse: 0.2436 - mae: 0.3798 - val_loss: 0.4212 - val_mse: 0.4212 - val_mae: 0.4931\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2546 - mse: 0.2546 - mae: 0.3843 - val_loss: 0.4036 - val_mse: 0.4036 - val_mae: 0.4943\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2524 - mse: 0.2524 - mae: 0.3899 - val_loss: 0.4466 - val_mse: 0.4466 - val_mae: 0.4997\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2208 - mse: 0.2208 - mae: 0.3628 - val_loss: 0.4098 - val_mse: 0.4098 - val_mae: 0.5020\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2358 - mse: 0.2358 - mae: 0.3775 - val_loss: 0.4060 - val_mse: 0.4060 - val_mae: 0.4877\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2294 - mse: 0.2294 - mae: 0.3697 - val_loss: 0.4214 - val_mse: 0.4214 - val_mae: 0.4850\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2367 - mse: 0.2367 - mae: 0.3750 - val_loss: 0.4071 - val_mse: 0.4071 - val_mae: 0.4831\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2498 - mse: 0.2498 - mae: 0.3873 - val_loss: 0.3924 - val_mse: 0.3924 - val_mae: 0.4821\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2473 - mse: 0.2473 - mae: 0.3923 - val_loss: 0.4669 - val_mse: 0.4669 - val_mae: 0.5104\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.2566 - mse: 0.2566 - mae: 0.3928 - val_loss: 0.3926 - val_mse: 0.3926 - val_mae: 0.4810\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2634 - mse: 0.2634 - mae: 0.3930 - val_loss: 0.3954 - val_mse: 0.3954 - val_mae: 0.4896\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2264 - mse: 0.2264 - mae: 0.3592 - val_loss: 0.4146 - val_mse: 0.4146 - val_mae: 0.4912\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2507 - mse: 0.2507 - mae: 0.3961 - val_loss: 0.4468 - val_mse: 0.4468 - val_mae: 0.5091\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2420 - mse: 0.2420 - mae: 0.3860 - val_loss: 0.3941 - val_mse: 0.3941 - val_mae: 0.4632\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2301 - mse: 0.2301 - mae: 0.3700 - val_loss: 0.4465 - val_mse: 0.4465 - val_mae: 0.5019\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2481 - mse: 0.2481 - mae: 0.3791 - val_loss: 0.4553 - val_mse: 0.4553 - val_mae: 0.4999\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2368 - mse: 0.2368 - mae: 0.3781 - val_loss: 0.4561 - val_mse: 0.4561 - val_mae: 0.5196\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2432 - mse: 0.2432 - mae: 0.3795 - val_loss: 0.4191 - val_mse: 0.4191 - val_mae: 0.4826\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2322 - mse: 0.2322 - mae: 0.3773 - val_loss: 0.4055 - val_mse: 0.4055 - val_mae: 0.4797\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2270 - mse: 0.2270 - mae: 0.3714 - val_loss: 0.4155 - val_mse: 0.4155 - val_mae: 0.4804\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2458 - mse: 0.2458 - mae: 0.3900 - val_loss: 0.4684 - val_mse: 0.4684 - val_mae: 0.5052\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2507 - mse: 0.2507 - mae: 0.3874 - val_loss: 0.4378 - val_mse: 0.4378 - val_mae: 0.4987\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2268 - mse: 0.2268 - mae: 0.3747 - val_loss: 0.4249 - val_mse: 0.4249 - val_mae: 0.4913\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2564 - mse: 0.2564 - mae: 0.3930 - val_loss: 0.4326 - val_mse: 0.4326 - val_mae: 0.5017\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2870 - mse: 0.2870 - mae: 0.4137 - val_loss: 0.4370 - val_mse: 0.4370 - val_mae: 0.5126\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2296 - mse: 0.2296 - mae: 0.3650 - val_loss: 0.4314 - val_mse: 0.4314 - val_mae: 0.4929\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2301 - mse: 0.2301 - mae: 0.3658 - val_loss: 0.4208 - val_mse: 0.4208 - val_mae: 0.4902\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2220 - mse: 0.2220 - mae: 0.3614 - val_loss: 0.4249 - val_mse: 0.4249 - val_mae: 0.4813\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2192 - mse: 0.2192 - mae: 0.3547 - val_loss: 0.4062 - val_mse: 0.4062 - val_mae: 0.4809\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2455 - mse: 0.2455 - mae: 0.3775 - val_loss: 0.4305 - val_mse: 0.4305 - val_mae: 0.4954\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2040 - mse: 0.2040 - mae: 0.3475 - val_loss: 0.4669 - val_mse: 0.4669 - val_mae: 0.5226\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2230 - mse: 0.2230 - mae: 0.3689 - val_loss: 0.4127 - val_mse: 0.4127 - val_mae: 0.4823\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2129 - mse: 0.2129 - mae: 0.3587 - val_loss: 0.4539 - val_mse: 0.4539 - val_mae: 0.5211\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2423 - mse: 0.2423 - mae: 0.3797 - val_loss: 0.4327 - val_mse: 0.4327 - val_mae: 0.5082\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2072 - mse: 0.2072 - mae: 0.3485 - val_loss: 0.4039 - val_mse: 0.4039 - val_mae: 0.4831\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2227 - mse: 0.2227 - mae: 0.3687 - val_loss: 0.4765 - val_mse: 0.4765 - val_mae: 0.5184\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2315 - mse: 0.2315 - mae: 0.3719 - val_loss: 0.4142 - val_mse: 0.4142 - val_mae: 0.4804\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2104 - mse: 0.2104 - mae: 0.3509 - val_loss: 0.3951 - val_mse: 0.3951 - val_mae: 0.4820\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2413 - mse: 0.2413 - mae: 0.3824 - val_loss: 0.3933 - val_mse: 0.3933 - val_mae: 0.4696\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2581 - mse: 0.2581 - mae: 0.3918 - val_loss: 0.4378 - val_mse: 0.4378 - val_mae: 0.5010\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2405 - mse: 0.2405 - mae: 0.3834 - val_loss: 0.4405 - val_mse: 0.4405 - val_mae: 0.4878\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2146 - mse: 0.2146 - mae: 0.3593 - val_loss: 0.4022 - val_mse: 0.4022 - val_mae: 0.4717\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2209 - mse: 0.2209 - mae: 0.3650 - val_loss: 0.4307 - val_mse: 0.4307 - val_mae: 0.5054\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2240 - mse: 0.2240 - mae: 0.3596 - val_loss: 0.4856 - val_mse: 0.4856 - val_mae: 0.5274\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2247 - mse: 0.2247 - mae: 0.3695 - val_loss: 0.4603 - val_mse: 0.4603 - val_mae: 0.5068\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2245 - mse: 0.2245 - mae: 0.3626 - val_loss: 0.4356 - val_mse: 0.4356 - val_mae: 0.4957\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2165 - mse: 0.2165 - mae: 0.3644 - val_loss: 0.4074 - val_mse: 0.4074 - val_mae: 0.5004\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2231 - mse: 0.2231 - mae: 0.3661 - val_loss: 0.4347 - val_mse: 0.4347 - val_mae: 0.5081\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2265 - mse: 0.2265 - mae: 0.3699 - val_loss: 0.4183 - val_mse: 0.4183 - val_mae: 0.5096\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2398 - mse: 0.2398 - mae: 0.3776 - val_loss: 0.4149 - val_mse: 0.4149 - val_mae: 0.5032\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1936 - mse: 0.1936 - mae: 0.3405 - val_loss: 0.4187 - val_mse: 0.4187 - val_mae: 0.4930\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2269 - mse: 0.2269 - mae: 0.3665 - val_loss: 0.4136 - val_mse: 0.4136 - val_mae: 0.4957\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2310 - mse: 0.2310 - mae: 0.3710 - val_loss: 0.3994 - val_mse: 0.3994 - val_mae: 0.4685\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2002 - mse: 0.2002 - mae: 0.3462 - val_loss: 0.4228 - val_mse: 0.4228 - val_mae: 0.4928\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2159 - mse: 0.2159 - mae: 0.3538 - val_loss: 0.4577 - val_mse: 0.4577 - val_mae: 0.5143\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2114 - mse: 0.2114 - mae: 0.3518 - val_loss: 0.4254 - val_mse: 0.4254 - val_mae: 0.4989\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2259 - mse: 0.2259 - mae: 0.3668 - val_loss: 0.4008 - val_mse: 0.4008 - val_mae: 0.4781\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2524 - mse: 0.2524 - mae: 0.3925 - val_loss: 0.4769 - val_mse: 0.4769 - val_mae: 0.5206\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1994 - mse: 0.1994 - mae: 0.3499 - val_loss: 0.4227 - val_mse: 0.4227 - val_mae: 0.4841\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2037 - mse: 0.2037 - mae: 0.3519 - val_loss: 0.3586 - val_mse: 0.3586 - val_mae: 0.4554\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2286 - mse: 0.2286 - mae: 0.3647 - val_loss: 0.4634 - val_mse: 0.4634 - val_mae: 0.5139\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2074 - mse: 0.2074 - mae: 0.3445 - val_loss: 0.3975 - val_mse: 0.3975 - val_mae: 0.4754\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2337 - mse: 0.2337 - mae: 0.3694 - val_loss: 0.4698 - val_mse: 0.4698 - val_mae: 0.5097\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2479 - mse: 0.2479 - mae: 0.3854 - val_loss: 0.4074 - val_mse: 0.4074 - val_mae: 0.4904\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2215 - mse: 0.2215 - mae: 0.3587 - val_loss: 0.3950 - val_mse: 0.3950 - val_mae: 0.4851\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2171 - mse: 0.2171 - mae: 0.3623 - val_loss: 0.4035 - val_mse: 0.4035 - val_mae: 0.4739\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1952 - mse: 0.1952 - mae: 0.3376 - val_loss: 0.4110 - val_mse: 0.4110 - val_mae: 0.4934\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2306 - mse: 0.2306 - mae: 0.3721 - val_loss: 0.4263 - val_mse: 0.4263 - val_mae: 0.4946\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2068 - mse: 0.2068 - mae: 0.3504 - val_loss: 0.3880 - val_mse: 0.3880 - val_mae: 0.4731\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2041 - mse: 0.2041 - mae: 0.3500 - val_loss: 0.3923 - val_mse: 0.3923 - val_mae: 0.4779\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2207 - mse: 0.2207 - mae: 0.3569 - val_loss: 0.3922 - val_mse: 0.3922 - val_mae: 0.4737\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1983 - mse: 0.1983 - mae: 0.3430 - val_loss: 0.4450 - val_mse: 0.4450 - val_mae: 0.5160\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2215 - mse: 0.2215 - mae: 0.3633 - val_loss: 0.4562 - val_mse: 0.4562 - val_mae: 0.5107\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2189 - mse: 0.2189 - mae: 0.3572 - val_loss: 0.4086 - val_mse: 0.4086 - val_mae: 0.4872\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2091 - mse: 0.2091 - mae: 0.3563 - val_loss: 0.4529 - val_mse: 0.4529 - val_mae: 0.5100\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1978 - mse: 0.1978 - mae: 0.3413 - val_loss: 0.4192 - val_mse: 0.4192 - val_mae: 0.4858\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2146 - mse: 0.2146 - mae: 0.3525 - val_loss: 0.4358 - val_mse: 0.4358 - val_mae: 0.4992\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2365 - mse: 0.2365 - mae: 0.3772 - val_loss: 0.4647 - val_mse: 0.4647 - val_mae: 0.5173\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2525 - mse: 0.2525 - mae: 0.3886 - val_loss: 0.4466 - val_mse: 0.4466 - val_mae: 0.5011\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2236 - mse: 0.2236 - mae: 0.3695 - val_loss: 0.4339 - val_mse: 0.4339 - val_mae: 0.5038\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2207 - mse: 0.2207 - mae: 0.3676 - val_loss: 0.3950 - val_mse: 0.3950 - val_mae: 0.4894\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2325 - mse: 0.2325 - mae: 0.3699 - val_loss: 0.4702 - val_mse: 0.4702 - val_mae: 0.5191\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1974 - mse: 0.1974 - mae: 0.3458 - val_loss: 0.3863 - val_mse: 0.3863 - val_mae: 0.4838\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2287 - mse: 0.2287 - mae: 0.3699 - val_loss: 0.4697 - val_mse: 0.4697 - val_mae: 0.5073\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1917 - mse: 0.1917 - mae: 0.3420 - val_loss: 0.3845 - val_mse: 0.3845 - val_mae: 0.4766\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2031 - mse: 0.2031 - mae: 0.3538 - val_loss: 0.3911 - val_mse: 0.3911 - val_mae: 0.4817\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1885 - mse: 0.1885 - mae: 0.3362 - val_loss: 0.3820 - val_mse: 0.3820 - val_mae: 0.4598\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2330 - mse: 0.2330 - mae: 0.3704 - val_loss: 0.4931 - val_mse: 0.4931 - val_mae: 0.5291\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2387 - mse: 0.2387 - mae: 0.3649 - val_loss: 0.4157 - val_mse: 0.4157 - val_mae: 0.4811\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2013 - mse: 0.2013 - mae: 0.3530 - val_loss: 0.3957 - val_mse: 0.3957 - val_mae: 0.4775\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2093 - mse: 0.2093 - mae: 0.3486 - val_loss: 0.4269 - val_mse: 0.4269 - val_mae: 0.4927\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1837 - mse: 0.1837 - mae: 0.3260 - val_loss: 0.4137 - val_mse: 0.4137 - val_mae: 0.4830\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2251 - mse: 0.2251 - mae: 0.3729 - val_loss: 0.3900 - val_mse: 0.3900 - val_mae: 0.4872\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2120 - mse: 0.2120 - mae: 0.3508 - val_loss: 0.4556 - val_mse: 0.4556 - val_mae: 0.5078\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1901 - mse: 0.1901 - mae: 0.3401 - val_loss: 0.3878 - val_mse: 0.3878 - val_mae: 0.4789\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1857 - mse: 0.1857 - mae: 0.3370 - val_loss: 0.4038 - val_mse: 0.4038 - val_mae: 0.4798\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2285 - mse: 0.2285 - mae: 0.3726 - val_loss: 0.3833 - val_mse: 0.3833 - val_mae: 0.4660\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2062 - mse: 0.2062 - mae: 0.3537 - val_loss: 0.3943 - val_mse: 0.3943 - val_mae: 0.4736\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1862 - mse: 0.1862 - mae: 0.3278 - val_loss: 0.4178 - val_mse: 0.4178 - val_mae: 0.4784\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2034 - mse: 0.2034 - mae: 0.3501 - val_loss: 0.3929 - val_mse: 0.3929 - val_mae: 0.4757\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2286 - mse: 0.2286 - mae: 0.3682 - val_loss: 0.3929 - val_mse: 0.3929 - val_mae: 0.4805\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2185 - mse: 0.2185 - mae: 0.3651 - val_loss: 0.4833 - val_mse: 0.4833 - val_mae: 0.5113\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1863 - mse: 0.1863 - mae: 0.3356 - val_loss: 0.4268 - val_mse: 0.4268 - val_mae: 0.5067\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1973 - mse: 0.1973 - mae: 0.3453 - val_loss: 0.4930 - val_mse: 0.4930 - val_mae: 0.5349\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2048 - mse: 0.2048 - mae: 0.3543 - val_loss: 0.3900 - val_mse: 0.3900 - val_mae: 0.4787\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1865 - mse: 0.1865 - mae: 0.3412 - val_loss: 0.5153 - val_mse: 0.5153 - val_mae: 0.5275\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1981 - mse: 0.1981 - mae: 0.3485 - val_loss: 0.4202 - val_mse: 0.4202 - val_mae: 0.4786\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2080 - mse: 0.2080 - mae: 0.3512 - val_loss: 0.5144 - val_mse: 0.5144 - val_mae: 0.5395\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1828 - mse: 0.1828 - mae: 0.3322 - val_loss: 0.3801 - val_mse: 0.3801 - val_mae: 0.4685\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1784 - mse: 0.1784 - mae: 0.3271 - val_loss: 0.4474 - val_mse: 0.4474 - val_mae: 0.4976\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2191 - mse: 0.2191 - mae: 0.3603 - val_loss: 0.4979 - val_mse: 0.4979 - val_mae: 0.5309\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2024 - mse: 0.2024 - mae: 0.3490 - val_loss: 0.3818 - val_mse: 0.3818 - val_mae: 0.4738\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2093 - mse: 0.2093 - mae: 0.3574 - val_loss: 0.3865 - val_mse: 0.3865 - val_mae: 0.4803\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1870 - mse: 0.1870 - mae: 0.3295 - val_loss: 0.3974 - val_mse: 0.3974 - val_mae: 0.4889\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1891 - mse: 0.1891 - mae: 0.3311 - val_loss: 0.4800 - val_mse: 0.4800 - val_mae: 0.5219\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1886 - mse: 0.1886 - mae: 0.3480 - val_loss: 0.4862 - val_mse: 0.4862 - val_mae: 0.5238\n",
            "Epoch 252/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2141 - mse: 0.2141 - mae: 0.3625 - val_loss: 0.4274 - val_mse: 0.4274 - val_mae: 0.4975\n",
            "Epoch 253/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1921 - mse: 0.1921 - mae: 0.3410 - val_loss: 0.4135 - val_mse: 0.4135 - val_mae: 0.4860\n",
            "Epoch 254/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1917 - mse: 0.1917 - mae: 0.3380 - val_loss: 0.4337 - val_mse: 0.4337 - val_mae: 0.4967\n",
            "Epoch 255/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1839 - mse: 0.1839 - mae: 0.3257 - val_loss: 0.4693 - val_mse: 0.4693 - val_mae: 0.5137\n",
            "Epoch 256/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1828 - mse: 0.1828 - mae: 0.3212 - val_loss: 0.4349 - val_mse: 0.4349 - val_mae: 0.4960\n",
            "Epoch 257/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1821 - mse: 0.1821 - mae: 0.3272 - val_loss: 0.4232 - val_mse: 0.4232 - val_mae: 0.4917\n",
            "Epoch 258/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1774 - mse: 0.1774 - mae: 0.3282 - val_loss: 0.3854 - val_mse: 0.3854 - val_mae: 0.4712\n",
            "Epoch 259/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1663 - mse: 0.1663 - mae: 0.3136 - val_loss: 0.4205 - val_mse: 0.4205 - val_mae: 0.4855\n",
            "Epoch 260/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1851 - mse: 0.1851 - mae: 0.3255 - val_loss: 0.4043 - val_mse: 0.4043 - val_mae: 0.4857\n",
            "Epoch 261/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1876 - mse: 0.1876 - mae: 0.3380 - val_loss: 0.4612 - val_mse: 0.4612 - val_mae: 0.5102\n",
            "Epoch 262/1000\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.1661 - mse: 0.1661 - mae: 0.3129 - val_loss: 0.4084 - val_mse: 0.4084 - val_mae: 0.4952\n",
            "Epoch 263/1000\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 0.1943 - mse: 0.1943 - mae: 0.3351 - val_loss: 0.4195 - val_mse: 0.4195 - val_mae: 0.4916\n",
            "Epoch 264/1000\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.1830 - mse: 0.1830 - mae: 0.3302 - val_loss: 0.4347 - val_mse: 0.4347 - val_mae: 0.5085\n",
            "Epoch 265/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1758 - mse: 0.1758 - mae: 0.3192 - val_loss: 0.4269 - val_mse: 0.4269 - val_mae: 0.5081\n",
            "Epoch 266/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1954 - mse: 0.1954 - mae: 0.3453 - val_loss: 0.4684 - val_mse: 0.4684 - val_mae: 0.5112\n",
            "Epoch 267/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1862 - mse: 0.1862 - mae: 0.3364 - val_loss: 0.4141 - val_mse: 0.4141 - val_mae: 0.4859\n",
            "Epoch 268/1000\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.1751 - mse: 0.1751 - mae: 0.3234 - val_loss: 0.4288 - val_mse: 0.4288 - val_mae: 0.4874\n",
            "Epoch 269/1000\n",
            "22/22 [==============================] - 1s 22ms/step - loss: 0.1780 - mse: 0.1780 - mae: 0.3171 - val_loss: 0.4298 - val_mse: 0.4298 - val_mae: 0.5047\n",
            "Epoch 270/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1644 - mse: 0.1644 - mae: 0.3084 - val_loss: 0.4443 - val_mse: 0.4443 - val_mae: 0.4939\n",
            "Epoch 271/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1653 - mse: 0.1653 - mae: 0.3082 - val_loss: 0.4186 - val_mse: 0.4186 - val_mae: 0.4855\n",
            "Epoch 272/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2018 - mse: 0.2018 - mae: 0.3490 - val_loss: 0.3977 - val_mse: 0.3977 - val_mae: 0.4724\n",
            "Epoch 273/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1801 - mse: 0.1801 - mae: 0.3282 - val_loss: 0.4604 - val_mse: 0.4604 - val_mae: 0.5024\n",
            "Epoch 274/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1806 - mse: 0.1806 - mae: 0.3302 - val_loss: 0.4047 - val_mse: 0.4047 - val_mae: 0.4791\n",
            "Epoch 275/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1879 - mse: 0.1879 - mae: 0.3358 - val_loss: 0.4054 - val_mse: 0.4054 - val_mae: 0.4823\n",
            "Epoch 276/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1831 - mse: 0.1831 - mae: 0.3318 - val_loss: 0.3989 - val_mse: 0.3989 - val_mae: 0.4780\n",
            "Epoch 277/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2225 - mse: 0.2225 - mae: 0.3652 - val_loss: 0.6598 - val_mse: 0.6598 - val_mae: 0.6353\n",
            "Epoch 278/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2539 - mse: 0.2539 - mae: 0.3917 - val_loss: 0.4075 - val_mse: 0.4075 - val_mae: 0.4920\n",
            "Epoch 279/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1795 - mse: 0.1795 - mae: 0.3306 - val_loss: 0.4151 - val_mse: 0.4151 - val_mae: 0.4886\n",
            "Epoch 280/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1904 - mse: 0.1904 - mae: 0.3399 - val_loss: 0.4444 - val_mse: 0.4444 - val_mae: 0.5033\n",
            "Epoch 281/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2016 - mse: 0.2016 - mae: 0.3435 - val_loss: 0.4089 - val_mse: 0.4089 - val_mae: 0.4888\n",
            "Epoch 282/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1686 - mse: 0.1686 - mae: 0.3170 - val_loss: 0.4156 - val_mse: 0.4156 - val_mae: 0.4931\n",
            "Epoch 283/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1566 - mse: 0.1566 - mae: 0.3012 - val_loss: 0.4275 - val_mse: 0.4275 - val_mae: 0.5083\n",
            "Epoch 284/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1673 - mse: 0.1673 - mae: 0.3083 - val_loss: 0.4238 - val_mse: 0.4238 - val_mae: 0.4892\n",
            "Epoch 285/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1710 - mse: 0.1710 - mae: 0.3162 - val_loss: 0.4165 - val_mse: 0.4165 - val_mae: 0.4971\n",
            "Epoch 286/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2003 - mse: 0.2003 - mae: 0.3458 - val_loss: 0.4127 - val_mse: 0.4127 - val_mae: 0.4907\n",
            "Epoch 287/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1747 - mse: 0.1747 - mae: 0.3207 - val_loss: 0.4198 - val_mse: 0.4198 - val_mae: 0.4950\n",
            "Epoch 288/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1787 - mse: 0.1787 - mae: 0.3226 - val_loss: 0.3977 - val_mse: 0.3977 - val_mae: 0.4814\n",
            "Epoch 289/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1704 - mse: 0.1704 - mae: 0.3226 - val_loss: 0.4144 - val_mse: 0.4144 - val_mae: 0.4845\n",
            "Epoch 290/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1771 - mse: 0.1771 - mae: 0.3270 - val_loss: 0.4167 - val_mse: 0.4167 - val_mae: 0.4887\n",
            "Epoch 291/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1813 - mse: 0.1813 - mae: 0.3289 - val_loss: 0.3882 - val_mse: 0.3882 - val_mae: 0.4785\n",
            "Epoch 292/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2278 - mse: 0.2278 - mae: 0.3718 - val_loss: 0.4626 - val_mse: 0.4626 - val_mae: 0.5186\n",
            "Epoch 293/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2034 - mse: 0.2034 - mae: 0.3463 - val_loss: 0.4273 - val_mse: 0.4273 - val_mae: 0.4966\n",
            "Epoch 294/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1993 - mse: 0.1993 - mae: 0.3409 - val_loss: 0.4706 - val_mse: 0.4706 - val_mae: 0.5184\n",
            "Epoch 295/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2060 - mse: 0.2060 - mae: 0.3553 - val_loss: 0.4275 - val_mse: 0.4275 - val_mae: 0.5040\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.5877 - mse: 0.5877 - mae: 0.5674\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 15ms/step - loss: 17.4355 - mse: 17.4355 - mae: 3.7542 - val_loss: 5.7004 - val_mse: 5.7004 - val_mae: 1.9801\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 4.2022 - mse: 4.2022 - mae: 1.6053 - val_loss: 3.4955 - val_mse: 3.4955 - val_mae: 1.4807\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.9803 - mse: 2.9803 - mae: 1.3900 - val_loss: 2.5126 - val_mse: 2.5126 - val_mae: 1.3018\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.2624 - mse: 2.2624 - mae: 1.1964 - val_loss: 2.0465 - val_mse: 2.0465 - val_mae: 1.1263\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.9760 - mse: 1.9760 - mae: 1.1248 - val_loss: 1.8947 - val_mse: 1.8947 - val_mae: 1.0854\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.9238 - mse: 1.9238 - mae: 1.0981 - val_loss: 1.8415 - val_mse: 1.8415 - val_mae: 1.0631\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.7784 - mse: 1.7784 - mae: 1.0514 - val_loss: 1.8221 - val_mse: 1.8221 - val_mae: 1.0430\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.6582 - mse: 1.6582 - mae: 1.0264 - val_loss: 1.8323 - val_mse: 1.8323 - val_mae: 1.0251\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.6357 - mse: 1.6357 - mae: 1.0273 - val_loss: 1.7629 - val_mse: 1.7629 - val_mae: 1.0085\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.5638 - mse: 1.5638 - mae: 0.9927 - val_loss: 1.6486 - val_mse: 1.6486 - val_mae: 0.9612\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.3746 - mse: 1.3746 - mae: 0.9405 - val_loss: 1.6176 - val_mse: 1.6176 - val_mae: 0.9448\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.3084 - mse: 1.3084 - mae: 0.9041 - val_loss: 1.4708 - val_mse: 1.4708 - val_mae: 0.9060\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.3249 - mse: 1.3249 - mae: 0.9168 - val_loss: 1.4509 - val_mse: 1.4509 - val_mae: 0.8953\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1908 - mse: 1.1908 - mae: 0.8743 - val_loss: 1.3587 - val_mse: 1.3587 - val_mae: 0.8637\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1558 - mse: 1.1558 - mae: 0.8656 - val_loss: 1.3164 - val_mse: 1.3164 - val_mae: 0.8553\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1574 - mse: 1.1574 - mae: 0.8568 - val_loss: 1.2536 - val_mse: 1.2536 - val_mae: 0.8321\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0149 - mse: 1.0149 - mae: 0.7929 - val_loss: 1.2632 - val_mse: 1.2632 - val_mae: 0.8276\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0420 - mse: 1.0420 - mae: 0.8078 - val_loss: 1.1982 - val_mse: 1.1982 - val_mae: 0.8048\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.9718 - mse: 0.9718 - mae: 0.7773 - val_loss: 1.2044 - val_mse: 1.2044 - val_mae: 0.8291\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.9140 - mse: 0.9140 - mae: 0.7514 - val_loss: 1.1366 - val_mse: 1.1366 - val_mae: 0.7839\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8801 - mse: 0.8801 - mae: 0.7360 - val_loss: 1.0811 - val_mse: 1.0811 - val_mae: 0.7763\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8785 - mse: 0.8785 - mae: 0.7527 - val_loss: 1.0240 - val_mse: 1.0240 - val_mae: 0.7503\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.8164 - mse: 0.8164 - mae: 0.7085 - val_loss: 0.9936 - val_mse: 0.9936 - val_mae: 0.7353\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8076 - mse: 0.8076 - mae: 0.7065 - val_loss: 0.9672 - val_mse: 0.9672 - val_mae: 0.7224\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7362 - mse: 0.7362 - mae: 0.6775 - val_loss: 0.9730 - val_mse: 0.9730 - val_mae: 0.7186\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7744 - mse: 0.7744 - mae: 0.7011 - val_loss: 0.8826 - val_mse: 0.8826 - val_mae: 0.7019\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7253 - mse: 0.7253 - mae: 0.6847 - val_loss: 0.8529 - val_mse: 0.8529 - val_mae: 0.6814\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6797 - mse: 0.6797 - mae: 0.6478 - val_loss: 0.9117 - val_mse: 0.9117 - val_mae: 0.6884\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6254 - mse: 0.6254 - mae: 0.6195 - val_loss: 0.7830 - val_mse: 0.7830 - val_mae: 0.6518\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6198 - mse: 0.6198 - mae: 0.6287 - val_loss: 0.8138 - val_mse: 0.8138 - val_mae: 0.6676\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6406 - mse: 0.6406 - mae: 0.6268 - val_loss: 0.7937 - val_mse: 0.7937 - val_mae: 0.6474\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5875 - mse: 0.5875 - mae: 0.5915 - val_loss: 0.7668 - val_mse: 0.7668 - val_mae: 0.6375\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6017 - mse: 0.6017 - mae: 0.6020 - val_loss: 0.8044 - val_mse: 0.8044 - val_mae: 0.6396\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5515 - mse: 0.5515 - mae: 0.5824 - val_loss: 0.9054 - val_mse: 0.9054 - val_mae: 0.6714\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5369 - mse: 0.5369 - mae: 0.5756 - val_loss: 0.8160 - val_mse: 0.8160 - val_mae: 0.6411\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5729 - mse: 0.5729 - mae: 0.5912 - val_loss: 0.7795 - val_mse: 0.7795 - val_mae: 0.6440\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5234 - mse: 0.5234 - mae: 0.5715 - val_loss: 0.7649 - val_mse: 0.7649 - val_mae: 0.6294\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5464 - mse: 0.5464 - mae: 0.5802 - val_loss: 0.6639 - val_mse: 0.6639 - val_mae: 0.5975\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5063 - mse: 0.5063 - mae: 0.5650 - val_loss: 0.6710 - val_mse: 0.6710 - val_mae: 0.5813\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5205 - mse: 0.5205 - mae: 0.5699 - val_loss: 0.7084 - val_mse: 0.7084 - val_mae: 0.6034\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4792 - mse: 0.4792 - mae: 0.5413 - val_loss: 0.6837 - val_mse: 0.6837 - val_mae: 0.5904\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4825 - mse: 0.4825 - mae: 0.5340 - val_loss: 0.6716 - val_mse: 0.6716 - val_mae: 0.5874\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4739 - mse: 0.4739 - mae: 0.5315 - val_loss: 0.6616 - val_mse: 0.6616 - val_mae: 0.5797\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4287 - mse: 0.4287 - mae: 0.5113 - val_loss: 0.6387 - val_mse: 0.6387 - val_mae: 0.5791\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4868 - mse: 0.4868 - mae: 0.5451 - val_loss: 0.6342 - val_mse: 0.6342 - val_mae: 0.5628\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4343 - mse: 0.4343 - mae: 0.5154 - val_loss: 0.5951 - val_mse: 0.5951 - val_mae: 0.5586\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4556 - mse: 0.4556 - mae: 0.5298 - val_loss: 0.6236 - val_mse: 0.6236 - val_mae: 0.5711\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4353 - mse: 0.4353 - mae: 0.5072 - val_loss: 0.6243 - val_mse: 0.6243 - val_mae: 0.5758\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4349 - mse: 0.4349 - mae: 0.5116 - val_loss: 0.5740 - val_mse: 0.5740 - val_mae: 0.5439\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3994 - mse: 0.3994 - mae: 0.4886 - val_loss: 0.5731 - val_mse: 0.5731 - val_mae: 0.5437\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4352 - mse: 0.4352 - mae: 0.5236 - val_loss: 0.5674 - val_mse: 0.5674 - val_mae: 0.5448\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3816 - mse: 0.3816 - mae: 0.4817 - val_loss: 0.5650 - val_mse: 0.5650 - val_mae: 0.5548\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3883 - mse: 0.3883 - mae: 0.4910 - val_loss: 0.5548 - val_mse: 0.5548 - val_mae: 0.5392\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3924 - mse: 0.3924 - mae: 0.4832 - val_loss: 0.5143 - val_mse: 0.5143 - val_mae: 0.5201\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4168 - mse: 0.4168 - mae: 0.5006 - val_loss: 0.6197 - val_mse: 0.6197 - val_mae: 0.5919\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3712 - mse: 0.3712 - mae: 0.4712 - val_loss: 0.5481 - val_mse: 0.5481 - val_mae: 0.5483\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3792 - mse: 0.3792 - mae: 0.4863 - val_loss: 0.5391 - val_mse: 0.5391 - val_mae: 0.5302\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4118 - mse: 0.4118 - mae: 0.4958 - val_loss: 0.5026 - val_mse: 0.5026 - val_mae: 0.5201\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3555 - mse: 0.3555 - mae: 0.4674 - val_loss: 0.5295 - val_mse: 0.5295 - val_mae: 0.5338\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3629 - mse: 0.3629 - mae: 0.4700 - val_loss: 0.5096 - val_mse: 0.5096 - val_mae: 0.5172\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3705 - mse: 0.3705 - mae: 0.4703 - val_loss: 0.5309 - val_mse: 0.5309 - val_mae: 0.5315\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3539 - mse: 0.3539 - mae: 0.4656 - val_loss: 0.5207 - val_mse: 0.5207 - val_mae: 0.5150\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3670 - mse: 0.3670 - mae: 0.4750 - val_loss: 0.5383 - val_mse: 0.5383 - val_mae: 0.5220\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3614 - mse: 0.3614 - mae: 0.4615 - val_loss: 0.5214 - val_mse: 0.5214 - val_mae: 0.5449\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3581 - mse: 0.3581 - mae: 0.4688 - val_loss: 0.5046 - val_mse: 0.5046 - val_mae: 0.5060\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3331 - mse: 0.3331 - mae: 0.4524 - val_loss: 0.4956 - val_mse: 0.4956 - val_mae: 0.5192\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3492 - mse: 0.3492 - mae: 0.4601 - val_loss: 0.5357 - val_mse: 0.5357 - val_mae: 0.5441\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3743 - mse: 0.3743 - mae: 0.4756 - val_loss: 0.5272 - val_mse: 0.5272 - val_mae: 0.5252\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3408 - mse: 0.3408 - mae: 0.4503 - val_loss: 0.5219 - val_mse: 0.5219 - val_mae: 0.5349\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3383 - mse: 0.3383 - mae: 0.4598 - val_loss: 0.5003 - val_mse: 0.5003 - val_mae: 0.5279\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2977 - mse: 0.2977 - mae: 0.4233 - val_loss: 0.5039 - val_mse: 0.5039 - val_mae: 0.5255\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3423 - mse: 0.3423 - mae: 0.4657 - val_loss: 0.4987 - val_mse: 0.4987 - val_mae: 0.5373\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3455 - mse: 0.3455 - mae: 0.4592 - val_loss: 0.4705 - val_mse: 0.4705 - val_mae: 0.5079\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3449 - mse: 0.3449 - mae: 0.4639 - val_loss: 0.4991 - val_mse: 0.4991 - val_mae: 0.5420\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3503 - mse: 0.3503 - mae: 0.4569 - val_loss: 0.4855 - val_mse: 0.4855 - val_mae: 0.5118\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3233 - mse: 0.3233 - mae: 0.4363 - val_loss: 0.5183 - val_mse: 0.5183 - val_mae: 0.5540\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3141 - mse: 0.3141 - mae: 0.4396 - val_loss: 0.4793 - val_mse: 0.4793 - val_mae: 0.5203\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3022 - mse: 0.3022 - mae: 0.4339 - val_loss: 0.5229 - val_mse: 0.5229 - val_mae: 0.5461\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3289 - mse: 0.3289 - mae: 0.4428 - val_loss: 0.4993 - val_mse: 0.4993 - val_mae: 0.5418\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3320 - mse: 0.3320 - mae: 0.4513 - val_loss: 0.4894 - val_mse: 0.4894 - val_mae: 0.5219\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3019 - mse: 0.3019 - mae: 0.4236 - val_loss: 0.4835 - val_mse: 0.4835 - val_mae: 0.5345\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3066 - mse: 0.3066 - mae: 0.4315 - val_loss: 0.4684 - val_mse: 0.4684 - val_mae: 0.5130\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2869 - mse: 0.2869 - mae: 0.4119 - val_loss: 0.4787 - val_mse: 0.4787 - val_mae: 0.5272\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2981 - mse: 0.2981 - mae: 0.4261 - val_loss: 0.4974 - val_mse: 0.4974 - val_mae: 0.5244\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2968 - mse: 0.2968 - mae: 0.4311 - val_loss: 0.4692 - val_mse: 0.4692 - val_mae: 0.5225\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2948 - mse: 0.2948 - mae: 0.4295 - val_loss: 0.5090 - val_mse: 0.5090 - val_mae: 0.5244\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2734 - mse: 0.2734 - mae: 0.4070 - val_loss: 0.4861 - val_mse: 0.4861 - val_mae: 0.5117\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2930 - mse: 0.2930 - mae: 0.4168 - val_loss: 0.4592 - val_mse: 0.4592 - val_mae: 0.4958\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2941 - mse: 0.2941 - mae: 0.4236 - val_loss: 0.4414 - val_mse: 0.4414 - val_mae: 0.5170\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2960 - mse: 0.2960 - mae: 0.4263 - val_loss: 0.4382 - val_mse: 0.4382 - val_mae: 0.5079\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3186 - mse: 0.3186 - mae: 0.4449 - val_loss: 0.4884 - val_mse: 0.4884 - val_mae: 0.5325\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3099 - mse: 0.3099 - mae: 0.4394 - val_loss: 0.4811 - val_mse: 0.4811 - val_mae: 0.5371\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2773 - mse: 0.2773 - mae: 0.4101 - val_loss: 0.4773 - val_mse: 0.4773 - val_mae: 0.5229\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3194 - mse: 0.3194 - mae: 0.4475 - val_loss: 0.4686 - val_mse: 0.4686 - val_mae: 0.5334\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3042 - mse: 0.3042 - mae: 0.4346 - val_loss: 0.4409 - val_mse: 0.4409 - val_mae: 0.5031\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2932 - mse: 0.2932 - mae: 0.4258 - val_loss: 0.4585 - val_mse: 0.4585 - val_mae: 0.5112\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2871 - mse: 0.2871 - mae: 0.4111 - val_loss: 0.4726 - val_mse: 0.4726 - val_mae: 0.5131\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2956 - mse: 0.2956 - mae: 0.4249 - val_loss: 0.4656 - val_mse: 0.4656 - val_mae: 0.5195\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.2857 - mse: 0.2857 - mae: 0.4148 - val_loss: 0.4572 - val_mse: 0.4572 - val_mae: 0.5039\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3309 - mse: 0.3309 - mae: 0.4508 - val_loss: 0.4682 - val_mse: 0.4682 - val_mae: 0.5061\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2723 - mse: 0.2723 - mae: 0.4044 - val_loss: 0.4446 - val_mse: 0.4446 - val_mae: 0.5037\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2533 - mse: 0.2533 - mae: 0.3950 - val_loss: 0.4531 - val_mse: 0.4531 - val_mae: 0.5128\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2569 - mse: 0.2569 - mae: 0.3929 - val_loss: 0.4610 - val_mse: 0.4610 - val_mae: 0.5249\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2580 - mse: 0.2580 - mae: 0.3947 - val_loss: 0.4450 - val_mse: 0.4450 - val_mae: 0.5078\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2720 - mse: 0.2720 - mae: 0.4083 - val_loss: 0.4881 - val_mse: 0.4881 - val_mae: 0.5441\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2685 - mse: 0.2685 - mae: 0.4035 - val_loss: 0.4814 - val_mse: 0.4814 - val_mae: 0.5362\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2646 - mse: 0.2646 - mae: 0.3960 - val_loss: 0.4420 - val_mse: 0.4420 - val_mae: 0.5048\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3004 - mse: 0.3004 - mae: 0.4294 - val_loss: 0.4933 - val_mse: 0.4933 - val_mae: 0.5532\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2400 - mse: 0.2400 - mae: 0.3912 - val_loss: 0.5338 - val_mse: 0.5338 - val_mae: 0.5788\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2655 - mse: 0.2655 - mae: 0.3976 - val_loss: 0.4525 - val_mse: 0.4525 - val_mae: 0.5140\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2347 - mse: 0.2347 - mae: 0.3819 - val_loss: 0.4497 - val_mse: 0.4497 - val_mae: 0.5049\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2707 - mse: 0.2707 - mae: 0.4062 - val_loss: 0.4451 - val_mse: 0.4451 - val_mae: 0.5147\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2759 - mse: 0.2759 - mae: 0.4182 - val_loss: 0.4643 - val_mse: 0.4643 - val_mae: 0.5235\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2606 - mse: 0.2606 - mae: 0.4017 - val_loss: 0.4974 - val_mse: 0.4974 - val_mae: 0.5581\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2798 - mse: 0.2798 - mae: 0.4119 - val_loss: 0.4409 - val_mse: 0.4409 - val_mae: 0.5277\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2567 - mse: 0.2567 - mae: 0.3936 - val_loss: 0.4407 - val_mse: 0.4407 - val_mae: 0.5050\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2590 - mse: 0.2590 - mae: 0.4010 - val_loss: 0.4381 - val_mse: 0.4381 - val_mae: 0.5091\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2784 - mse: 0.2784 - mae: 0.4134 - val_loss: 0.4270 - val_mse: 0.4270 - val_mae: 0.5088\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2543 - mse: 0.2543 - mae: 0.3888 - val_loss: 0.4230 - val_mse: 0.4230 - val_mae: 0.5187\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2457 - mse: 0.2457 - mae: 0.3899 - val_loss: 0.4228 - val_mse: 0.4228 - val_mae: 0.5043\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2587 - mse: 0.2587 - mae: 0.4017 - val_loss: 0.4428 - val_mse: 0.4428 - val_mae: 0.5113\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2529 - mse: 0.2529 - mae: 0.3830 - val_loss: 0.4669 - val_mse: 0.4669 - val_mae: 0.5161\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2673 - mse: 0.2673 - mae: 0.4094 - val_loss: 0.4257 - val_mse: 0.4257 - val_mae: 0.5065\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2639 - mse: 0.2639 - mae: 0.3959 - val_loss: 0.4222 - val_mse: 0.4222 - val_mae: 0.4948\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2605 - mse: 0.2605 - mae: 0.3962 - val_loss: 0.4292 - val_mse: 0.4292 - val_mae: 0.5024\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3050 - mse: 0.3050 - mae: 0.4263 - val_loss: 0.4284 - val_mse: 0.4284 - val_mae: 0.5090\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2623 - mse: 0.2623 - mae: 0.3989 - val_loss: 0.4481 - val_mse: 0.4481 - val_mae: 0.5287\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2720 - mse: 0.2720 - mae: 0.4077 - val_loss: 0.4551 - val_mse: 0.4551 - val_mae: 0.5284\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2484 - mse: 0.2484 - mae: 0.3790 - val_loss: 0.4525 - val_mse: 0.4525 - val_mae: 0.5339\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.2307 - mse: 0.2307 - mae: 0.3760 - val_loss: 0.4434 - val_mse: 0.4434 - val_mae: 0.5173\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2605 - mse: 0.2605 - mae: 0.3947 - val_loss: 0.4279 - val_mse: 0.4279 - val_mae: 0.5096\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2597 - mse: 0.2597 - mae: 0.3996 - val_loss: 0.4909 - val_mse: 0.4909 - val_mae: 0.5593\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2478 - mse: 0.2478 - mae: 0.3936 - val_loss: 0.4214 - val_mse: 0.4214 - val_mae: 0.4902\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2512 - mse: 0.2512 - mae: 0.3901 - val_loss: 0.4270 - val_mse: 0.4270 - val_mae: 0.5052\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2563 - mse: 0.2563 - mae: 0.3884 - val_loss: 0.4582 - val_mse: 0.4582 - val_mae: 0.5321\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2336 - mse: 0.2336 - mae: 0.3833 - val_loss: 0.4144 - val_mse: 0.4144 - val_mae: 0.4883\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2557 - mse: 0.2557 - mae: 0.3899 - val_loss: 0.4245 - val_mse: 0.4245 - val_mae: 0.4888\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2489 - mse: 0.2489 - mae: 0.3911 - val_loss: 0.4297 - val_mse: 0.4297 - val_mae: 0.5173\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2484 - mse: 0.2484 - mae: 0.3882 - val_loss: 0.4259 - val_mse: 0.4259 - val_mae: 0.5028\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2358 - mse: 0.2358 - mae: 0.3823 - val_loss: 0.4480 - val_mse: 0.4480 - val_mae: 0.5129\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2378 - mse: 0.2378 - mae: 0.3829 - val_loss: 0.4560 - val_mse: 0.4560 - val_mae: 0.5144\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2460 - mse: 0.2460 - mae: 0.3918 - val_loss: 0.4555 - val_mse: 0.4555 - val_mae: 0.5047\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2526 - mse: 0.2526 - mae: 0.3847 - val_loss: 0.4176 - val_mse: 0.4176 - val_mae: 0.4977\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2419 - mse: 0.2419 - mae: 0.3844 - val_loss: 0.4572 - val_mse: 0.4572 - val_mae: 0.5309\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2498 - mse: 0.2498 - mae: 0.3879 - val_loss: 0.4976 - val_mse: 0.4976 - val_mae: 0.5594\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2640 - mse: 0.2640 - mae: 0.4024 - val_loss: 0.4567 - val_mse: 0.4567 - val_mae: 0.5111\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2323 - mse: 0.2323 - mae: 0.3756 - val_loss: 0.4296 - val_mse: 0.4296 - val_mae: 0.5108\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2276 - mse: 0.2276 - mae: 0.3708 - val_loss: 0.4295 - val_mse: 0.4295 - val_mae: 0.5044\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2267 - mse: 0.2267 - mae: 0.3778 - val_loss: 0.4334 - val_mse: 0.4334 - val_mae: 0.4934\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2237 - mse: 0.2237 - mae: 0.3717 - val_loss: 0.4439 - val_mse: 0.4439 - val_mae: 0.5089\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2246 - mse: 0.2246 - mae: 0.3669 - val_loss: 0.4131 - val_mse: 0.4131 - val_mae: 0.4933\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2420 - mse: 0.2420 - mae: 0.3867 - val_loss: 0.4695 - val_mse: 0.4695 - val_mae: 0.5193\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2411 - mse: 0.2411 - mae: 0.3854 - val_loss: 0.4338 - val_mse: 0.4338 - val_mae: 0.5106\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2334 - mse: 0.2334 - mae: 0.3763 - val_loss: 0.4426 - val_mse: 0.4426 - val_mae: 0.5186\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2330 - mse: 0.2330 - mae: 0.3692 - val_loss: 0.4550 - val_mse: 0.4550 - val_mae: 0.5139\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2445 - mse: 0.2445 - mae: 0.3844 - val_loss: 0.4350 - val_mse: 0.4350 - val_mae: 0.4954\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2410 - mse: 0.2410 - mae: 0.3838 - val_loss: 0.4561 - val_mse: 0.4561 - val_mae: 0.5330\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2153 - mse: 0.2153 - mae: 0.3605 - val_loss: 0.4280 - val_mse: 0.4280 - val_mae: 0.4982\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2157 - mse: 0.2157 - mae: 0.3590 - val_loss: 0.4274 - val_mse: 0.4274 - val_mae: 0.5055\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2276 - mse: 0.2276 - mae: 0.3752 - val_loss: 0.4971 - val_mse: 0.4971 - val_mae: 0.5493\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2099 - mse: 0.2099 - mae: 0.3565 - val_loss: 0.4655 - val_mse: 0.4655 - val_mae: 0.5322\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2455 - mse: 0.2455 - mae: 0.3860 - val_loss: 0.4590 - val_mse: 0.4590 - val_mae: 0.5213\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2307 - mse: 0.2307 - mae: 0.3771 - val_loss: 0.4523 - val_mse: 0.4523 - val_mae: 0.5070\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2544 - mse: 0.2544 - mae: 0.3969 - val_loss: 0.4485 - val_mse: 0.4485 - val_mae: 0.5125\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2313 - mse: 0.2313 - mae: 0.3696 - val_loss: 0.4458 - val_mse: 0.4458 - val_mae: 0.5005\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2272 - mse: 0.2272 - mae: 0.3743 - val_loss: 0.4455 - val_mse: 0.4455 - val_mae: 0.5190\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2110 - mse: 0.2110 - mae: 0.3571 - val_loss: 0.4411 - val_mse: 0.4411 - val_mae: 0.5118\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2621 - mse: 0.2621 - mae: 0.3980 - val_loss: 0.4502 - val_mse: 0.4502 - val_mae: 0.5102\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2248 - mse: 0.2248 - mae: 0.3716 - val_loss: 0.4704 - val_mse: 0.4704 - val_mae: 0.5504\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1995 - mse: 0.1995 - mae: 0.3503 - val_loss: 0.4796 - val_mse: 0.4796 - val_mae: 0.5415\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2024 - mse: 0.2024 - mae: 0.3546 - val_loss: 0.4322 - val_mse: 0.4322 - val_mae: 0.5058\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2137 - mse: 0.2137 - mae: 0.3595 - val_loss: 0.4383 - val_mse: 0.4383 - val_mae: 0.5221\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2238 - mse: 0.2238 - mae: 0.3696 - val_loss: 0.4596 - val_mse: 0.4596 - val_mae: 0.5394\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2345 - mse: 0.2345 - mae: 0.3747 - val_loss: 0.4426 - val_mse: 0.4426 - val_mae: 0.5077\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2571 - mse: 0.2571 - mae: 0.3900 - val_loss: 0.4348 - val_mse: 0.4348 - val_mae: 0.5125\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2333 - mse: 0.2333 - mae: 0.3776 - val_loss: 0.4560 - val_mse: 0.4560 - val_mae: 0.5150\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2376 - mse: 0.2376 - mae: 0.3803 - val_loss: 0.5303 - val_mse: 0.5303 - val_mae: 0.5759\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2346 - mse: 0.2346 - mae: 0.3771 - val_loss: 0.4273 - val_mse: 0.4273 - val_mae: 0.4975\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2421 - mse: 0.2421 - mae: 0.3876 - val_loss: 0.4618 - val_mse: 0.4618 - val_mae: 0.5392\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2283 - mse: 0.2283 - mae: 0.3790 - val_loss: 0.4311 - val_mse: 0.4311 - val_mae: 0.5021\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2288 - mse: 0.2288 - mae: 0.3685 - val_loss: 0.4367 - val_mse: 0.4367 - val_mae: 0.5134\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2471 - mse: 0.2471 - mae: 0.3884 - val_loss: 0.4556 - val_mse: 0.4556 - val_mae: 0.5295\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2390 - mse: 0.2390 - mae: 0.3837 - val_loss: 0.4305 - val_mse: 0.4305 - val_mae: 0.5117\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2313 - mse: 0.2313 - mae: 0.3806 - val_loss: 0.5085 - val_mse: 0.5085 - val_mae: 0.5662\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2064 - mse: 0.2064 - mae: 0.3477 - val_loss: 0.4738 - val_mse: 0.4738 - val_mae: 0.5154\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2320 - mse: 0.2320 - mae: 0.3765 - val_loss: 0.4486 - val_mse: 0.4486 - val_mae: 0.5182\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2203 - mse: 0.2203 - mae: 0.3638 - val_loss: 0.4563 - val_mse: 0.4563 - val_mae: 0.5258\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2197 - mse: 0.2197 - mae: 0.3666 - val_loss: 0.4658 - val_mse: 0.4658 - val_mae: 0.5333\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2016 - mse: 0.2016 - mae: 0.3483 - val_loss: 0.4770 - val_mse: 0.4770 - val_mae: 0.5447\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2068 - mse: 0.2068 - mae: 0.3525 - val_loss: 0.4406 - val_mse: 0.4406 - val_mae: 0.5160\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1946 - mse: 0.1946 - mae: 0.3496 - val_loss: 0.4500 - val_mse: 0.4500 - val_mae: 0.5196\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1936 - mse: 0.1936 - mae: 0.3394 - val_loss: 0.4568 - val_mse: 0.4568 - val_mae: 0.5198\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2156 - mse: 0.2156 - mae: 0.3587 - val_loss: 0.4785 - val_mse: 0.4785 - val_mae: 0.5331\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2023 - mse: 0.2023 - mae: 0.3487 - val_loss: 0.4170 - val_mse: 0.4170 - val_mae: 0.4975\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1983 - mse: 0.1983 - mae: 0.3440 - val_loss: 0.4515 - val_mse: 0.4515 - val_mae: 0.5221\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2313 - mse: 0.2313 - mae: 0.3765 - val_loss: 0.4503 - val_mse: 0.4503 - val_mae: 0.5234\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2024 - mse: 0.2024 - mae: 0.3460 - val_loss: 0.4390 - val_mse: 0.4390 - val_mae: 0.5064\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2328 - mse: 0.2328 - mae: 0.3737 - val_loss: 0.6067 - val_mse: 0.6067 - val_mae: 0.6168\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2237 - mse: 0.2237 - mae: 0.3655 - val_loss: 0.4745 - val_mse: 0.4745 - val_mae: 0.5334\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2051 - mse: 0.2051 - mae: 0.3461 - val_loss: 0.4410 - val_mse: 0.4410 - val_mae: 0.5158\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1873 - mse: 0.1873 - mae: 0.3360 - val_loss: 0.4661 - val_mse: 0.4661 - val_mae: 0.5378\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2122 - mse: 0.2122 - mae: 0.3581 - val_loss: 0.4766 - val_mse: 0.4766 - val_mae: 0.5439\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2040 - mse: 0.2040 - mae: 0.3506 - val_loss: 0.4461 - val_mse: 0.4461 - val_mae: 0.5210\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2081 - mse: 0.2081 - mae: 0.3595 - val_loss: 0.4456 - val_mse: 0.4456 - val_mae: 0.5152\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2158 - mse: 0.2158 - mae: 0.3571 - val_loss: 0.4748 - val_mse: 0.4748 - val_mae: 0.5353\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2162 - mse: 0.2162 - mae: 0.3598 - val_loss: 0.4525 - val_mse: 0.4525 - val_mae: 0.5158\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2391 - mse: 0.2391 - mae: 0.3851 - val_loss: 0.4381 - val_mse: 0.4381 - val_mae: 0.5071\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2409 - mse: 0.2409 - mae: 0.3910 - val_loss: 0.5176 - val_mse: 0.5176 - val_mae: 0.5658\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2183 - mse: 0.2183 - mae: 0.3636 - val_loss: 0.4666 - val_mse: 0.4666 - val_mae: 0.5235\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2145 - mse: 0.2145 - mae: 0.3545 - val_loss: 0.4537 - val_mse: 0.4537 - val_mae: 0.5225\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2025 - mse: 0.2025 - mae: 0.3528 - val_loss: 0.4787 - val_mse: 0.4787 - val_mae: 0.5192\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2211 - mse: 0.2211 - mae: 0.3647 - val_loss: 0.4803 - val_mse: 0.4803 - val_mae: 0.5234\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2155 - mse: 0.2155 - mae: 0.3627 - val_loss: 0.4702 - val_mse: 0.4702 - val_mae: 0.5145\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2172 - mse: 0.2172 - mae: 0.3659 - val_loss: 0.5341 - val_mse: 0.5341 - val_mae: 0.5717\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2147 - mse: 0.2147 - mae: 0.3608 - val_loss: 0.4653 - val_mse: 0.4653 - val_mae: 0.5249\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1956 - mse: 0.1956 - mae: 0.3395 - val_loss: 0.4665 - val_mse: 0.4665 - val_mae: 0.5207\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2079 - mse: 0.2079 - mae: 0.3495 - val_loss: 0.4929 - val_mse: 0.4929 - val_mae: 0.5230\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2095 - mse: 0.2095 - mae: 0.3519 - val_loss: 0.4432 - val_mse: 0.4432 - val_mae: 0.5047\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2140 - mse: 0.2140 - mae: 0.3587 - val_loss: 0.4457 - val_mse: 0.4457 - val_mae: 0.5103\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2039 - mse: 0.2039 - mae: 0.3502 - val_loss: 0.4741 - val_mse: 0.4741 - val_mae: 0.5346\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2164 - mse: 0.2164 - mae: 0.3567 - val_loss: 0.4668 - val_mse: 0.4668 - val_mae: 0.5162\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2156 - mse: 0.2156 - mae: 0.3653 - val_loss: 0.4838 - val_mse: 0.4838 - val_mae: 0.5273\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2116 - mse: 0.2116 - mae: 0.3611 - val_loss: 0.4291 - val_mse: 0.4291 - val_mae: 0.4973\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2240 - mse: 0.2240 - mae: 0.3606 - val_loss: 0.4444 - val_mse: 0.4444 - val_mae: 0.5060\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.1914 - mse: 0.1914 - mae: 0.3280 - val_loss: 0.4569 - val_mse: 0.4569 - val_mae: 0.5142\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1981 - mse: 0.1981 - mae: 0.3532 - val_loss: 0.5218 - val_mse: 0.5218 - val_mae: 0.5658\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2125 - mse: 0.2125 - mae: 0.3587 - val_loss: 0.4583 - val_mse: 0.4583 - val_mae: 0.5073\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2036 - mse: 0.2036 - mae: 0.3543 - val_loss: 0.4753 - val_mse: 0.4753 - val_mae: 0.5333\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2113 - mse: 0.2113 - mae: 0.3617 - val_loss: 0.4884 - val_mse: 0.4884 - val_mae: 0.5243\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2071 - mse: 0.2071 - mae: 0.3573 - val_loss: 0.4493 - val_mse: 0.4493 - val_mae: 0.4992\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1769 - mse: 0.1769 - mae: 0.3271 - val_loss: 0.4813 - val_mse: 0.4813 - val_mae: 0.5248\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2081 - mse: 0.2081 - mae: 0.3567 - val_loss: 0.4871 - val_mse: 0.4871 - val_mae: 0.5362\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1883 - mse: 0.1883 - mae: 0.3434 - val_loss: 0.4769 - val_mse: 0.4769 - val_mae: 0.5326\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1952 - mse: 0.1952 - mae: 0.3458 - val_loss: 0.4983 - val_mse: 0.4983 - val_mae: 0.5583\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1860 - mse: 0.1860 - mae: 0.3364 - val_loss: 0.4481 - val_mse: 0.4481 - val_mae: 0.5086\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1954 - mse: 0.1954 - mae: 0.3456 - val_loss: 0.4930 - val_mse: 0.4930 - val_mae: 0.5512\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2290 - mse: 0.2290 - mae: 0.3700 - val_loss: 0.4721 - val_mse: 0.4721 - val_mae: 0.5388\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2109 - mse: 0.2109 - mae: 0.3669 - val_loss: 0.4699 - val_mse: 0.4699 - val_mae: 0.5285\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2156 - mse: 0.2156 - mae: 0.3678 - val_loss: 0.4195 - val_mse: 0.4195 - val_mae: 0.4905\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2111 - mse: 0.2111 - mae: 0.3605 - val_loss: 0.5327 - val_mse: 0.5327 - val_mae: 0.5713\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2081 - mse: 0.2081 - mae: 0.3521 - val_loss: 0.4524 - val_mse: 0.4524 - val_mae: 0.5125\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2127 - mse: 0.2127 - mae: 0.3539 - val_loss: 0.4651 - val_mse: 0.4651 - val_mae: 0.5301\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2145 - mse: 0.2145 - mae: 0.3548 - val_loss: 0.4641 - val_mse: 0.4641 - val_mae: 0.5241\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2036 - mse: 0.2036 - mae: 0.3525 - val_loss: 0.4389 - val_mse: 0.4389 - val_mae: 0.5072\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2175 - mse: 0.2175 - mae: 0.3615 - val_loss: 0.4725 - val_mse: 0.4725 - val_mae: 0.5268\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2255 - mse: 0.2255 - mae: 0.3706 - val_loss: 0.4281 - val_mse: 0.4281 - val_mae: 0.4970\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2046 - mse: 0.2046 - mae: 0.3495 - val_loss: 0.4377 - val_mse: 0.4377 - val_mae: 0.5077\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1987 - mse: 0.1987 - mae: 0.3440 - val_loss: 0.4520 - val_mse: 0.4520 - val_mae: 0.5176\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1851 - mse: 0.1851 - mae: 0.3375 - val_loss: 0.4404 - val_mse: 0.4404 - val_mae: 0.5018\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1844 - mse: 0.1844 - mae: 0.3342 - val_loss: 0.4804 - val_mse: 0.4804 - val_mae: 0.5302\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.1818 - mse: 0.1818 - mae: 0.3275 - val_loss: 0.4403 - val_mse: 0.4403 - val_mae: 0.5027\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.4915 - mse: 0.4915 - mae: 0.5127\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 2s 30ms/step - loss: 44.0570 - mse: 44.0570 - mae: 6.5061 - val_loss: 39.7210 - val_mse: 39.7210 - val_mae: 6.1760\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 35.5405 - mse: 35.5405 - mae: 5.8239 - val_loss: 32.3606 - val_mse: 32.3606 - val_mae: 5.5569\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 28.6204 - mse: 28.6204 - mae: 5.2075 - val_loss: 26.2263 - val_mse: 26.2263 - val_mae: 4.9716\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 22.9314 - mse: 22.9314 - mae: 4.6188 - val_loss: 21.0099 - val_mse: 21.0099 - val_mae: 4.4069\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 18.1782 - mse: 18.1782 - mae: 4.0413 - val_loss: 16.5006 - val_mse: 16.5006 - val_mae: 3.8607\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 14.0293 - mse: 14.0293 - mae: 3.4884 - val_loss: 12.7449 - val_mse: 12.7449 - val_mae: 3.3457\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 10.7811 - mse: 10.7811 - mae: 3.0053 - val_loss: 9.5632 - val_mse: 9.5632 - val_mae: 2.8404\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 8.1626 - mse: 8.1626 - mae: 2.5536 - val_loss: 7.0277 - val_mse: 7.0277 - val_mae: 2.3709\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 6.1014 - mse: 6.1014 - mae: 2.1608 - val_loss: 5.1102 - val_mse: 5.1102 - val_mae: 1.9769\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 4.4986 - mse: 4.4986 - mae: 1.8164 - val_loss: 3.7072 - val_mse: 3.7072 - val_mae: 1.6517\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 3.6226 - mse: 3.6226 - mae: 1.5843 - val_loss: 2.7638 - val_mse: 2.7638 - val_mae: 1.3867\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.8569 - mse: 2.8569 - mae: 1.3748 - val_loss: 2.2147 - val_mse: 2.2147 - val_mae: 1.2044\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.5024 - mse: 2.5024 - mae: 1.2870 - val_loss: 1.9233 - val_mse: 1.9233 - val_mae: 1.1077\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.1448 - mse: 2.1448 - mae: 1.1714 - val_loss: 1.7357 - val_mse: 1.7357 - val_mae: 1.0520\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 2.0664 - mse: 2.0664 - mae: 1.1317 - val_loss: 1.6110 - val_mse: 1.6110 - val_mae: 1.0198\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.1007 - mse: 2.1007 - mae: 1.1307 - val_loss: 1.5327 - val_mse: 1.5327 - val_mae: 0.9957\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.8885 - mse: 1.8885 - mae: 1.0912 - val_loss: 1.4703 - val_mse: 1.4703 - val_mae: 0.9787\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.8023 - mse: 1.8023 - mae: 1.0530 - val_loss: 1.4212 - val_mse: 1.4212 - val_mae: 0.9627\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.7672 - mse: 1.7672 - mae: 1.0516 - val_loss: 1.3747 - val_mse: 1.3747 - val_mae: 0.9466\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.6508 - mse: 1.6508 - mae: 1.0216 - val_loss: 1.3401 - val_mse: 1.3401 - val_mae: 0.9324\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.5556 - mse: 1.5556 - mae: 0.9996 - val_loss: 1.3023 - val_mse: 1.3023 - val_mae: 0.9189\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.5609 - mse: 1.5609 - mae: 1.0029 - val_loss: 1.2712 - val_mse: 1.2712 - val_mae: 0.9069\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.5283 - mse: 1.5283 - mae: 0.9925 - val_loss: 1.2370 - val_mse: 1.2370 - val_mae: 0.8946\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.5163 - mse: 1.5163 - mae: 0.9578 - val_loss: 1.2094 - val_mse: 1.2094 - val_mae: 0.8829\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.3857 - mse: 1.3857 - mae: 0.9251 - val_loss: 1.1847 - val_mse: 1.1847 - val_mae: 0.8715\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.3913 - mse: 1.3913 - mae: 0.9331 - val_loss: 1.1613 - val_mse: 1.1613 - val_mae: 0.8621\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.3885 - mse: 1.3885 - mae: 0.9414 - val_loss: 1.1410 - val_mse: 1.1410 - val_mae: 0.8529\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.3261 - mse: 1.3261 - mae: 0.9086 - val_loss: 1.1184 - val_mse: 1.1184 - val_mae: 0.8437\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.3064 - mse: 1.3064 - mae: 0.8917 - val_loss: 1.0903 - val_mse: 1.0903 - val_mae: 0.8288\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.2679 - mse: 1.2679 - mae: 0.8864 - val_loss: 1.0709 - val_mse: 1.0709 - val_mae: 0.8197\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.2115 - mse: 1.2115 - mae: 0.8694 - val_loss: 1.0563 - val_mse: 1.0563 - val_mae: 0.8105\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.2257 - mse: 1.2257 - mae: 0.8708 - val_loss: 1.0345 - val_mse: 1.0345 - val_mae: 0.8006\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.1482 - mse: 1.1482 - mae: 0.8416 - val_loss: 1.0174 - val_mse: 1.0174 - val_mae: 0.7939\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.1803 - mse: 1.1803 - mae: 0.8505 - val_loss: 1.0019 - val_mse: 1.0019 - val_mae: 0.7853\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.1350 - mse: 1.1350 - mae: 0.8298 - val_loss: 0.9876 - val_mse: 0.9876 - val_mae: 0.7760\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0974 - mse: 1.0974 - mae: 0.8369 - val_loss: 0.9724 - val_mse: 0.9724 - val_mae: 0.7700\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.1454 - mse: 1.1454 - mae: 0.8402 - val_loss: 0.9619 - val_mse: 0.9619 - val_mae: 0.7639\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1221 - mse: 1.1221 - mae: 0.8308 - val_loss: 0.9488 - val_mse: 0.9488 - val_mae: 0.7554\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0240 - mse: 1.0240 - mae: 0.7908 - val_loss: 0.9373 - val_mse: 0.9373 - val_mae: 0.7512\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0331 - mse: 1.0331 - mae: 0.7912 - val_loss: 0.9276 - val_mse: 0.9276 - val_mae: 0.7488\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0861 - mse: 1.0861 - mae: 0.8120 - val_loss: 0.9104 - val_mse: 0.9104 - val_mae: 0.7406\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9925 - mse: 0.9925 - mae: 0.7786 - val_loss: 0.9037 - val_mse: 0.9037 - val_mae: 0.7346\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0440 - mse: 1.0440 - mae: 0.7970 - val_loss: 0.8944 - val_mse: 0.8944 - val_mae: 0.7316\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0683 - mse: 1.0683 - mae: 0.8159 - val_loss: 0.8876 - val_mse: 0.8876 - val_mae: 0.7289\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.0114 - mse: 1.0114 - mae: 0.7824 - val_loss: 0.8796 - val_mse: 0.8796 - val_mae: 0.7210\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.9318 - mse: 0.9318 - mae: 0.7534 - val_loss: 0.8720 - val_mse: 0.8720 - val_mae: 0.7163\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9815 - mse: 0.9815 - mae: 0.7781 - val_loss: 0.8616 - val_mse: 0.8616 - val_mae: 0.7133\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9757 - mse: 0.9757 - mae: 0.7651 - val_loss: 0.8493 - val_mse: 0.8493 - val_mae: 0.7050\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.9547 - mse: 0.9547 - mae: 0.7650 - val_loss: 0.8386 - val_mse: 0.8386 - val_mae: 0.6994\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8852 - mse: 0.8852 - mae: 0.7363 - val_loss: 0.8316 - val_mse: 0.8316 - val_mae: 0.6978\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.9116 - mse: 0.9116 - mae: 0.7464 - val_loss: 0.8258 - val_mse: 0.8258 - val_mae: 0.6952\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8848 - mse: 0.8848 - mae: 0.7290 - val_loss: 0.8173 - val_mse: 0.8173 - val_mae: 0.6918\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8512 - mse: 0.8512 - mae: 0.7257 - val_loss: 0.8090 - val_mse: 0.8090 - val_mae: 0.6841\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8610 - mse: 0.8610 - mae: 0.7300 - val_loss: 0.8006 - val_mse: 0.8006 - val_mae: 0.6800\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.8235 - mse: 0.8235 - mae: 0.7015 - val_loss: 0.7942 - val_mse: 0.7942 - val_mae: 0.6779\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.8510 - mse: 0.8510 - mae: 0.7186 - val_loss: 0.7851 - val_mse: 0.7851 - val_mae: 0.6738\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.8646 - mse: 0.8646 - mae: 0.7298 - val_loss: 0.7812 - val_mse: 0.7812 - val_mae: 0.6716\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.8491 - mse: 0.8491 - mae: 0.7193 - val_loss: 0.7784 - val_mse: 0.7784 - val_mae: 0.6694\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8231 - mse: 0.8231 - mae: 0.6980 - val_loss: 0.7693 - val_mse: 0.7693 - val_mae: 0.6619\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8043 - mse: 0.8043 - mae: 0.7031 - val_loss: 0.7632 - val_mse: 0.7632 - val_mae: 0.6618\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8008 - mse: 0.8008 - mae: 0.6837 - val_loss: 0.7600 - val_mse: 0.7600 - val_mae: 0.6642\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7481 - mse: 0.7481 - mae: 0.6858 - val_loss: 0.7489 - val_mse: 0.7489 - val_mae: 0.6552\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7868 - mse: 0.7868 - mae: 0.6922 - val_loss: 0.7467 - val_mse: 0.7467 - val_mae: 0.6492\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7427 - mse: 0.7427 - mae: 0.6670 - val_loss: 0.7398 - val_mse: 0.7398 - val_mae: 0.6478\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7848 - mse: 0.7848 - mae: 0.6858 - val_loss: 0.7341 - val_mse: 0.7341 - val_mae: 0.6419\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7805 - mse: 0.7805 - mae: 0.6819 - val_loss: 0.7264 - val_mse: 0.7264 - val_mae: 0.6399\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7976 - mse: 0.7976 - mae: 0.6972 - val_loss: 0.7211 - val_mse: 0.7211 - val_mae: 0.6430\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7418 - mse: 0.7418 - mae: 0.6706 - val_loss: 0.7153 - val_mse: 0.7153 - val_mae: 0.6366\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7568 - mse: 0.7568 - mae: 0.6754 - val_loss: 0.7107 - val_mse: 0.7107 - val_mae: 0.6321\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7490 - mse: 0.7490 - mae: 0.6667 - val_loss: 0.7027 - val_mse: 0.7027 - val_mae: 0.6314\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7642 - mse: 0.7642 - mae: 0.6810 - val_loss: 0.6958 - val_mse: 0.6958 - val_mae: 0.6293\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7298 - mse: 0.7298 - mae: 0.6692 - val_loss: 0.6872 - val_mse: 0.6872 - val_mae: 0.6227\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7341 - mse: 0.7341 - mae: 0.6546 - val_loss: 0.6815 - val_mse: 0.6815 - val_mae: 0.6168\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7558 - mse: 0.7558 - mae: 0.6695 - val_loss: 0.6752 - val_mse: 0.6752 - val_mae: 0.6148\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6868 - mse: 0.6868 - mae: 0.6435 - val_loss: 0.6734 - val_mse: 0.6734 - val_mae: 0.6120\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7081 - mse: 0.7081 - mae: 0.6589 - val_loss: 0.6685 - val_mse: 0.6685 - val_mae: 0.6093\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7083 - mse: 0.7083 - mae: 0.6514 - val_loss: 0.6627 - val_mse: 0.6627 - val_mae: 0.6053\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7151 - mse: 0.7151 - mae: 0.6552 - val_loss: 0.6570 - val_mse: 0.6570 - val_mae: 0.6059\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6943 - mse: 0.6943 - mae: 0.6478 - val_loss: 0.6530 - val_mse: 0.6530 - val_mae: 0.5998\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6640 - mse: 0.6640 - mae: 0.6280 - val_loss: 0.6488 - val_mse: 0.6488 - val_mae: 0.5986\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6562 - mse: 0.6562 - mae: 0.6326 - val_loss: 0.6452 - val_mse: 0.6452 - val_mae: 0.5990\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6748 - mse: 0.6748 - mae: 0.6355 - val_loss: 0.6405 - val_mse: 0.6405 - val_mae: 0.5940\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6617 - mse: 0.6617 - mae: 0.6359 - val_loss: 0.6370 - val_mse: 0.6370 - val_mae: 0.5917\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6507 - mse: 0.6507 - mae: 0.6179 - val_loss: 0.6299 - val_mse: 0.6299 - val_mae: 0.5890\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6282 - mse: 0.6282 - mae: 0.6106 - val_loss: 0.6297 - val_mse: 0.6297 - val_mae: 0.5875\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6242 - mse: 0.6242 - mae: 0.6018 - val_loss: 0.6261 - val_mse: 0.6261 - val_mae: 0.5860\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6250 - mse: 0.6250 - mae: 0.6081 - val_loss: 0.6187 - val_mse: 0.6187 - val_mae: 0.5826\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6285 - mse: 0.6285 - mae: 0.6114 - val_loss: 0.6144 - val_mse: 0.6144 - val_mae: 0.5823\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6578 - mse: 0.6578 - mae: 0.6350 - val_loss: 0.6068 - val_mse: 0.6068 - val_mae: 0.5754\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6301 - mse: 0.6301 - mae: 0.6118 - val_loss: 0.6031 - val_mse: 0.6031 - val_mae: 0.5740\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6088 - mse: 0.6088 - mae: 0.6035 - val_loss: 0.5963 - val_mse: 0.5963 - val_mae: 0.5725\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6105 - mse: 0.6105 - mae: 0.5996 - val_loss: 0.5936 - val_mse: 0.5936 - val_mae: 0.5726\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5977 - mse: 0.5977 - mae: 0.6055 - val_loss: 0.5904 - val_mse: 0.5904 - val_mae: 0.5688\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5983 - mse: 0.5983 - mae: 0.5896 - val_loss: 0.5847 - val_mse: 0.5847 - val_mae: 0.5681\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6214 - mse: 0.6214 - mae: 0.6037 - val_loss: 0.5836 - val_mse: 0.5836 - val_mae: 0.5642\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5963 - mse: 0.5963 - mae: 0.5929 - val_loss: 0.5774 - val_mse: 0.5774 - val_mae: 0.5636\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6039 - mse: 0.6039 - mae: 0.6021 - val_loss: 0.5738 - val_mse: 0.5738 - val_mae: 0.5601\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5696 - mse: 0.5696 - mae: 0.5846 - val_loss: 0.5733 - val_mse: 0.5733 - val_mae: 0.5580\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5734 - mse: 0.5734 - mae: 0.5817 - val_loss: 0.5688 - val_mse: 0.5688 - val_mae: 0.5572\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5915 - mse: 0.5915 - mae: 0.5917 - val_loss: 0.5644 - val_mse: 0.5644 - val_mae: 0.5555\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5870 - mse: 0.5870 - mae: 0.5918 - val_loss: 0.5654 - val_mse: 0.5654 - val_mae: 0.5532\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5593 - mse: 0.5593 - mae: 0.5802 - val_loss: 0.5655 - val_mse: 0.5655 - val_mae: 0.5538\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5568 - mse: 0.5568 - mae: 0.5774 - val_loss: 0.5599 - val_mse: 0.5599 - val_mae: 0.5517\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5935 - mse: 0.5935 - mae: 0.5967 - val_loss: 0.5531 - val_mse: 0.5531 - val_mae: 0.5486\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5664 - mse: 0.5664 - mae: 0.5807 - val_loss: 0.5550 - val_mse: 0.5550 - val_mae: 0.5498\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5474 - mse: 0.5474 - mae: 0.5653 - val_loss: 0.5446 - val_mse: 0.5446 - val_mae: 0.5460\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5364 - mse: 0.5364 - mae: 0.5730 - val_loss: 0.5426 - val_mse: 0.5426 - val_mae: 0.5444\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5334 - mse: 0.5334 - mae: 0.5623 - val_loss: 0.5355 - val_mse: 0.5355 - val_mae: 0.5433\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5651 - mse: 0.5651 - mae: 0.5766 - val_loss: 0.5338 - val_mse: 0.5338 - val_mae: 0.5413\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5580 - mse: 0.5580 - mae: 0.5781 - val_loss: 0.5356 - val_mse: 0.5356 - val_mae: 0.5390\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5426 - mse: 0.5426 - mae: 0.5691 - val_loss: 0.5291 - val_mse: 0.5291 - val_mae: 0.5362\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5270 - mse: 0.5270 - mae: 0.5603 - val_loss: 0.5255 - val_mse: 0.5255 - val_mae: 0.5362\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5624 - mse: 0.5624 - mae: 0.5781 - val_loss: 0.5224 - val_mse: 0.5224 - val_mae: 0.5333\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5396 - mse: 0.5396 - mae: 0.5656 - val_loss: 0.5221 - val_mse: 0.5221 - val_mae: 0.5347\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5245 - mse: 0.5245 - mae: 0.5582 - val_loss: 0.5212 - val_mse: 0.5212 - val_mae: 0.5355\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5369 - mse: 0.5369 - mae: 0.5666 - val_loss: 0.5153 - val_mse: 0.5153 - val_mae: 0.5301\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5094 - mse: 0.5094 - mae: 0.5516 - val_loss: 0.5131 - val_mse: 0.5131 - val_mae: 0.5288\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5079 - mse: 0.5079 - mae: 0.5428 - val_loss: 0.5093 - val_mse: 0.5093 - val_mae: 0.5273\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5292 - mse: 0.5292 - mae: 0.5595 - val_loss: 0.5085 - val_mse: 0.5085 - val_mae: 0.5273\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4913 - mse: 0.4913 - mae: 0.5454 - val_loss: 0.5098 - val_mse: 0.5098 - val_mae: 0.5290\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5040 - mse: 0.5040 - mae: 0.5455 - val_loss: 0.5034 - val_mse: 0.5034 - val_mae: 0.5254\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4941 - mse: 0.4941 - mae: 0.5396 - val_loss: 0.5013 - val_mse: 0.5013 - val_mae: 0.5245\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4746 - mse: 0.4746 - mae: 0.5333 - val_loss: 0.4974 - val_mse: 0.4974 - val_mae: 0.5229\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5041 - mse: 0.5041 - mae: 0.5418 - val_loss: 0.4900 - val_mse: 0.4900 - val_mae: 0.5202\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5090 - mse: 0.5090 - mae: 0.5436 - val_loss: 0.4933 - val_mse: 0.4933 - val_mae: 0.5226\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4980 - mse: 0.4980 - mae: 0.5418 - val_loss: 0.4895 - val_mse: 0.4895 - val_mae: 0.5204\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4750 - mse: 0.4750 - mae: 0.5274 - val_loss: 0.4903 - val_mse: 0.4903 - val_mae: 0.5194\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5050 - mse: 0.5050 - mae: 0.5522 - val_loss: 0.4856 - val_mse: 0.4856 - val_mae: 0.5170\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4906 - mse: 0.4906 - mae: 0.5364 - val_loss: 0.4858 - val_mse: 0.4858 - val_mae: 0.5180\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4988 - mse: 0.4988 - mae: 0.5431 - val_loss: 0.4863 - val_mse: 0.4863 - val_mae: 0.5198\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4836 - mse: 0.4836 - mae: 0.5330 - val_loss: 0.4835 - val_mse: 0.4835 - val_mae: 0.5185\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4758 - mse: 0.4758 - mae: 0.5299 - val_loss: 0.4783 - val_mse: 0.4783 - val_mae: 0.5153\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4888 - mse: 0.4888 - mae: 0.5394 - val_loss: 0.4784 - val_mse: 0.4784 - val_mae: 0.5170\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4783 - mse: 0.4783 - mae: 0.5345 - val_loss: 0.4754 - val_mse: 0.4754 - val_mae: 0.5151\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4738 - mse: 0.4738 - mae: 0.5284 - val_loss: 0.4732 - val_mse: 0.4732 - val_mae: 0.5139\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4865 - mse: 0.4865 - mae: 0.5437 - val_loss: 0.4716 - val_mse: 0.4716 - val_mae: 0.5119\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4922 - mse: 0.4922 - mae: 0.5391 - val_loss: 0.4769 - val_mse: 0.4769 - val_mae: 0.5137\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4490 - mse: 0.4490 - mae: 0.5200 - val_loss: 0.4686 - val_mse: 0.4686 - val_mae: 0.5104\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4763 - mse: 0.4763 - mae: 0.5342 - val_loss: 0.4676 - val_mse: 0.4676 - val_mae: 0.5107\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4838 - mse: 0.4838 - mae: 0.5382 - val_loss: 0.4600 - val_mse: 0.4600 - val_mae: 0.5061\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4672 - mse: 0.4672 - mae: 0.5271 - val_loss: 0.4622 - val_mse: 0.4622 - val_mae: 0.5075\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4620 - mse: 0.4620 - mae: 0.5243 - val_loss: 0.4627 - val_mse: 0.4627 - val_mae: 0.5075\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4537 - mse: 0.4537 - mae: 0.5209 - val_loss: 0.4562 - val_mse: 0.4562 - val_mae: 0.5050\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4679 - mse: 0.4679 - mae: 0.5210 - val_loss: 0.4550 - val_mse: 0.4550 - val_mae: 0.5040\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4587 - mse: 0.4587 - mae: 0.5216 - val_loss: 0.4536 - val_mse: 0.4536 - val_mae: 0.5033\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4547 - mse: 0.4547 - mae: 0.5230 - val_loss: 0.4512 - val_mse: 0.4512 - val_mae: 0.5002\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4824 - mse: 0.4824 - mae: 0.5295 - val_loss: 0.4573 - val_mse: 0.4573 - val_mae: 0.5052\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4570 - mse: 0.4570 - mae: 0.5219 - val_loss: 0.4514 - val_mse: 0.4514 - val_mae: 0.5041\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4590 - mse: 0.4590 - mae: 0.5287 - val_loss: 0.4493 - val_mse: 0.4493 - val_mae: 0.5021\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4646 - mse: 0.4646 - mae: 0.5283 - val_loss: 0.4467 - val_mse: 0.4467 - val_mae: 0.5013\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4324 - mse: 0.4324 - mae: 0.5086 - val_loss: 0.4389 - val_mse: 0.4389 - val_mae: 0.4971\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4357 - mse: 0.4357 - mae: 0.5070 - val_loss: 0.4394 - val_mse: 0.4394 - val_mae: 0.4981\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4514 - mse: 0.4514 - mae: 0.5142 - val_loss: 0.4408 - val_mse: 0.4408 - val_mae: 0.4988\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4270 - mse: 0.4270 - mae: 0.5017 - val_loss: 0.4414 - val_mse: 0.4414 - val_mae: 0.4995\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4510 - mse: 0.4510 - mae: 0.5104 - val_loss: 0.4394 - val_mse: 0.4394 - val_mae: 0.4983\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4341 - mse: 0.4341 - mae: 0.5137 - val_loss: 0.4354 - val_mse: 0.4354 - val_mae: 0.4962\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4331 - mse: 0.4331 - mae: 0.5079 - val_loss: 0.4358 - val_mse: 0.4358 - val_mae: 0.4969\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4408 - mse: 0.4408 - mae: 0.5114 - val_loss: 0.4368 - val_mse: 0.4368 - val_mae: 0.4973\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4381 - mse: 0.4381 - mae: 0.5094 - val_loss: 0.4347 - val_mse: 0.4347 - val_mae: 0.4960\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4332 - mse: 0.4332 - mae: 0.5110 - val_loss: 0.4297 - val_mse: 0.4297 - val_mae: 0.4914\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4388 - mse: 0.4388 - mae: 0.5100 - val_loss: 0.4335 - val_mse: 0.4335 - val_mae: 0.4938\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4465 - mse: 0.4465 - mae: 0.5153 - val_loss: 0.4255 - val_mse: 0.4255 - val_mae: 0.4902\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4387 - mse: 0.4387 - mae: 0.5060 - val_loss: 0.4340 - val_mse: 0.4340 - val_mae: 0.4979\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4353 - mse: 0.4353 - mae: 0.5106 - val_loss: 0.4248 - val_mse: 0.4248 - val_mae: 0.4916\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4285 - mse: 0.4285 - mae: 0.4968 - val_loss: 0.4291 - val_mse: 0.4291 - val_mae: 0.4940\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4367 - mse: 0.4367 - mae: 0.5071 - val_loss: 0.4292 - val_mse: 0.4292 - val_mae: 0.4952\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4440 - mse: 0.4440 - mae: 0.5157 - val_loss: 0.4236 - val_mse: 0.4236 - val_mae: 0.4890\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4230 - mse: 0.4230 - mae: 0.4981 - val_loss: 0.4322 - val_mse: 0.4322 - val_mae: 0.4941\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4343 - mse: 0.4343 - mae: 0.5076 - val_loss: 0.4256 - val_mse: 0.4256 - val_mae: 0.4914\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4304 - mse: 0.4304 - mae: 0.5040 - val_loss: 0.4248 - val_mse: 0.4248 - val_mae: 0.4920\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4237 - mse: 0.4237 - mae: 0.4991 - val_loss: 0.4244 - val_mse: 0.4244 - val_mae: 0.4919\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4290 - mse: 0.4290 - mae: 0.4998 - val_loss: 0.4241 - val_mse: 0.4241 - val_mae: 0.4913\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4205 - mse: 0.4205 - mae: 0.5001 - val_loss: 0.4178 - val_mse: 0.4178 - val_mae: 0.4879\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4234 - mse: 0.4234 - mae: 0.5000 - val_loss: 0.4175 - val_mse: 0.4175 - val_mae: 0.4896\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3960 - mse: 0.3960 - mae: 0.4872 - val_loss: 0.4194 - val_mse: 0.4194 - val_mae: 0.4884\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4198 - mse: 0.4198 - mae: 0.4971 - val_loss: 0.4249 - val_mse: 0.4249 - val_mae: 0.4919\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4329 - mse: 0.4329 - mae: 0.5047 - val_loss: 0.4187 - val_mse: 0.4187 - val_mae: 0.4894\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4313 - mse: 0.4313 - mae: 0.5059 - val_loss: 0.4260 - val_mse: 0.4260 - val_mae: 0.4932\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4094 - mse: 0.4094 - mae: 0.4980 - val_loss: 0.4140 - val_mse: 0.4140 - val_mae: 0.4880\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4128 - mse: 0.4128 - mae: 0.4953 - val_loss: 0.4146 - val_mse: 0.4146 - val_mae: 0.4886\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4070 - mse: 0.4070 - mae: 0.4881 - val_loss: 0.4130 - val_mse: 0.4130 - val_mae: 0.4853\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4246 - mse: 0.4246 - mae: 0.5043 - val_loss: 0.4121 - val_mse: 0.4121 - val_mae: 0.4870\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4157 - mse: 0.4157 - mae: 0.4986 - val_loss: 0.4091 - val_mse: 0.4091 - val_mae: 0.4826\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4186 - mse: 0.4186 - mae: 0.4996 - val_loss: 0.4156 - val_mse: 0.4156 - val_mae: 0.4892\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4092 - mse: 0.4092 - mae: 0.4931 - val_loss: 0.4071 - val_mse: 0.4071 - val_mae: 0.4839\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4132 - mse: 0.4132 - mae: 0.5017 - val_loss: 0.4197 - val_mse: 0.4197 - val_mae: 0.4925\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4320 - mse: 0.4320 - mae: 0.5075 - val_loss: 0.4099 - val_mse: 0.4099 - val_mae: 0.4861\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4101 - mse: 0.4101 - mae: 0.4908 - val_loss: 0.4033 - val_mse: 0.4033 - val_mae: 0.4843\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4091 - mse: 0.4091 - mae: 0.4900 - val_loss: 0.4088 - val_mse: 0.4088 - val_mae: 0.4842\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4121 - mse: 0.4121 - mae: 0.4997 - val_loss: 0.4075 - val_mse: 0.4075 - val_mae: 0.4823\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3956 - mse: 0.3956 - mae: 0.4793 - val_loss: 0.4046 - val_mse: 0.4046 - val_mae: 0.4822\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4089 - mse: 0.4089 - mae: 0.4964 - val_loss: 0.4067 - val_mse: 0.4067 - val_mae: 0.4837\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4111 - mse: 0.4111 - mae: 0.4950 - val_loss: 0.4066 - val_mse: 0.4066 - val_mae: 0.4856\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4103 - mse: 0.4103 - mae: 0.4934 - val_loss: 0.4064 - val_mse: 0.4064 - val_mae: 0.4839\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4206 - mse: 0.4206 - mae: 0.5024 - val_loss: 0.3974 - val_mse: 0.3974 - val_mae: 0.4786\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4051 - mse: 0.4051 - mae: 0.4941 - val_loss: 0.3984 - val_mse: 0.3984 - val_mae: 0.4780\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4095 - mse: 0.4095 - mae: 0.4908 - val_loss: 0.4056 - val_mse: 0.4056 - val_mae: 0.4849\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4099 - mse: 0.4099 - mae: 0.4923 - val_loss: 0.3952 - val_mse: 0.3952 - val_mae: 0.4794\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4143 - mse: 0.4143 - mae: 0.4959 - val_loss: 0.3980 - val_mse: 0.3980 - val_mae: 0.4825\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3884 - mse: 0.3884 - mae: 0.4857 - val_loss: 0.3987 - val_mse: 0.3987 - val_mae: 0.4819\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4055 - mse: 0.4055 - mae: 0.4919 - val_loss: 0.4032 - val_mse: 0.4032 - val_mae: 0.4842\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3933 - mse: 0.3933 - mae: 0.4869 - val_loss: 0.4014 - val_mse: 0.4014 - val_mae: 0.4832\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4027 - mse: 0.4027 - mae: 0.4883 - val_loss: 0.3950 - val_mse: 0.3950 - val_mae: 0.4783\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4202 - mse: 0.4202 - mae: 0.5022 - val_loss: 0.3958 - val_mse: 0.3958 - val_mae: 0.4813\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4010 - mse: 0.4010 - mae: 0.4914 - val_loss: 0.4021 - val_mse: 0.4021 - val_mae: 0.4844\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3892 - mse: 0.3892 - mae: 0.4791 - val_loss: 0.3954 - val_mse: 0.3954 - val_mae: 0.4793\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4007 - mse: 0.4007 - mae: 0.4959 - val_loss: 0.3953 - val_mse: 0.3953 - val_mae: 0.4818\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3967 - mse: 0.3967 - mae: 0.4944 - val_loss: 0.3931 - val_mse: 0.3931 - val_mae: 0.4805\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3889 - mse: 0.3889 - mae: 0.4848 - val_loss: 0.3921 - val_mse: 0.3921 - val_mae: 0.4802\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3998 - mse: 0.3998 - mae: 0.4836 - val_loss: 0.3928 - val_mse: 0.3928 - val_mae: 0.4788\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3982 - mse: 0.3982 - mae: 0.4868 - val_loss: 0.3979 - val_mse: 0.3979 - val_mae: 0.4818\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3943 - mse: 0.3943 - mae: 0.4831 - val_loss: 0.3996 - val_mse: 0.3996 - val_mae: 0.4830\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4040 - mse: 0.4040 - mae: 0.4941 - val_loss: 0.3941 - val_mse: 0.3941 - val_mae: 0.4800\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3885 - mse: 0.3885 - mae: 0.4822 - val_loss: 0.3880 - val_mse: 0.3880 - val_mae: 0.4760\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3943 - mse: 0.3943 - mae: 0.4806 - val_loss: 0.3904 - val_mse: 0.3904 - val_mae: 0.4789\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3906 - mse: 0.3906 - mae: 0.4838 - val_loss: 0.3955 - val_mse: 0.3955 - val_mae: 0.4822\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3988 - mse: 0.3988 - mae: 0.4920 - val_loss: 0.3894 - val_mse: 0.3894 - val_mae: 0.4784\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4008 - mse: 0.4008 - mae: 0.4881 - val_loss: 0.3917 - val_mse: 0.3917 - val_mae: 0.4788\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3809 - mse: 0.3809 - mae: 0.4716 - val_loss: 0.3897 - val_mse: 0.3897 - val_mae: 0.4783\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3832 - mse: 0.3832 - mae: 0.4865 - val_loss: 0.3910 - val_mse: 0.3910 - val_mae: 0.4789\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3937 - mse: 0.3937 - mae: 0.4866 - val_loss: 0.3917 - val_mse: 0.3917 - val_mae: 0.4796\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3807 - mse: 0.3807 - mae: 0.4777 - val_loss: 0.3954 - val_mse: 0.3954 - val_mae: 0.4815\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3959 - mse: 0.3959 - mae: 0.4886 - val_loss: 0.3837 - val_mse: 0.3837 - val_mae: 0.4768\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3756 - mse: 0.3756 - mae: 0.4788 - val_loss: 0.3827 - val_mse: 0.3827 - val_mae: 0.4753\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3872 - mse: 0.3872 - mae: 0.4754 - val_loss: 0.3857 - val_mse: 0.3857 - val_mae: 0.4772\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4106 - mse: 0.4106 - mae: 0.4983 - val_loss: 0.3991 - val_mse: 0.3991 - val_mae: 0.4831\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3981 - mse: 0.3981 - mae: 0.4954 - val_loss: 0.3913 - val_mse: 0.3913 - val_mae: 0.4796\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3989 - mse: 0.3989 - mae: 0.4880 - val_loss: 0.3907 - val_mse: 0.3907 - val_mae: 0.4799\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3984 - mse: 0.3984 - mae: 0.4857 - val_loss: 0.3891 - val_mse: 0.3891 - val_mae: 0.4785\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3841 - mse: 0.3841 - mae: 0.4827 - val_loss: 0.3883 - val_mse: 0.3883 - val_mae: 0.4790\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3905 - mse: 0.3905 - mae: 0.4821 - val_loss: 0.4008 - val_mse: 0.4008 - val_mae: 0.4887\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3845 - mse: 0.3845 - mae: 0.4772 - val_loss: 0.3865 - val_mse: 0.3865 - val_mae: 0.4777\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3819 - mse: 0.3819 - mae: 0.4787 - val_loss: 0.4059 - val_mse: 0.4059 - val_mae: 0.4891\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3775 - mse: 0.3775 - mae: 0.4761 - val_loss: 0.3845 - val_mse: 0.3845 - val_mae: 0.4760\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3870 - mse: 0.3870 - mae: 0.4790 - val_loss: 0.3972 - val_mse: 0.3972 - val_mae: 0.4824\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3993 - mse: 0.3993 - mae: 0.4918 - val_loss: 0.3964 - val_mse: 0.3964 - val_mae: 0.4797\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4120 - mse: 0.4120 - mae: 0.4920 - val_loss: 0.3846 - val_mse: 0.3846 - val_mae: 0.4763\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3852 - mse: 0.3852 - mae: 0.4800 - val_loss: 0.3860 - val_mse: 0.3860 - val_mae: 0.4802\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3751 - mse: 0.3751 - mae: 0.4720 - val_loss: 0.3894 - val_mse: 0.3894 - val_mae: 0.4812\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3954 - mse: 0.3954 - mae: 0.4830 - val_loss: 0.3804 - val_mse: 0.3804 - val_mae: 0.4751\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4031 - mse: 0.4031 - mae: 0.4924 - val_loss: 0.3839 - val_mse: 0.3839 - val_mae: 0.4762\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3721 - mse: 0.3721 - mae: 0.4747 - val_loss: 0.3801 - val_mse: 0.3801 - val_mae: 0.4731\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3986 - mse: 0.3986 - mae: 0.4862 - val_loss: 0.3905 - val_mse: 0.3905 - val_mae: 0.4814\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3697 - mse: 0.3697 - mae: 0.4680 - val_loss: 0.3816 - val_mse: 0.3816 - val_mae: 0.4770\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4016 - mse: 0.4016 - mae: 0.4927 - val_loss: 0.3840 - val_mse: 0.3840 - val_mae: 0.4790\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3804 - mse: 0.3804 - mae: 0.4804 - val_loss: 0.3927 - val_mse: 0.3927 - val_mae: 0.4862\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3759 - mse: 0.3759 - mae: 0.4766 - val_loss: 0.3836 - val_mse: 0.3836 - val_mae: 0.4770\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3797 - mse: 0.3797 - mae: 0.4808 - val_loss: 0.3851 - val_mse: 0.3851 - val_mae: 0.4787\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3842 - mse: 0.3842 - mae: 0.4780 - val_loss: 0.3957 - val_mse: 0.3957 - val_mae: 0.4853\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3782 - mse: 0.3782 - mae: 0.4796 - val_loss: 0.3884 - val_mse: 0.3884 - val_mae: 0.4788\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3936 - mse: 0.3936 - mae: 0.4827 - val_loss: 0.3938 - val_mse: 0.3938 - val_mae: 0.4815\n",
            "Epoch 252/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3873 - mse: 0.3873 - mae: 0.4823 - val_loss: 0.3810 - val_mse: 0.3810 - val_mae: 0.4760\n",
            "Epoch 253/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3883 - mse: 0.3883 - mae: 0.4821 - val_loss: 0.3878 - val_mse: 0.3878 - val_mae: 0.4819\n",
            "Epoch 254/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3778 - mse: 0.3778 - mae: 0.4726 - val_loss: 0.3808 - val_mse: 0.3808 - val_mae: 0.4761\n",
            "Epoch 255/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3817 - mse: 0.3817 - mae: 0.4769 - val_loss: 0.3806 - val_mse: 0.3806 - val_mae: 0.4750\n",
            "Epoch 256/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3795 - mse: 0.3795 - mae: 0.4745 - val_loss: 0.3830 - val_mse: 0.3830 - val_mae: 0.4775\n",
            "Epoch 257/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3710 - mse: 0.3710 - mae: 0.4711 - val_loss: 0.3917 - val_mse: 0.3917 - val_mae: 0.4833\n",
            "Epoch 258/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3742 - mse: 0.3742 - mae: 0.4788 - val_loss: 0.3860 - val_mse: 0.3860 - val_mae: 0.4811\n",
            "Epoch 259/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3824 - mse: 0.3824 - mae: 0.4846 - val_loss: 0.3785 - val_mse: 0.3785 - val_mae: 0.4766\n",
            "Epoch 260/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3638 - mse: 0.3638 - mae: 0.4657 - val_loss: 0.3907 - val_mse: 0.3907 - val_mae: 0.4848\n",
            "Epoch 261/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3715 - mse: 0.3715 - mae: 0.4777 - val_loss: 0.3840 - val_mse: 0.3840 - val_mae: 0.4790\n",
            "Epoch 262/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3689 - mse: 0.3689 - mae: 0.4720 - val_loss: 0.3808 - val_mse: 0.3808 - val_mae: 0.4772\n",
            "Epoch 263/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3731 - mse: 0.3731 - mae: 0.4793 - val_loss: 0.3798 - val_mse: 0.3798 - val_mae: 0.4776\n",
            "Epoch 264/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3789 - mse: 0.3789 - mae: 0.4821 - val_loss: 0.3861 - val_mse: 0.3861 - val_mae: 0.4795\n",
            "Epoch 265/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3814 - mse: 0.3814 - mae: 0.4776 - val_loss: 0.3926 - val_mse: 0.3926 - val_mae: 0.4836\n",
            "Epoch 266/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3933 - mse: 0.3933 - mae: 0.4943 - val_loss: 0.3786 - val_mse: 0.3786 - val_mae: 0.4768\n",
            "Epoch 267/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3693 - mse: 0.3693 - mae: 0.4738 - val_loss: 0.3820 - val_mse: 0.3820 - val_mae: 0.4786\n",
            "Epoch 268/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3753 - mse: 0.3753 - mae: 0.4720 - val_loss: 0.3823 - val_mse: 0.3823 - val_mae: 0.4766\n",
            "Epoch 269/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3751 - mse: 0.3751 - mae: 0.4685 - val_loss: 0.3800 - val_mse: 0.3800 - val_mae: 0.4764\n",
            "Epoch 270/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3713 - mse: 0.3713 - mae: 0.4760 - val_loss: 0.3794 - val_mse: 0.3794 - val_mae: 0.4760\n",
            "Epoch 271/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3819 - mse: 0.3819 - mae: 0.4808 - val_loss: 0.3864 - val_mse: 0.3864 - val_mae: 0.4819\n",
            "Epoch 272/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3910 - mse: 0.3910 - mae: 0.4896 - val_loss: 0.3768 - val_mse: 0.3768 - val_mae: 0.4760\n",
            "Epoch 273/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3921 - mse: 0.3921 - mae: 0.4874 - val_loss: 0.3779 - val_mse: 0.3779 - val_mae: 0.4726\n",
            "Epoch 274/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3941 - mse: 0.3941 - mae: 0.4838 - val_loss: 0.3968 - val_mse: 0.3968 - val_mae: 0.4885\n",
            "Epoch 275/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3789 - mse: 0.3789 - mae: 0.4792 - val_loss: 0.3762 - val_mse: 0.3762 - val_mae: 0.4750\n",
            "Epoch 276/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3592 - mse: 0.3592 - mae: 0.4624 - val_loss: 0.3933 - val_mse: 0.3933 - val_mae: 0.4870\n",
            "Epoch 277/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3694 - mse: 0.3694 - mae: 0.4817 - val_loss: 0.3781 - val_mse: 0.3781 - val_mae: 0.4767\n",
            "Epoch 278/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3938 - mse: 0.3938 - mae: 0.4923 - val_loss: 0.3811 - val_mse: 0.3811 - val_mae: 0.4771\n",
            "Epoch 279/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3595 - mse: 0.3595 - mae: 0.4656 - val_loss: 0.3922 - val_mse: 0.3922 - val_mae: 0.4855\n",
            "Epoch 280/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3653 - mse: 0.3653 - mae: 0.4723 - val_loss: 0.3746 - val_mse: 0.3746 - val_mae: 0.4741\n",
            "Epoch 281/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3624 - mse: 0.3624 - mae: 0.4612 - val_loss: 0.3788 - val_mse: 0.3788 - val_mae: 0.4786\n",
            "Epoch 282/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3662 - mse: 0.3662 - mae: 0.4694 - val_loss: 0.3826 - val_mse: 0.3826 - val_mae: 0.4789\n",
            "Epoch 283/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3823 - mse: 0.3823 - mae: 0.4829 - val_loss: 0.3848 - val_mse: 0.3848 - val_mae: 0.4787\n",
            "Epoch 284/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3768 - mse: 0.3768 - mae: 0.4725 - val_loss: 0.3809 - val_mse: 0.3809 - val_mae: 0.4804\n",
            "Epoch 285/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3635 - mse: 0.3635 - mae: 0.4672 - val_loss: 0.3809 - val_mse: 0.3809 - val_mae: 0.4746\n",
            "Epoch 286/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3770 - mse: 0.3770 - mae: 0.4781 - val_loss: 0.3791 - val_mse: 0.3791 - val_mae: 0.4792\n",
            "Epoch 287/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3606 - mse: 0.3606 - mae: 0.4687 - val_loss: 0.3751 - val_mse: 0.3751 - val_mae: 0.4747\n",
            "Epoch 288/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3674 - mse: 0.3674 - mae: 0.4746 - val_loss: 0.3809 - val_mse: 0.3809 - val_mae: 0.4780\n",
            "Epoch 289/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3908 - mse: 0.3908 - mae: 0.4836 - val_loss: 0.3789 - val_mse: 0.3789 - val_mae: 0.4799\n",
            "Epoch 290/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3587 - mse: 0.3587 - mae: 0.4678 - val_loss: 0.3771 - val_mse: 0.3771 - val_mae: 0.4751\n",
            "Epoch 291/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3827 - mse: 0.3827 - mae: 0.4753 - val_loss: 0.3851 - val_mse: 0.3851 - val_mae: 0.4822\n",
            "Epoch 292/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3861 - mse: 0.3861 - mae: 0.4841 - val_loss: 0.3802 - val_mse: 0.3802 - val_mae: 0.4794\n",
            "Epoch 293/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3782 - mse: 0.3782 - mae: 0.4765 - val_loss: 0.3796 - val_mse: 0.3796 - val_mae: 0.4771\n",
            "Epoch 294/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3645 - mse: 0.3645 - mae: 0.4662 - val_loss: 0.3716 - val_mse: 0.3716 - val_mae: 0.4742\n",
            "Epoch 295/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3589 - mse: 0.3589 - mae: 0.4688 - val_loss: 0.3854 - val_mse: 0.3854 - val_mae: 0.4831\n",
            "Epoch 296/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3602 - mse: 0.3602 - mae: 0.4656 - val_loss: 0.3842 - val_mse: 0.3842 - val_mae: 0.4805\n",
            "Epoch 297/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3729 - mse: 0.3729 - mae: 0.4692 - val_loss: 0.4094 - val_mse: 0.4094 - val_mae: 0.5013\n",
            "Epoch 298/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3772 - mse: 0.3772 - mae: 0.4790 - val_loss: 0.3942 - val_mse: 0.3942 - val_mae: 0.4920\n",
            "Epoch 299/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3708 - mse: 0.3708 - mae: 0.4776 - val_loss: 0.3792 - val_mse: 0.3792 - val_mae: 0.4782\n",
            "Epoch 300/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3606 - mse: 0.3606 - mae: 0.4624 - val_loss: 0.3875 - val_mse: 0.3875 - val_mae: 0.4849\n",
            "Epoch 301/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3723 - mse: 0.3723 - mae: 0.4754 - val_loss: 0.3822 - val_mse: 0.3822 - val_mae: 0.4797\n",
            "Epoch 302/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3562 - mse: 0.3562 - mae: 0.4668 - val_loss: 0.3937 - val_mse: 0.3937 - val_mae: 0.4871\n",
            "Epoch 303/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3662 - mse: 0.3662 - mae: 0.4757 - val_loss: 0.3846 - val_mse: 0.3846 - val_mae: 0.4803\n",
            "Epoch 304/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3650 - mse: 0.3650 - mae: 0.4650 - val_loss: 0.3811 - val_mse: 0.3811 - val_mae: 0.4796\n",
            "Epoch 305/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3576 - mse: 0.3576 - mae: 0.4651 - val_loss: 0.3794 - val_mse: 0.3794 - val_mae: 0.4803\n",
            "Epoch 306/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3696 - mse: 0.3696 - mae: 0.4693 - val_loss: 0.3912 - val_mse: 0.3912 - val_mae: 0.4878\n",
            "Epoch 307/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3817 - mse: 0.3817 - mae: 0.4761 - val_loss: 0.3874 - val_mse: 0.3874 - val_mae: 0.4829\n",
            "Epoch 308/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3760 - mse: 0.3760 - mae: 0.4789 - val_loss: 0.3839 - val_mse: 0.3839 - val_mae: 0.4826\n",
            "Epoch 309/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3709 - mse: 0.3709 - mae: 0.4697 - val_loss: 0.3822 - val_mse: 0.3822 - val_mae: 0.4810\n",
            "Epoch 310/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3584 - mse: 0.3584 - mae: 0.4651 - val_loss: 0.3788 - val_mse: 0.3788 - val_mae: 0.4781\n",
            "Epoch 311/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3720 - mse: 0.3720 - mae: 0.4717 - val_loss: 0.3852 - val_mse: 0.3852 - val_mae: 0.4848\n",
            "Epoch 312/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3620 - mse: 0.3620 - mae: 0.4666 - val_loss: 0.3866 - val_mse: 0.3866 - val_mae: 0.4832\n",
            "Epoch 313/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3749 - mse: 0.3749 - mae: 0.4724 - val_loss: 0.3796 - val_mse: 0.3796 - val_mae: 0.4789\n",
            "Epoch 314/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3841 - mse: 0.3841 - mae: 0.4859 - val_loss: 0.3812 - val_mse: 0.3812 - val_mae: 0.4747\n",
            "Epoch 315/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3881 - mse: 0.3881 - mae: 0.4784 - val_loss: 0.3934 - val_mse: 0.3934 - val_mae: 0.4894\n",
            "Epoch 316/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3677 - mse: 0.3677 - mae: 0.4630 - val_loss: 0.3728 - val_mse: 0.3728 - val_mae: 0.4764\n",
            "Epoch 317/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3682 - mse: 0.3682 - mae: 0.4659 - val_loss: 0.3895 - val_mse: 0.3895 - val_mae: 0.4858\n",
            "Epoch 318/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3730 - mse: 0.3730 - mae: 0.4788 - val_loss: 0.3705 - val_mse: 0.3705 - val_mae: 0.4730\n",
            "Epoch 319/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3600 - mse: 0.3600 - mae: 0.4634 - val_loss: 0.3758 - val_mse: 0.3758 - val_mae: 0.4796\n",
            "Epoch 320/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3528 - mse: 0.3528 - mae: 0.4573 - val_loss: 0.3723 - val_mse: 0.3723 - val_mae: 0.4754\n",
            "Epoch 321/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3531 - mse: 0.3531 - mae: 0.4605 - val_loss: 0.3740 - val_mse: 0.3740 - val_mae: 0.4771\n",
            "Epoch 322/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3541 - mse: 0.3541 - mae: 0.4626 - val_loss: 0.3793 - val_mse: 0.3793 - val_mae: 0.4826\n",
            "Epoch 323/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3750 - mse: 0.3750 - mae: 0.4739 - val_loss: 0.3835 - val_mse: 0.3835 - val_mae: 0.4853\n",
            "Epoch 324/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3689 - mse: 0.3689 - mae: 0.4662 - val_loss: 0.3823 - val_mse: 0.3823 - val_mae: 0.4836\n",
            "Epoch 325/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3809 - mse: 0.3809 - mae: 0.4818 - val_loss: 0.3748 - val_mse: 0.3748 - val_mae: 0.4738\n",
            "Epoch 326/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3739 - mse: 0.3739 - mae: 0.4720 - val_loss: 0.3809 - val_mse: 0.3809 - val_mae: 0.4804\n",
            "Epoch 327/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3511 - mse: 0.3511 - mae: 0.4554 - val_loss: 0.3719 - val_mse: 0.3719 - val_mae: 0.4756\n",
            "Epoch 328/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3613 - mse: 0.3613 - mae: 0.4665 - val_loss: 0.3739 - val_mse: 0.3739 - val_mae: 0.4747\n",
            "Epoch 329/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3596 - mse: 0.3596 - mae: 0.4627 - val_loss: 0.3777 - val_mse: 0.3777 - val_mae: 0.4782\n",
            "Epoch 330/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3722 - mse: 0.3722 - mae: 0.4684 - val_loss: 0.3809 - val_mse: 0.3809 - val_mae: 0.4832\n",
            "Epoch 331/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3564 - mse: 0.3564 - mae: 0.4669 - val_loss: 0.3699 - val_mse: 0.3699 - val_mae: 0.4752\n",
            "Epoch 332/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3611 - mse: 0.3611 - mae: 0.4643 - val_loss: 0.3692 - val_mse: 0.3692 - val_mae: 0.4730\n",
            "Epoch 333/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3770 - mse: 0.3770 - mae: 0.4652 - val_loss: 0.3702 - val_mse: 0.3702 - val_mae: 0.4727\n",
            "Epoch 334/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3713 - mse: 0.3713 - mae: 0.4666 - val_loss: 0.3742 - val_mse: 0.3742 - val_mae: 0.4797\n",
            "Epoch 335/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3686 - mse: 0.3686 - mae: 0.4712 - val_loss: 0.3714 - val_mse: 0.3714 - val_mae: 0.4765\n",
            "Epoch 336/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.3524 - mse: 0.3524 - mae: 0.4599 - val_loss: 0.3790 - val_mse: 0.3790 - val_mae: 0.4822\n",
            "Epoch 337/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3620 - mse: 0.3620 - mae: 0.4678 - val_loss: 0.3735 - val_mse: 0.3735 - val_mae: 0.4766\n",
            "Epoch 338/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3753 - mse: 0.3753 - mae: 0.4753 - val_loss: 0.3888 - val_mse: 0.3888 - val_mae: 0.4907\n",
            "Epoch 339/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3625 - mse: 0.3625 - mae: 0.4582 - val_loss: 0.3735 - val_mse: 0.3735 - val_mae: 0.4767\n",
            "Epoch 340/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3681 - mse: 0.3681 - mae: 0.4740 - val_loss: 0.3688 - val_mse: 0.3688 - val_mae: 0.4718\n",
            "Epoch 341/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3637 - mse: 0.3637 - mae: 0.4644 - val_loss: 0.3912 - val_mse: 0.3912 - val_mae: 0.4905\n",
            "Epoch 342/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3723 - mse: 0.3723 - mae: 0.4693 - val_loss: 0.3727 - val_mse: 0.3727 - val_mae: 0.4755\n",
            "Epoch 343/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3697 - mse: 0.3697 - mae: 0.4705 - val_loss: 0.3760 - val_mse: 0.3760 - val_mae: 0.4787\n",
            "Epoch 344/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3735 - mse: 0.3735 - mae: 0.4768 - val_loss: 0.3745 - val_mse: 0.3745 - val_mae: 0.4766\n",
            "Epoch 345/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3520 - mse: 0.3520 - mae: 0.4554 - val_loss: 0.3855 - val_mse: 0.3855 - val_mae: 0.4867\n",
            "Epoch 346/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3708 - mse: 0.3708 - mae: 0.4705 - val_loss: 0.3715 - val_mse: 0.3715 - val_mae: 0.4735\n",
            "Epoch 347/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3630 - mse: 0.3630 - mae: 0.4702 - val_loss: 0.3875 - val_mse: 0.3875 - val_mae: 0.4888\n",
            "Epoch 348/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3618 - mse: 0.3618 - mae: 0.4661 - val_loss: 0.3790 - val_mse: 0.3790 - val_mae: 0.4808\n",
            "Epoch 349/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3507 - mse: 0.3507 - mae: 0.4668 - val_loss: 0.3729 - val_mse: 0.3729 - val_mae: 0.4754\n",
            "Epoch 350/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3602 - mse: 0.3602 - mae: 0.4639 - val_loss: 0.3865 - val_mse: 0.3865 - val_mae: 0.4879\n",
            "Epoch 351/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3602 - mse: 0.3602 - mae: 0.4682 - val_loss: 0.3736 - val_mse: 0.3736 - val_mae: 0.4748\n",
            "Epoch 352/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3788 - mse: 0.3788 - mae: 0.4868 - val_loss: 0.3764 - val_mse: 0.3764 - val_mae: 0.4786\n",
            "Epoch 353/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3594 - mse: 0.3594 - mae: 0.4645 - val_loss: 0.4059 - val_mse: 0.4059 - val_mae: 0.5009\n",
            "Epoch 354/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3617 - mse: 0.3617 - mae: 0.4700 - val_loss: 0.3785 - val_mse: 0.3785 - val_mae: 0.4788\n",
            "Epoch 355/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3643 - mse: 0.3643 - mae: 0.4705 - val_loss: 0.3808 - val_mse: 0.3808 - val_mae: 0.4845\n",
            "Epoch 356/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3557 - mse: 0.3557 - mae: 0.4627 - val_loss: 0.3756 - val_mse: 0.3756 - val_mae: 0.4782\n",
            "Epoch 357/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3682 - mse: 0.3682 - mae: 0.4728 - val_loss: 0.3883 - val_mse: 0.3883 - val_mae: 0.4891\n",
            "Epoch 358/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3623 - mse: 0.3623 - mae: 0.4694 - val_loss: 0.3745 - val_mse: 0.3745 - val_mae: 0.4738\n",
            "Epoch 359/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3479 - mse: 0.3479 - mae: 0.4587 - val_loss: 0.3841 - val_mse: 0.3841 - val_mae: 0.4845\n",
            "Epoch 360/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3563 - mse: 0.3563 - mae: 0.4627 - val_loss: 0.3768 - val_mse: 0.3768 - val_mae: 0.4760\n",
            "Epoch 361/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3569 - mse: 0.3569 - mae: 0.4692 - val_loss: 0.3962 - val_mse: 0.3962 - val_mae: 0.4970\n",
            "Epoch 362/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3674 - mse: 0.3674 - mae: 0.4723 - val_loss: 0.3708 - val_mse: 0.3708 - val_mae: 0.4747\n",
            "Epoch 363/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3681 - mse: 0.3681 - mae: 0.4701 - val_loss: 0.3716 - val_mse: 0.3716 - val_mae: 0.4760\n",
            "Epoch 364/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3483 - mse: 0.3483 - mae: 0.4621 - val_loss: 0.3822 - val_mse: 0.3822 - val_mae: 0.4856\n",
            "Epoch 365/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3618 - mse: 0.3618 - mae: 0.4690 - val_loss: 0.3785 - val_mse: 0.3785 - val_mae: 0.4821\n",
            "Epoch 366/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3513 - mse: 0.3513 - mae: 0.4657 - val_loss: 0.3964 - val_mse: 0.3964 - val_mae: 0.4959\n",
            "Epoch 367/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3473 - mse: 0.3473 - mae: 0.4621 - val_loss: 0.3742 - val_mse: 0.3742 - val_mae: 0.4772\n",
            "Epoch 368/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3535 - mse: 0.3535 - mae: 0.4695 - val_loss: 0.3747 - val_mse: 0.3747 - val_mae: 0.4757\n",
            "Epoch 369/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3613 - mse: 0.3613 - mae: 0.4691 - val_loss: 0.3737 - val_mse: 0.3737 - val_mae: 0.4724\n",
            "Epoch 370/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3615 - mse: 0.3615 - mae: 0.4617 - val_loss: 0.3870 - val_mse: 0.3870 - val_mae: 0.4877\n",
            "Epoch 371/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3758 - mse: 0.3758 - mae: 0.4813 - val_loss: 0.3750 - val_mse: 0.3750 - val_mae: 0.4745\n",
            "Epoch 372/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3563 - mse: 0.3563 - mae: 0.4650 - val_loss: 0.3943 - val_mse: 0.3943 - val_mae: 0.4947\n",
            "Epoch 373/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3543 - mse: 0.3543 - mae: 0.4637 - val_loss: 0.3774 - val_mse: 0.3774 - val_mae: 0.4785\n",
            "Epoch 374/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3637 - mse: 0.3637 - mae: 0.4737 - val_loss: 0.3819 - val_mse: 0.3819 - val_mae: 0.4828\n",
            "Epoch 375/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3643 - mse: 0.3643 - mae: 0.4641 - val_loss: 0.3966 - val_mse: 0.3966 - val_mae: 0.4950\n",
            "Epoch 376/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3489 - mse: 0.3489 - mae: 0.4598 - val_loss: 0.3846 - val_mse: 0.3846 - val_mae: 0.4788\n",
            "Epoch 377/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3518 - mse: 0.3518 - mae: 0.4608 - val_loss: 0.4044 - val_mse: 0.4044 - val_mae: 0.4997\n",
            "Epoch 378/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3492 - mse: 0.3492 - mae: 0.4571 - val_loss: 0.3738 - val_mse: 0.3738 - val_mae: 0.4784\n",
            "Epoch 379/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3572 - mse: 0.3572 - mae: 0.4619 - val_loss: 0.3781 - val_mse: 0.3781 - val_mae: 0.4802\n",
            "Epoch 380/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3505 - mse: 0.3505 - mae: 0.4606 - val_loss: 0.3789 - val_mse: 0.3789 - val_mae: 0.4821\n",
            "Epoch 381/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3601 - mse: 0.3601 - mae: 0.4710 - val_loss: 0.3799 - val_mse: 0.3799 - val_mae: 0.4818\n",
            "Epoch 382/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3626 - mse: 0.3626 - mae: 0.4698 - val_loss: 0.3749 - val_mse: 0.3749 - val_mae: 0.4778\n",
            "Epoch 383/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3631 - mse: 0.3631 - mae: 0.4679 - val_loss: 0.3792 - val_mse: 0.3792 - val_mae: 0.4841\n",
            "Epoch 384/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3521 - mse: 0.3521 - mae: 0.4702 - val_loss: 0.3888 - val_mse: 0.3888 - val_mae: 0.4879\n",
            "Epoch 385/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3402 - mse: 0.3402 - mae: 0.4518 - val_loss: 0.3734 - val_mse: 0.3734 - val_mae: 0.4748\n",
            "Epoch 386/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3487 - mse: 0.3487 - mae: 0.4609 - val_loss: 0.3743 - val_mse: 0.3743 - val_mae: 0.4793\n",
            "Epoch 387/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3464 - mse: 0.3464 - mae: 0.4591 - val_loss: 0.3799 - val_mse: 0.3799 - val_mae: 0.4836\n",
            "Epoch 388/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3556 - mse: 0.3556 - mae: 0.4630 - val_loss: 0.3826 - val_mse: 0.3826 - val_mae: 0.4850\n",
            "Epoch 389/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3499 - mse: 0.3499 - mae: 0.4588 - val_loss: 0.3793 - val_mse: 0.3793 - val_mae: 0.4801\n",
            "Epoch 390/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3520 - mse: 0.3520 - mae: 0.4578 - val_loss: 0.3759 - val_mse: 0.3759 - val_mae: 0.4780\n",
            "Epoch 391/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3428 - mse: 0.3428 - mae: 0.4534 - val_loss: 0.3802 - val_mse: 0.3802 - val_mae: 0.4824\n",
            "Epoch 392/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3466 - mse: 0.3466 - mae: 0.4593 - val_loss: 0.3801 - val_mse: 0.3801 - val_mae: 0.4798\n",
            "Epoch 393/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3675 - mse: 0.3675 - mae: 0.4714 - val_loss: 0.3742 - val_mse: 0.3742 - val_mae: 0.4779\n",
            "Epoch 394/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3548 - mse: 0.3548 - mae: 0.4627 - val_loss: 0.3832 - val_mse: 0.3832 - val_mae: 0.4817\n",
            "Epoch 395/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3755 - mse: 0.3755 - mae: 0.4755 - val_loss: 0.3791 - val_mse: 0.3791 - val_mae: 0.4783\n",
            "Epoch 396/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3526 - mse: 0.3526 - mae: 0.4627 - val_loss: 0.3757 - val_mse: 0.3757 - val_mae: 0.4760\n",
            "Epoch 397/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3627 - mse: 0.3627 - mae: 0.4664 - val_loss: 0.3785 - val_mse: 0.3785 - val_mae: 0.4759\n",
            "Epoch 398/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3516 - mse: 0.3516 - mae: 0.4644 - val_loss: 0.3857 - val_mse: 0.3857 - val_mae: 0.4874\n",
            "Epoch 399/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3559 - mse: 0.3559 - mae: 0.4673 - val_loss: 0.3770 - val_mse: 0.3770 - val_mae: 0.4764\n",
            "Epoch 400/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3593 - mse: 0.3593 - mae: 0.4659 - val_loss: 0.3760 - val_mse: 0.3760 - val_mae: 0.4780\n",
            "Epoch 401/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3482 - mse: 0.3482 - mae: 0.4551 - val_loss: 0.3684 - val_mse: 0.3684 - val_mae: 0.4725\n",
            "Epoch 402/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3507 - mse: 0.3507 - mae: 0.4623 - val_loss: 0.3713 - val_mse: 0.3713 - val_mae: 0.4728\n",
            "Epoch 403/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3341 - mse: 0.3341 - mae: 0.4513 - val_loss: 0.3785 - val_mse: 0.3785 - val_mae: 0.4820\n",
            "Epoch 404/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3594 - mse: 0.3594 - mae: 0.4646 - val_loss: 0.3803 - val_mse: 0.3803 - val_mae: 0.4825\n",
            "Epoch 405/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3468 - mse: 0.3468 - mae: 0.4639 - val_loss: 0.3746 - val_mse: 0.3746 - val_mae: 0.4757\n",
            "Epoch 406/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3530 - mse: 0.3530 - mae: 0.4669 - val_loss: 0.3693 - val_mse: 0.3693 - val_mae: 0.4714\n",
            "Epoch 407/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3575 - mse: 0.3575 - mae: 0.4552 - val_loss: 0.3727 - val_mse: 0.3727 - val_mae: 0.4710\n",
            "Epoch 408/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3578 - mse: 0.3578 - mae: 0.4620 - val_loss: 0.3823 - val_mse: 0.3823 - val_mae: 0.4806\n",
            "Epoch 409/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3639 - mse: 0.3639 - mae: 0.4678 - val_loss: 0.3747 - val_mse: 0.3747 - val_mae: 0.4748\n",
            "Epoch 410/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3563 - mse: 0.3563 - mae: 0.4681 - val_loss: 0.3704 - val_mse: 0.3704 - val_mae: 0.4711\n",
            "Epoch 411/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3427 - mse: 0.3427 - mae: 0.4526 - val_loss: 0.3689 - val_mse: 0.3689 - val_mae: 0.4694\n",
            "Epoch 412/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3520 - mse: 0.3520 - mae: 0.4638 - val_loss: 0.3717 - val_mse: 0.3717 - val_mae: 0.4723\n",
            "Epoch 413/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3555 - mse: 0.3555 - mae: 0.4674 - val_loss: 0.3725 - val_mse: 0.3725 - val_mae: 0.4740\n",
            "Epoch 414/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3565 - mse: 0.3565 - mae: 0.4667 - val_loss: 0.3782 - val_mse: 0.3782 - val_mae: 0.4792\n",
            "Epoch 415/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3424 - mse: 0.3424 - mae: 0.4572 - val_loss: 0.3729 - val_mse: 0.3729 - val_mae: 0.4741\n",
            "Epoch 416/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3646 - mse: 0.3646 - mae: 0.4670 - val_loss: 0.3778 - val_mse: 0.3778 - val_mae: 0.4757\n",
            "Epoch 417/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3582 - mse: 0.3582 - mae: 0.4643 - val_loss: 0.3718 - val_mse: 0.3718 - val_mae: 0.4707\n",
            "Epoch 418/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3615 - mse: 0.3615 - mae: 0.4611 - val_loss: 0.3892 - val_mse: 0.3892 - val_mae: 0.4858\n",
            "Epoch 419/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3523 - mse: 0.3523 - mae: 0.4613 - val_loss: 0.3793 - val_mse: 0.3793 - val_mae: 0.4808\n",
            "Epoch 420/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3687 - mse: 0.3687 - mae: 0.4744 - val_loss: 0.3733 - val_mse: 0.3733 - val_mae: 0.4760\n",
            "Epoch 421/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3569 - mse: 0.3569 - mae: 0.4686 - val_loss: 0.3697 - val_mse: 0.3697 - val_mae: 0.4682\n",
            "Epoch 422/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3333 - mse: 0.3333 - mae: 0.4449 - val_loss: 0.3764 - val_mse: 0.3764 - val_mae: 0.4748\n",
            "Epoch 423/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3597 - mse: 0.3597 - mae: 0.4643 - val_loss: 0.3699 - val_mse: 0.3699 - val_mae: 0.4712\n",
            "Epoch 424/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3519 - mse: 0.3519 - mae: 0.4648 - val_loss: 0.3742 - val_mse: 0.3742 - val_mae: 0.4767\n",
            "Epoch 425/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3710 - mse: 0.3710 - mae: 0.4749 - val_loss: 0.3896 - val_mse: 0.3896 - val_mae: 0.4856\n",
            "Epoch 426/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3423 - mse: 0.3423 - mae: 0.4523 - val_loss: 0.3743 - val_mse: 0.3743 - val_mae: 0.4725\n",
            "Epoch 427/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3566 - mse: 0.3566 - mae: 0.4577 - val_loss: 0.3728 - val_mse: 0.3728 - val_mae: 0.4734\n",
            "Epoch 428/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3552 - mse: 0.3552 - mae: 0.4670 - val_loss: 0.3724 - val_mse: 0.3724 - val_mae: 0.4700\n",
            "Epoch 429/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3501 - mse: 0.3501 - mae: 0.4625 - val_loss: 0.3882 - val_mse: 0.3882 - val_mae: 0.4871\n",
            "Epoch 430/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3510 - mse: 0.3510 - mae: 0.4672 - val_loss: 0.3768 - val_mse: 0.3768 - val_mae: 0.4739\n",
            "Epoch 431/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3415 - mse: 0.3415 - mae: 0.4532 - val_loss: 0.3777 - val_mse: 0.3777 - val_mae: 0.4748\n",
            "Epoch 432/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3460 - mse: 0.3460 - mae: 0.4561 - val_loss: 0.3727 - val_mse: 0.3727 - val_mae: 0.4718\n",
            "Epoch 433/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3385 - mse: 0.3385 - mae: 0.4595 - val_loss: 0.3803 - val_mse: 0.3803 - val_mae: 0.4804\n",
            "Epoch 434/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3472 - mse: 0.3472 - mae: 0.4578 - val_loss: 0.3785 - val_mse: 0.3785 - val_mae: 0.4753\n",
            "Epoch 435/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3575 - mse: 0.3575 - mae: 0.4595 - val_loss: 0.3729 - val_mse: 0.3729 - val_mae: 0.4726\n",
            "Epoch 436/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3429 - mse: 0.3429 - mae: 0.4594 - val_loss: 0.3688 - val_mse: 0.3688 - val_mae: 0.4655\n",
            "Epoch 437/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3510 - mse: 0.3510 - mae: 0.4661 - val_loss: 0.3752 - val_mse: 0.3752 - val_mae: 0.4735\n",
            "Epoch 438/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3500 - mse: 0.3500 - mae: 0.4577 - val_loss: 0.3790 - val_mse: 0.3790 - val_mae: 0.4774\n",
            "Epoch 439/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3435 - mse: 0.3435 - mae: 0.4544 - val_loss: 0.3782 - val_mse: 0.3782 - val_mae: 0.4749\n",
            "Epoch 440/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3742 - mse: 0.3742 - mae: 0.4770 - val_loss: 0.3715 - val_mse: 0.3715 - val_mae: 0.4701\n",
            "Epoch 441/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3540 - mse: 0.3540 - mae: 0.4670 - val_loss: 0.3718 - val_mse: 0.3718 - val_mae: 0.4726\n",
            "Epoch 442/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3326 - mse: 0.3326 - mae: 0.4521 - val_loss: 0.3781 - val_mse: 0.3781 - val_mae: 0.4727\n",
            "Epoch 443/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3577 - mse: 0.3577 - mae: 0.4676 - val_loss: 0.3680 - val_mse: 0.3680 - val_mae: 0.4671\n",
            "Epoch 444/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3490 - mse: 0.3490 - mae: 0.4594 - val_loss: 0.3704 - val_mse: 0.3704 - val_mae: 0.4703\n",
            "Epoch 445/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3449 - mse: 0.3449 - mae: 0.4523 - val_loss: 0.3755 - val_mse: 0.3755 - val_mae: 0.4767\n",
            "Epoch 446/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3435 - mse: 0.3435 - mae: 0.4552 - val_loss: 0.3802 - val_mse: 0.3802 - val_mae: 0.4791\n",
            "Epoch 447/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3458 - mse: 0.3458 - mae: 0.4497 - val_loss: 0.3919 - val_mse: 0.3919 - val_mae: 0.4851\n",
            "Epoch 448/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3485 - mse: 0.3485 - mae: 0.4575 - val_loss: 0.3759 - val_mse: 0.3759 - val_mae: 0.4744\n",
            "Epoch 449/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3540 - mse: 0.3540 - mae: 0.4613 - val_loss: 0.3740 - val_mse: 0.3740 - val_mae: 0.4717\n",
            "Epoch 450/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3596 - mse: 0.3596 - mae: 0.4658 - val_loss: 0.3882 - val_mse: 0.3882 - val_mae: 0.4845\n",
            "Epoch 451/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3508 - mse: 0.3508 - mae: 0.4600 - val_loss: 0.3702 - val_mse: 0.3702 - val_mae: 0.4712\n",
            "Epoch 452/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3588 - mse: 0.3588 - mae: 0.4649 - val_loss: 0.3669 - val_mse: 0.3669 - val_mae: 0.4674\n",
            "Epoch 453/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3433 - mse: 0.3433 - mae: 0.4590 - val_loss: 0.3686 - val_mse: 0.3686 - val_mae: 0.4693\n",
            "Epoch 454/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3602 - mse: 0.3602 - mae: 0.4595 - val_loss: 0.3732 - val_mse: 0.3732 - val_mae: 0.4700\n",
            "Epoch 455/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3427 - mse: 0.3427 - mae: 0.4586 - val_loss: 0.3749 - val_mse: 0.3749 - val_mae: 0.4696\n",
            "Epoch 456/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3466 - mse: 0.3466 - mae: 0.4521 - val_loss: 0.3786 - val_mse: 0.3786 - val_mae: 0.4774\n",
            "Epoch 457/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3463 - mse: 0.3463 - mae: 0.4606 - val_loss: 0.3778 - val_mse: 0.3778 - val_mae: 0.4779\n",
            "Epoch 458/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3496 - mse: 0.3496 - mae: 0.4598 - val_loss: 0.3800 - val_mse: 0.3800 - val_mae: 0.4804\n",
            "Epoch 459/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3499 - mse: 0.3499 - mae: 0.4640 - val_loss: 0.3766 - val_mse: 0.3766 - val_mae: 0.4737\n",
            "Epoch 460/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3408 - mse: 0.3408 - mae: 0.4539 - val_loss: 0.3837 - val_mse: 0.3837 - val_mae: 0.4778\n",
            "Epoch 461/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3370 - mse: 0.3370 - mae: 0.4524 - val_loss: 0.3749 - val_mse: 0.3749 - val_mae: 0.4738\n",
            "Epoch 462/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3403 - mse: 0.3403 - mae: 0.4482 - val_loss: 0.3816 - val_mse: 0.3816 - val_mae: 0.4790\n",
            "Epoch 463/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3404 - mse: 0.3404 - mae: 0.4525 - val_loss: 0.3714 - val_mse: 0.3714 - val_mae: 0.4708\n",
            "Epoch 464/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3323 - mse: 0.3323 - mae: 0.4496 - val_loss: 0.3791 - val_mse: 0.3791 - val_mae: 0.4758\n",
            "Epoch 465/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3275 - mse: 0.3275 - mae: 0.4468 - val_loss: 0.3746 - val_mse: 0.3746 - val_mae: 0.4694\n",
            "Epoch 466/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3406 - mse: 0.3406 - mae: 0.4593 - val_loss: 0.3790 - val_mse: 0.3790 - val_mae: 0.4755\n",
            "Epoch 467/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3319 - mse: 0.3319 - mae: 0.4495 - val_loss: 0.3783 - val_mse: 0.3783 - val_mae: 0.4745\n",
            "Epoch 468/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3478 - mse: 0.3478 - mae: 0.4599 - val_loss: 0.3797 - val_mse: 0.3797 - val_mae: 0.4758\n",
            "Epoch 469/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3352 - mse: 0.3352 - mae: 0.4534 - val_loss: 0.3770 - val_mse: 0.3770 - val_mae: 0.4744\n",
            "Epoch 470/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3480 - mse: 0.3480 - mae: 0.4679 - val_loss: 0.3742 - val_mse: 0.3742 - val_mae: 0.4748\n",
            "Epoch 471/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3399 - mse: 0.3399 - mae: 0.4484 - val_loss: 0.3842 - val_mse: 0.3842 - val_mae: 0.4819\n",
            "Epoch 472/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3448 - mse: 0.3448 - mae: 0.4612 - val_loss: 0.3731 - val_mse: 0.3731 - val_mae: 0.4717\n",
            "Epoch 473/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3486 - mse: 0.3486 - mae: 0.4580 - val_loss: 0.3938 - val_mse: 0.3938 - val_mae: 0.4840\n",
            "Epoch 474/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3513 - mse: 0.3513 - mae: 0.4615 - val_loss: 0.3761 - val_mse: 0.3761 - val_mae: 0.4744\n",
            "Epoch 475/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3626 - mse: 0.3626 - mae: 0.4716 - val_loss: 0.3752 - val_mse: 0.3752 - val_mae: 0.4744\n",
            "Epoch 476/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3513 - mse: 0.3513 - mae: 0.4660 - val_loss: 0.3760 - val_mse: 0.3760 - val_mae: 0.4744\n",
            "Epoch 477/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3369 - mse: 0.3369 - mae: 0.4504 - val_loss: 0.3937 - val_mse: 0.3937 - val_mae: 0.4882\n",
            "Epoch 478/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3331 - mse: 0.3331 - mae: 0.4531 - val_loss: 0.3784 - val_mse: 0.3784 - val_mae: 0.4744\n",
            "Epoch 479/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3349 - mse: 0.3349 - mae: 0.4527 - val_loss: 0.3809 - val_mse: 0.3809 - val_mae: 0.4783\n",
            "Epoch 480/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3532 - mse: 0.3532 - mae: 0.4642 - val_loss: 0.3796 - val_mse: 0.3796 - val_mae: 0.4789\n",
            "Epoch 481/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3503 - mse: 0.3503 - mae: 0.4644 - val_loss: 0.3983 - val_mse: 0.3983 - val_mae: 0.4926\n",
            "Epoch 482/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3398 - mse: 0.3398 - mae: 0.4611 - val_loss: 0.3786 - val_mse: 0.3786 - val_mae: 0.4765\n",
            "Epoch 483/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3404 - mse: 0.3404 - mae: 0.4576 - val_loss: 0.3735 - val_mse: 0.3735 - val_mae: 0.4698\n",
            "Epoch 484/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3390 - mse: 0.3390 - mae: 0.4554 - val_loss: 0.3800 - val_mse: 0.3800 - val_mae: 0.4787\n",
            "Epoch 485/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3315 - mse: 0.3315 - mae: 0.4559 - val_loss: 0.3749 - val_mse: 0.3749 - val_mae: 0.4739\n",
            "Epoch 486/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3302 - mse: 0.3302 - mae: 0.4522 - val_loss: 0.3795 - val_mse: 0.3795 - val_mae: 0.4777\n",
            "Epoch 487/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3457 - mse: 0.3457 - mae: 0.4635 - val_loss: 0.3763 - val_mse: 0.3763 - val_mae: 0.4729\n",
            "Epoch 488/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3345 - mse: 0.3345 - mae: 0.4475 - val_loss: 0.3752 - val_mse: 0.3752 - val_mae: 0.4765\n",
            "Epoch 489/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3507 - mse: 0.3507 - mae: 0.4611 - val_loss: 0.3731 - val_mse: 0.3731 - val_mae: 0.4735\n",
            "Epoch 490/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3507 - mse: 0.3507 - mae: 0.4527 - val_loss: 0.3779 - val_mse: 0.3779 - val_mae: 0.4737\n",
            "Epoch 491/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3449 - mse: 0.3449 - mae: 0.4602 - val_loss: 0.3711 - val_mse: 0.3711 - val_mae: 0.4716\n",
            "Epoch 492/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3398 - mse: 0.3398 - mae: 0.4568 - val_loss: 0.3793 - val_mse: 0.3793 - val_mae: 0.4742\n",
            "Epoch 493/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3399 - mse: 0.3399 - mae: 0.4572 - val_loss: 0.3757 - val_mse: 0.3757 - val_mae: 0.4699\n",
            "Epoch 494/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3511 - mse: 0.3511 - mae: 0.4594 - val_loss: 0.3937 - val_mse: 0.3937 - val_mae: 0.4866\n",
            "Epoch 495/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3519 - mse: 0.3519 - mae: 0.4662 - val_loss: 0.3789 - val_mse: 0.3789 - val_mae: 0.4756\n",
            "Epoch 496/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3301 - mse: 0.3301 - mae: 0.4483 - val_loss: 0.3794 - val_mse: 0.3794 - val_mae: 0.4757\n",
            "Epoch 497/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3436 - mse: 0.3436 - mae: 0.4542 - val_loss: 0.3736 - val_mse: 0.3736 - val_mae: 0.4712\n",
            "Epoch 498/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3443 - mse: 0.3443 - mae: 0.4491 - val_loss: 0.3746 - val_mse: 0.3746 - val_mae: 0.4698\n",
            "Epoch 499/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3413 - mse: 0.3413 - mae: 0.4465 - val_loss: 0.3689 - val_mse: 0.3689 - val_mae: 0.4705\n",
            "Epoch 500/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3440 - mse: 0.3440 - mae: 0.4638 - val_loss: 0.3741 - val_mse: 0.3741 - val_mae: 0.4725\n",
            "Epoch 501/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3349 - mse: 0.3349 - mae: 0.4493 - val_loss: 0.3677 - val_mse: 0.3677 - val_mae: 0.4706\n",
            "Epoch 502/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3446 - mse: 0.3446 - mae: 0.4597 - val_loss: 0.3766 - val_mse: 0.3766 - val_mae: 0.4739\n",
            "Epoch 503/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3410 - mse: 0.3410 - mae: 0.4516 - val_loss: 0.3733 - val_mse: 0.3733 - val_mae: 0.4719\n",
            "Epoch 504/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3293 - mse: 0.3293 - mae: 0.4437 - val_loss: 0.3724 - val_mse: 0.3724 - val_mae: 0.4694\n",
            "Epoch 505/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3295 - mse: 0.3295 - mae: 0.4560 - val_loss: 0.3697 - val_mse: 0.3697 - val_mae: 0.4663\n",
            "Epoch 506/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3395 - mse: 0.3395 - mae: 0.4501 - val_loss: 0.3781 - val_mse: 0.3781 - val_mae: 0.4771\n",
            "Epoch 507/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3441 - mse: 0.3441 - mae: 0.4613 - val_loss: 0.3743 - val_mse: 0.3743 - val_mae: 0.4718\n",
            "Epoch 508/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3357 - mse: 0.3357 - mae: 0.4485 - val_loss: 0.3796 - val_mse: 0.3796 - val_mae: 0.4710\n",
            "Epoch 509/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3284 - mse: 0.3284 - mae: 0.4450 - val_loss: 0.3772 - val_mse: 0.3772 - val_mae: 0.4699\n",
            "Epoch 510/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3325 - mse: 0.3325 - mae: 0.4552 - val_loss: 0.3737 - val_mse: 0.3737 - val_mae: 0.4699\n",
            "Epoch 511/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3546 - mse: 0.3546 - mae: 0.4581 - val_loss: 0.4062 - val_mse: 0.4062 - val_mae: 0.4952\n",
            "Epoch 512/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3476 - mse: 0.3476 - mae: 0.4657 - val_loss: 0.3719 - val_mse: 0.3719 - val_mae: 0.4667\n",
            "Epoch 513/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3344 - mse: 0.3344 - mae: 0.4526 - val_loss: 0.3776 - val_mse: 0.3776 - val_mae: 0.4731\n",
            "Epoch 514/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3532 - mse: 0.3532 - mae: 0.4609 - val_loss: 0.3728 - val_mse: 0.3728 - val_mae: 0.4700\n",
            "Epoch 515/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3438 - mse: 0.3438 - mae: 0.4529 - val_loss: 0.3774 - val_mse: 0.3774 - val_mae: 0.4717\n",
            "Epoch 516/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3373 - mse: 0.3373 - mae: 0.4540 - val_loss: 0.3772 - val_mse: 0.3772 - val_mae: 0.4694\n",
            "Epoch 517/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3454 - mse: 0.3454 - mae: 0.4565 - val_loss: 0.3830 - val_mse: 0.3830 - val_mae: 0.4740\n",
            "Epoch 518/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3370 - mse: 0.3370 - mae: 0.4490 - val_loss: 0.3751 - val_mse: 0.3751 - val_mae: 0.4709\n",
            "Epoch 519/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3335 - mse: 0.3335 - mae: 0.4517 - val_loss: 0.3842 - val_mse: 0.3842 - val_mae: 0.4766\n",
            "Epoch 520/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3388 - mse: 0.3388 - mae: 0.4574 - val_loss: 0.3869 - val_mse: 0.3869 - val_mae: 0.4813\n",
            "Epoch 521/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3591 - mse: 0.3591 - mae: 0.4672 - val_loss: 0.3758 - val_mse: 0.3758 - val_mae: 0.4703\n",
            "Epoch 522/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3216 - mse: 0.3216 - mae: 0.4406 - val_loss: 0.3809 - val_mse: 0.3809 - val_mae: 0.4734\n",
            "Epoch 523/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3461 - mse: 0.3461 - mae: 0.4548 - val_loss: 0.3733 - val_mse: 0.3733 - val_mae: 0.4704\n",
            "Epoch 524/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3558 - mse: 0.3558 - mae: 0.4721 - val_loss: 0.3835 - val_mse: 0.3835 - val_mae: 0.4804\n",
            "Epoch 525/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3235 - mse: 0.3235 - mae: 0.4454 - val_loss: 0.3837 - val_mse: 0.3837 - val_mae: 0.4778\n",
            "Epoch 526/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3337 - mse: 0.3337 - mae: 0.4546 - val_loss: 0.3843 - val_mse: 0.3843 - val_mae: 0.4761\n",
            "Epoch 527/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3412 - mse: 0.3412 - mae: 0.4548 - val_loss: 0.3796 - val_mse: 0.3796 - val_mae: 0.4719\n",
            "Epoch 528/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3415 - mse: 0.3415 - mae: 0.4570 - val_loss: 0.3820 - val_mse: 0.3820 - val_mae: 0.4774\n",
            "Epoch 529/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3339 - mse: 0.3339 - mae: 0.4452 - val_loss: 0.3843 - val_mse: 0.3843 - val_mae: 0.4748\n",
            "Epoch 530/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3443 - mse: 0.3443 - mae: 0.4556 - val_loss: 0.3756 - val_mse: 0.3756 - val_mae: 0.4736\n",
            "Epoch 531/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3474 - mse: 0.3474 - mae: 0.4595 - val_loss: 0.3783 - val_mse: 0.3783 - val_mae: 0.4701\n",
            "Epoch 532/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3354 - mse: 0.3354 - mae: 0.4527 - val_loss: 0.3790 - val_mse: 0.3790 - val_mae: 0.4745\n",
            "Epoch 533/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3388 - mse: 0.3388 - mae: 0.4545 - val_loss: 0.3808 - val_mse: 0.3808 - val_mae: 0.4741\n",
            "Epoch 534/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3421 - mse: 0.3421 - mae: 0.4512 - val_loss: 0.3805 - val_mse: 0.3805 - val_mae: 0.4736\n",
            "Epoch 535/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3525 - mse: 0.3525 - mae: 0.4630 - val_loss: 0.3787 - val_mse: 0.3787 - val_mae: 0.4731\n",
            "Epoch 536/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3551 - mse: 0.3551 - mae: 0.4685 - val_loss: 0.3783 - val_mse: 0.3783 - val_mae: 0.4697\n",
            "Epoch 537/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3401 - mse: 0.3401 - mae: 0.4546 - val_loss: 0.3791 - val_mse: 0.3791 - val_mae: 0.4687\n",
            "Epoch 538/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3477 - mse: 0.3477 - mae: 0.4598 - val_loss: 0.3869 - val_mse: 0.3869 - val_mae: 0.4746\n",
            "Epoch 539/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3501 - mse: 0.3501 - mae: 0.4578 - val_loss: 0.3791 - val_mse: 0.3791 - val_mae: 0.4695\n",
            "Epoch 540/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3325 - mse: 0.3325 - mae: 0.4570 - val_loss: 0.3749 - val_mse: 0.3749 - val_mae: 0.4669\n",
            "Epoch 541/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3467 - mse: 0.3467 - mae: 0.4567 - val_loss: 0.3865 - val_mse: 0.3865 - val_mae: 0.4743\n",
            "Epoch 542/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3328 - mse: 0.3328 - mae: 0.4442 - val_loss: 0.3825 - val_mse: 0.3825 - val_mae: 0.4713\n",
            "Epoch 543/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3462 - mse: 0.3462 - mae: 0.4582 - val_loss: 0.3749 - val_mse: 0.3749 - val_mae: 0.4682\n",
            "Epoch 544/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3472 - mse: 0.3472 - mae: 0.4599 - val_loss: 0.3767 - val_mse: 0.3767 - val_mae: 0.4709\n",
            "Epoch 545/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3318 - mse: 0.3318 - mae: 0.4476 - val_loss: 0.3899 - val_mse: 0.3899 - val_mae: 0.4791\n",
            "Epoch 546/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3325 - mse: 0.3325 - mae: 0.4494 - val_loss: 0.3771 - val_mse: 0.3771 - val_mae: 0.4697\n",
            "Epoch 547/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3685 - mse: 0.3685 - mae: 0.4781 - val_loss: 0.3817 - val_mse: 0.3817 - val_mae: 0.4777\n",
            "Epoch 548/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3377 - mse: 0.3377 - mae: 0.4533 - val_loss: 0.3810 - val_mse: 0.3810 - val_mae: 0.4729\n",
            "Epoch 549/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3512 - mse: 0.3512 - mae: 0.4604 - val_loss: 0.3817 - val_mse: 0.3817 - val_mae: 0.4756\n",
            "Epoch 550/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3167 - mse: 0.3167 - mae: 0.4379 - val_loss: 0.3792 - val_mse: 0.3792 - val_mae: 0.4722\n",
            "Epoch 551/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3366 - mse: 0.3366 - mae: 0.4526 - val_loss: 0.3818 - val_mse: 0.3818 - val_mae: 0.4787\n",
            "Epoch 552/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3331 - mse: 0.3331 - mae: 0.4525 - val_loss: 0.3918 - val_mse: 0.3918 - val_mae: 0.4811\n",
            "14/14 [==============================] - 0s 2ms/step - loss: 0.4587 - mse: 0.4587 - mae: 0.5347\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 13ms/step - loss: 13.7524 - mse: 13.7524 - mae: 3.4452 - val_loss: 11.1842 - val_mse: 11.1842 - val_mae: 3.0885\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 9.4035 - mse: 9.4035 - mae: 2.7636 - val_loss: 7.5678 - val_mse: 7.5678 - val_mae: 2.4861\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 6.2934 - mse: 6.2934 - mae: 2.2068 - val_loss: 5.2175 - val_mse: 5.2175 - val_mae: 2.0045\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 4.2865 - mse: 4.2865 - mae: 1.7544 - val_loss: 3.8941 - val_mse: 3.8941 - val_mae: 1.6828\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 3.4075 - mse: 3.4075 - mae: 1.5404 - val_loss: 3.2386 - val_mse: 3.2386 - val_mae: 1.4865\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.8668 - mse: 2.8668 - mae: 1.3743 - val_loss: 2.9537 - val_mse: 2.9537 - val_mae: 1.3842\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.6847 - mse: 2.6847 - mae: 1.3168 - val_loss: 2.7803 - val_mse: 2.7803 - val_mae: 1.3277\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.4797 - mse: 2.4797 - mae: 1.2587 - val_loss: 2.6578 - val_mse: 2.6578 - val_mae: 1.2926\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.4362 - mse: 2.4362 - mae: 1.2462 - val_loss: 2.5686 - val_mse: 2.5686 - val_mae: 1.2673\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.3838 - mse: 2.3838 - mae: 1.2236 - val_loss: 2.4932 - val_mse: 2.4932 - val_mae: 1.2472\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.3213 - mse: 2.3213 - mae: 1.2248 - val_loss: 2.4010 - val_mse: 2.4010 - val_mae: 1.2187\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.0417 - mse: 2.0417 - mae: 1.1490 - val_loss: 2.3400 - val_mse: 2.3400 - val_mae: 1.1954\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.0764 - mse: 2.0764 - mae: 1.1604 - val_loss: 2.2832 - val_mse: 2.2832 - val_mae: 1.1800\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.9989 - mse: 1.9989 - mae: 1.1055 - val_loss: 2.2337 - val_mse: 2.2337 - val_mae: 1.1680\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.9114 - mse: 1.9114 - mae: 1.0893 - val_loss: 2.1843 - val_mse: 2.1843 - val_mae: 1.1506\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.9364 - mse: 1.9364 - mae: 1.0981 - val_loss: 2.1410 - val_mse: 2.1410 - val_mae: 1.1360\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.9397 - mse: 1.9397 - mae: 1.1092 - val_loss: 2.0910 - val_mse: 2.0910 - val_mae: 1.1217\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.7836 - mse: 1.7836 - mae: 1.0569 - val_loss: 2.0399 - val_mse: 2.0399 - val_mae: 1.1054\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.7762 - mse: 1.7762 - mae: 1.0500 - val_loss: 1.9994 - val_mse: 1.9994 - val_mae: 1.0943\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.6922 - mse: 1.6922 - mae: 1.0305 - val_loss: 1.9729 - val_mse: 1.9729 - val_mae: 1.0820\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.7035 - mse: 1.7035 - mae: 1.0386 - val_loss: 1.9384 - val_mse: 1.9384 - val_mae: 1.0697\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.6353 - mse: 1.6353 - mae: 1.0060 - val_loss: 1.9002 - val_mse: 1.9002 - val_mae: 1.0534\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.5173 - mse: 1.5173 - mae: 0.9694 - val_loss: 1.8592 - val_mse: 1.8592 - val_mae: 1.0401\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.6925 - mse: 1.6925 - mae: 1.0207 - val_loss: 1.8148 - val_mse: 1.8148 - val_mae: 1.0258\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.4529 - mse: 1.4529 - mae: 0.9485 - val_loss: 1.7865 - val_mse: 1.7865 - val_mae: 1.0190\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.4777 - mse: 1.4777 - mae: 0.9561 - val_loss: 1.7581 - val_mse: 1.7581 - val_mae: 1.0047\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.4565 - mse: 1.4565 - mae: 0.9474 - val_loss: 1.7100 - val_mse: 1.7100 - val_mae: 0.9901\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.4231 - mse: 1.4231 - mae: 0.9522 - val_loss: 1.6872 - val_mse: 1.6872 - val_mae: 0.9804\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.4642 - mse: 1.4642 - mae: 0.9578 - val_loss: 1.6438 - val_mse: 1.6438 - val_mae: 0.9691\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.3581 - mse: 1.3581 - mae: 0.9194 - val_loss: 1.6062 - val_mse: 1.6062 - val_mae: 0.9515\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.3280 - mse: 1.3280 - mae: 0.9096 - val_loss: 1.5814 - val_mse: 1.5814 - val_mae: 0.9431\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.2558 - mse: 1.2558 - mae: 0.8726 - val_loss: 1.5443 - val_mse: 1.5443 - val_mae: 0.9292\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.2973 - mse: 1.2973 - mae: 0.8980 - val_loss: 1.5143 - val_mse: 1.5143 - val_mae: 0.9191\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1955 - mse: 1.1955 - mae: 0.8586 - val_loss: 1.4756 - val_mse: 1.4756 - val_mae: 0.9096\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.1841 - mse: 1.1841 - mae: 0.8559 - val_loss: 1.4466 - val_mse: 1.4466 - val_mae: 0.9016\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1818 - mse: 1.1818 - mae: 0.8649 - val_loss: 1.4114 - val_mse: 1.4114 - val_mae: 0.8845\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1233 - mse: 1.1233 - mae: 0.8354 - val_loss: 1.3821 - val_mse: 1.3821 - val_mae: 0.8729\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1209 - mse: 1.1209 - mae: 0.8332 - val_loss: 1.3665 - val_mse: 1.3665 - val_mae: 0.8689\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0501 - mse: 1.0501 - mae: 0.8209 - val_loss: 1.3300 - val_mse: 1.3300 - val_mae: 0.8542\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1262 - mse: 1.1262 - mae: 0.8392 - val_loss: 1.3146 - val_mse: 1.3146 - val_mae: 0.8454\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0872 - mse: 1.0872 - mae: 0.8266 - val_loss: 1.2826 - val_mse: 1.2826 - val_mae: 0.8333\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.9583 - mse: 0.9583 - mae: 0.7737 - val_loss: 1.2544 - val_mse: 1.2544 - val_mae: 0.8212\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0131 - mse: 1.0131 - mae: 0.8002 - val_loss: 1.2291 - val_mse: 1.2291 - val_mae: 0.8162\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0199 - mse: 1.0199 - mae: 0.7987 - val_loss: 1.2148 - val_mse: 1.2148 - val_mae: 0.8093\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.9931 - mse: 0.9931 - mae: 0.7828 - val_loss: 1.1949 - val_mse: 1.1949 - val_mae: 0.8005\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9544 - mse: 0.9544 - mae: 0.7777 - val_loss: 1.1633 - val_mse: 1.1633 - val_mae: 0.7880\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9327 - mse: 0.9327 - mae: 0.7590 - val_loss: 1.1445 - val_mse: 1.1445 - val_mae: 0.7879\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8730 - mse: 0.8730 - mae: 0.7386 - val_loss: 1.1138 - val_mse: 1.1138 - val_mae: 0.7815\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9092 - mse: 0.9092 - mae: 0.7520 - val_loss: 1.0976 - val_mse: 1.0976 - val_mae: 0.7720\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8868 - mse: 0.8868 - mae: 0.7435 - val_loss: 1.0587 - val_mse: 1.0587 - val_mae: 0.7582\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8825 - mse: 0.8825 - mae: 0.7363 - val_loss: 1.0425 - val_mse: 1.0425 - val_mae: 0.7489\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8057 - mse: 0.8057 - mae: 0.7078 - val_loss: 1.0210 - val_mse: 1.0210 - val_mae: 0.7398\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8272 - mse: 0.8272 - mae: 0.7237 - val_loss: 0.9930 - val_mse: 0.9930 - val_mae: 0.7371\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7858 - mse: 0.7858 - mae: 0.7012 - val_loss: 0.9834 - val_mse: 0.9834 - val_mae: 0.7291\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7915 - mse: 0.7915 - mae: 0.6969 - val_loss: 0.9702 - val_mse: 0.9702 - val_mae: 0.7217\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7955 - mse: 0.7955 - mae: 0.6977 - val_loss: 0.9416 - val_mse: 0.9416 - val_mae: 0.7137\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7961 - mse: 0.7961 - mae: 0.7036 - val_loss: 0.9335 - val_mse: 0.9335 - val_mae: 0.7082\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7541 - mse: 0.7541 - mae: 0.6893 - val_loss: 0.9055 - val_mse: 0.9055 - val_mae: 0.7025\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7287 - mse: 0.7287 - mae: 0.6739 - val_loss: 0.8921 - val_mse: 0.8921 - val_mae: 0.6931\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7306 - mse: 0.7306 - mae: 0.6796 - val_loss: 0.8669 - val_mse: 0.8669 - val_mae: 0.6904\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7106 - mse: 0.7106 - mae: 0.6524 - val_loss: 0.8626 - val_mse: 0.8626 - val_mae: 0.6813\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6810 - mse: 0.6810 - mae: 0.6438 - val_loss: 0.8559 - val_mse: 0.8559 - val_mae: 0.6759\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6920 - mse: 0.6920 - mae: 0.6505 - val_loss: 0.8323 - val_mse: 0.8323 - val_mae: 0.6703\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6733 - mse: 0.6733 - mae: 0.6510 - val_loss: 0.8126 - val_mse: 0.8126 - val_mae: 0.6630\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6585 - mse: 0.6585 - mae: 0.6421 - val_loss: 0.8130 - val_mse: 0.8130 - val_mae: 0.6589\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6379 - mse: 0.6379 - mae: 0.6293 - val_loss: 0.8080 - val_mse: 0.8080 - val_mae: 0.6586\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6394 - mse: 0.6394 - mae: 0.6235 - val_loss: 0.7754 - val_mse: 0.7754 - val_mae: 0.6448\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6275 - mse: 0.6275 - mae: 0.6228 - val_loss: 0.7648 - val_mse: 0.7648 - val_mae: 0.6422\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6194 - mse: 0.6194 - mae: 0.6109 - val_loss: 0.7512 - val_mse: 0.7512 - val_mae: 0.6362\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5773 - mse: 0.5773 - mae: 0.5913 - val_loss: 0.7486 - val_mse: 0.7486 - val_mae: 0.6315\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6066 - mse: 0.6066 - mae: 0.6117 - val_loss: 0.7443 - val_mse: 0.7443 - val_mae: 0.6324\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5987 - mse: 0.5987 - mae: 0.6043 - val_loss: 0.7196 - val_mse: 0.7196 - val_mae: 0.6233\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6035 - mse: 0.6035 - mae: 0.6175 - val_loss: 0.7103 - val_mse: 0.7103 - val_mae: 0.6220\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5793 - mse: 0.5793 - mae: 0.5966 - val_loss: 0.7041 - val_mse: 0.7041 - val_mae: 0.6178\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6139 - mse: 0.6139 - mae: 0.6135 - val_loss: 0.6839 - val_mse: 0.6839 - val_mae: 0.6164\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5668 - mse: 0.5668 - mae: 0.5931 - val_loss: 0.6863 - val_mse: 0.6863 - val_mae: 0.6147\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5656 - mse: 0.5656 - mae: 0.5978 - val_loss: 0.6779 - val_mse: 0.6779 - val_mae: 0.6077\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5470 - mse: 0.5470 - mae: 0.5730 - val_loss: 0.6717 - val_mse: 0.6717 - val_mae: 0.6049\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5439 - mse: 0.5439 - mae: 0.5808 - val_loss: 0.6702 - val_mse: 0.6702 - val_mae: 0.6035\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5661 - mse: 0.5661 - mae: 0.5911 - val_loss: 0.6425 - val_mse: 0.6425 - val_mae: 0.5955\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5327 - mse: 0.5327 - mae: 0.5776 - val_loss: 0.6439 - val_mse: 0.6439 - val_mae: 0.5964\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5455 - mse: 0.5455 - mae: 0.5794 - val_loss: 0.6394 - val_mse: 0.6394 - val_mae: 0.5928\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5307 - mse: 0.5307 - mae: 0.5689 - val_loss: 0.6301 - val_mse: 0.6301 - val_mae: 0.5909\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5243 - mse: 0.5243 - mae: 0.5700 - val_loss: 0.6234 - val_mse: 0.6234 - val_mae: 0.5848\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5481 - mse: 0.5481 - mae: 0.5754 - val_loss: 0.6173 - val_mse: 0.6173 - val_mae: 0.5833\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5247 - mse: 0.5247 - mae: 0.5609 - val_loss: 0.6132 - val_mse: 0.6132 - val_mae: 0.5816\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5026 - mse: 0.5026 - mae: 0.5500 - val_loss: 0.6000 - val_mse: 0.6000 - val_mae: 0.5790\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5115 - mse: 0.5115 - mae: 0.5630 - val_loss: 0.5952 - val_mse: 0.5952 - val_mae: 0.5771\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5187 - mse: 0.5187 - mae: 0.5659 - val_loss: 0.5936 - val_mse: 0.5936 - val_mae: 0.5712\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4943 - mse: 0.4943 - mae: 0.5559 - val_loss: 0.5759 - val_mse: 0.5759 - val_mae: 0.5637\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4963 - mse: 0.4963 - mae: 0.5547 - val_loss: 0.5812 - val_mse: 0.5812 - val_mae: 0.5716\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4951 - mse: 0.4951 - mae: 0.5574 - val_loss: 0.5731 - val_mse: 0.5731 - val_mae: 0.5704\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4877 - mse: 0.4877 - mae: 0.5526 - val_loss: 0.5720 - val_mse: 0.5720 - val_mae: 0.5606\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4824 - mse: 0.4824 - mae: 0.5467 - val_loss: 0.5572 - val_mse: 0.5572 - val_mae: 0.5580\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4691 - mse: 0.4691 - mae: 0.5389 - val_loss: 0.5485 - val_mse: 0.5485 - val_mae: 0.5546\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4645 - mse: 0.4645 - mae: 0.5314 - val_loss: 0.5497 - val_mse: 0.5497 - val_mae: 0.5555\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4393 - mse: 0.4393 - mae: 0.5172 - val_loss: 0.5394 - val_mse: 0.5394 - val_mae: 0.5479\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4723 - mse: 0.4723 - mae: 0.5349 - val_loss: 0.5440 - val_mse: 0.5440 - val_mae: 0.5522\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4711 - mse: 0.4711 - mae: 0.5429 - val_loss: 0.5291 - val_mse: 0.5291 - val_mae: 0.5434\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4398 - mse: 0.4398 - mae: 0.5172 - val_loss: 0.5310 - val_mse: 0.5310 - val_mae: 0.5500\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4570 - mse: 0.4570 - mae: 0.5250 - val_loss: 0.5263 - val_mse: 0.5263 - val_mae: 0.5449\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4509 - mse: 0.4509 - mae: 0.5200 - val_loss: 0.5170 - val_mse: 0.5170 - val_mae: 0.5409\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4470 - mse: 0.4470 - mae: 0.5235 - val_loss: 0.5188 - val_mse: 0.5188 - val_mae: 0.5431\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4483 - mse: 0.4483 - mae: 0.5285 - val_loss: 0.5217 - val_mse: 0.5217 - val_mae: 0.5428\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4360 - mse: 0.4360 - mae: 0.5185 - val_loss: 0.5077 - val_mse: 0.5077 - val_mae: 0.5393\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4529 - mse: 0.4529 - mae: 0.5237 - val_loss: 0.5147 - val_mse: 0.5147 - val_mae: 0.5387\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4484 - mse: 0.4484 - mae: 0.5208 - val_loss: 0.5007 - val_mse: 0.5007 - val_mae: 0.5375\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4592 - mse: 0.4592 - mae: 0.5353 - val_loss: 0.4969 - val_mse: 0.4969 - val_mae: 0.5333\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4276 - mse: 0.4276 - mae: 0.5039 - val_loss: 0.4948 - val_mse: 0.4948 - val_mae: 0.5331\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4382 - mse: 0.4382 - mae: 0.5129 - val_loss: 0.4936 - val_mse: 0.4936 - val_mae: 0.5263\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4504 - mse: 0.4504 - mae: 0.5266 - val_loss: 0.4919 - val_mse: 0.4919 - val_mae: 0.5298\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4321 - mse: 0.4321 - mae: 0.5141 - val_loss: 0.4820 - val_mse: 0.4820 - val_mae: 0.5258\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4266 - mse: 0.4266 - mae: 0.5045 - val_loss: 0.4811 - val_mse: 0.4811 - val_mae: 0.5271\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4389 - mse: 0.4389 - mae: 0.5175 - val_loss: 0.4749 - val_mse: 0.4749 - val_mae: 0.5216\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4087 - mse: 0.4087 - mae: 0.5013 - val_loss: 0.4850 - val_mse: 0.4850 - val_mae: 0.5246\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4261 - mse: 0.4261 - mae: 0.5156 - val_loss: 0.4700 - val_mse: 0.4700 - val_mae: 0.5149\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4051 - mse: 0.4051 - mae: 0.4964 - val_loss: 0.4720 - val_mse: 0.4720 - val_mae: 0.5186\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4372 - mse: 0.4372 - mae: 0.5164 - val_loss: 0.4704 - val_mse: 0.4704 - val_mae: 0.5202\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4142 - mse: 0.4142 - mae: 0.5094 - val_loss: 0.4665 - val_mse: 0.4665 - val_mae: 0.5177\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4100 - mse: 0.4100 - mae: 0.4946 - val_loss: 0.4660 - val_mse: 0.4660 - val_mae: 0.5131\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4166 - mse: 0.4166 - mae: 0.5021 - val_loss: 0.4658 - val_mse: 0.4658 - val_mae: 0.5152\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3915 - mse: 0.3915 - mae: 0.4877 - val_loss: 0.4615 - val_mse: 0.4615 - val_mae: 0.5118\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4148 - mse: 0.4148 - mae: 0.4970 - val_loss: 0.4712 - val_mse: 0.4712 - val_mae: 0.5176\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3998 - mse: 0.3998 - mae: 0.4941 - val_loss: 0.4511 - val_mse: 0.4511 - val_mae: 0.5095\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4181 - mse: 0.4181 - mae: 0.5015 - val_loss: 0.4605 - val_mse: 0.4605 - val_mae: 0.5146\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3990 - mse: 0.3990 - mae: 0.5015 - val_loss: 0.4578 - val_mse: 0.4578 - val_mae: 0.5154\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3994 - mse: 0.3994 - mae: 0.5006 - val_loss: 0.4497 - val_mse: 0.4497 - val_mae: 0.5071\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4046 - mse: 0.4046 - mae: 0.4955 - val_loss: 0.4466 - val_mse: 0.4466 - val_mae: 0.5094\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3932 - mse: 0.3932 - mae: 0.4882 - val_loss: 0.4642 - val_mse: 0.4642 - val_mae: 0.5115\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4026 - mse: 0.4026 - mae: 0.4942 - val_loss: 0.4467 - val_mse: 0.4467 - val_mae: 0.5086\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4173 - mse: 0.4173 - mae: 0.5030 - val_loss: 0.4436 - val_mse: 0.4436 - val_mae: 0.5074\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4132 - mse: 0.4132 - mae: 0.5057 - val_loss: 0.4544 - val_mse: 0.4544 - val_mae: 0.5121\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4062 - mse: 0.4062 - mae: 0.4930 - val_loss: 0.4412 - val_mse: 0.4412 - val_mae: 0.5055\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3867 - mse: 0.3867 - mae: 0.4861 - val_loss: 0.4441 - val_mse: 0.4441 - val_mae: 0.5050\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4100 - mse: 0.4100 - mae: 0.5020 - val_loss: 0.4435 - val_mse: 0.4435 - val_mae: 0.5079\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3965 - mse: 0.3965 - mae: 0.4884 - val_loss: 0.4290 - val_mse: 0.4290 - val_mae: 0.5025\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3935 - mse: 0.3935 - mae: 0.4971 - val_loss: 0.4421 - val_mse: 0.4421 - val_mae: 0.5068\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3922 - mse: 0.3922 - mae: 0.4859 - val_loss: 0.4295 - val_mse: 0.4295 - val_mae: 0.4984\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4145 - mse: 0.4145 - mae: 0.4969 - val_loss: 0.4316 - val_mse: 0.4316 - val_mae: 0.4990\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3804 - mse: 0.3804 - mae: 0.4857 - val_loss: 0.4309 - val_mse: 0.4309 - val_mae: 0.5030\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3951 - mse: 0.3951 - mae: 0.4853 - val_loss: 0.4296 - val_mse: 0.4296 - val_mae: 0.5001\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4096 - mse: 0.4096 - mae: 0.5040 - val_loss: 0.4197 - val_mse: 0.4197 - val_mae: 0.4986\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3854 - mse: 0.3854 - mae: 0.4834 - val_loss: 0.4280 - val_mse: 0.4280 - val_mae: 0.5008\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4151 - mse: 0.4151 - mae: 0.5046 - val_loss: 0.4211 - val_mse: 0.4211 - val_mae: 0.4954\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3863 - mse: 0.3863 - mae: 0.4837 - val_loss: 0.4181 - val_mse: 0.4181 - val_mae: 0.4925\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3952 - mse: 0.3952 - mae: 0.4832 - val_loss: 0.4125 - val_mse: 0.4125 - val_mae: 0.4912\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4018 - mse: 0.4018 - mae: 0.4953 - val_loss: 0.4207 - val_mse: 0.4207 - val_mae: 0.4995\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3974 - mse: 0.3974 - mae: 0.4934 - val_loss: 0.4131 - val_mse: 0.4131 - val_mae: 0.4962\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3889 - mse: 0.3889 - mae: 0.4906 - val_loss: 0.4069 - val_mse: 0.4069 - val_mae: 0.4913\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3849 - mse: 0.3849 - mae: 0.4802 - val_loss: 0.4149 - val_mse: 0.4149 - val_mae: 0.4944\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3840 - mse: 0.3840 - mae: 0.4826 - val_loss: 0.4036 - val_mse: 0.4036 - val_mae: 0.4918\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3852 - mse: 0.3852 - mae: 0.4862 - val_loss: 0.4101 - val_mse: 0.4101 - val_mae: 0.4907\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3836 - mse: 0.3836 - mae: 0.4857 - val_loss: 0.3975 - val_mse: 0.3975 - val_mae: 0.4865\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3800 - mse: 0.3800 - mae: 0.4769 - val_loss: 0.4099 - val_mse: 0.4099 - val_mae: 0.4905\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3753 - mse: 0.3753 - mae: 0.4796 - val_loss: 0.4043 - val_mse: 0.4043 - val_mae: 0.4907\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3700 - mse: 0.3700 - mae: 0.4742 - val_loss: 0.4016 - val_mse: 0.4016 - val_mae: 0.4876\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3882 - mse: 0.3882 - mae: 0.4806 - val_loss: 0.4094 - val_mse: 0.4094 - val_mae: 0.4914\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3645 - mse: 0.3645 - mae: 0.4693 - val_loss: 0.4085 - val_mse: 0.4085 - val_mae: 0.4909\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3902 - mse: 0.3902 - mae: 0.4915 - val_loss: 0.3977 - val_mse: 0.3977 - val_mae: 0.4894\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3792 - mse: 0.3792 - mae: 0.4759 - val_loss: 0.3989 - val_mse: 0.3989 - val_mae: 0.4857\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3551 - mse: 0.3551 - mae: 0.4631 - val_loss: 0.3962 - val_mse: 0.3962 - val_mae: 0.4834\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3884 - mse: 0.3884 - mae: 0.4818 - val_loss: 0.3982 - val_mse: 0.3982 - val_mae: 0.4892\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3750 - mse: 0.3750 - mae: 0.4761 - val_loss: 0.3973 - val_mse: 0.3973 - val_mae: 0.4900\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3839 - mse: 0.3839 - mae: 0.4831 - val_loss: 0.3966 - val_mse: 0.3966 - val_mae: 0.4877\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3824 - mse: 0.3824 - mae: 0.4823 - val_loss: 0.4025 - val_mse: 0.4025 - val_mae: 0.4892\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3740 - mse: 0.3740 - mae: 0.4771 - val_loss: 0.3956 - val_mse: 0.3956 - val_mae: 0.4877\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3790 - mse: 0.3790 - mae: 0.4861 - val_loss: 0.3974 - val_mse: 0.3974 - val_mae: 0.4936\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3811 - mse: 0.3811 - mae: 0.4779 - val_loss: 0.3970 - val_mse: 0.3970 - val_mae: 0.4875\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3639 - mse: 0.3639 - mae: 0.4672 - val_loss: 0.3896 - val_mse: 0.3896 - val_mae: 0.4871\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3743 - mse: 0.3743 - mae: 0.4750 - val_loss: 0.3878 - val_mse: 0.3878 - val_mae: 0.4850\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3754 - mse: 0.3754 - mae: 0.4754 - val_loss: 0.4085 - val_mse: 0.4085 - val_mae: 0.4931\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3693 - mse: 0.3693 - mae: 0.4751 - val_loss: 0.3934 - val_mse: 0.3934 - val_mae: 0.4824\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4056 - mse: 0.4056 - mae: 0.4979 - val_loss: 0.3886 - val_mse: 0.3886 - val_mae: 0.4830\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3683 - mse: 0.3683 - mae: 0.4640 - val_loss: 0.3921 - val_mse: 0.3921 - val_mae: 0.4867\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3727 - mse: 0.3727 - mae: 0.4770 - val_loss: 0.3858 - val_mse: 0.3858 - val_mae: 0.4845\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3759 - mse: 0.3759 - mae: 0.4755 - val_loss: 0.3896 - val_mse: 0.3896 - val_mae: 0.4862\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3740 - mse: 0.3740 - mae: 0.4729 - val_loss: 0.3853 - val_mse: 0.3853 - val_mae: 0.4848\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3819 - mse: 0.3819 - mae: 0.4842 - val_loss: 0.3933 - val_mse: 0.3933 - val_mae: 0.4868\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3761 - mse: 0.3761 - mae: 0.4762 - val_loss: 0.3884 - val_mse: 0.3884 - val_mae: 0.4850\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3843 - mse: 0.3843 - mae: 0.4842 - val_loss: 0.3803 - val_mse: 0.3803 - val_mae: 0.4832\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3824 - mse: 0.3824 - mae: 0.4856 - val_loss: 0.3847 - val_mse: 0.3847 - val_mae: 0.4857\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3828 - mse: 0.3828 - mae: 0.4822 - val_loss: 0.3820 - val_mse: 0.3820 - val_mae: 0.4858\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3749 - mse: 0.3749 - mae: 0.4738 - val_loss: 0.3979 - val_mse: 0.3979 - val_mae: 0.4880\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3680 - mse: 0.3680 - mae: 0.4793 - val_loss: 0.3821 - val_mse: 0.3821 - val_mae: 0.4814\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3576 - mse: 0.3576 - mae: 0.4647 - val_loss: 0.3789 - val_mse: 0.3789 - val_mae: 0.4759\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3596 - mse: 0.3596 - mae: 0.4666 - val_loss: 0.3826 - val_mse: 0.3826 - val_mae: 0.4787\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3719 - mse: 0.3719 - mae: 0.4701 - val_loss: 0.3823 - val_mse: 0.3823 - val_mae: 0.4799\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3622 - mse: 0.3622 - mae: 0.4645 - val_loss: 0.3763 - val_mse: 0.3763 - val_mae: 0.4773\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3632 - mse: 0.3632 - mae: 0.4704 - val_loss: 0.3785 - val_mse: 0.3785 - val_mae: 0.4776\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3745 - mse: 0.3745 - mae: 0.4819 - val_loss: 0.3823 - val_mse: 0.3823 - val_mae: 0.4805\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3607 - mse: 0.3607 - mae: 0.4694 - val_loss: 0.3864 - val_mse: 0.3864 - val_mae: 0.4802\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3705 - mse: 0.3705 - mae: 0.4693 - val_loss: 0.3796 - val_mse: 0.3796 - val_mae: 0.4783\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3670 - mse: 0.3670 - mae: 0.4738 - val_loss: 0.3783 - val_mse: 0.3783 - val_mae: 0.4782\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3551 - mse: 0.3551 - mae: 0.4679 - val_loss: 0.3794 - val_mse: 0.3794 - val_mae: 0.4778\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3524 - mse: 0.3524 - mae: 0.4579 - val_loss: 0.3744 - val_mse: 0.3744 - val_mae: 0.4731\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3618 - mse: 0.3618 - mae: 0.4696 - val_loss: 0.3719 - val_mse: 0.3719 - val_mae: 0.4757\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3861 - mse: 0.3861 - mae: 0.4843 - val_loss: 0.3857 - val_mse: 0.3857 - val_mae: 0.4833\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3773 - mse: 0.3773 - mae: 0.4788 - val_loss: 0.3729 - val_mse: 0.3729 - val_mae: 0.4774\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3545 - mse: 0.3545 - mae: 0.4637 - val_loss: 0.3747 - val_mse: 0.3747 - val_mae: 0.4764\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3483 - mse: 0.3483 - mae: 0.4567 - val_loss: 0.3734 - val_mse: 0.3734 - val_mae: 0.4756\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3563 - mse: 0.3563 - mae: 0.4650 - val_loss: 0.3722 - val_mse: 0.3722 - val_mae: 0.4755\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3716 - mse: 0.3716 - mae: 0.4789 - val_loss: 0.3714 - val_mse: 0.3714 - val_mae: 0.4746\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3624 - mse: 0.3624 - mae: 0.4703 - val_loss: 0.3723 - val_mse: 0.3723 - val_mae: 0.4723\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3769 - mse: 0.3769 - mae: 0.4714 - val_loss: 0.3784 - val_mse: 0.3784 - val_mae: 0.4797\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3578 - mse: 0.3578 - mae: 0.4607 - val_loss: 0.3744 - val_mse: 0.3744 - val_mae: 0.4746\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3606 - mse: 0.3606 - mae: 0.4658 - val_loss: 0.3883 - val_mse: 0.3883 - val_mae: 0.4825\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3726 - mse: 0.3726 - mae: 0.4800 - val_loss: 0.3722 - val_mse: 0.3722 - val_mae: 0.4759\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3496 - mse: 0.3496 - mae: 0.4561 - val_loss: 0.3785 - val_mse: 0.3785 - val_mae: 0.4769\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3551 - mse: 0.3551 - mae: 0.4659 - val_loss: 0.3742 - val_mse: 0.3742 - val_mae: 0.4755\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3637 - mse: 0.3637 - mae: 0.4684 - val_loss: 0.3743 - val_mse: 0.3743 - val_mae: 0.4740\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3628 - mse: 0.3628 - mae: 0.4745 - val_loss: 0.3723 - val_mse: 0.3723 - val_mae: 0.4713\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3629 - mse: 0.3629 - mae: 0.4682 - val_loss: 0.3721 - val_mse: 0.3721 - val_mae: 0.4775\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3731 - mse: 0.3731 - mae: 0.4751 - val_loss: 0.3748 - val_mse: 0.3748 - val_mae: 0.4729\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3558 - mse: 0.3558 - mae: 0.4633 - val_loss: 0.3674 - val_mse: 0.3674 - val_mae: 0.4758\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3679 - mse: 0.3679 - mae: 0.4790 - val_loss: 0.3710 - val_mse: 0.3710 - val_mae: 0.4710\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3512 - mse: 0.3512 - mae: 0.4641 - val_loss: 0.3813 - val_mse: 0.3813 - val_mae: 0.4771\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3599 - mse: 0.3599 - mae: 0.4693 - val_loss: 0.3706 - val_mse: 0.3706 - val_mae: 0.4700\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3726 - mse: 0.3726 - mae: 0.4696 - val_loss: 0.3731 - val_mse: 0.3731 - val_mae: 0.4733\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3598 - mse: 0.3598 - mae: 0.4627 - val_loss: 0.3701 - val_mse: 0.3701 - val_mae: 0.4728\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3630 - mse: 0.3630 - mae: 0.4672 - val_loss: 0.3754 - val_mse: 0.3754 - val_mae: 0.4724\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3582 - mse: 0.3582 - mae: 0.4657 - val_loss: 0.3682 - val_mse: 0.3682 - val_mae: 0.4680\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3484 - mse: 0.3484 - mae: 0.4654 - val_loss: 0.3728 - val_mse: 0.3728 - val_mae: 0.4695\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3666 - mse: 0.3666 - mae: 0.4707 - val_loss: 0.3679 - val_mse: 0.3679 - val_mae: 0.4717\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3500 - mse: 0.3500 - mae: 0.4600 - val_loss: 0.3731 - val_mse: 0.3731 - val_mae: 0.4712\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3815 - mse: 0.3815 - mae: 0.4851 - val_loss: 0.3663 - val_mse: 0.3663 - val_mae: 0.4678\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3617 - mse: 0.3617 - mae: 0.4709 - val_loss: 0.3609 - val_mse: 0.3609 - val_mae: 0.4674\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3571 - mse: 0.3571 - mae: 0.4657 - val_loss: 0.3795 - val_mse: 0.3795 - val_mae: 0.4755\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3614 - mse: 0.3614 - mae: 0.4717 - val_loss: 0.3670 - val_mse: 0.3670 - val_mae: 0.4700\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3473 - mse: 0.3473 - mae: 0.4592 - val_loss: 0.3661 - val_mse: 0.3661 - val_mae: 0.4663\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3690 - mse: 0.3690 - mae: 0.4671 - val_loss: 0.3713 - val_mse: 0.3713 - val_mae: 0.4697\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3584 - mse: 0.3584 - mae: 0.4655 - val_loss: 0.3641 - val_mse: 0.3641 - val_mae: 0.4712\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3584 - mse: 0.3584 - mae: 0.4631 - val_loss: 0.3870 - val_mse: 0.3870 - val_mae: 0.4793\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3680 - mse: 0.3680 - mae: 0.4745 - val_loss: 0.3684 - val_mse: 0.3684 - val_mae: 0.4696\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3571 - mse: 0.3571 - mae: 0.4637 - val_loss: 0.3638 - val_mse: 0.3638 - val_mae: 0.4676\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3478 - mse: 0.3478 - mae: 0.4586 - val_loss: 0.3610 - val_mse: 0.3610 - val_mae: 0.4661\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3533 - mse: 0.3533 - mae: 0.4611 - val_loss: 0.3689 - val_mse: 0.3689 - val_mae: 0.4752\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3498 - mse: 0.3498 - mae: 0.4627 - val_loss: 0.3604 - val_mse: 0.3604 - val_mae: 0.4679\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3671 - mse: 0.3671 - mae: 0.4745 - val_loss: 0.3624 - val_mse: 0.3624 - val_mae: 0.4673\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3496 - mse: 0.3496 - mae: 0.4578 - val_loss: 0.3673 - val_mse: 0.3673 - val_mae: 0.4680\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3493 - mse: 0.3493 - mae: 0.4584 - val_loss: 0.3701 - val_mse: 0.3701 - val_mae: 0.4742\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3486 - mse: 0.3486 - mae: 0.4606 - val_loss: 0.3714 - val_mse: 0.3714 - val_mae: 0.4707\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3552 - mse: 0.3552 - mae: 0.4642 - val_loss: 0.3569 - val_mse: 0.3569 - val_mae: 0.4648\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3484 - mse: 0.3484 - mae: 0.4595 - val_loss: 0.3628 - val_mse: 0.3628 - val_mae: 0.4667\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3575 - mse: 0.3575 - mae: 0.4595 - val_loss: 0.3719 - val_mse: 0.3719 - val_mae: 0.4722\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3595 - mse: 0.3595 - mae: 0.4725 - val_loss: 0.3604 - val_mse: 0.3604 - val_mae: 0.4648\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3547 - mse: 0.3547 - mae: 0.4582 - val_loss: 0.3732 - val_mse: 0.3732 - val_mae: 0.4728\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3471 - mse: 0.3471 - mae: 0.4622 - val_loss: 0.3588 - val_mse: 0.3588 - val_mae: 0.4637\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3327 - mse: 0.3327 - mae: 0.4450 - val_loss: 0.3667 - val_mse: 0.3667 - val_mae: 0.4661\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3586 - mse: 0.3586 - mae: 0.4714 - val_loss: 0.3637 - val_mse: 0.3637 - val_mae: 0.4717\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3469 - mse: 0.3469 - mae: 0.4557 - val_loss: 0.3704 - val_mse: 0.3704 - val_mae: 0.4702\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3435 - mse: 0.3435 - mae: 0.4570 - val_loss: 0.3557 - val_mse: 0.3557 - val_mae: 0.4639\n",
            "Epoch 252/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3611 - mse: 0.3611 - mae: 0.4656 - val_loss: 0.3569 - val_mse: 0.3569 - val_mae: 0.4612\n",
            "Epoch 253/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3492 - mse: 0.3492 - mae: 0.4594 - val_loss: 0.3529 - val_mse: 0.3529 - val_mae: 0.4653\n",
            "Epoch 254/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3536 - mse: 0.3536 - mae: 0.4622 - val_loss: 0.3546 - val_mse: 0.3546 - val_mae: 0.4643\n",
            "Epoch 255/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3518 - mse: 0.3518 - mae: 0.4644 - val_loss: 0.3703 - val_mse: 0.3703 - val_mae: 0.4698\n",
            "Epoch 256/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3510 - mse: 0.3510 - mae: 0.4674 - val_loss: 0.3612 - val_mse: 0.3612 - val_mae: 0.4633\n",
            "Epoch 257/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3432 - mse: 0.3432 - mae: 0.4584 - val_loss: 0.3627 - val_mse: 0.3627 - val_mae: 0.4657\n",
            "Epoch 258/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3480 - mse: 0.3480 - mae: 0.4611 - val_loss: 0.3584 - val_mse: 0.3584 - val_mae: 0.4625\n",
            "Epoch 259/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3596 - mse: 0.3596 - mae: 0.4628 - val_loss: 0.3655 - val_mse: 0.3655 - val_mae: 0.4676\n",
            "Epoch 260/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3619 - mse: 0.3619 - mae: 0.4685 - val_loss: 0.3538 - val_mse: 0.3538 - val_mae: 0.4643\n",
            "Epoch 261/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3418 - mse: 0.3418 - mae: 0.4521 - val_loss: 0.3665 - val_mse: 0.3665 - val_mae: 0.4669\n",
            "Epoch 262/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3382 - mse: 0.3382 - mae: 0.4559 - val_loss: 0.3627 - val_mse: 0.3627 - val_mae: 0.4650\n",
            "Epoch 263/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3365 - mse: 0.3365 - mae: 0.4520 - val_loss: 0.3645 - val_mse: 0.3645 - val_mae: 0.4669\n",
            "Epoch 264/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3425 - mse: 0.3425 - mae: 0.4554 - val_loss: 0.3614 - val_mse: 0.3614 - val_mae: 0.4666\n",
            "Epoch 265/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3525 - mse: 0.3525 - mae: 0.4621 - val_loss: 0.3607 - val_mse: 0.3607 - val_mae: 0.4630\n",
            "Epoch 266/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3533 - mse: 0.3533 - mae: 0.4592 - val_loss: 0.3574 - val_mse: 0.3574 - val_mae: 0.4613\n",
            "Epoch 267/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3574 - mse: 0.3574 - mae: 0.4642 - val_loss: 0.3580 - val_mse: 0.3580 - val_mae: 0.4639\n",
            "Epoch 268/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3558 - mse: 0.3558 - mae: 0.4678 - val_loss: 0.3614 - val_mse: 0.3614 - val_mae: 0.4668\n",
            "Epoch 269/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3401 - mse: 0.3401 - mae: 0.4510 - val_loss: 0.3665 - val_mse: 0.3665 - val_mae: 0.4647\n",
            "Epoch 270/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3543 - mse: 0.3543 - mae: 0.4608 - val_loss: 0.3555 - val_mse: 0.3555 - val_mae: 0.4595\n",
            "Epoch 271/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3505 - mse: 0.3505 - mae: 0.4641 - val_loss: 0.3539 - val_mse: 0.3539 - val_mae: 0.4623\n",
            "Epoch 272/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3485 - mse: 0.3485 - mae: 0.4637 - val_loss: 0.3645 - val_mse: 0.3645 - val_mae: 0.4653\n",
            "Epoch 273/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3475 - mse: 0.3475 - mae: 0.4570 - val_loss: 0.3553 - val_mse: 0.3553 - val_mae: 0.4604\n",
            "Epoch 274/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3462 - mse: 0.3462 - mae: 0.4567 - val_loss: 0.3679 - val_mse: 0.3679 - val_mae: 0.4654\n",
            "Epoch 275/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3462 - mse: 0.3462 - mae: 0.4562 - val_loss: 0.3659 - val_mse: 0.3659 - val_mae: 0.4669\n",
            "Epoch 276/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3622 - mse: 0.3622 - mae: 0.4675 - val_loss: 0.3581 - val_mse: 0.3581 - val_mae: 0.4682\n",
            "Epoch 277/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3594 - mse: 0.3594 - mae: 0.4695 - val_loss: 0.3754 - val_mse: 0.3754 - val_mae: 0.4729\n",
            "Epoch 278/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3441 - mse: 0.3441 - mae: 0.4560 - val_loss: 0.3596 - val_mse: 0.3596 - val_mae: 0.4633\n",
            "Epoch 279/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3444 - mse: 0.3444 - mae: 0.4556 - val_loss: 0.3598 - val_mse: 0.3598 - val_mae: 0.4644\n",
            "Epoch 280/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3446 - mse: 0.3446 - mae: 0.4545 - val_loss: 0.3645 - val_mse: 0.3645 - val_mae: 0.4673\n",
            "Epoch 281/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3580 - mse: 0.3580 - mae: 0.4690 - val_loss: 0.3661 - val_mse: 0.3661 - val_mae: 0.4663\n",
            "Epoch 282/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3493 - mse: 0.3493 - mae: 0.4647 - val_loss: 0.3634 - val_mse: 0.3634 - val_mae: 0.4660\n",
            "Epoch 283/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3542 - mse: 0.3542 - mae: 0.4596 - val_loss: 0.3633 - val_mse: 0.3633 - val_mae: 0.4673\n",
            "Epoch 284/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3559 - mse: 0.3559 - mae: 0.4674 - val_loss: 0.3595 - val_mse: 0.3595 - val_mae: 0.4671\n",
            "Epoch 285/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3579 - mse: 0.3579 - mae: 0.4714 - val_loss: 0.3598 - val_mse: 0.3598 - val_mae: 0.4660\n",
            "Epoch 286/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3545 - mse: 0.3545 - mae: 0.4621 - val_loss: 0.3649 - val_mse: 0.3649 - val_mae: 0.4653\n",
            "Epoch 287/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3444 - mse: 0.3444 - mae: 0.4561 - val_loss: 0.3577 - val_mse: 0.3577 - val_mae: 0.4618\n",
            "Epoch 288/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3447 - mse: 0.3447 - mae: 0.4580 - val_loss: 0.3548 - val_mse: 0.3548 - val_mae: 0.4642\n",
            "Epoch 289/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3356 - mse: 0.3356 - mae: 0.4506 - val_loss: 0.3489 - val_mse: 0.3489 - val_mae: 0.4567\n",
            "Epoch 290/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3369 - mse: 0.3369 - mae: 0.4548 - val_loss: 0.3618 - val_mse: 0.3618 - val_mae: 0.4634\n",
            "Epoch 291/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3353 - mse: 0.3353 - mae: 0.4459 - val_loss: 0.3580 - val_mse: 0.3580 - val_mae: 0.4593\n",
            "Epoch 292/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3357 - mse: 0.3357 - mae: 0.4501 - val_loss: 0.3500 - val_mse: 0.3500 - val_mae: 0.4584\n",
            "Epoch 293/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3458 - mse: 0.3458 - mae: 0.4564 - val_loss: 0.3593 - val_mse: 0.3593 - val_mae: 0.4589\n",
            "Epoch 294/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3417 - mse: 0.3417 - mae: 0.4571 - val_loss: 0.3603 - val_mse: 0.3603 - val_mae: 0.4613\n",
            "Epoch 295/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3408 - mse: 0.3408 - mae: 0.4521 - val_loss: 0.3537 - val_mse: 0.3537 - val_mae: 0.4607\n",
            "Epoch 296/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3599 - mse: 0.3599 - mae: 0.4601 - val_loss: 0.3532 - val_mse: 0.3532 - val_mae: 0.4611\n",
            "Epoch 297/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3458 - mse: 0.3458 - mae: 0.4567 - val_loss: 0.3556 - val_mse: 0.3556 - val_mae: 0.4602\n",
            "Epoch 298/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3386 - mse: 0.3386 - mae: 0.4454 - val_loss: 0.3663 - val_mse: 0.3663 - val_mae: 0.4639\n",
            "Epoch 299/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3530 - mse: 0.3530 - mae: 0.4598 - val_loss: 0.3678 - val_mse: 0.3678 - val_mae: 0.4655\n",
            "Epoch 300/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3609 - mse: 0.3609 - mae: 0.4650 - val_loss: 0.3640 - val_mse: 0.3640 - val_mae: 0.4633\n",
            "Epoch 301/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3513 - mse: 0.3513 - mae: 0.4699 - val_loss: 0.3613 - val_mse: 0.3613 - val_mae: 0.4646\n",
            "Epoch 302/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3432 - mse: 0.3432 - mae: 0.4577 - val_loss: 0.3552 - val_mse: 0.3552 - val_mae: 0.4597\n",
            "Epoch 303/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3649 - mse: 0.3649 - mae: 0.4704 - val_loss: 0.3529 - val_mse: 0.3529 - val_mae: 0.4613\n",
            "Epoch 304/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3429 - mse: 0.3429 - mae: 0.4493 - val_loss: 0.3643 - val_mse: 0.3643 - val_mae: 0.4652\n",
            "Epoch 305/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3406 - mse: 0.3406 - mae: 0.4476 - val_loss: 0.3554 - val_mse: 0.3554 - val_mae: 0.4612\n",
            "Epoch 306/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3495 - mse: 0.3495 - mae: 0.4631 - val_loss: 0.3530 - val_mse: 0.3530 - val_mae: 0.4584\n",
            "Epoch 307/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3140 - mse: 0.3140 - mae: 0.4346 - val_loss: 0.3598 - val_mse: 0.3598 - val_mae: 0.4615\n",
            "Epoch 308/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3420 - mse: 0.3420 - mae: 0.4551 - val_loss: 0.3493 - val_mse: 0.3493 - val_mae: 0.4561\n",
            "Epoch 309/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3337 - mse: 0.3337 - mae: 0.4476 - val_loss: 0.3500 - val_mse: 0.3500 - val_mae: 0.4558\n",
            "Epoch 310/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3491 - mse: 0.3491 - mae: 0.4589 - val_loss: 0.3488 - val_mse: 0.3488 - val_mae: 0.4615\n",
            "Epoch 311/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3350 - mse: 0.3350 - mae: 0.4498 - val_loss: 0.3592 - val_mse: 0.3592 - val_mae: 0.4629\n",
            "Epoch 312/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3416 - mse: 0.3416 - mae: 0.4602 - val_loss: 0.3482 - val_mse: 0.3482 - val_mae: 0.4569\n",
            "Epoch 313/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3321 - mse: 0.3321 - mae: 0.4509 - val_loss: 0.3558 - val_mse: 0.3558 - val_mae: 0.4588\n",
            "Epoch 314/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3392 - mse: 0.3392 - mae: 0.4514 - val_loss: 0.3550 - val_mse: 0.3550 - val_mae: 0.4598\n",
            "Epoch 315/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3417 - mse: 0.3417 - mae: 0.4518 - val_loss: 0.3589 - val_mse: 0.3589 - val_mae: 0.4591\n",
            "Epoch 316/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3547 - mse: 0.3547 - mae: 0.4636 - val_loss: 0.3632 - val_mse: 0.3632 - val_mae: 0.4636\n",
            "Epoch 317/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3511 - mse: 0.3511 - mae: 0.4564 - val_loss: 0.3618 - val_mse: 0.3618 - val_mae: 0.4610\n",
            "Epoch 318/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3362 - mse: 0.3362 - mae: 0.4502 - val_loss: 0.3571 - val_mse: 0.3571 - val_mae: 0.4597\n",
            "Epoch 319/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3538 - mse: 0.3538 - mae: 0.4640 - val_loss: 0.3529 - val_mse: 0.3529 - val_mae: 0.4590\n",
            "Epoch 320/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3561 - mse: 0.3561 - mae: 0.4628 - val_loss: 0.3539 - val_mse: 0.3539 - val_mae: 0.4600\n",
            "Epoch 321/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3456 - mse: 0.3456 - mae: 0.4554 - val_loss: 0.3484 - val_mse: 0.3484 - val_mae: 0.4596\n",
            "Epoch 322/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3308 - mse: 0.3308 - mae: 0.4415 - val_loss: 0.3534 - val_mse: 0.3534 - val_mae: 0.4601\n",
            "Epoch 323/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3362 - mse: 0.3362 - mae: 0.4507 - val_loss: 0.3542 - val_mse: 0.3542 - val_mae: 0.4595\n",
            "Epoch 324/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3332 - mse: 0.3332 - mae: 0.4479 - val_loss: 0.3613 - val_mse: 0.3613 - val_mae: 0.4616\n",
            "Epoch 325/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3442 - mse: 0.3442 - mae: 0.4549 - val_loss: 0.3502 - val_mse: 0.3502 - val_mae: 0.4574\n",
            "Epoch 326/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3472 - mse: 0.3472 - mae: 0.4595 - val_loss: 0.3538 - val_mse: 0.3538 - val_mae: 0.4615\n",
            "Epoch 327/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3494 - mse: 0.3494 - mae: 0.4581 - val_loss: 0.3657 - val_mse: 0.3657 - val_mae: 0.4638\n",
            "Epoch 328/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3450 - mse: 0.3450 - mae: 0.4614 - val_loss: 0.3513 - val_mse: 0.3513 - val_mae: 0.4570\n",
            "Epoch 329/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3283 - mse: 0.3283 - mae: 0.4412 - val_loss: 0.3445 - val_mse: 0.3445 - val_mae: 0.4592\n",
            "Epoch 330/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3409 - mse: 0.3409 - mae: 0.4543 - val_loss: 0.3522 - val_mse: 0.3522 - val_mae: 0.4607\n",
            "Epoch 331/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3378 - mse: 0.3378 - mae: 0.4503 - val_loss: 0.3461 - val_mse: 0.3461 - val_mae: 0.4579\n",
            "Epoch 332/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3532 - mse: 0.3532 - mae: 0.4594 - val_loss: 0.3459 - val_mse: 0.3459 - val_mae: 0.4598\n",
            "Epoch 333/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3534 - mse: 0.3534 - mae: 0.4605 - val_loss: 0.3560 - val_mse: 0.3560 - val_mae: 0.4608\n",
            "Epoch 334/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3384 - mse: 0.3384 - mae: 0.4541 - val_loss: 0.3559 - val_mse: 0.3559 - val_mae: 0.4605\n",
            "Epoch 335/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3312 - mse: 0.3312 - mae: 0.4516 - val_loss: 0.3493 - val_mse: 0.3493 - val_mae: 0.4578\n",
            "Epoch 336/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3293 - mse: 0.3293 - mae: 0.4425 - val_loss: 0.3612 - val_mse: 0.3612 - val_mae: 0.4633\n",
            "Epoch 337/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3440 - mse: 0.3440 - mae: 0.4600 - val_loss: 0.3486 - val_mse: 0.3486 - val_mae: 0.4577\n",
            "Epoch 338/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3421 - mse: 0.3421 - mae: 0.4544 - val_loss: 0.3473 - val_mse: 0.3473 - val_mae: 0.4606\n",
            "Epoch 339/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3497 - mse: 0.3497 - mae: 0.4567 - val_loss: 0.3592 - val_mse: 0.3592 - val_mae: 0.4658\n",
            "Epoch 340/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3300 - mse: 0.3300 - mae: 0.4493 - val_loss: 0.3544 - val_mse: 0.3544 - val_mae: 0.4586\n",
            "Epoch 341/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3371 - mse: 0.3371 - mae: 0.4507 - val_loss: 0.3574 - val_mse: 0.3574 - val_mae: 0.4566\n",
            "Epoch 342/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3423 - mse: 0.3423 - mae: 0.4599 - val_loss: 0.3696 - val_mse: 0.3696 - val_mae: 0.4635\n",
            "Epoch 343/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3483 - mse: 0.3483 - mae: 0.4588 - val_loss: 0.3545 - val_mse: 0.3545 - val_mae: 0.4566\n",
            "Epoch 344/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3333 - mse: 0.3333 - mae: 0.4525 - val_loss: 0.3492 - val_mse: 0.3492 - val_mae: 0.4584\n",
            "Epoch 345/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3298 - mse: 0.3298 - mae: 0.4451 - val_loss: 0.3480 - val_mse: 0.3480 - val_mae: 0.4535\n",
            "Epoch 346/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3448 - mse: 0.3448 - mae: 0.4566 - val_loss: 0.3559 - val_mse: 0.3559 - val_mae: 0.4559\n",
            "Epoch 347/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3327 - mse: 0.3327 - mae: 0.4514 - val_loss: 0.3518 - val_mse: 0.3518 - val_mae: 0.4573\n",
            "Epoch 348/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3433 - mse: 0.3433 - mae: 0.4547 - val_loss: 0.3568 - val_mse: 0.3568 - val_mae: 0.4564\n",
            "Epoch 349/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3390 - mse: 0.3390 - mae: 0.4563 - val_loss: 0.3645 - val_mse: 0.3645 - val_mae: 0.4617\n",
            "Epoch 350/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3316 - mse: 0.3316 - mae: 0.4495 - val_loss: 0.3553 - val_mse: 0.3553 - val_mae: 0.4603\n",
            "Epoch 351/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3217 - mse: 0.3217 - mae: 0.4418 - val_loss: 0.3542 - val_mse: 0.3542 - val_mae: 0.4561\n",
            "Epoch 352/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3272 - mse: 0.3272 - mae: 0.4431 - val_loss: 0.3452 - val_mse: 0.3452 - val_mae: 0.4518\n",
            "Epoch 353/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3311 - mse: 0.3311 - mae: 0.4466 - val_loss: 0.3644 - val_mse: 0.3644 - val_mae: 0.4609\n",
            "Epoch 354/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3328 - mse: 0.3328 - mae: 0.4495 - val_loss: 0.3485 - val_mse: 0.3485 - val_mae: 0.4607\n",
            "Epoch 355/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3226 - mse: 0.3226 - mae: 0.4350 - val_loss: 0.3640 - val_mse: 0.3640 - val_mae: 0.4599\n",
            "Epoch 356/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3331 - mse: 0.3331 - mae: 0.4431 - val_loss: 0.3601 - val_mse: 0.3601 - val_mae: 0.4558\n",
            "Epoch 357/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3264 - mse: 0.3264 - mae: 0.4437 - val_loss: 0.3498 - val_mse: 0.3498 - val_mae: 0.4559\n",
            "Epoch 358/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3404 - mse: 0.3404 - mae: 0.4605 - val_loss: 0.3554 - val_mse: 0.3554 - val_mae: 0.4653\n",
            "Epoch 359/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3362 - mse: 0.3362 - mae: 0.4455 - val_loss: 0.3549 - val_mse: 0.3549 - val_mae: 0.4536\n",
            "Epoch 360/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3321 - mse: 0.3321 - mae: 0.4498 - val_loss: 0.3547 - val_mse: 0.3547 - val_mae: 0.4569\n",
            "Epoch 361/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3318 - mse: 0.3318 - mae: 0.4553 - val_loss: 0.3526 - val_mse: 0.3526 - val_mae: 0.4628\n",
            "Epoch 362/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3435 - mse: 0.3435 - mae: 0.4560 - val_loss: 0.3527 - val_mse: 0.3527 - val_mae: 0.4560\n",
            "Epoch 363/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3429 - mse: 0.3429 - mae: 0.4534 - val_loss: 0.3535 - val_mse: 0.3535 - val_mae: 0.4560\n",
            "Epoch 364/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3334 - mse: 0.3334 - mae: 0.4475 - val_loss: 0.3534 - val_mse: 0.3534 - val_mae: 0.4549\n",
            "Epoch 365/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3455 - mse: 0.3455 - mae: 0.4568 - val_loss: 0.3485 - val_mse: 0.3485 - val_mae: 0.4521\n",
            "Epoch 366/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3318 - mse: 0.3318 - mae: 0.4471 - val_loss: 0.3455 - val_mse: 0.3455 - val_mae: 0.4551\n",
            "Epoch 367/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3294 - mse: 0.3294 - mae: 0.4463 - val_loss: 0.3532 - val_mse: 0.3532 - val_mae: 0.4540\n",
            "Epoch 368/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3262 - mse: 0.3262 - mae: 0.4439 - val_loss: 0.3473 - val_mse: 0.3473 - val_mae: 0.4561\n",
            "Epoch 369/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3428 - mse: 0.3428 - mae: 0.4469 - val_loss: 0.3577 - val_mse: 0.3577 - val_mae: 0.4596\n",
            "Epoch 370/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3156 - mse: 0.3156 - mae: 0.4360 - val_loss: 0.3474 - val_mse: 0.3474 - val_mae: 0.4588\n",
            "Epoch 371/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3520 - mse: 0.3520 - mae: 0.4612 - val_loss: 0.3597 - val_mse: 0.3597 - val_mae: 0.4579\n",
            "Epoch 372/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3240 - mse: 0.3240 - mae: 0.4451 - val_loss: 0.3497 - val_mse: 0.3497 - val_mae: 0.4537\n",
            "Epoch 373/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3214 - mse: 0.3214 - mae: 0.4407 - val_loss: 0.3518 - val_mse: 0.3518 - val_mae: 0.4583\n",
            "Epoch 374/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3396 - mse: 0.3396 - mae: 0.4558 - val_loss: 0.3532 - val_mse: 0.3532 - val_mae: 0.4596\n",
            "Epoch 375/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3473 - mse: 0.3473 - mae: 0.4530 - val_loss: 0.3690 - val_mse: 0.3690 - val_mae: 0.4635\n",
            "Epoch 376/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3337 - mse: 0.3337 - mae: 0.4526 - val_loss: 0.3496 - val_mse: 0.3496 - val_mae: 0.4611\n",
            "Epoch 377/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3394 - mse: 0.3394 - mae: 0.4582 - val_loss: 0.3603 - val_mse: 0.3603 - val_mae: 0.4580\n",
            "Epoch 378/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3341 - mse: 0.3341 - mae: 0.4428 - val_loss: 0.3488 - val_mse: 0.3488 - val_mae: 0.4600\n",
            "Epoch 379/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3225 - mse: 0.3225 - mae: 0.4400 - val_loss: 0.3542 - val_mse: 0.3542 - val_mae: 0.4556\n",
            "Epoch 380/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3423 - mse: 0.3423 - mae: 0.4571 - val_loss: 0.3426 - val_mse: 0.3426 - val_mae: 0.4570\n",
            "Epoch 381/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3211 - mse: 0.3211 - mae: 0.4393 - val_loss: 0.3544 - val_mse: 0.3544 - val_mae: 0.4563\n",
            "Epoch 382/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3393 - mse: 0.3393 - mae: 0.4506 - val_loss: 0.3416 - val_mse: 0.3416 - val_mae: 0.4557\n",
            "Epoch 383/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3311 - mse: 0.3311 - mae: 0.4451 - val_loss: 0.3529 - val_mse: 0.3529 - val_mae: 0.4541\n",
            "Epoch 384/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3329 - mse: 0.3329 - mae: 0.4508 - val_loss: 0.3467 - val_mse: 0.3467 - val_mae: 0.4536\n",
            "Epoch 385/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3272 - mse: 0.3272 - mae: 0.4464 - val_loss: 0.3412 - val_mse: 0.3412 - val_mae: 0.4521\n",
            "Epoch 386/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3233 - mse: 0.3233 - mae: 0.4443 - val_loss: 0.3482 - val_mse: 0.3482 - val_mae: 0.4551\n",
            "Epoch 387/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3307 - mse: 0.3307 - mae: 0.4496 - val_loss: 0.3468 - val_mse: 0.3468 - val_mae: 0.4494\n",
            "Epoch 388/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3094 - mse: 0.3094 - mae: 0.4305 - val_loss: 0.3534 - val_mse: 0.3534 - val_mae: 0.4551\n",
            "Epoch 389/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3323 - mse: 0.3323 - mae: 0.4456 - val_loss: 0.3463 - val_mse: 0.3463 - val_mae: 0.4537\n",
            "Epoch 390/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3416 - mse: 0.3416 - mae: 0.4591 - val_loss: 0.3630 - val_mse: 0.3630 - val_mae: 0.4604\n",
            "Epoch 391/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3289 - mse: 0.3289 - mae: 0.4382 - val_loss: 0.3405 - val_mse: 0.3405 - val_mae: 0.4526\n",
            "Epoch 392/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3318 - mse: 0.3318 - mae: 0.4519 - val_loss: 0.3552 - val_mse: 0.3552 - val_mae: 0.4531\n",
            "Epoch 393/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3422 - mse: 0.3422 - mae: 0.4548 - val_loss: 0.3439 - val_mse: 0.3439 - val_mae: 0.4547\n",
            "Epoch 394/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3399 - mse: 0.3399 - mae: 0.4621 - val_loss: 0.3486 - val_mse: 0.3486 - val_mae: 0.4543\n",
            "Epoch 395/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3420 - mse: 0.3420 - mae: 0.4557 - val_loss: 0.3513 - val_mse: 0.3513 - val_mae: 0.4551\n",
            "Epoch 396/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3326 - mse: 0.3326 - mae: 0.4522 - val_loss: 0.3478 - val_mse: 0.3478 - val_mae: 0.4529\n",
            "Epoch 397/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3402 - mse: 0.3402 - mae: 0.4558 - val_loss: 0.3430 - val_mse: 0.3430 - val_mae: 0.4528\n",
            "Epoch 398/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3311 - mse: 0.3311 - mae: 0.4467 - val_loss: 0.3458 - val_mse: 0.3458 - val_mae: 0.4548\n",
            "Epoch 399/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3491 - mse: 0.3491 - mae: 0.4581 - val_loss: 0.3625 - val_mse: 0.3625 - val_mae: 0.4586\n",
            "Epoch 400/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3297 - mse: 0.3297 - mae: 0.4473 - val_loss: 0.3573 - val_mse: 0.3573 - val_mae: 0.4555\n",
            "Epoch 401/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3283 - mse: 0.3283 - mae: 0.4383 - val_loss: 0.3470 - val_mse: 0.3470 - val_mae: 0.4563\n",
            "Epoch 402/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3326 - mse: 0.3326 - mae: 0.4497 - val_loss: 0.3617 - val_mse: 0.3617 - val_mae: 0.4589\n",
            "Epoch 403/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3288 - mse: 0.3288 - mae: 0.4472 - val_loss: 0.3436 - val_mse: 0.3436 - val_mae: 0.4517\n",
            "Epoch 404/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3290 - mse: 0.3290 - mae: 0.4478 - val_loss: 0.3466 - val_mse: 0.3466 - val_mae: 0.4535\n",
            "Epoch 405/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3326 - mse: 0.3326 - mae: 0.4477 - val_loss: 0.3567 - val_mse: 0.3567 - val_mae: 0.4558\n",
            "Epoch 406/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3378 - mse: 0.3378 - mae: 0.4518 - val_loss: 0.3460 - val_mse: 0.3460 - val_mae: 0.4568\n",
            "Epoch 407/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3264 - mse: 0.3264 - mae: 0.4441 - val_loss: 0.3602 - val_mse: 0.3602 - val_mae: 0.4600\n",
            "Epoch 408/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3393 - mse: 0.3393 - mae: 0.4542 - val_loss: 0.3495 - val_mse: 0.3495 - val_mae: 0.4582\n",
            "Epoch 409/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3341 - mse: 0.3341 - mae: 0.4411 - val_loss: 0.3480 - val_mse: 0.3480 - val_mae: 0.4551\n",
            "Epoch 410/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3350 - mse: 0.3350 - mae: 0.4486 - val_loss: 0.3522 - val_mse: 0.3522 - val_mae: 0.4561\n",
            "Epoch 411/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3329 - mse: 0.3329 - mae: 0.4506 - val_loss: 0.3530 - val_mse: 0.3530 - val_mae: 0.4608\n",
            "Epoch 412/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3214 - mse: 0.3214 - mae: 0.4396 - val_loss: 0.3552 - val_mse: 0.3552 - val_mae: 0.4565\n",
            "Epoch 413/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3232 - mse: 0.3232 - mae: 0.4409 - val_loss: 0.3542 - val_mse: 0.3542 - val_mae: 0.4559\n",
            "Epoch 414/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3288 - mse: 0.3288 - mae: 0.4562 - val_loss: 0.3516 - val_mse: 0.3516 - val_mae: 0.4531\n",
            "Epoch 415/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3333 - mse: 0.3333 - mae: 0.4500 - val_loss: 0.3601 - val_mse: 0.3601 - val_mae: 0.4570\n",
            "Epoch 416/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3256 - mse: 0.3256 - mae: 0.4435 - val_loss: 0.3597 - val_mse: 0.3597 - val_mae: 0.4588\n",
            "Epoch 417/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3313 - mse: 0.3313 - mae: 0.4487 - val_loss: 0.3509 - val_mse: 0.3509 - val_mae: 0.4586\n",
            "Epoch 418/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3417 - mse: 0.3417 - mae: 0.4570 - val_loss: 0.3575 - val_mse: 0.3575 - val_mae: 0.4584\n",
            "Epoch 419/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3111 - mse: 0.3111 - mae: 0.4307 - val_loss: 0.3548 - val_mse: 0.3548 - val_mae: 0.4548\n",
            "Epoch 420/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3250 - mse: 0.3250 - mae: 0.4457 - val_loss: 0.3612 - val_mse: 0.3612 - val_mae: 0.4584\n",
            "Epoch 421/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3574 - mse: 0.3574 - mae: 0.4734 - val_loss: 0.3582 - val_mse: 0.3582 - val_mae: 0.4666\n",
            "Epoch 422/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3485 - mse: 0.3485 - mae: 0.4551 - val_loss: 0.3803 - val_mse: 0.3803 - val_mae: 0.4685\n",
            "Epoch 423/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3331 - mse: 0.3331 - mae: 0.4472 - val_loss: 0.3487 - val_mse: 0.3487 - val_mae: 0.4563\n",
            "Epoch 424/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3377 - mse: 0.3377 - mae: 0.4543 - val_loss: 0.3597 - val_mse: 0.3597 - val_mae: 0.4582\n",
            "Epoch 425/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3262 - mse: 0.3262 - mae: 0.4471 - val_loss: 0.3484 - val_mse: 0.3484 - val_mae: 0.4592\n",
            "Epoch 426/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3250 - mse: 0.3250 - mae: 0.4478 - val_loss: 0.3589 - val_mse: 0.3589 - val_mae: 0.4576\n",
            "Epoch 427/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3220 - mse: 0.3220 - mae: 0.4392 - val_loss: 0.3596 - val_mse: 0.3596 - val_mae: 0.4574\n",
            "Epoch 428/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3373 - mse: 0.3373 - mae: 0.4548 - val_loss: 0.3456 - val_mse: 0.3456 - val_mae: 0.4539\n",
            "Epoch 429/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3396 - mse: 0.3396 - mae: 0.4539 - val_loss: 0.3637 - val_mse: 0.3637 - val_mae: 0.4582\n",
            "Epoch 430/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3267 - mse: 0.3267 - mae: 0.4421 - val_loss: 0.3493 - val_mse: 0.3493 - val_mae: 0.4570\n",
            "Epoch 431/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3168 - mse: 0.3168 - mae: 0.4383 - val_loss: 0.3491 - val_mse: 0.3491 - val_mae: 0.4599\n",
            "Epoch 432/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3342 - mse: 0.3342 - mae: 0.4487 - val_loss: 0.3542 - val_mse: 0.3542 - val_mae: 0.4550\n",
            "Epoch 433/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3268 - mse: 0.3268 - mae: 0.4436 - val_loss: 0.3643 - val_mse: 0.3643 - val_mae: 0.4590\n",
            "Epoch 434/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3275 - mse: 0.3275 - mae: 0.4465 - val_loss: 0.3501 - val_mse: 0.3501 - val_mae: 0.4547\n",
            "Epoch 435/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3246 - mse: 0.3246 - mae: 0.4381 - val_loss: 0.3604 - val_mse: 0.3604 - val_mae: 0.4604\n",
            "Epoch 436/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3302 - mse: 0.3302 - mae: 0.4425 - val_loss: 0.3482 - val_mse: 0.3482 - val_mae: 0.4546\n",
            "Epoch 437/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3291 - mse: 0.3291 - mae: 0.4433 - val_loss: 0.3535 - val_mse: 0.3535 - val_mae: 0.4540\n",
            "Epoch 438/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3270 - mse: 0.3270 - mae: 0.4428 - val_loss: 0.3514 - val_mse: 0.3514 - val_mae: 0.4539\n",
            "Epoch 439/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3318 - mse: 0.3318 - mae: 0.4507 - val_loss: 0.3505 - val_mse: 0.3505 - val_mae: 0.4546\n",
            "Epoch 440/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3365 - mse: 0.3365 - mae: 0.4480 - val_loss: 0.3543 - val_mse: 0.3543 - val_mae: 0.4552\n",
            "Epoch 441/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3219 - mse: 0.3219 - mae: 0.4387 - val_loss: 0.3499 - val_mse: 0.3499 - val_mae: 0.4569\n",
            "Epoch 442/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3280 - mse: 0.3280 - mae: 0.4407 - val_loss: 0.3583 - val_mse: 0.3583 - val_mae: 0.4542\n",
            "Epoch 443/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3250 - mse: 0.3250 - mae: 0.4417 - val_loss: 0.3534 - val_mse: 0.3534 - val_mae: 0.4530\n",
            "Epoch 444/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3292 - mse: 0.3292 - mae: 0.4435 - val_loss: 0.3571 - val_mse: 0.3571 - val_mae: 0.4563\n",
            "Epoch 445/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3167 - mse: 0.3167 - mae: 0.4364 - val_loss: 0.3512 - val_mse: 0.3512 - val_mae: 0.4523\n",
            "Epoch 446/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3286 - mse: 0.3286 - mae: 0.4420 - val_loss: 0.3483 - val_mse: 0.3483 - val_mae: 0.4529\n",
            "Epoch 447/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3107 - mse: 0.3107 - mae: 0.4349 - val_loss: 0.3576 - val_mse: 0.3576 - val_mae: 0.4569\n",
            "Epoch 448/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3193 - mse: 0.3193 - mae: 0.4413 - val_loss: 0.3483 - val_mse: 0.3483 - val_mae: 0.4541\n",
            "Epoch 449/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3153 - mse: 0.3153 - mae: 0.4367 - val_loss: 0.3499 - val_mse: 0.3499 - val_mae: 0.4557\n",
            "Epoch 450/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3168 - mse: 0.3168 - mae: 0.4360 - val_loss: 0.3590 - val_mse: 0.3590 - val_mae: 0.4569\n",
            "Epoch 451/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3425 - mse: 0.3425 - mae: 0.4586 - val_loss: 0.3542 - val_mse: 0.3542 - val_mae: 0.4671\n",
            "Epoch 452/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3266 - mse: 0.3266 - mae: 0.4407 - val_loss: 0.3595 - val_mse: 0.3595 - val_mae: 0.4555\n",
            "Epoch 453/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3333 - mse: 0.3333 - mae: 0.4472 - val_loss: 0.3496 - val_mse: 0.3496 - val_mae: 0.4599\n",
            "Epoch 454/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3280 - mse: 0.3280 - mae: 0.4398 - val_loss: 0.3664 - val_mse: 0.3664 - val_mae: 0.4573\n",
            "Epoch 455/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3298 - mse: 0.3298 - mae: 0.4512 - val_loss: 0.3526 - val_mse: 0.3526 - val_mae: 0.4547\n",
            "Epoch 456/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3145 - mse: 0.3145 - mae: 0.4334 - val_loss: 0.3485 - val_mse: 0.3485 - val_mae: 0.4530\n",
            "Epoch 457/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3130 - mse: 0.3130 - mae: 0.4353 - val_loss: 0.3536 - val_mse: 0.3536 - val_mae: 0.4534\n",
            "Epoch 458/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3354 - mse: 0.3354 - mae: 0.4430 - val_loss: 0.3646 - val_mse: 0.3646 - val_mae: 0.4578\n",
            "Epoch 459/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3352 - mse: 0.3352 - mae: 0.4515 - val_loss: 0.3520 - val_mse: 0.3520 - val_mae: 0.4534\n",
            "Epoch 460/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3331 - mse: 0.3331 - mae: 0.4502 - val_loss: 0.3543 - val_mse: 0.3543 - val_mae: 0.4675\n",
            "Epoch 461/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3469 - mse: 0.3469 - mae: 0.4553 - val_loss: 0.3515 - val_mse: 0.3515 - val_mae: 0.4525\n",
            "Epoch 462/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3191 - mse: 0.3191 - mae: 0.4376 - val_loss: 0.3466 - val_mse: 0.3466 - val_mae: 0.4519\n",
            "Epoch 463/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3178 - mse: 0.3178 - mae: 0.4359 - val_loss: 0.3537 - val_mse: 0.3537 - val_mae: 0.4547\n",
            "Epoch 464/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3201 - mse: 0.3201 - mae: 0.4383 - val_loss: 0.3496 - val_mse: 0.3496 - val_mae: 0.4502\n",
            "Epoch 465/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3245 - mse: 0.3245 - mae: 0.4486 - val_loss: 0.3461 - val_mse: 0.3461 - val_mae: 0.4512\n",
            "Epoch 466/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3276 - mse: 0.3276 - mae: 0.4486 - val_loss: 0.3579 - val_mse: 0.3579 - val_mae: 0.4571\n",
            "Epoch 467/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3398 - mse: 0.3398 - mae: 0.4505 - val_loss: 0.3700 - val_mse: 0.3700 - val_mae: 0.4563\n",
            "Epoch 468/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3208 - mse: 0.3208 - mae: 0.4379 - val_loss: 0.3500 - val_mse: 0.3500 - val_mae: 0.4543\n",
            "Epoch 469/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3236 - mse: 0.3236 - mae: 0.4421 - val_loss: 0.3504 - val_mse: 0.3504 - val_mae: 0.4555\n",
            "Epoch 470/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3248 - mse: 0.3248 - mae: 0.4422 - val_loss: 0.3577 - val_mse: 0.3577 - val_mae: 0.4567\n",
            "Epoch 471/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3239 - mse: 0.3239 - mae: 0.4452 - val_loss: 0.3522 - val_mse: 0.3522 - val_mae: 0.4548\n",
            "Epoch 472/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3173 - mse: 0.3173 - mae: 0.4385 - val_loss: 0.3535 - val_mse: 0.3535 - val_mae: 0.4552\n",
            "Epoch 473/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3143 - mse: 0.3143 - mae: 0.4397 - val_loss: 0.3549 - val_mse: 0.3549 - val_mae: 0.4556\n",
            "Epoch 474/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3259 - mse: 0.3259 - mae: 0.4535 - val_loss: 0.3603 - val_mse: 0.3603 - val_mae: 0.4556\n",
            "Epoch 475/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3271 - mse: 0.3271 - mae: 0.4448 - val_loss: 0.3524 - val_mse: 0.3524 - val_mae: 0.4554\n",
            "Epoch 476/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3234 - mse: 0.3234 - mae: 0.4491 - val_loss: 0.3671 - val_mse: 0.3671 - val_mae: 0.4616\n",
            "Epoch 477/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3378 - mse: 0.3378 - mae: 0.4513 - val_loss: 0.3526 - val_mse: 0.3526 - val_mae: 0.4535\n",
            "Epoch 478/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3154 - mse: 0.3154 - mae: 0.4387 - val_loss: 0.3520 - val_mse: 0.3520 - val_mae: 0.4514\n",
            "Epoch 479/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3198 - mse: 0.3198 - mae: 0.4357 - val_loss: 0.3547 - val_mse: 0.3547 - val_mae: 0.4553\n",
            "Epoch 480/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3105 - mse: 0.3105 - mae: 0.4339 - val_loss: 0.3530 - val_mse: 0.3530 - val_mae: 0.4543\n",
            "Epoch 481/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3392 - mse: 0.3392 - mae: 0.4528 - val_loss: 0.3593 - val_mse: 0.3593 - val_mae: 0.4563\n",
            "Epoch 482/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3321 - mse: 0.3321 - mae: 0.4488 - val_loss: 0.3546 - val_mse: 0.3546 - val_mae: 0.4598\n",
            "Epoch 483/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3167 - mse: 0.3167 - mae: 0.4366 - val_loss: 0.3590 - val_mse: 0.3590 - val_mae: 0.4573\n",
            "Epoch 484/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3184 - mse: 0.3184 - mae: 0.4417 - val_loss: 0.3547 - val_mse: 0.3547 - val_mae: 0.4555\n",
            "Epoch 485/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3271 - mse: 0.3271 - mae: 0.4405 - val_loss: 0.3580 - val_mse: 0.3580 - val_mae: 0.4578\n",
            "Epoch 486/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3278 - mse: 0.3278 - mae: 0.4458 - val_loss: 0.3483 - val_mse: 0.3483 - val_mae: 0.4574\n",
            "Epoch 487/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3265 - mse: 0.3265 - mae: 0.4418 - val_loss: 0.3745 - val_mse: 0.3745 - val_mae: 0.4638\n",
            "Epoch 488/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3307 - mse: 0.3307 - mae: 0.4441 - val_loss: 0.3457 - val_mse: 0.3457 - val_mae: 0.4556\n",
            "Epoch 489/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3197 - mse: 0.3197 - mae: 0.4387 - val_loss: 0.3611 - val_mse: 0.3611 - val_mae: 0.4560\n",
            "Epoch 490/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3387 - mse: 0.3387 - mae: 0.4533 - val_loss: 0.3545 - val_mse: 0.3545 - val_mae: 0.4562\n",
            "Epoch 491/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3229 - mse: 0.3229 - mae: 0.4477 - val_loss: 0.3591 - val_mse: 0.3591 - val_mae: 0.4578\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.5194 - mse: 0.5194 - mae: 0.5433\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 14ms/step - loss: 18.8050 - mse: 18.8050 - mae: 4.1003 - val_loss: 16.2894 - val_mse: 16.2894 - val_mae: 3.8155\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 13.6243 - mse: 13.6243 - mae: 3.3920 - val_loss: 11.6704 - val_mse: 11.6704 - val_mae: 3.1581\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 9.8761 - mse: 9.8761 - mae: 2.7950 - val_loss: 8.4605 - val_mse: 8.4605 - val_mae: 2.6064\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 7.2494 - mse: 7.2494 - mae: 2.3249 - val_loss: 6.4407 - val_mse: 6.4407 - val_mae: 2.2228\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5.5359 - mse: 5.5359 - mae: 1.9731 - val_loss: 5.1626 - val_mse: 5.1626 - val_mae: 1.9450\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 4.3517 - mse: 4.3517 - mae: 1.7282 - val_loss: 4.3751 - val_mse: 4.3751 - val_mae: 1.7555\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 3.7792 - mse: 3.7792 - mae: 1.6018 - val_loss: 3.8326 - val_mse: 3.8326 - val_mae: 1.6184\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 3.5199 - mse: 3.5199 - mae: 1.5279 - val_loss: 3.4024 - val_mse: 3.4024 - val_mae: 1.5086\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 3.0177 - mse: 3.0177 - mae: 1.3995 - val_loss: 3.1509 - val_mse: 3.1509 - val_mae: 1.4369\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.7289 - mse: 2.7289 - mae: 1.3337 - val_loss: 2.9353 - val_mse: 2.9353 - val_mae: 1.3795\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.5723 - mse: 2.5723 - mae: 1.2892 - val_loss: 2.7430 - val_mse: 2.7430 - val_mae: 1.3242\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.4764 - mse: 2.4764 - mae: 1.2567 - val_loss: 2.6039 - val_mse: 2.6039 - val_mae: 1.2825\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 2.3342 - mse: 2.3342 - mae: 1.2271 - val_loss: 2.4927 - val_mse: 2.4927 - val_mae: 1.2457\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.1665 - mse: 2.1665 - mae: 1.1881 - val_loss: 2.4139 - val_mse: 2.4139 - val_mae: 1.2179\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.0124 - mse: 2.0124 - mae: 1.1458 - val_loss: 2.3206 - val_mse: 2.3206 - val_mae: 1.1879\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.0293 - mse: 2.0293 - mae: 1.1316 - val_loss: 2.2527 - val_mse: 2.2527 - val_mae: 1.1665\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.9904 - mse: 1.9904 - mae: 1.1226 - val_loss: 2.1820 - val_mse: 2.1820 - val_mae: 1.1465\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.9494 - mse: 1.9494 - mae: 1.1224 - val_loss: 2.1421 - val_mse: 2.1421 - val_mae: 1.1359\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.9633 - mse: 1.9633 - mae: 1.1268 - val_loss: 2.0947 - val_mse: 2.0947 - val_mae: 1.1171\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.8224 - mse: 1.8224 - mae: 1.0730 - val_loss: 2.0270 - val_mse: 2.0270 - val_mae: 1.0983\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.8289 - mse: 1.8289 - mae: 1.0831 - val_loss: 1.9727 - val_mse: 1.9727 - val_mae: 1.0814\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.6721 - mse: 1.6721 - mae: 1.0169 - val_loss: 1.9254 - val_mse: 1.9254 - val_mae: 1.0675\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.6892 - mse: 1.6892 - mae: 1.0398 - val_loss: 1.8913 - val_mse: 1.8913 - val_mae: 1.0573\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.6624 - mse: 1.6624 - mae: 1.0463 - val_loss: 1.8601 - val_mse: 1.8601 - val_mae: 1.0424\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.6436 - mse: 1.6436 - mae: 1.0213 - val_loss: 1.8201 - val_mse: 1.8201 - val_mae: 1.0297\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.6259 - mse: 1.6259 - mae: 1.0001 - val_loss: 1.7625 - val_mse: 1.7625 - val_mae: 1.0102\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.5657 - mse: 1.5657 - mae: 1.0079 - val_loss: 1.7292 - val_mse: 1.7292 - val_mae: 1.0013\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.5082 - mse: 1.5082 - mae: 0.9792 - val_loss: 1.6918 - val_mse: 1.6918 - val_mae: 0.9879\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.4945 - mse: 1.4945 - mae: 0.9695 - val_loss: 1.6450 - val_mse: 1.6450 - val_mae: 0.9738\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.5001 - mse: 1.5001 - mae: 0.9817 - val_loss: 1.5993 - val_mse: 1.5993 - val_mae: 0.9627\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.3990 - mse: 1.3990 - mae: 0.9550 - val_loss: 1.5653 - val_mse: 1.5653 - val_mae: 0.9551\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.3780 - mse: 1.3780 - mae: 0.9127 - val_loss: 1.5437 - val_mse: 1.5437 - val_mae: 0.9367\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.4012 - mse: 1.4012 - mae: 0.9465 - val_loss: 1.5079 - val_mse: 1.5079 - val_mae: 0.9250\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.4127 - mse: 1.4127 - mae: 0.9447 - val_loss: 1.4764 - val_mse: 1.4764 - val_mae: 0.9196\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.3623 - mse: 1.3623 - mae: 0.9203 - val_loss: 1.4488 - val_mse: 1.4488 - val_mae: 0.9033\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2377 - mse: 1.2377 - mae: 0.8900 - val_loss: 1.4241 - val_mse: 1.4241 - val_mae: 0.8999\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.2885 - mse: 1.2885 - mae: 0.9055 - val_loss: 1.4021 - val_mse: 1.4021 - val_mae: 0.8862\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.2460 - mse: 1.2460 - mae: 0.8827 - val_loss: 1.3635 - val_mse: 1.3635 - val_mae: 0.8718\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2317 - mse: 1.2317 - mae: 0.8828 - val_loss: 1.3315 - val_mse: 1.3315 - val_mae: 0.8660\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1181 - mse: 1.1181 - mae: 0.8499 - val_loss: 1.3164 - val_mse: 1.3164 - val_mae: 0.8523\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1379 - mse: 1.1379 - mae: 0.8472 - val_loss: 1.2743 - val_mse: 1.2743 - val_mae: 0.8438\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0466 - mse: 1.0466 - mae: 0.8069 - val_loss: 1.2549 - val_mse: 1.2549 - val_mae: 0.8324\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0769 - mse: 1.0769 - mae: 0.8257 - val_loss: 1.2264 - val_mse: 1.2264 - val_mae: 0.8188\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0845 - mse: 1.0845 - mae: 0.8252 - val_loss: 1.2011 - val_mse: 1.2011 - val_mae: 0.8157\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0829 - mse: 1.0829 - mae: 0.8195 - val_loss: 1.1733 - val_mse: 1.1733 - val_mae: 0.8073\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0304 - mse: 1.0304 - mae: 0.7965 - val_loss: 1.1454 - val_mse: 1.1454 - val_mae: 0.7939\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0174 - mse: 1.0174 - mae: 0.7899 - val_loss: 1.1131 - val_mse: 1.1131 - val_mae: 0.7807\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.9504 - mse: 0.9504 - mae: 0.7620 - val_loss: 1.0963 - val_mse: 1.0963 - val_mae: 0.7730\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.9346 - mse: 0.9346 - mae: 0.7688 - val_loss: 1.0831 - val_mse: 1.0831 - val_mae: 0.7655\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.9372 - mse: 0.9372 - mae: 0.7750 - val_loss: 1.0544 - val_mse: 1.0544 - val_mae: 0.7577\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.8972 - mse: 0.8972 - mae: 0.7495 - val_loss: 1.0259 - val_mse: 1.0259 - val_mae: 0.7454\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8753 - mse: 0.8753 - mae: 0.7342 - val_loss: 1.0231 - val_mse: 1.0231 - val_mae: 0.7498\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.8602 - mse: 0.8602 - mae: 0.7349 - val_loss: 1.0023 - val_mse: 1.0023 - val_mae: 0.7343\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8766 - mse: 0.8766 - mae: 0.7439 - val_loss: 0.9900 - val_mse: 0.9900 - val_mae: 0.7259\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8883 - mse: 0.8883 - mae: 0.7490 - val_loss: 0.9626 - val_mse: 0.9626 - val_mae: 0.7176\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.9112 - mse: 0.9112 - mae: 0.7518 - val_loss: 0.9532 - val_mse: 0.9532 - val_mae: 0.7110\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7940 - mse: 0.7940 - mae: 0.6915 - val_loss: 0.9263 - val_mse: 0.9263 - val_mae: 0.7038\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8205 - mse: 0.8205 - mae: 0.7153 - val_loss: 0.8989 - val_mse: 0.8989 - val_mae: 0.6951\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.8021 - mse: 0.8021 - mae: 0.6972 - val_loss: 0.8713 - val_mse: 0.8713 - val_mae: 0.6848\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.8086 - mse: 0.8086 - mae: 0.6955 - val_loss: 0.8685 - val_mse: 0.8685 - val_mae: 0.6842\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7610 - mse: 0.7610 - mae: 0.6822 - val_loss: 0.8517 - val_mse: 0.8517 - val_mae: 0.6732\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7709 - mse: 0.7709 - mae: 0.6974 - val_loss: 0.8388 - val_mse: 0.8388 - val_mae: 0.6730\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7589 - mse: 0.7589 - mae: 0.6846 - val_loss: 0.8356 - val_mse: 0.8356 - val_mae: 0.6677\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7388 - mse: 0.7388 - mae: 0.6769 - val_loss: 0.8290 - val_mse: 0.8290 - val_mae: 0.6640\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7622 - mse: 0.7622 - mae: 0.6839 - val_loss: 0.8134 - val_mse: 0.8134 - val_mae: 0.6588\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7427 - mse: 0.7427 - mae: 0.6820 - val_loss: 0.8144 - val_mse: 0.8144 - val_mae: 0.6639\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7347 - mse: 0.7347 - mae: 0.6719 - val_loss: 0.7843 - val_mse: 0.7843 - val_mae: 0.6454\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6760 - mse: 0.6760 - mae: 0.6562 - val_loss: 0.7721 - val_mse: 0.7721 - val_mae: 0.6438\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6692 - mse: 0.6692 - mae: 0.6313 - val_loss: 0.7637 - val_mse: 0.7637 - val_mae: 0.6464\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6631 - mse: 0.6631 - mae: 0.6347 - val_loss: 0.7563 - val_mse: 0.7563 - val_mae: 0.6346\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6498 - mse: 0.6498 - mae: 0.6388 - val_loss: 0.7429 - val_mse: 0.7429 - val_mae: 0.6354\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6796 - mse: 0.6796 - mae: 0.6379 - val_loss: 0.7434 - val_mse: 0.7434 - val_mae: 0.6342\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6218 - mse: 0.6218 - mae: 0.6150 - val_loss: 0.7245 - val_mse: 0.7245 - val_mae: 0.6318\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6141 - mse: 0.6141 - mae: 0.6127 - val_loss: 0.7158 - val_mse: 0.7158 - val_mae: 0.6253\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6254 - mse: 0.6254 - mae: 0.6208 - val_loss: 0.7105 - val_mse: 0.7105 - val_mae: 0.6211\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6184 - mse: 0.6184 - mae: 0.6123 - val_loss: 0.7169 - val_mse: 0.7169 - val_mae: 0.6255\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6033 - mse: 0.6033 - mae: 0.6100 - val_loss: 0.7011 - val_mse: 0.7011 - val_mae: 0.6151\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6051 - mse: 0.6051 - mae: 0.6115 - val_loss: 0.6840 - val_mse: 0.6840 - val_mae: 0.6137\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5888 - mse: 0.5888 - mae: 0.6032 - val_loss: 0.6777 - val_mse: 0.6777 - val_mae: 0.6125\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5937 - mse: 0.5937 - mae: 0.6013 - val_loss: 0.6715 - val_mse: 0.6715 - val_mae: 0.6060\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5699 - mse: 0.5699 - mae: 0.5889 - val_loss: 0.6783 - val_mse: 0.6783 - val_mae: 0.6132\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6003 - mse: 0.6003 - mae: 0.6087 - val_loss: 0.6689 - val_mse: 0.6689 - val_mae: 0.6053\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5793 - mse: 0.5793 - mae: 0.6044 - val_loss: 0.6576 - val_mse: 0.6576 - val_mae: 0.6010\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5761 - mse: 0.5761 - mae: 0.5891 - val_loss: 0.6418 - val_mse: 0.6418 - val_mae: 0.5940\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5747 - mse: 0.5747 - mae: 0.5933 - val_loss: 0.6306 - val_mse: 0.6306 - val_mae: 0.5928\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5464 - mse: 0.5464 - mae: 0.5730 - val_loss: 0.6422 - val_mse: 0.6422 - val_mae: 0.5914\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5684 - mse: 0.5684 - mae: 0.5975 - val_loss: 0.6377 - val_mse: 0.6377 - val_mae: 0.5916\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5604 - mse: 0.5604 - mae: 0.5902 - val_loss: 0.6316 - val_mse: 0.6316 - val_mae: 0.5902\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5405 - mse: 0.5405 - mae: 0.5804 - val_loss: 0.6256 - val_mse: 0.6256 - val_mae: 0.5870\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5342 - mse: 0.5342 - mae: 0.5726 - val_loss: 0.6303 - val_mse: 0.6303 - val_mae: 0.5828\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5420 - mse: 0.5420 - mae: 0.5717 - val_loss: 0.6094 - val_mse: 0.6094 - val_mae: 0.5790\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5348 - mse: 0.5348 - mae: 0.5731 - val_loss: 0.6122 - val_mse: 0.6122 - val_mae: 0.5801\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5412 - mse: 0.5412 - mae: 0.5749 - val_loss: 0.6097 - val_mse: 0.6097 - val_mae: 0.5748\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5410 - mse: 0.5410 - mae: 0.5738 - val_loss: 0.6052 - val_mse: 0.6052 - val_mae: 0.5747\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5303 - mse: 0.5303 - mae: 0.5749 - val_loss: 0.6043 - val_mse: 0.6043 - val_mae: 0.5747\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5415 - mse: 0.5415 - mae: 0.5717 - val_loss: 0.6072 - val_mse: 0.6072 - val_mae: 0.5760\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5119 - mse: 0.5119 - mae: 0.5588 - val_loss: 0.5926 - val_mse: 0.5926 - val_mae: 0.5688\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4961 - mse: 0.4961 - mae: 0.5511 - val_loss: 0.5972 - val_mse: 0.5972 - val_mae: 0.5697\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5122 - mse: 0.5122 - mae: 0.5673 - val_loss: 0.5858 - val_mse: 0.5858 - val_mae: 0.5640\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4972 - mse: 0.4972 - mae: 0.5489 - val_loss: 0.5877 - val_mse: 0.5877 - val_mae: 0.5657\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4751 - mse: 0.4751 - mae: 0.5391 - val_loss: 0.5829 - val_mse: 0.5829 - val_mae: 0.5640\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5077 - mse: 0.5077 - mae: 0.5538 - val_loss: 0.5657 - val_mse: 0.5657 - val_mae: 0.5558\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4815 - mse: 0.4815 - mae: 0.5537 - val_loss: 0.5627 - val_mse: 0.5627 - val_mae: 0.5565\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4900 - mse: 0.4900 - mae: 0.5439 - val_loss: 0.5686 - val_mse: 0.5686 - val_mae: 0.5615\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4900 - mse: 0.4900 - mae: 0.5462 - val_loss: 0.5620 - val_mse: 0.5620 - val_mae: 0.5556\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4845 - mse: 0.4845 - mae: 0.5398 - val_loss: 0.5616 - val_mse: 0.5616 - val_mae: 0.5591\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4864 - mse: 0.4864 - mae: 0.5491 - val_loss: 0.5582 - val_mse: 0.5582 - val_mae: 0.5529\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4741 - mse: 0.4741 - mae: 0.5375 - val_loss: 0.5590 - val_mse: 0.5590 - val_mae: 0.5527\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4720 - mse: 0.4720 - mae: 0.5354 - val_loss: 0.5470 - val_mse: 0.5470 - val_mae: 0.5498\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4670 - mse: 0.4670 - mae: 0.5376 - val_loss: 0.5477 - val_mse: 0.5477 - val_mae: 0.5472\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4662 - mse: 0.4662 - mae: 0.5337 - val_loss: 0.5476 - val_mse: 0.5476 - val_mae: 0.5454\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4880 - mse: 0.4880 - mae: 0.5460 - val_loss: 0.5503 - val_mse: 0.5503 - val_mae: 0.5487\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4505 - mse: 0.4505 - mae: 0.5257 - val_loss: 0.5362 - val_mse: 0.5362 - val_mae: 0.5430\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4461 - mse: 0.4461 - mae: 0.5264 - val_loss: 0.5341 - val_mse: 0.5341 - val_mae: 0.5412\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4513 - mse: 0.4513 - mae: 0.5347 - val_loss: 0.5334 - val_mse: 0.5334 - val_mae: 0.5415\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4502 - mse: 0.4502 - mae: 0.5234 - val_loss: 0.5361 - val_mse: 0.5361 - val_mae: 0.5437\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4513 - mse: 0.4513 - mae: 0.5214 - val_loss: 0.5398 - val_mse: 0.5398 - val_mae: 0.5458\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4194 - mse: 0.4194 - mae: 0.5058 - val_loss: 0.5338 - val_mse: 0.5338 - val_mae: 0.5389\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4378 - mse: 0.4378 - mae: 0.5113 - val_loss: 0.5341 - val_mse: 0.5341 - val_mae: 0.5393\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4414 - mse: 0.4414 - mae: 0.5169 - val_loss: 0.5287 - val_mse: 0.5287 - val_mae: 0.5385\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4399 - mse: 0.4399 - mae: 0.5255 - val_loss: 0.5304 - val_mse: 0.5304 - val_mae: 0.5401\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4541 - mse: 0.4541 - mae: 0.5268 - val_loss: 0.5219 - val_mse: 0.5219 - val_mae: 0.5404\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4445 - mse: 0.4445 - mae: 0.5158 - val_loss: 0.5327 - val_mse: 0.5327 - val_mae: 0.5387\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4355 - mse: 0.4355 - mae: 0.5165 - val_loss: 0.5181 - val_mse: 0.5181 - val_mae: 0.5322\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4417 - mse: 0.4417 - mae: 0.5166 - val_loss: 0.5262 - val_mse: 0.5262 - val_mae: 0.5372\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4321 - mse: 0.4321 - mae: 0.5163 - val_loss: 0.5219 - val_mse: 0.5219 - val_mae: 0.5374\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4293 - mse: 0.4293 - mae: 0.5140 - val_loss: 0.5237 - val_mse: 0.5237 - val_mae: 0.5358\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4614 - mse: 0.4614 - mae: 0.5213 - val_loss: 0.5176 - val_mse: 0.5176 - val_mae: 0.5402\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4388 - mse: 0.4388 - mae: 0.5233 - val_loss: 0.5072 - val_mse: 0.5072 - val_mae: 0.5319\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4348 - mse: 0.4348 - mae: 0.5127 - val_loss: 0.5123 - val_mse: 0.5123 - val_mae: 0.5327\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4416 - mse: 0.4416 - mae: 0.5216 - val_loss: 0.5140 - val_mse: 0.5140 - val_mae: 0.5340\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4267 - mse: 0.4267 - mae: 0.5114 - val_loss: 0.5058 - val_mse: 0.5058 - val_mae: 0.5289\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4295 - mse: 0.4295 - mae: 0.5120 - val_loss: 0.5223 - val_mse: 0.5223 - val_mae: 0.5336\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4042 - mse: 0.4042 - mae: 0.4952 - val_loss: 0.5028 - val_mse: 0.5028 - val_mae: 0.5256\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4129 - mse: 0.4129 - mae: 0.4969 - val_loss: 0.5006 - val_mse: 0.5006 - val_mae: 0.5280\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4106 - mse: 0.4106 - mae: 0.5024 - val_loss: 0.5146 - val_mse: 0.5146 - val_mae: 0.5331\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4263 - mse: 0.4263 - mae: 0.5136 - val_loss: 0.5011 - val_mse: 0.5011 - val_mae: 0.5302\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4246 - mse: 0.4246 - mae: 0.5058 - val_loss: 0.5078 - val_mse: 0.5078 - val_mae: 0.5311\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4120 - mse: 0.4120 - mae: 0.5014 - val_loss: 0.5120 - val_mse: 0.5120 - val_mae: 0.5333\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4091 - mse: 0.4091 - mae: 0.5016 - val_loss: 0.5003 - val_mse: 0.5003 - val_mae: 0.5274\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4191 - mse: 0.4191 - mae: 0.5027 - val_loss: 0.5067 - val_mse: 0.5067 - val_mae: 0.5273\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4043 - mse: 0.4043 - mae: 0.4961 - val_loss: 0.4907 - val_mse: 0.4907 - val_mae: 0.5254\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4223 - mse: 0.4223 - mae: 0.5088 - val_loss: 0.4969 - val_mse: 0.4969 - val_mae: 0.5251\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3963 - mse: 0.3963 - mae: 0.4924 - val_loss: 0.4978 - val_mse: 0.4978 - val_mae: 0.5266\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4243 - mse: 0.4243 - mae: 0.5069 - val_loss: 0.5008 - val_mse: 0.5008 - val_mae: 0.5299\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4177 - mse: 0.4177 - mae: 0.5100 - val_loss: 0.4926 - val_mse: 0.4926 - val_mae: 0.5272\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4029 - mse: 0.4029 - mae: 0.4981 - val_loss: 0.4974 - val_mse: 0.4974 - val_mae: 0.5259\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3966 - mse: 0.3966 - mae: 0.4922 - val_loss: 0.4950 - val_mse: 0.4950 - val_mae: 0.5232\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4140 - mse: 0.4140 - mae: 0.4934 - val_loss: 0.4859 - val_mse: 0.4859 - val_mae: 0.5245\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4020 - mse: 0.4020 - mae: 0.4965 - val_loss: 0.4987 - val_mse: 0.4987 - val_mae: 0.5322\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4008 - mse: 0.4008 - mae: 0.4889 - val_loss: 0.4911 - val_mse: 0.4911 - val_mae: 0.5264\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4040 - mse: 0.4040 - mae: 0.4991 - val_loss: 0.4927 - val_mse: 0.4927 - val_mae: 0.5283\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4022 - mse: 0.4022 - mae: 0.4908 - val_loss: 0.4940 - val_mse: 0.4940 - val_mae: 0.5266\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3913 - mse: 0.3913 - mae: 0.4835 - val_loss: 0.4951 - val_mse: 0.4951 - val_mae: 0.5244\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3992 - mse: 0.3992 - mae: 0.4884 - val_loss: 0.4956 - val_mse: 0.4956 - val_mae: 0.5289\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4075 - mse: 0.4075 - mae: 0.4989 - val_loss: 0.4903 - val_mse: 0.4903 - val_mae: 0.5232\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4048 - mse: 0.4048 - mae: 0.5015 - val_loss: 0.4872 - val_mse: 0.4872 - val_mae: 0.5292\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3765 - mse: 0.3765 - mae: 0.4844 - val_loss: 0.4893 - val_mse: 0.4893 - val_mae: 0.5213\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3990 - mse: 0.3990 - mae: 0.4914 - val_loss: 0.5030 - val_mse: 0.5030 - val_mae: 0.5332\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4290 - mse: 0.4290 - mae: 0.5103 - val_loss: 0.4865 - val_mse: 0.4865 - val_mae: 0.5249\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3955 - mse: 0.3955 - mae: 0.4913 - val_loss: 0.4803 - val_mse: 0.4803 - val_mae: 0.5201\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3762 - mse: 0.3762 - mae: 0.4823 - val_loss: 0.4908 - val_mse: 0.4908 - val_mae: 0.5253\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3731 - mse: 0.3731 - mae: 0.4758 - val_loss: 0.4766 - val_mse: 0.4766 - val_mae: 0.5218\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4075 - mse: 0.4075 - mae: 0.4976 - val_loss: 0.4810 - val_mse: 0.4810 - val_mae: 0.5215\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3940 - mse: 0.3940 - mae: 0.4897 - val_loss: 0.4829 - val_mse: 0.4829 - val_mae: 0.5245\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3843 - mse: 0.3843 - mae: 0.4878 - val_loss: 0.4842 - val_mse: 0.4842 - val_mae: 0.5234\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3867 - mse: 0.3867 - mae: 0.4878 - val_loss: 0.4734 - val_mse: 0.4734 - val_mae: 0.5190\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3878 - mse: 0.3878 - mae: 0.4864 - val_loss: 0.4822 - val_mse: 0.4822 - val_mae: 0.5228\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3971 - mse: 0.3971 - mae: 0.4932 - val_loss: 0.4757 - val_mse: 0.4757 - val_mae: 0.5224\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3879 - mse: 0.3879 - mae: 0.4924 - val_loss: 0.4855 - val_mse: 0.4855 - val_mae: 0.5263\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3750 - mse: 0.3750 - mae: 0.4685 - val_loss: 0.4904 - val_mse: 0.4904 - val_mae: 0.5241\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3901 - mse: 0.3901 - mae: 0.4864 - val_loss: 0.4690 - val_mse: 0.4690 - val_mae: 0.5147\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3979 - mse: 0.3979 - mae: 0.4910 - val_loss: 0.4853 - val_mse: 0.4853 - val_mae: 0.5249\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3820 - mse: 0.3820 - mae: 0.4806 - val_loss: 0.4767 - val_mse: 0.4767 - val_mae: 0.5240\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3810 - mse: 0.3810 - mae: 0.4869 - val_loss: 0.4738 - val_mse: 0.4738 - val_mae: 0.5198\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3824 - mse: 0.3824 - mae: 0.4867 - val_loss: 0.4775 - val_mse: 0.4775 - val_mae: 0.5258\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3803 - mse: 0.3803 - mae: 0.4792 - val_loss: 0.4653 - val_mse: 0.4653 - val_mae: 0.5138\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3745 - mse: 0.3745 - mae: 0.4746 - val_loss: 0.4689 - val_mse: 0.4689 - val_mae: 0.5176\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3680 - mse: 0.3680 - mae: 0.4717 - val_loss: 0.4749 - val_mse: 0.4749 - val_mae: 0.5271\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3757 - mse: 0.3757 - mae: 0.4826 - val_loss: 0.4657 - val_mse: 0.4657 - val_mae: 0.5205\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3975 - mse: 0.3975 - mae: 0.4904 - val_loss: 0.4849 - val_mse: 0.4849 - val_mae: 0.5354\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3802 - mse: 0.3802 - mae: 0.4804 - val_loss: 0.4605 - val_mse: 0.4605 - val_mae: 0.5151\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3789 - mse: 0.3789 - mae: 0.4879 - val_loss: 0.4841 - val_mse: 0.4841 - val_mae: 0.5279\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3680 - mse: 0.3680 - mae: 0.4760 - val_loss: 0.4621 - val_mse: 0.4621 - val_mae: 0.5156\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3762 - mse: 0.3762 - mae: 0.4740 - val_loss: 0.4731 - val_mse: 0.4731 - val_mae: 0.5239\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3765 - mse: 0.3765 - mae: 0.4832 - val_loss: 0.4657 - val_mse: 0.4657 - val_mae: 0.5258\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3784 - mse: 0.3784 - mae: 0.4755 - val_loss: 0.4838 - val_mse: 0.4838 - val_mae: 0.5299\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3735 - mse: 0.3735 - mae: 0.4767 - val_loss: 0.4632 - val_mse: 0.4632 - val_mae: 0.5133\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3844 - mse: 0.3844 - mae: 0.4835 - val_loss: 0.4694 - val_mse: 0.4694 - val_mae: 0.5225\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3753 - mse: 0.3753 - mae: 0.4773 - val_loss: 0.4739 - val_mse: 0.4739 - val_mae: 0.5297\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3581 - mse: 0.3581 - mae: 0.4735 - val_loss: 0.4705 - val_mse: 0.4705 - val_mae: 0.5209\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3839 - mse: 0.3839 - mae: 0.4846 - val_loss: 0.4600 - val_mse: 0.4600 - val_mae: 0.5240\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3765 - mse: 0.3765 - mae: 0.4757 - val_loss: 0.4735 - val_mse: 0.4735 - val_mae: 0.5264\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3794 - mse: 0.3794 - mae: 0.4897 - val_loss: 0.4556 - val_mse: 0.4556 - val_mae: 0.5121\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3787 - mse: 0.3787 - mae: 0.4810 - val_loss: 0.4719 - val_mse: 0.4719 - val_mae: 0.5243\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3756 - mse: 0.3756 - mae: 0.4877 - val_loss: 0.4648 - val_mse: 0.4648 - val_mae: 0.5176\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3677 - mse: 0.3677 - mae: 0.4639 - val_loss: 0.4551 - val_mse: 0.4551 - val_mae: 0.5109\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3691 - mse: 0.3691 - mae: 0.4812 - val_loss: 0.4693 - val_mse: 0.4693 - val_mae: 0.5284\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3840 - mse: 0.3840 - mae: 0.4859 - val_loss: 0.4530 - val_mse: 0.4530 - val_mae: 0.5176\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3701 - mse: 0.3701 - mae: 0.4723 - val_loss: 0.4546 - val_mse: 0.4546 - val_mae: 0.5147\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3725 - mse: 0.3725 - mae: 0.4736 - val_loss: 0.4557 - val_mse: 0.4557 - val_mae: 0.5103\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3546 - mse: 0.3546 - mae: 0.4670 - val_loss: 0.4745 - val_mse: 0.4745 - val_mae: 0.5311\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3811 - mse: 0.3811 - mae: 0.4790 - val_loss: 0.4697 - val_mse: 0.4697 - val_mae: 0.5257\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3790 - mse: 0.3790 - mae: 0.4853 - val_loss: 0.4572 - val_mse: 0.4572 - val_mae: 0.5131\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3671 - mse: 0.3671 - mae: 0.4665 - val_loss: 0.4679 - val_mse: 0.4679 - val_mae: 0.5275\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3765 - mse: 0.3765 - mae: 0.4777 - val_loss: 0.4610 - val_mse: 0.4610 - val_mae: 0.5205\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3607 - mse: 0.3607 - mae: 0.4693 - val_loss: 0.4550 - val_mse: 0.4550 - val_mae: 0.5148\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3656 - mse: 0.3656 - mae: 0.4777 - val_loss: 0.4638 - val_mse: 0.4638 - val_mae: 0.5235\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3737 - mse: 0.3737 - mae: 0.4793 - val_loss: 0.4496 - val_mse: 0.4496 - val_mae: 0.5136\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3735 - mse: 0.3735 - mae: 0.4764 - val_loss: 0.4525 - val_mse: 0.4525 - val_mae: 0.5167\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3618 - mse: 0.3618 - mae: 0.4711 - val_loss: 0.4674 - val_mse: 0.4674 - val_mae: 0.5207\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3658 - mse: 0.3658 - mae: 0.4673 - val_loss: 0.4637 - val_mse: 0.4637 - val_mae: 0.5168\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3661 - mse: 0.3661 - mae: 0.4727 - val_loss: 0.4629 - val_mse: 0.4629 - val_mae: 0.5205\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3618 - mse: 0.3618 - mae: 0.4708 - val_loss: 0.4597 - val_mse: 0.4597 - val_mae: 0.5172\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3790 - mse: 0.3790 - mae: 0.4809 - val_loss: 0.4644 - val_mse: 0.4644 - val_mae: 0.5145\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3575 - mse: 0.3575 - mae: 0.4628 - val_loss: 0.4571 - val_mse: 0.4571 - val_mae: 0.5225\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3631 - mse: 0.3631 - mae: 0.4741 - val_loss: 0.4595 - val_mse: 0.4595 - val_mae: 0.5134\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3603 - mse: 0.3603 - mae: 0.4690 - val_loss: 0.4637 - val_mse: 0.4637 - val_mae: 0.5199\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3648 - mse: 0.3648 - mae: 0.4673 - val_loss: 0.4583 - val_mse: 0.4583 - val_mae: 0.5166\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3869 - mse: 0.3869 - mae: 0.4865 - val_loss: 0.4582 - val_mse: 0.4582 - val_mae: 0.5168\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3714 - mse: 0.3714 - mae: 0.4802 - val_loss: 0.4676 - val_mse: 0.4676 - val_mae: 0.5243\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3599 - mse: 0.3599 - mae: 0.4650 - val_loss: 0.4498 - val_mse: 0.4498 - val_mae: 0.5130\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3753 - mse: 0.3753 - mae: 0.4775 - val_loss: 0.4539 - val_mse: 0.4539 - val_mae: 0.5163\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3618 - mse: 0.3618 - mae: 0.4713 - val_loss: 0.4505 - val_mse: 0.4505 - val_mae: 0.5144\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3539 - mse: 0.3539 - mae: 0.4603 - val_loss: 0.4655 - val_mse: 0.4655 - val_mae: 0.5205\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3561 - mse: 0.3561 - mae: 0.4691 - val_loss: 0.4517 - val_mse: 0.4517 - val_mae: 0.5127\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3753 - mse: 0.3753 - mae: 0.4732 - val_loss: 0.4570 - val_mse: 0.4570 - val_mae: 0.5213\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3658 - mse: 0.3658 - mae: 0.4717 - val_loss: 0.4597 - val_mse: 0.4597 - val_mae: 0.5215\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3734 - mse: 0.3734 - mae: 0.4823 - val_loss: 0.4600 - val_mse: 0.4600 - val_mae: 0.5160\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3707 - mse: 0.3707 - mae: 0.4694 - val_loss: 0.4702 - val_mse: 0.4702 - val_mae: 0.5308\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3510 - mse: 0.3510 - mae: 0.4691 - val_loss: 0.4447 - val_mse: 0.4447 - val_mae: 0.5131\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3454 - mse: 0.3454 - mae: 0.4588 - val_loss: 0.4654 - val_mse: 0.4654 - val_mae: 0.5228\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3515 - mse: 0.3515 - mae: 0.4635 - val_loss: 0.4630 - val_mse: 0.4630 - val_mae: 0.5165\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3488 - mse: 0.3488 - mae: 0.4583 - val_loss: 0.4604 - val_mse: 0.4604 - val_mae: 0.5202\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3538 - mse: 0.3538 - mae: 0.4643 - val_loss: 0.4518 - val_mse: 0.4518 - val_mae: 0.5149\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3545 - mse: 0.3545 - mae: 0.4687 - val_loss: 0.4537 - val_mse: 0.4537 - val_mae: 0.5175\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3616 - mse: 0.3616 - mae: 0.4693 - val_loss: 0.4632 - val_mse: 0.4632 - val_mae: 0.5281\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3684 - mse: 0.3684 - mae: 0.4745 - val_loss: 0.4604 - val_mse: 0.4604 - val_mae: 0.5149\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3751 - mse: 0.3751 - mae: 0.4709 - val_loss: 0.4770 - val_mse: 0.4770 - val_mae: 0.5368\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3876 - mse: 0.3876 - mae: 0.4867 - val_loss: 0.4506 - val_mse: 0.4506 - val_mae: 0.5117\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3628 - mse: 0.3628 - mae: 0.4679 - val_loss: 0.4539 - val_mse: 0.4539 - val_mae: 0.5167\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3635 - mse: 0.3635 - mae: 0.4693 - val_loss: 0.4629 - val_mse: 0.4629 - val_mae: 0.5215\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3363 - mse: 0.3363 - mae: 0.4518 - val_loss: 0.4576 - val_mse: 0.4576 - val_mae: 0.5141\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3639 - mse: 0.3639 - mae: 0.4680 - val_loss: 0.4637 - val_mse: 0.4637 - val_mae: 0.5197\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3646 - mse: 0.3646 - mae: 0.4704 - val_loss: 0.4514 - val_mse: 0.4514 - val_mae: 0.5198\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3639 - mse: 0.3639 - mae: 0.4654 - val_loss: 0.4515 - val_mse: 0.4515 - val_mae: 0.5182\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3508 - mse: 0.3508 - mae: 0.4672 - val_loss: 0.4737 - val_mse: 0.4737 - val_mae: 0.5266\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3585 - mse: 0.3585 - mae: 0.4740 - val_loss: 0.4479 - val_mse: 0.4479 - val_mae: 0.5113\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3477 - mse: 0.3477 - mae: 0.4569 - val_loss: 0.4658 - val_mse: 0.4658 - val_mae: 0.5219\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3710 - mse: 0.3710 - mae: 0.4798 - val_loss: 0.4606 - val_mse: 0.4606 - val_mae: 0.5218\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3279 - mse: 0.3279 - mae: 0.4518 - val_loss: 0.4426 - val_mse: 0.4426 - val_mae: 0.5056\n",
            "Epoch 252/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3835 - mse: 0.3835 - mae: 0.4803 - val_loss: 0.4487 - val_mse: 0.4487 - val_mae: 0.5088\n",
            "Epoch 253/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3716 - mse: 0.3716 - mae: 0.4701 - val_loss: 0.4703 - val_mse: 0.4703 - val_mae: 0.5336\n",
            "Epoch 254/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3497 - mse: 0.3497 - mae: 0.4574 - val_loss: 0.4452 - val_mse: 0.4452 - val_mae: 0.5095\n",
            "Epoch 255/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3636 - mse: 0.3636 - mae: 0.4701 - val_loss: 0.4583 - val_mse: 0.4583 - val_mae: 0.5203\n",
            "Epoch 256/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3523 - mse: 0.3523 - mae: 0.4609 - val_loss: 0.4540 - val_mse: 0.4540 - val_mae: 0.5186\n",
            "Epoch 257/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3511 - mse: 0.3511 - mae: 0.4597 - val_loss: 0.4655 - val_mse: 0.4655 - val_mae: 0.5167\n",
            "Epoch 258/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3642 - mse: 0.3642 - mae: 0.4692 - val_loss: 0.4662 - val_mse: 0.4662 - val_mae: 0.5234\n",
            "Epoch 259/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3687 - mse: 0.3687 - mae: 0.4813 - val_loss: 0.4539 - val_mse: 0.4539 - val_mae: 0.5161\n",
            "Epoch 260/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3552 - mse: 0.3552 - mae: 0.4691 - val_loss: 0.4448 - val_mse: 0.4448 - val_mae: 0.5066\n",
            "Epoch 261/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3688 - mse: 0.3688 - mae: 0.4676 - val_loss: 0.4619 - val_mse: 0.4619 - val_mae: 0.5212\n",
            "Epoch 262/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3550 - mse: 0.3550 - mae: 0.4649 - val_loss: 0.4476 - val_mse: 0.4476 - val_mae: 0.5093\n",
            "Epoch 263/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3539 - mse: 0.3539 - mae: 0.4629 - val_loss: 0.4582 - val_mse: 0.4582 - val_mae: 0.5150\n",
            "Epoch 264/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3535 - mse: 0.3535 - mae: 0.4672 - val_loss: 0.4525 - val_mse: 0.4525 - val_mae: 0.5133\n",
            "Epoch 265/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3788 - mse: 0.3788 - mae: 0.4773 - val_loss: 0.4803 - val_mse: 0.4803 - val_mae: 0.5354\n",
            "Epoch 266/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3667 - mse: 0.3667 - mae: 0.4739 - val_loss: 0.4447 - val_mse: 0.4447 - val_mae: 0.5035\n",
            "Epoch 267/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3521 - mse: 0.3521 - mae: 0.4651 - val_loss: 0.4573 - val_mse: 0.4573 - val_mae: 0.5204\n",
            "Epoch 268/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3499 - mse: 0.3499 - mae: 0.4678 - val_loss: 0.4602 - val_mse: 0.4602 - val_mae: 0.5225\n",
            "Epoch 269/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3574 - mse: 0.3574 - mae: 0.4632 - val_loss: 0.4582 - val_mse: 0.4582 - val_mae: 0.5230\n",
            "Epoch 270/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3710 - mse: 0.3710 - mae: 0.4807 - val_loss: 0.4501 - val_mse: 0.4501 - val_mae: 0.5111\n",
            "Epoch 271/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3515 - mse: 0.3515 - mae: 0.4618 - val_loss: 0.4490 - val_mse: 0.4490 - val_mae: 0.5137\n",
            "Epoch 272/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3496 - mse: 0.3496 - mae: 0.4567 - val_loss: 0.4509 - val_mse: 0.4509 - val_mae: 0.5147\n",
            "Epoch 273/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3477 - mse: 0.3477 - mae: 0.4649 - val_loss: 0.4518 - val_mse: 0.4518 - val_mae: 0.5095\n",
            "Epoch 274/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3539 - mse: 0.3539 - mae: 0.4665 - val_loss: 0.4547 - val_mse: 0.4547 - val_mae: 0.5171\n",
            "Epoch 275/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3521 - mse: 0.3521 - mae: 0.4626 - val_loss: 0.4463 - val_mse: 0.4463 - val_mae: 0.5144\n",
            "Epoch 276/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3477 - mse: 0.3477 - mae: 0.4650 - val_loss: 0.4454 - val_mse: 0.4454 - val_mae: 0.5083\n",
            "Epoch 277/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3757 - mse: 0.3757 - mae: 0.4722 - val_loss: 0.4419 - val_mse: 0.4419 - val_mae: 0.5072\n",
            "Epoch 278/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3392 - mse: 0.3392 - mae: 0.4528 - val_loss: 0.4469 - val_mse: 0.4469 - val_mae: 0.5063\n",
            "Epoch 279/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3403 - mse: 0.3403 - mae: 0.4530 - val_loss: 0.4421 - val_mse: 0.4421 - val_mae: 0.5090\n",
            "Epoch 280/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3523 - mse: 0.3523 - mae: 0.4665 - val_loss: 0.4444 - val_mse: 0.4444 - val_mae: 0.5120\n",
            "Epoch 281/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3471 - mse: 0.3471 - mae: 0.4638 - val_loss: 0.4624 - val_mse: 0.4624 - val_mae: 0.5201\n",
            "Epoch 282/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3548 - mse: 0.3548 - mae: 0.4678 - val_loss: 0.4463 - val_mse: 0.4463 - val_mae: 0.5092\n",
            "Epoch 283/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3464 - mse: 0.3464 - mae: 0.4623 - val_loss: 0.4563 - val_mse: 0.4563 - val_mae: 0.5189\n",
            "Epoch 284/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3508 - mse: 0.3508 - mae: 0.4695 - val_loss: 0.4503 - val_mse: 0.4503 - val_mae: 0.5123\n",
            "Epoch 285/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3423 - mse: 0.3423 - mae: 0.4560 - val_loss: 0.4478 - val_mse: 0.4478 - val_mae: 0.5117\n",
            "Epoch 286/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3531 - mse: 0.3531 - mae: 0.4590 - val_loss: 0.4563 - val_mse: 0.4563 - val_mae: 0.5167\n",
            "Epoch 287/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3612 - mse: 0.3612 - mae: 0.4692 - val_loss: 0.4612 - val_mse: 0.4612 - val_mae: 0.5203\n",
            "Epoch 288/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3608 - mse: 0.3608 - mae: 0.4635 - val_loss: 0.4563 - val_mse: 0.4563 - val_mae: 0.5162\n",
            "Epoch 289/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3600 - mse: 0.3600 - mae: 0.4644 - val_loss: 0.4498 - val_mse: 0.4498 - val_mae: 0.5180\n",
            "Epoch 290/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3687 - mse: 0.3687 - mae: 0.4733 - val_loss: 0.4566 - val_mse: 0.4566 - val_mae: 0.5175\n",
            "Epoch 291/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3485 - mse: 0.3485 - mae: 0.4597 - val_loss: 0.4420 - val_mse: 0.4420 - val_mae: 0.5103\n",
            "Epoch 292/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3576 - mse: 0.3576 - mae: 0.4707 - val_loss: 0.4551 - val_mse: 0.4551 - val_mae: 0.5191\n",
            "Epoch 293/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3441 - mse: 0.3441 - mae: 0.4598 - val_loss: 0.4489 - val_mse: 0.4489 - val_mae: 0.5091\n",
            "Epoch 294/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3392 - mse: 0.3392 - mae: 0.4549 - val_loss: 0.4595 - val_mse: 0.4595 - val_mae: 0.5199\n",
            "Epoch 295/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3489 - mse: 0.3489 - mae: 0.4645 - val_loss: 0.4400 - val_mse: 0.4400 - val_mae: 0.5056\n",
            "Epoch 296/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3580 - mse: 0.3580 - mae: 0.4568 - val_loss: 0.4581 - val_mse: 0.4581 - val_mae: 0.5223\n",
            "Epoch 297/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3731 - mse: 0.3731 - mae: 0.4756 - val_loss: 0.4544 - val_mse: 0.4544 - val_mae: 0.5192\n",
            "Epoch 298/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3629 - mse: 0.3629 - mae: 0.4690 - val_loss: 0.4405 - val_mse: 0.4405 - val_mae: 0.5056\n",
            "Epoch 299/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3470 - mse: 0.3470 - mae: 0.4615 - val_loss: 0.4399 - val_mse: 0.4399 - val_mae: 0.5082\n",
            "Epoch 300/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3502 - mse: 0.3502 - mae: 0.4669 - val_loss: 0.4522 - val_mse: 0.4522 - val_mae: 0.5045\n",
            "Epoch 301/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3476 - mse: 0.3476 - mae: 0.4533 - val_loss: 0.4657 - val_mse: 0.4657 - val_mae: 0.5264\n",
            "Epoch 302/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3548 - mse: 0.3548 - mae: 0.4639 - val_loss: 0.4525 - val_mse: 0.4525 - val_mae: 0.5075\n",
            "Epoch 303/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3567 - mse: 0.3567 - mae: 0.4671 - val_loss: 0.4548 - val_mse: 0.4548 - val_mae: 0.5117\n",
            "Epoch 304/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3512 - mse: 0.3512 - mae: 0.4637 - val_loss: 0.4439 - val_mse: 0.4439 - val_mae: 0.5076\n",
            "Epoch 305/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3434 - mse: 0.3434 - mae: 0.4577 - val_loss: 0.4524 - val_mse: 0.4524 - val_mae: 0.5088\n",
            "Epoch 306/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3503 - mse: 0.3503 - mae: 0.4586 - val_loss: 0.4518 - val_mse: 0.4518 - val_mae: 0.5065\n",
            "Epoch 307/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3393 - mse: 0.3393 - mae: 0.4498 - val_loss: 0.4567 - val_mse: 0.4567 - val_mae: 0.5188\n",
            "Epoch 308/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3551 - mse: 0.3551 - mae: 0.4628 - val_loss: 0.4460 - val_mse: 0.4460 - val_mae: 0.5066\n",
            "Epoch 309/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3613 - mse: 0.3613 - mae: 0.4718 - val_loss: 0.4523 - val_mse: 0.4523 - val_mae: 0.5188\n",
            "Epoch 310/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3501 - mse: 0.3501 - mae: 0.4591 - val_loss: 0.4447 - val_mse: 0.4447 - val_mae: 0.5103\n",
            "Epoch 311/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3288 - mse: 0.3288 - mae: 0.4464 - val_loss: 0.4439 - val_mse: 0.4439 - val_mae: 0.5090\n",
            "Epoch 312/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3346 - mse: 0.3346 - mae: 0.4533 - val_loss: 0.4501 - val_mse: 0.4501 - val_mae: 0.5084\n",
            "Epoch 313/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3449 - mse: 0.3449 - mae: 0.4547 - val_loss: 0.4474 - val_mse: 0.4474 - val_mae: 0.5107\n",
            "Epoch 314/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3396 - mse: 0.3396 - mae: 0.4492 - val_loss: 0.4508 - val_mse: 0.4508 - val_mae: 0.5132\n",
            "Epoch 315/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3431 - mse: 0.3431 - mae: 0.4592 - val_loss: 0.4423 - val_mse: 0.4423 - val_mae: 0.5107\n",
            "Epoch 316/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3427 - mse: 0.3427 - mae: 0.4582 - val_loss: 0.4396 - val_mse: 0.4396 - val_mae: 0.5069\n",
            "Epoch 317/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3501 - mse: 0.3501 - mae: 0.4667 - val_loss: 0.4511 - val_mse: 0.4511 - val_mae: 0.5141\n",
            "Epoch 318/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3362 - mse: 0.3362 - mae: 0.4568 - val_loss: 0.4546 - val_mse: 0.4546 - val_mae: 0.5077\n",
            "Epoch 319/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3326 - mse: 0.3326 - mae: 0.4505 - val_loss: 0.4499 - val_mse: 0.4499 - val_mae: 0.5097\n",
            "Epoch 320/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3357 - mse: 0.3357 - mae: 0.4526 - val_loss: 0.4410 - val_mse: 0.4410 - val_mae: 0.5054\n",
            "Epoch 321/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3333 - mse: 0.3333 - mae: 0.4501 - val_loss: 0.4435 - val_mse: 0.4435 - val_mae: 0.5023\n",
            "Epoch 322/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3452 - mse: 0.3452 - mae: 0.4568 - val_loss: 0.4502 - val_mse: 0.4502 - val_mae: 0.5133\n",
            "Epoch 323/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3329 - mse: 0.3329 - mae: 0.4505 - val_loss: 0.4530 - val_mse: 0.4530 - val_mae: 0.5136\n",
            "Epoch 324/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3447 - mse: 0.3447 - mae: 0.4624 - val_loss: 0.4350 - val_mse: 0.4350 - val_mae: 0.5005\n",
            "Epoch 325/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3456 - mse: 0.3456 - mae: 0.4606 - val_loss: 0.4426 - val_mse: 0.4426 - val_mae: 0.5060\n",
            "Epoch 326/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3459 - mse: 0.3459 - mae: 0.4655 - val_loss: 0.4473 - val_mse: 0.4473 - val_mae: 0.5102\n",
            "Epoch 327/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3250 - mse: 0.3250 - mae: 0.4403 - val_loss: 0.4390 - val_mse: 0.4390 - val_mae: 0.5037\n",
            "Epoch 328/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3441 - mse: 0.3441 - mae: 0.4619 - val_loss: 0.4546 - val_mse: 0.4546 - val_mae: 0.5137\n",
            "Epoch 329/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3538 - mse: 0.3538 - mae: 0.4619 - val_loss: 0.4585 - val_mse: 0.4585 - val_mae: 0.5157\n",
            "Epoch 330/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3219 - mse: 0.3219 - mae: 0.4467 - val_loss: 0.4668 - val_mse: 0.4668 - val_mae: 0.5213\n",
            "Epoch 331/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3440 - mse: 0.3440 - mae: 0.4610 - val_loss: 0.4509 - val_mse: 0.4509 - val_mae: 0.5043\n",
            "Epoch 332/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3500 - mse: 0.3500 - mae: 0.4565 - val_loss: 0.4475 - val_mse: 0.4475 - val_mae: 0.5102\n",
            "Epoch 333/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3331 - mse: 0.3331 - mae: 0.4439 - val_loss: 0.4601 - val_mse: 0.4601 - val_mae: 0.5115\n",
            "Epoch 334/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3346 - mse: 0.3346 - mae: 0.4516 - val_loss: 0.4479 - val_mse: 0.4479 - val_mae: 0.5104\n",
            "Epoch 335/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3514 - mse: 0.3514 - mae: 0.4600 - val_loss: 0.4487 - val_mse: 0.4487 - val_mae: 0.5073\n",
            "Epoch 336/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3405 - mse: 0.3405 - mae: 0.4575 - val_loss: 0.4537 - val_mse: 0.4537 - val_mae: 0.5189\n",
            "Epoch 337/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3537 - mse: 0.3537 - mae: 0.4679 - val_loss: 0.4514 - val_mse: 0.4514 - val_mae: 0.5040\n",
            "Epoch 338/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3362 - mse: 0.3362 - mae: 0.4501 - val_loss: 0.4482 - val_mse: 0.4482 - val_mae: 0.5129\n",
            "Epoch 339/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3325 - mse: 0.3325 - mae: 0.4485 - val_loss: 0.4478 - val_mse: 0.4478 - val_mae: 0.5070\n",
            "Epoch 340/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3349 - mse: 0.3349 - mae: 0.4513 - val_loss: 0.4460 - val_mse: 0.4460 - val_mae: 0.5087\n",
            "Epoch 341/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3326 - mse: 0.3326 - mae: 0.4511 - val_loss: 0.4503 - val_mse: 0.4503 - val_mae: 0.5074\n",
            "Epoch 342/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3239 - mse: 0.3239 - mae: 0.4428 - val_loss: 0.4398 - val_mse: 0.4398 - val_mae: 0.5015\n",
            "Epoch 343/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3428 - mse: 0.3428 - mae: 0.4527 - val_loss: 0.4556 - val_mse: 0.4556 - val_mae: 0.5158\n",
            "Epoch 344/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3370 - mse: 0.3370 - mae: 0.4492 - val_loss: 0.4484 - val_mse: 0.4484 - val_mae: 0.5060\n",
            "Epoch 345/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3314 - mse: 0.3314 - mae: 0.4452 - val_loss: 0.4446 - val_mse: 0.4446 - val_mae: 0.5066\n",
            "Epoch 346/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3274 - mse: 0.3274 - mae: 0.4483 - val_loss: 0.4490 - val_mse: 0.4490 - val_mae: 0.5031\n",
            "Epoch 347/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3437 - mse: 0.3437 - mae: 0.4535 - val_loss: 0.4472 - val_mse: 0.4472 - val_mae: 0.5063\n",
            "Epoch 348/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3441 - mse: 0.3441 - mae: 0.4509 - val_loss: 0.4459 - val_mse: 0.4459 - val_mae: 0.5100\n",
            "Epoch 349/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3378 - mse: 0.3378 - mae: 0.4471 - val_loss: 0.4522 - val_mse: 0.4522 - val_mae: 0.5158\n",
            "Epoch 350/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3576 - mse: 0.3576 - mae: 0.4623 - val_loss: 0.4403 - val_mse: 0.4403 - val_mae: 0.4985\n",
            "Epoch 351/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3498 - mse: 0.3498 - mae: 0.4549 - val_loss: 0.4440 - val_mse: 0.4440 - val_mae: 0.5061\n",
            "Epoch 352/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3424 - mse: 0.3424 - mae: 0.4552 - val_loss: 0.4585 - val_mse: 0.4585 - val_mae: 0.5196\n",
            "Epoch 353/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3560 - mse: 0.3560 - mae: 0.4640 - val_loss: 0.4644 - val_mse: 0.4644 - val_mae: 0.5300\n",
            "Epoch 354/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3321 - mse: 0.3321 - mae: 0.4518 - val_loss: 0.4479 - val_mse: 0.4479 - val_mae: 0.5083\n",
            "Epoch 355/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3471 - mse: 0.3471 - mae: 0.4527 - val_loss: 0.4638 - val_mse: 0.4638 - val_mae: 0.5229\n",
            "Epoch 356/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3274 - mse: 0.3274 - mae: 0.4492 - val_loss: 0.4468 - val_mse: 0.4468 - val_mae: 0.4991\n",
            "Epoch 357/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3617 - mse: 0.3617 - mae: 0.4572 - val_loss: 0.4554 - val_mse: 0.4554 - val_mae: 0.5227\n",
            "Epoch 358/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3161 - mse: 0.3161 - mae: 0.4359 - val_loss: 0.4486 - val_mse: 0.4486 - val_mae: 0.5071\n",
            "Epoch 359/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3364 - mse: 0.3364 - mae: 0.4422 - val_loss: 0.4501 - val_mse: 0.4501 - val_mae: 0.5090\n",
            "Epoch 360/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3348 - mse: 0.3348 - mae: 0.4589 - val_loss: 0.4464 - val_mse: 0.4464 - val_mae: 0.5068\n",
            "Epoch 361/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3293 - mse: 0.3293 - mae: 0.4400 - val_loss: 0.4450 - val_mse: 0.4450 - val_mae: 0.5089\n",
            "Epoch 362/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3590 - mse: 0.3590 - mae: 0.4655 - val_loss: 0.4471 - val_mse: 0.4471 - val_mae: 0.5079\n",
            "Epoch 363/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3361 - mse: 0.3361 - mae: 0.4449 - val_loss: 0.4466 - val_mse: 0.4466 - val_mae: 0.5027\n",
            "Epoch 364/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3487 - mse: 0.3487 - mae: 0.4564 - val_loss: 0.4654 - val_mse: 0.4654 - val_mae: 0.5329\n",
            "Epoch 365/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3316 - mse: 0.3316 - mae: 0.4494 - val_loss: 0.4506 - val_mse: 0.4506 - val_mae: 0.5158\n",
            "Epoch 366/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3233 - mse: 0.3233 - mae: 0.4435 - val_loss: 0.4417 - val_mse: 0.4417 - val_mae: 0.4990\n",
            "Epoch 367/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3350 - mse: 0.3350 - mae: 0.4411 - val_loss: 0.4509 - val_mse: 0.4509 - val_mae: 0.5148\n",
            "Epoch 368/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3526 - mse: 0.3526 - mae: 0.4569 - val_loss: 0.4541 - val_mse: 0.4541 - val_mae: 0.5045\n",
            "Epoch 369/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3364 - mse: 0.3364 - mae: 0.4494 - val_loss: 0.4529 - val_mse: 0.4529 - val_mae: 0.5066\n",
            "Epoch 370/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3290 - mse: 0.3290 - mae: 0.4495 - val_loss: 0.4540 - val_mse: 0.4540 - val_mae: 0.5085\n",
            "Epoch 371/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3437 - mse: 0.3437 - mae: 0.4590 - val_loss: 0.4427 - val_mse: 0.4427 - val_mae: 0.5036\n",
            "Epoch 372/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3366 - mse: 0.3366 - mae: 0.4490 - val_loss: 0.4549 - val_mse: 0.4549 - val_mae: 0.5139\n",
            "Epoch 373/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3345 - mse: 0.3345 - mae: 0.4510 - val_loss: 0.4437 - val_mse: 0.4437 - val_mae: 0.5024\n",
            "Epoch 374/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3374 - mse: 0.3374 - mae: 0.4474 - val_loss: 0.4544 - val_mse: 0.4544 - val_mae: 0.5156\n",
            "Epoch 375/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3504 - mse: 0.3504 - mae: 0.4609 - val_loss: 0.4620 - val_mse: 0.4620 - val_mae: 0.5089\n",
            "Epoch 376/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3329 - mse: 0.3329 - mae: 0.4513 - val_loss: 0.4547 - val_mse: 0.4547 - val_mae: 0.5115\n",
            "Epoch 377/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3291 - mse: 0.3291 - mae: 0.4440 - val_loss: 0.4548 - val_mse: 0.4548 - val_mae: 0.5135\n",
            "Epoch 378/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3203 - mse: 0.3203 - mae: 0.4421 - val_loss: 0.4479 - val_mse: 0.4479 - val_mae: 0.5025\n",
            "Epoch 379/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3154 - mse: 0.3154 - mae: 0.4336 - val_loss: 0.4620 - val_mse: 0.4620 - val_mae: 0.5085\n",
            "Epoch 380/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3301 - mse: 0.3301 - mae: 0.4404 - val_loss: 0.4578 - val_mse: 0.4578 - val_mae: 0.5112\n",
            "Epoch 381/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3454 - mse: 0.3454 - mae: 0.4600 - val_loss: 0.4510 - val_mse: 0.4510 - val_mae: 0.5083\n",
            "Epoch 382/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3313 - mse: 0.3313 - mae: 0.4453 - val_loss: 0.4517 - val_mse: 0.4517 - val_mae: 0.5038\n",
            "Epoch 383/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3269 - mse: 0.3269 - mae: 0.4407 - val_loss: 0.4494 - val_mse: 0.4494 - val_mae: 0.5029\n",
            "Epoch 384/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3394 - mse: 0.3394 - mae: 0.4572 - val_loss: 0.4505 - val_mse: 0.4505 - val_mae: 0.5021\n",
            "Epoch 385/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3380 - mse: 0.3380 - mae: 0.4540 - val_loss: 0.4583 - val_mse: 0.4583 - val_mae: 0.5143\n",
            "Epoch 386/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3239 - mse: 0.3239 - mae: 0.4507 - val_loss: 0.4582 - val_mse: 0.4582 - val_mae: 0.5011\n",
            "Epoch 387/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3441 - mse: 0.3441 - mae: 0.4483 - val_loss: 0.4765 - val_mse: 0.4765 - val_mae: 0.5282\n",
            "Epoch 388/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3324 - mse: 0.3324 - mae: 0.4492 - val_loss: 0.4462 - val_mse: 0.4462 - val_mae: 0.4961\n",
            "Epoch 389/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3311 - mse: 0.3311 - mae: 0.4430 - val_loss: 0.4572 - val_mse: 0.4572 - val_mae: 0.5191\n",
            "Epoch 390/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3225 - mse: 0.3225 - mae: 0.4399 - val_loss: 0.4440 - val_mse: 0.4440 - val_mae: 0.5001\n",
            "Epoch 391/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3443 - mse: 0.3443 - mae: 0.4508 - val_loss: 0.4615 - val_mse: 0.4615 - val_mae: 0.5077\n",
            "Epoch 392/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3340 - mse: 0.3340 - mae: 0.4477 - val_loss: 0.4578 - val_mse: 0.4578 - val_mae: 0.5200\n",
            "Epoch 393/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3535 - mse: 0.3535 - mae: 0.4737 - val_loss: 0.4456 - val_mse: 0.4456 - val_mae: 0.5058\n",
            "Epoch 394/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3371 - mse: 0.3371 - mae: 0.4483 - val_loss: 0.4603 - val_mse: 0.4603 - val_mae: 0.5001\n",
            "Epoch 395/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3282 - mse: 0.3282 - mae: 0.4427 - val_loss: 0.4581 - val_mse: 0.4581 - val_mae: 0.5181\n",
            "Epoch 396/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3309 - mse: 0.3309 - mae: 0.4492 - val_loss: 0.4562 - val_mse: 0.4562 - val_mae: 0.5170\n",
            "Epoch 397/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3246 - mse: 0.3246 - mae: 0.4457 - val_loss: 0.4453 - val_mse: 0.4453 - val_mae: 0.5034\n",
            "Epoch 398/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3522 - mse: 0.3522 - mae: 0.4636 - val_loss: 0.4591 - val_mse: 0.4591 - val_mae: 0.5142\n",
            "Epoch 399/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3364 - mse: 0.3364 - mae: 0.4540 - val_loss: 0.4502 - val_mse: 0.4502 - val_mae: 0.5101\n",
            "Epoch 400/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3141 - mse: 0.3141 - mae: 0.4353 - val_loss: 0.4598 - val_mse: 0.4598 - val_mae: 0.5136\n",
            "Epoch 401/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3366 - mse: 0.3366 - mae: 0.4529 - val_loss: 0.4495 - val_mse: 0.4495 - val_mae: 0.5033\n",
            "Epoch 402/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3262 - mse: 0.3262 - mae: 0.4416 - val_loss: 0.4499 - val_mse: 0.4499 - val_mae: 0.5126\n",
            "Epoch 403/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3056 - mse: 0.3056 - mae: 0.4276 - val_loss: 0.4459 - val_mse: 0.4459 - val_mae: 0.5021\n",
            "Epoch 404/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3408 - mse: 0.3408 - mae: 0.4521 - val_loss: 0.4473 - val_mse: 0.4473 - val_mae: 0.5075\n",
            "Epoch 405/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3174 - mse: 0.3174 - mae: 0.4390 - val_loss: 0.4575 - val_mse: 0.4575 - val_mae: 0.5080\n",
            "Epoch 406/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3430 - mse: 0.3430 - mae: 0.4477 - val_loss: 0.4566 - val_mse: 0.4566 - val_mae: 0.5192\n",
            "Epoch 407/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3199 - mse: 0.3199 - mae: 0.4402 - val_loss: 0.4462 - val_mse: 0.4462 - val_mae: 0.5012\n",
            "Epoch 408/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3355 - mse: 0.3355 - mae: 0.4515 - val_loss: 0.4478 - val_mse: 0.4478 - val_mae: 0.5057\n",
            "Epoch 409/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3392 - mse: 0.3392 - mae: 0.4536 - val_loss: 0.4454 - val_mse: 0.4454 - val_mae: 0.5019\n",
            "Epoch 410/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3310 - mse: 0.3310 - mae: 0.4465 - val_loss: 0.4550 - val_mse: 0.4550 - val_mae: 0.5141\n",
            "Epoch 411/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3154 - mse: 0.3154 - mae: 0.4372 - val_loss: 0.4494 - val_mse: 0.4494 - val_mae: 0.5111\n",
            "Epoch 412/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3294 - mse: 0.3294 - mae: 0.4417 - val_loss: 0.4479 - val_mse: 0.4479 - val_mae: 0.4946\n",
            "Epoch 413/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3258 - mse: 0.3258 - mae: 0.4414 - val_loss: 0.4631 - val_mse: 0.4631 - val_mae: 0.5220\n",
            "Epoch 414/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3166 - mse: 0.3166 - mae: 0.4357 - val_loss: 0.4394 - val_mse: 0.4394 - val_mae: 0.4956\n",
            "Epoch 415/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3145 - mse: 0.3145 - mae: 0.4348 - val_loss: 0.4573 - val_mse: 0.4573 - val_mae: 0.5197\n",
            "Epoch 416/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3305 - mse: 0.3305 - mae: 0.4494 - val_loss: 0.4548 - val_mse: 0.4548 - val_mae: 0.5219\n",
            "Epoch 417/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3183 - mse: 0.3183 - mae: 0.4419 - val_loss: 0.4458 - val_mse: 0.4458 - val_mae: 0.4996\n",
            "Epoch 418/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3451 - mse: 0.3451 - mae: 0.4572 - val_loss: 0.4509 - val_mse: 0.4509 - val_mae: 0.5016\n",
            "Epoch 419/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3164 - mse: 0.3164 - mae: 0.4367 - val_loss: 0.4495 - val_mse: 0.4495 - val_mae: 0.5145\n",
            "Epoch 420/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3254 - mse: 0.3254 - mae: 0.4433 - val_loss: 0.4453 - val_mse: 0.4453 - val_mae: 0.5018\n",
            "Epoch 421/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3326 - mse: 0.3326 - mae: 0.4568 - val_loss: 0.4415 - val_mse: 0.4415 - val_mae: 0.4982\n",
            "Epoch 422/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3230 - mse: 0.3230 - mae: 0.4384 - val_loss: 0.4643 - val_mse: 0.4643 - val_mae: 0.5241\n",
            "Epoch 423/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3273 - mse: 0.3273 - mae: 0.4453 - val_loss: 0.4422 - val_mse: 0.4422 - val_mae: 0.4930\n",
            "Epoch 424/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3491 - mse: 0.3491 - mae: 0.4562 - val_loss: 0.4505 - val_mse: 0.4505 - val_mae: 0.4983\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.4544 - mse: 0.4544 - mae: 0.4976\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 14ms/step - loss: 31.5075 - mse: 31.5075 - mae: 5.4400 - val_loss: 26.1058 - val_mse: 26.1058 - val_mae: 4.9562\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 24.3256 - mse: 24.3256 - mae: 4.7287 - val_loss: 19.8793 - val_mse: 19.8793 - val_mae: 4.2592\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 18.5447 - mse: 18.5447 - mae: 4.0614 - val_loss: 15.0032 - val_mse: 15.0032 - val_mae: 3.6451\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 14.3773 - mse: 14.3773 - mae: 3.4978 - val_loss: 11.2164 - val_mse: 11.2164 - val_mae: 3.0830\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 11.0679 - mse: 11.0679 - mae: 3.0151 - val_loss: 8.5521 - val_mse: 8.5521 - val_mae: 2.6260\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 8.9128 - mse: 8.9128 - mae: 2.6298 - val_loss: 6.7436 - val_mse: 6.7436 - val_mae: 2.2744\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 7.1772 - mse: 7.1772 - mae: 2.3132 - val_loss: 5.5538 - val_mse: 5.5538 - val_mae: 2.0228\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5.9439 - mse: 5.9439 - mae: 2.0758 - val_loss: 4.7577 - val_mse: 4.7577 - val_mae: 1.8425\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5.3630 - mse: 5.3630 - mae: 1.9711 - val_loss: 4.1833 - val_mse: 4.1833 - val_mae: 1.7097\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 4.7510 - mse: 4.7510 - mae: 1.8231 - val_loss: 3.7395 - val_mse: 3.7395 - val_mae: 1.6060\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 4.0378 - mse: 4.0378 - mae: 1.6746 - val_loss: 3.4048 - val_mse: 3.4048 - val_mae: 1.5233\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 3.6952 - mse: 3.6952 - mae: 1.5820 - val_loss: 3.1404 - val_mse: 3.1404 - val_mae: 1.4519\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 3.5020 - mse: 3.5020 - mae: 1.5271 - val_loss: 2.9006 - val_mse: 2.9006 - val_mae: 1.3884\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 3.1836 - mse: 3.1836 - mae: 1.4309 - val_loss: 2.7019 - val_mse: 2.7019 - val_mae: 1.3383\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 2.8868 - mse: 2.8868 - mae: 1.3692 - val_loss: 2.5413 - val_mse: 2.5413 - val_mae: 1.2917\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.7241 - mse: 2.7241 - mae: 1.3278 - val_loss: 2.4010 - val_mse: 2.4010 - val_mae: 1.2485\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.4575 - mse: 2.4575 - mae: 1.2683 - val_loss: 2.2920 - val_mse: 2.2920 - val_mae: 1.2159\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 2.4754 - mse: 2.4754 - mae: 1.2565 - val_loss: 2.1858 - val_mse: 2.1858 - val_mae: 1.1889\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.3687 - mse: 2.3687 - mae: 1.2367 - val_loss: 2.0863 - val_mse: 2.0863 - val_mae: 1.1641\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.1846 - mse: 2.1846 - mae: 1.1949 - val_loss: 2.0035 - val_mse: 2.0035 - val_mae: 1.1418\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.0429 - mse: 2.0429 - mae: 1.1316 - val_loss: 1.9218 - val_mse: 1.9218 - val_mae: 1.1188\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 2.0094 - mse: 2.0094 - mae: 1.1353 - val_loss: 1.8587 - val_mse: 1.8587 - val_mae: 1.1015\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.8871 - mse: 1.8871 - mae: 1.0990 - val_loss: 1.7867 - val_mse: 1.7867 - val_mae: 1.0812\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.8019 - mse: 1.8019 - mae: 1.0794 - val_loss: 1.7161 - val_mse: 1.7161 - val_mae: 1.0570\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.8414 - mse: 1.8414 - mae: 1.0767 - val_loss: 1.6515 - val_mse: 1.6515 - val_mae: 1.0344\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.7105 - mse: 1.7105 - mae: 1.0394 - val_loss: 1.6005 - val_mse: 1.6005 - val_mae: 1.0178\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.6072 - mse: 1.6072 - mae: 1.0011 - val_loss: 1.5536 - val_mse: 1.5536 - val_mae: 1.0006\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.5929 - mse: 1.5929 - mae: 1.0149 - val_loss: 1.5027 - val_mse: 1.5027 - val_mae: 0.9815\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.4398 - mse: 1.4398 - mae: 0.9401 - val_loss: 1.4605 - val_mse: 1.4605 - val_mae: 0.9641\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.3930 - mse: 1.3930 - mae: 0.9328 - val_loss: 1.4194 - val_mse: 1.4194 - val_mae: 0.9498\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.4057 - mse: 1.4057 - mae: 0.9342 - val_loss: 1.3809 - val_mse: 1.3809 - val_mae: 0.9330\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.3811 - mse: 1.3811 - mae: 0.9356 - val_loss: 1.3509 - val_mse: 1.3509 - val_mae: 0.9225\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.2986 - mse: 1.2986 - mae: 0.9022 - val_loss: 1.3152 - val_mse: 1.3152 - val_mae: 0.9084\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.3253 - mse: 1.3253 - mae: 0.9048 - val_loss: 1.2878 - val_mse: 1.2878 - val_mae: 0.8966\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.3016 - mse: 1.3016 - mae: 0.9017 - val_loss: 1.2526 - val_mse: 1.2526 - val_mae: 0.8835\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.2023 - mse: 1.2023 - mae: 0.8502 - val_loss: 1.2254 - val_mse: 1.2254 - val_mae: 0.8733\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.2890 - mse: 1.2890 - mae: 0.8856 - val_loss: 1.1998 - val_mse: 1.1998 - val_mae: 0.8639\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.2180 - mse: 1.2180 - mae: 0.8668 - val_loss: 1.1735 - val_mse: 1.1735 - val_mae: 0.8540\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.2166 - mse: 1.2166 - mae: 0.8665 - val_loss: 1.1493 - val_mse: 1.1493 - val_mae: 0.8433\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1429 - mse: 1.1429 - mae: 0.8387 - val_loss: 1.1141 - val_mse: 1.1141 - val_mae: 0.8282\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.1311 - mse: 1.1311 - mae: 0.8269 - val_loss: 1.0962 - val_mse: 1.0962 - val_mae: 0.8166\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1287 - mse: 1.1287 - mae: 0.8355 - val_loss: 1.0816 - val_mse: 1.0816 - val_mae: 0.8049\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0262 - mse: 1.0262 - mae: 0.7858 - val_loss: 1.0526 - val_mse: 1.0526 - val_mae: 0.7938\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0684 - mse: 1.0684 - mae: 0.8165 - val_loss: 1.0362 - val_mse: 1.0362 - val_mae: 0.7875\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0896 - mse: 1.0896 - mae: 0.8116 - val_loss: 1.0098 - val_mse: 1.0098 - val_mae: 0.7797\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0517 - mse: 1.0517 - mae: 0.8000 - val_loss: 0.9902 - val_mse: 0.9902 - val_mae: 0.7713\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.9892 - mse: 0.9892 - mae: 0.7903 - val_loss: 0.9729 - val_mse: 0.9729 - val_mae: 0.7646\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.0146 - mse: 1.0146 - mae: 0.7883 - val_loss: 0.9473 - val_mse: 0.9473 - val_mae: 0.7539\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.9529 - mse: 0.9529 - mae: 0.7579 - val_loss: 0.9320 - val_mse: 0.9320 - val_mae: 0.7440\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9338 - mse: 0.9338 - mae: 0.7546 - val_loss: 0.9179 - val_mse: 0.9179 - val_mae: 0.7317\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8899 - mse: 0.8899 - mae: 0.7290 - val_loss: 0.9068 - val_mse: 0.9068 - val_mae: 0.7290\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.9218 - mse: 0.9218 - mae: 0.7591 - val_loss: 0.9038 - val_mse: 0.9038 - val_mae: 0.7227\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8607 - mse: 0.8607 - mae: 0.7293 - val_loss: 0.8937 - val_mse: 0.8937 - val_mae: 0.7214\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.8412 - mse: 0.8412 - mae: 0.7080 - val_loss: 0.8751 - val_mse: 0.8751 - val_mae: 0.7153\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8742 - mse: 0.8742 - mae: 0.7309 - val_loss: 0.8567 - val_mse: 0.8567 - val_mae: 0.7067\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8143 - mse: 0.8143 - mae: 0.7057 - val_loss: 0.8482 - val_mse: 0.8482 - val_mae: 0.7007\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.8148 - mse: 0.8148 - mae: 0.7031 - val_loss: 0.8328 - val_mse: 0.8328 - val_mae: 0.6930\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7768 - mse: 0.7768 - mae: 0.6874 - val_loss: 0.8241 - val_mse: 0.8241 - val_mae: 0.6865\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7826 - mse: 0.7826 - mae: 0.6845 - val_loss: 0.8136 - val_mse: 0.8136 - val_mae: 0.6847\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7624 - mse: 0.7624 - mae: 0.6833 - val_loss: 0.7999 - val_mse: 0.7999 - val_mae: 0.6800\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7932 - mse: 0.7932 - mae: 0.6892 - val_loss: 0.7913 - val_mse: 0.7913 - val_mae: 0.6742\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8049 - mse: 0.8049 - mae: 0.7029 - val_loss: 0.7870 - val_mse: 0.7870 - val_mae: 0.6744\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7826 - mse: 0.7826 - mae: 0.6834 - val_loss: 0.7783 - val_mse: 0.7783 - val_mae: 0.6712\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7994 - mse: 0.7994 - mae: 0.6935 - val_loss: 0.7667 - val_mse: 0.7667 - val_mae: 0.6635\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7266 - mse: 0.7266 - mae: 0.6670 - val_loss: 0.7531 - val_mse: 0.7531 - val_mae: 0.6581\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7131 - mse: 0.7131 - mae: 0.6561 - val_loss: 0.7443 - val_mse: 0.7443 - val_mae: 0.6516\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6825 - mse: 0.6825 - mae: 0.6469 - val_loss: 0.7319 - val_mse: 0.7319 - val_mae: 0.6469\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7098 - mse: 0.7098 - mae: 0.6471 - val_loss: 0.7285 - val_mse: 0.7285 - val_mae: 0.6444\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7064 - mse: 0.7064 - mae: 0.6544 - val_loss: 0.7199 - val_mse: 0.7199 - val_mae: 0.6391\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7039 - mse: 0.7039 - mae: 0.6534 - val_loss: 0.7167 - val_mse: 0.7167 - val_mae: 0.6365\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6692 - mse: 0.6692 - mae: 0.6392 - val_loss: 0.7049 - val_mse: 0.7049 - val_mae: 0.6359\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6885 - mse: 0.6885 - mae: 0.6383 - val_loss: 0.6958 - val_mse: 0.6958 - val_mae: 0.6279\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6934 - mse: 0.6934 - mae: 0.6453 - val_loss: 0.6850 - val_mse: 0.6850 - val_mae: 0.6293\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7082 - mse: 0.7082 - mae: 0.6557 - val_loss: 0.6771 - val_mse: 0.6771 - val_mae: 0.6174\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6756 - mse: 0.6756 - mae: 0.6466 - val_loss: 0.6652 - val_mse: 0.6652 - val_mae: 0.6177\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6233 - mse: 0.6233 - mae: 0.6122 - val_loss: 0.6639 - val_mse: 0.6639 - val_mae: 0.6133\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6467 - mse: 0.6467 - mae: 0.6189 - val_loss: 0.6602 - val_mse: 0.6602 - val_mae: 0.6162\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6086 - mse: 0.6086 - mae: 0.5975 - val_loss: 0.6569 - val_mse: 0.6569 - val_mae: 0.6103\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6429 - mse: 0.6429 - mae: 0.6158 - val_loss: 0.6532 - val_mse: 0.6532 - val_mae: 0.6066\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5983 - mse: 0.5983 - mae: 0.5987 - val_loss: 0.6373 - val_mse: 0.6373 - val_mae: 0.6024\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6213 - mse: 0.6213 - mae: 0.6064 - val_loss: 0.6313 - val_mse: 0.6313 - val_mae: 0.6001\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6320 - mse: 0.6320 - mae: 0.6223 - val_loss: 0.6256 - val_mse: 0.6256 - val_mae: 0.5956\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6111 - mse: 0.6111 - mae: 0.6098 - val_loss: 0.6218 - val_mse: 0.6218 - val_mae: 0.5951\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5807 - mse: 0.5807 - mae: 0.5985 - val_loss: 0.6160 - val_mse: 0.6160 - val_mae: 0.5916\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5830 - mse: 0.5830 - mae: 0.5871 - val_loss: 0.6178 - val_mse: 0.6178 - val_mae: 0.5885\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5946 - mse: 0.5946 - mae: 0.5972 - val_loss: 0.6115 - val_mse: 0.6115 - val_mae: 0.5859\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5739 - mse: 0.5739 - mae: 0.5874 - val_loss: 0.6100 - val_mse: 0.6100 - val_mae: 0.5867\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5726 - mse: 0.5726 - mae: 0.5847 - val_loss: 0.6031 - val_mse: 0.6031 - val_mae: 0.5865\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6091 - mse: 0.6091 - mae: 0.6048 - val_loss: 0.5955 - val_mse: 0.5955 - val_mae: 0.5822\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5788 - mse: 0.5788 - mae: 0.5889 - val_loss: 0.5926 - val_mse: 0.5926 - val_mae: 0.5786\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5683 - mse: 0.5683 - mae: 0.5859 - val_loss: 0.5892 - val_mse: 0.5892 - val_mae: 0.5776\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5513 - mse: 0.5513 - mae: 0.5743 - val_loss: 0.5857 - val_mse: 0.5857 - val_mae: 0.5761\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5609 - mse: 0.5609 - mae: 0.5768 - val_loss: 0.5852 - val_mse: 0.5852 - val_mae: 0.5729\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5622 - mse: 0.5622 - mae: 0.5893 - val_loss: 0.5787 - val_mse: 0.5787 - val_mae: 0.5729\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5243 - mse: 0.5243 - mae: 0.5609 - val_loss: 0.5734 - val_mse: 0.5734 - val_mae: 0.5700\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5315 - mse: 0.5315 - mae: 0.5667 - val_loss: 0.5715 - val_mse: 0.5715 - val_mae: 0.5698\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5477 - mse: 0.5477 - mae: 0.5696 - val_loss: 0.5694 - val_mse: 0.5694 - val_mae: 0.5690\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5359 - mse: 0.5359 - mae: 0.5635 - val_loss: 0.5704 - val_mse: 0.5704 - val_mae: 0.5702\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5317 - mse: 0.5317 - mae: 0.5638 - val_loss: 0.5595 - val_mse: 0.5595 - val_mae: 0.5656\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5277 - mse: 0.5277 - mae: 0.5546 - val_loss: 0.5588 - val_mse: 0.5588 - val_mae: 0.5622\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5461 - mse: 0.5461 - mae: 0.5629 - val_loss: 0.5507 - val_mse: 0.5507 - val_mae: 0.5581\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5185 - mse: 0.5185 - mae: 0.5544 - val_loss: 0.5414 - val_mse: 0.5414 - val_mae: 0.5558\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5337 - mse: 0.5337 - mae: 0.5516 - val_loss: 0.5506 - val_mse: 0.5506 - val_mae: 0.5551\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5052 - mse: 0.5052 - mae: 0.5447 - val_loss: 0.5484 - val_mse: 0.5484 - val_mae: 0.5604\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5253 - mse: 0.5253 - mae: 0.5606 - val_loss: 0.5351 - val_mse: 0.5351 - val_mae: 0.5508\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5099 - mse: 0.5099 - mae: 0.5554 - val_loss: 0.5399 - val_mse: 0.5399 - val_mae: 0.5509\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5187 - mse: 0.5187 - mae: 0.5467 - val_loss: 0.5289 - val_mse: 0.5289 - val_mae: 0.5467\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4892 - mse: 0.4892 - mae: 0.5408 - val_loss: 0.5278 - val_mse: 0.5278 - val_mae: 0.5461\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4811 - mse: 0.4811 - mae: 0.5365 - val_loss: 0.5230 - val_mse: 0.5230 - val_mae: 0.5447\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4774 - mse: 0.4774 - mae: 0.5360 - val_loss: 0.5134 - val_mse: 0.5134 - val_mae: 0.5414\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5014 - mse: 0.5014 - mae: 0.5478 - val_loss: 0.5130 - val_mse: 0.5130 - val_mae: 0.5404\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5088 - mse: 0.5088 - mae: 0.5542 - val_loss: 0.5117 - val_mse: 0.5117 - val_mae: 0.5356\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4949 - mse: 0.4949 - mae: 0.5361 - val_loss: 0.5068 - val_mse: 0.5068 - val_mae: 0.5341\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4915 - mse: 0.4915 - mae: 0.5413 - val_loss: 0.5093 - val_mse: 0.5093 - val_mae: 0.5353\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5091 - mse: 0.5091 - mae: 0.5527 - val_loss: 0.5103 - val_mse: 0.5103 - val_mae: 0.5379\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4980 - mse: 0.4980 - mae: 0.5419 - val_loss: 0.5011 - val_mse: 0.5011 - val_mae: 0.5315\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4826 - mse: 0.4826 - mae: 0.5388 - val_loss: 0.4998 - val_mse: 0.4998 - val_mae: 0.5303\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4706 - mse: 0.4706 - mae: 0.5322 - val_loss: 0.4961 - val_mse: 0.4961 - val_mae: 0.5258\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4955 - mse: 0.4955 - mae: 0.5423 - val_loss: 0.4939 - val_mse: 0.4939 - val_mae: 0.5285\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4733 - mse: 0.4733 - mae: 0.5312 - val_loss: 0.4935 - val_mse: 0.4935 - val_mae: 0.5252\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4591 - mse: 0.4591 - mae: 0.5234 - val_loss: 0.4859 - val_mse: 0.4859 - val_mae: 0.5238\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4616 - mse: 0.4616 - mae: 0.5242 - val_loss: 0.4834 - val_mse: 0.4834 - val_mae: 0.5220\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4623 - mse: 0.4623 - mae: 0.5227 - val_loss: 0.4881 - val_mse: 0.4881 - val_mae: 0.5224\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4842 - mse: 0.4842 - mae: 0.5370 - val_loss: 0.4858 - val_mse: 0.4858 - val_mae: 0.5252\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4763 - mse: 0.4763 - mae: 0.5330 - val_loss: 0.4794 - val_mse: 0.4794 - val_mae: 0.5184\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4939 - mse: 0.4939 - mae: 0.5433 - val_loss: 0.4765 - val_mse: 0.4765 - val_mae: 0.5166\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4810 - mse: 0.4810 - mae: 0.5298 - val_loss: 0.4726 - val_mse: 0.4726 - val_mae: 0.5172\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4527 - mse: 0.4527 - mae: 0.5228 - val_loss: 0.4662 - val_mse: 0.4662 - val_mae: 0.5161\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4739 - mse: 0.4739 - mae: 0.5372 - val_loss: 0.4718 - val_mse: 0.4718 - val_mae: 0.5176\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4453 - mse: 0.4453 - mae: 0.5202 - val_loss: 0.4633 - val_mse: 0.4633 - val_mae: 0.5117\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4639 - mse: 0.4639 - mae: 0.5256 - val_loss: 0.4606 - val_mse: 0.4606 - val_mae: 0.5093\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4447 - mse: 0.4447 - mae: 0.5146 - val_loss: 0.4626 - val_mse: 0.4626 - val_mae: 0.5112\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4426 - mse: 0.4426 - mae: 0.5096 - val_loss: 0.4611 - val_mse: 0.4611 - val_mae: 0.5102\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4356 - mse: 0.4356 - mae: 0.5116 - val_loss: 0.4570 - val_mse: 0.4570 - val_mae: 0.5076\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4588 - mse: 0.4588 - mae: 0.5243 - val_loss: 0.4496 - val_mse: 0.4496 - val_mae: 0.5060\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4461 - mse: 0.4461 - mae: 0.5136 - val_loss: 0.4483 - val_mse: 0.4483 - val_mae: 0.5032\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4330 - mse: 0.4330 - mae: 0.5077 - val_loss: 0.4541 - val_mse: 0.4541 - val_mae: 0.5061\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4383 - mse: 0.4383 - mae: 0.5073 - val_loss: 0.4475 - val_mse: 0.4475 - val_mae: 0.5038\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4548 - mse: 0.4548 - mae: 0.5306 - val_loss: 0.4400 - val_mse: 0.4400 - val_mae: 0.5016\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4403 - mse: 0.4403 - mae: 0.5159 - val_loss: 0.4402 - val_mse: 0.4402 - val_mae: 0.4991\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4461 - mse: 0.4461 - mae: 0.5064 - val_loss: 0.4466 - val_mse: 0.4466 - val_mae: 0.5008\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4419 - mse: 0.4419 - mae: 0.5145 - val_loss: 0.4377 - val_mse: 0.4377 - val_mae: 0.4979\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4150 - mse: 0.4150 - mae: 0.4984 - val_loss: 0.4433 - val_mse: 0.4433 - val_mae: 0.5007\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4203 - mse: 0.4203 - mae: 0.4967 - val_loss: 0.4422 - val_mse: 0.4422 - val_mae: 0.4994\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4241 - mse: 0.4241 - mae: 0.4964 - val_loss: 0.4363 - val_mse: 0.4363 - val_mae: 0.4967\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4261 - mse: 0.4261 - mae: 0.5090 - val_loss: 0.4293 - val_mse: 0.4293 - val_mae: 0.4923\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4494 - mse: 0.4494 - mae: 0.5129 - val_loss: 0.4358 - val_mse: 0.4358 - val_mae: 0.4950\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4248 - mse: 0.4248 - mae: 0.5076 - val_loss: 0.4258 - val_mse: 0.4258 - val_mae: 0.4884\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4229 - mse: 0.4229 - mae: 0.4939 - val_loss: 0.4387 - val_mse: 0.4387 - val_mae: 0.4979\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4273 - mse: 0.4273 - mae: 0.5040 - val_loss: 0.4234 - val_mse: 0.4234 - val_mae: 0.4895\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4200 - mse: 0.4200 - mae: 0.5056 - val_loss: 0.4278 - val_mse: 0.4278 - val_mae: 0.4930\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4405 - mse: 0.4405 - mae: 0.5129 - val_loss: 0.4274 - val_mse: 0.4274 - val_mae: 0.4950\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4131 - mse: 0.4131 - mae: 0.4981 - val_loss: 0.4223 - val_mse: 0.4223 - val_mae: 0.4885\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4259 - mse: 0.4259 - mae: 0.5035 - val_loss: 0.4237 - val_mse: 0.4237 - val_mae: 0.4875\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4235 - mse: 0.4235 - mae: 0.5043 - val_loss: 0.4221 - val_mse: 0.4221 - val_mae: 0.4893\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4160 - mse: 0.4160 - mae: 0.5008 - val_loss: 0.4188 - val_mse: 0.4188 - val_mae: 0.4895\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4481 - mse: 0.4481 - mae: 0.5087 - val_loss: 0.4243 - val_mse: 0.4243 - val_mae: 0.4875\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4354 - mse: 0.4354 - mae: 0.5145 - val_loss: 0.4093 - val_mse: 0.4093 - val_mae: 0.4795\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4292 - mse: 0.4292 - mae: 0.5031 - val_loss: 0.4154 - val_mse: 0.4154 - val_mae: 0.4842\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4073 - mse: 0.4073 - mae: 0.4921 - val_loss: 0.4192 - val_mse: 0.4192 - val_mae: 0.4851\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4277 - mse: 0.4277 - mae: 0.5101 - val_loss: 0.4058 - val_mse: 0.4058 - val_mae: 0.4796\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4144 - mse: 0.4144 - mae: 0.4976 - val_loss: 0.4082 - val_mse: 0.4082 - val_mae: 0.4806\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4211 - mse: 0.4211 - mae: 0.4979 - val_loss: 0.4154 - val_mse: 0.4154 - val_mae: 0.4850\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4383 - mse: 0.4383 - mae: 0.5207 - val_loss: 0.4101 - val_mse: 0.4101 - val_mae: 0.4835\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4261 - mse: 0.4261 - mae: 0.5105 - val_loss: 0.4122 - val_mse: 0.4122 - val_mae: 0.4798\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4265 - mse: 0.4265 - mae: 0.5064 - val_loss: 0.4158 - val_mse: 0.4158 - val_mae: 0.4874\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3884 - mse: 0.3884 - mae: 0.4897 - val_loss: 0.4028 - val_mse: 0.4028 - val_mae: 0.4757\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4007 - mse: 0.4007 - mae: 0.4925 - val_loss: 0.3992 - val_mse: 0.3992 - val_mae: 0.4756\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3973 - mse: 0.3973 - mae: 0.4902 - val_loss: 0.3996 - val_mse: 0.3996 - val_mae: 0.4770\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4099 - mse: 0.4099 - mae: 0.4970 - val_loss: 0.4033 - val_mse: 0.4033 - val_mae: 0.4761\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3982 - mse: 0.3982 - mae: 0.4896 - val_loss: 0.4081 - val_mse: 0.4081 - val_mae: 0.4782\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3956 - mse: 0.3956 - mae: 0.4854 - val_loss: 0.4003 - val_mse: 0.4003 - val_mae: 0.4742\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4113 - mse: 0.4113 - mae: 0.4956 - val_loss: 0.3997 - val_mse: 0.3997 - val_mae: 0.4721\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3995 - mse: 0.3995 - mae: 0.4933 - val_loss: 0.4073 - val_mse: 0.4073 - val_mae: 0.4788\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4144 - mse: 0.4144 - mae: 0.4975 - val_loss: 0.3993 - val_mse: 0.3993 - val_mae: 0.4767\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4120 - mse: 0.4120 - mae: 0.4939 - val_loss: 0.3973 - val_mse: 0.3973 - val_mae: 0.4750\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4015 - mse: 0.4015 - mae: 0.4777 - val_loss: 0.3948 - val_mse: 0.3948 - val_mae: 0.4728\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3955 - mse: 0.3955 - mae: 0.4840 - val_loss: 0.3982 - val_mse: 0.3982 - val_mae: 0.4765\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4050 - mse: 0.4050 - mae: 0.4932 - val_loss: 0.3935 - val_mse: 0.3935 - val_mae: 0.4726\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3890 - mse: 0.3890 - mae: 0.4847 - val_loss: 0.3942 - val_mse: 0.3942 - val_mae: 0.4745\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3953 - mse: 0.3953 - mae: 0.4801 - val_loss: 0.4055 - val_mse: 0.4055 - val_mae: 0.4826\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3859 - mse: 0.3859 - mae: 0.4840 - val_loss: 0.3966 - val_mse: 0.3966 - val_mae: 0.4745\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4168 - mse: 0.4168 - mae: 0.4999 - val_loss: 0.3905 - val_mse: 0.3905 - val_mae: 0.4725\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3916 - mse: 0.3916 - mae: 0.4839 - val_loss: 0.3929 - val_mse: 0.3929 - val_mae: 0.4717\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3974 - mse: 0.3974 - mae: 0.4850 - val_loss: 0.3947 - val_mse: 0.3947 - val_mae: 0.4730\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3829 - mse: 0.3829 - mae: 0.4782 - val_loss: 0.3855 - val_mse: 0.3855 - val_mae: 0.4698\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4027 - mse: 0.4027 - mae: 0.4945 - val_loss: 0.3865 - val_mse: 0.3865 - val_mae: 0.4708\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4091 - mse: 0.4091 - mae: 0.4984 - val_loss: 0.3912 - val_mse: 0.3912 - val_mae: 0.4738\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3765 - mse: 0.3765 - mae: 0.4782 - val_loss: 0.3847 - val_mse: 0.3847 - val_mae: 0.4679\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3877 - mse: 0.3877 - mae: 0.4763 - val_loss: 0.3942 - val_mse: 0.3942 - val_mae: 0.4753\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3851 - mse: 0.3851 - mae: 0.4804 - val_loss: 0.3871 - val_mse: 0.3871 - val_mae: 0.4685\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3933 - mse: 0.3933 - mae: 0.4858 - val_loss: 0.3859 - val_mse: 0.3859 - val_mae: 0.4698\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3909 - mse: 0.3909 - mae: 0.4829 - val_loss: 0.4023 - val_mse: 0.4023 - val_mae: 0.4815\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3994 - mse: 0.3994 - mae: 0.4938 - val_loss: 0.3836 - val_mse: 0.3836 - val_mae: 0.4652\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3928 - mse: 0.3928 - mae: 0.4846 - val_loss: 0.3965 - val_mse: 0.3965 - val_mae: 0.4785\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3867 - mse: 0.3867 - mae: 0.4921 - val_loss: 0.3827 - val_mse: 0.3827 - val_mae: 0.4656\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3848 - mse: 0.3848 - mae: 0.4771 - val_loss: 0.4007 - val_mse: 0.4007 - val_mae: 0.4798\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3925 - mse: 0.3925 - mae: 0.4909 - val_loss: 0.3839 - val_mse: 0.3839 - val_mae: 0.4663\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3864 - mse: 0.3864 - mae: 0.4814 - val_loss: 0.3840 - val_mse: 0.3840 - val_mae: 0.4689\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3917 - mse: 0.3917 - mae: 0.4804 - val_loss: 0.3794 - val_mse: 0.3794 - val_mae: 0.4676\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3845 - mse: 0.3845 - mae: 0.4778 - val_loss: 0.3837 - val_mse: 0.3837 - val_mae: 0.4705\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3872 - mse: 0.3872 - mae: 0.4847 - val_loss: 0.3866 - val_mse: 0.3866 - val_mae: 0.4736\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3863 - mse: 0.3863 - mae: 0.4801 - val_loss: 0.3794 - val_mse: 0.3794 - val_mae: 0.4694\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3819 - mse: 0.3819 - mae: 0.4770 - val_loss: 0.3827 - val_mse: 0.3827 - val_mae: 0.4734\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3897 - mse: 0.3897 - mae: 0.4775 - val_loss: 0.4023 - val_mse: 0.4023 - val_mae: 0.4854\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4036 - mse: 0.4036 - mae: 0.4867 - val_loss: 0.3727 - val_mse: 0.3727 - val_mae: 0.4633\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3866 - mse: 0.3866 - mae: 0.4820 - val_loss: 0.3759 - val_mse: 0.3759 - val_mae: 0.4656\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3764 - mse: 0.3764 - mae: 0.4706 - val_loss: 0.3845 - val_mse: 0.3845 - val_mae: 0.4736\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3748 - mse: 0.3748 - mae: 0.4814 - val_loss: 0.3801 - val_mse: 0.3801 - val_mae: 0.4717\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3760 - mse: 0.3760 - mae: 0.4647 - val_loss: 0.3870 - val_mse: 0.3870 - val_mae: 0.4757\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3837 - mse: 0.3837 - mae: 0.4897 - val_loss: 0.3728 - val_mse: 0.3728 - val_mae: 0.4648\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3847 - mse: 0.3847 - mae: 0.4810 - val_loss: 0.3768 - val_mse: 0.3768 - val_mae: 0.4684\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3640 - mse: 0.3640 - mae: 0.4638 - val_loss: 0.3724 - val_mse: 0.3724 - val_mae: 0.4623\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3841 - mse: 0.3841 - mae: 0.4726 - val_loss: 0.3907 - val_mse: 0.3907 - val_mae: 0.4743\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3865 - mse: 0.3865 - mae: 0.4786 - val_loss: 0.3728 - val_mse: 0.3728 - val_mae: 0.4648\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3843 - mse: 0.3843 - mae: 0.4822 - val_loss: 0.3811 - val_mse: 0.3811 - val_mae: 0.4700\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3709 - mse: 0.3709 - mae: 0.4766 - val_loss: 0.3774 - val_mse: 0.3774 - val_mae: 0.4683\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3736 - mse: 0.3736 - mae: 0.4798 - val_loss: 0.3794 - val_mse: 0.3794 - val_mae: 0.4692\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3801 - mse: 0.3801 - mae: 0.4851 - val_loss: 0.3716 - val_mse: 0.3716 - val_mae: 0.4636\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3857 - mse: 0.3857 - mae: 0.4836 - val_loss: 0.3726 - val_mse: 0.3726 - val_mae: 0.4640\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3693 - mse: 0.3693 - mae: 0.4681 - val_loss: 0.3838 - val_mse: 0.3838 - val_mae: 0.4726\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3875 - mse: 0.3875 - mae: 0.4814 - val_loss: 0.3663 - val_mse: 0.3663 - val_mae: 0.4609\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3676 - mse: 0.3676 - mae: 0.4814 - val_loss: 0.3682 - val_mse: 0.3682 - val_mae: 0.4638\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3670 - mse: 0.3670 - mae: 0.4688 - val_loss: 0.3684 - val_mse: 0.3684 - val_mae: 0.4601\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3726 - mse: 0.3726 - mae: 0.4680 - val_loss: 0.3732 - val_mse: 0.3732 - val_mae: 0.4666\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3793 - mse: 0.3793 - mae: 0.4821 - val_loss: 0.3684 - val_mse: 0.3684 - val_mae: 0.4632\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3874 - mse: 0.3874 - mae: 0.4796 - val_loss: 0.3920 - val_mse: 0.3920 - val_mae: 0.4813\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3720 - mse: 0.3720 - mae: 0.4715 - val_loss: 0.3670 - val_mse: 0.3670 - val_mae: 0.4606\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3981 - mse: 0.3981 - mae: 0.4893 - val_loss: 0.3882 - val_mse: 0.3882 - val_mae: 0.4758\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3629 - mse: 0.3629 - mae: 0.4636 - val_loss: 0.3729 - val_mse: 0.3729 - val_mae: 0.4667\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3561 - mse: 0.3561 - mae: 0.4655 - val_loss: 0.3720 - val_mse: 0.3720 - val_mae: 0.4673\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3880 - mse: 0.3880 - mae: 0.4756 - val_loss: 0.3736 - val_mse: 0.3736 - val_mae: 0.4694\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3680 - mse: 0.3680 - mae: 0.4690 - val_loss: 0.3622 - val_mse: 0.3622 - val_mae: 0.4599\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3656 - mse: 0.3656 - mae: 0.4683 - val_loss: 0.3632 - val_mse: 0.3632 - val_mae: 0.4601\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3673 - mse: 0.3673 - mae: 0.4631 - val_loss: 0.3737 - val_mse: 0.3737 - val_mae: 0.4681\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3776 - mse: 0.3776 - mae: 0.4756 - val_loss: 0.3682 - val_mse: 0.3682 - val_mae: 0.4641\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3720 - mse: 0.3720 - mae: 0.4722 - val_loss: 0.3711 - val_mse: 0.3711 - val_mae: 0.4665\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3728 - mse: 0.3728 - mae: 0.4724 - val_loss: 0.3651 - val_mse: 0.3651 - val_mae: 0.4603\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3972 - mse: 0.3972 - mae: 0.4916 - val_loss: 0.3688 - val_mse: 0.3688 - val_mae: 0.4657\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3590 - mse: 0.3590 - mae: 0.4642 - val_loss: 0.3862 - val_mse: 0.3862 - val_mae: 0.4762\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3657 - mse: 0.3657 - mae: 0.4730 - val_loss: 0.3656 - val_mse: 0.3656 - val_mae: 0.4603\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3826 - mse: 0.3826 - mae: 0.4726 - val_loss: 0.3829 - val_mse: 0.3829 - val_mae: 0.4743\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3878 - mse: 0.3878 - mae: 0.4843 - val_loss: 0.3707 - val_mse: 0.3707 - val_mae: 0.4677\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3673 - mse: 0.3673 - mae: 0.4655 - val_loss: 0.3683 - val_mse: 0.3683 - val_mae: 0.4641\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3673 - mse: 0.3673 - mae: 0.4705 - val_loss: 0.3689 - val_mse: 0.3689 - val_mae: 0.4669\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3729 - mse: 0.3729 - mae: 0.4748 - val_loss: 0.3743 - val_mse: 0.3743 - val_mae: 0.4692\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3700 - mse: 0.3700 - mae: 0.4766 - val_loss: 0.3751 - val_mse: 0.3751 - val_mae: 0.4683\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3697 - mse: 0.3697 - mae: 0.4710 - val_loss: 0.3727 - val_mse: 0.3727 - val_mae: 0.4705\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3530 - mse: 0.3530 - mae: 0.4630 - val_loss: 0.3680 - val_mse: 0.3680 - val_mae: 0.4639\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3934 - mse: 0.3934 - mae: 0.4811 - val_loss: 0.3683 - val_mse: 0.3683 - val_mae: 0.4647\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3791 - mse: 0.3791 - mae: 0.4755 - val_loss: 0.3687 - val_mse: 0.3687 - val_mae: 0.4666\n",
            "Epoch 252/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3680 - mse: 0.3680 - mae: 0.4665 - val_loss: 0.3699 - val_mse: 0.3699 - val_mae: 0.4681\n",
            "Epoch 253/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3703 - mse: 0.3703 - mae: 0.4763 - val_loss: 0.3741 - val_mse: 0.3741 - val_mae: 0.4701\n",
            "Epoch 254/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3581 - mse: 0.3581 - mae: 0.4595 - val_loss: 0.3693 - val_mse: 0.3693 - val_mae: 0.4638\n",
            "Epoch 255/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3586 - mse: 0.3586 - mae: 0.4649 - val_loss: 0.3704 - val_mse: 0.3704 - val_mae: 0.4672\n",
            "Epoch 256/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3587 - mse: 0.3587 - mae: 0.4640 - val_loss: 0.3746 - val_mse: 0.3746 - val_mae: 0.4703\n",
            "Epoch 257/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3779 - mse: 0.3779 - mae: 0.4778 - val_loss: 0.3666 - val_mse: 0.3666 - val_mae: 0.4641\n",
            "Epoch 258/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3563 - mse: 0.3563 - mae: 0.4667 - val_loss: 0.3783 - val_mse: 0.3783 - val_mae: 0.4747\n",
            "Epoch 259/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3678 - mse: 0.3678 - mae: 0.4664 - val_loss: 0.3683 - val_mse: 0.3683 - val_mae: 0.4654\n",
            "Epoch 260/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3637 - mse: 0.3637 - mae: 0.4661 - val_loss: 0.3744 - val_mse: 0.3744 - val_mae: 0.4729\n",
            "Epoch 261/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3532 - mse: 0.3532 - mae: 0.4595 - val_loss: 0.3736 - val_mse: 0.3736 - val_mae: 0.4703\n",
            "Epoch 262/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3659 - mse: 0.3659 - mae: 0.4724 - val_loss: 0.3766 - val_mse: 0.3766 - val_mae: 0.4718\n",
            "Epoch 263/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3468 - mse: 0.3468 - mae: 0.4553 - val_loss: 0.3719 - val_mse: 0.3719 - val_mae: 0.4679\n",
            "Epoch 264/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3799 - mse: 0.3799 - mae: 0.4729 - val_loss: 0.3688 - val_mse: 0.3688 - val_mae: 0.4662\n",
            "Epoch 265/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3679 - mse: 0.3679 - mae: 0.4666 - val_loss: 0.3751 - val_mse: 0.3751 - val_mae: 0.4700\n",
            "Epoch 266/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3515 - mse: 0.3515 - mae: 0.4598 - val_loss: 0.3728 - val_mse: 0.3728 - val_mae: 0.4664\n",
            "Epoch 267/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3620 - mse: 0.3620 - mae: 0.4626 - val_loss: 0.3788 - val_mse: 0.3788 - val_mae: 0.4762\n",
            "Epoch 268/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3534 - mse: 0.3534 - mae: 0.4689 - val_loss: 0.3720 - val_mse: 0.3720 - val_mae: 0.4692\n",
            "Epoch 269/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3603 - mse: 0.3603 - mae: 0.4696 - val_loss: 0.3704 - val_mse: 0.3704 - val_mae: 0.4659\n",
            "Epoch 270/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3809 - mse: 0.3809 - mae: 0.4786 - val_loss: 0.3779 - val_mse: 0.3779 - val_mae: 0.4751\n",
            "Epoch 271/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3709 - mse: 0.3709 - mae: 0.4754 - val_loss: 0.3684 - val_mse: 0.3684 - val_mae: 0.4672\n",
            "Epoch 272/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3656 - mse: 0.3656 - mae: 0.4696 - val_loss: 0.3693 - val_mse: 0.3693 - val_mae: 0.4685\n",
            "Epoch 273/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3739 - mse: 0.3739 - mae: 0.4737 - val_loss: 0.3587 - val_mse: 0.3587 - val_mae: 0.4598\n",
            "Epoch 274/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3727 - mse: 0.3727 - mae: 0.4700 - val_loss: 0.3856 - val_mse: 0.3856 - val_mae: 0.4792\n",
            "Epoch 275/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3735 - mse: 0.3735 - mae: 0.4757 - val_loss: 0.3686 - val_mse: 0.3686 - val_mae: 0.4645\n",
            "Epoch 276/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3636 - mse: 0.3636 - mae: 0.4693 - val_loss: 0.3700 - val_mse: 0.3700 - val_mae: 0.4690\n",
            "Epoch 277/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3605 - mse: 0.3605 - mae: 0.4707 - val_loss: 0.3816 - val_mse: 0.3816 - val_mae: 0.4760\n",
            "Epoch 278/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3362 - mse: 0.3362 - mae: 0.4453 - val_loss: 0.3674 - val_mse: 0.3674 - val_mae: 0.4672\n",
            "Epoch 279/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3622 - mse: 0.3622 - mae: 0.4688 - val_loss: 0.3646 - val_mse: 0.3646 - val_mae: 0.4634\n",
            "Epoch 280/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3540 - mse: 0.3540 - mae: 0.4561 - val_loss: 0.3798 - val_mse: 0.3798 - val_mae: 0.4726\n",
            "Epoch 281/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3563 - mse: 0.3563 - mae: 0.4626 - val_loss: 0.3687 - val_mse: 0.3687 - val_mae: 0.4675\n",
            "Epoch 282/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3461 - mse: 0.3461 - mae: 0.4575 - val_loss: 0.3754 - val_mse: 0.3754 - val_mae: 0.4717\n",
            "Epoch 283/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3701 - mse: 0.3701 - mae: 0.4606 - val_loss: 0.3701 - val_mse: 0.3701 - val_mae: 0.4683\n",
            "Epoch 284/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3528 - mse: 0.3528 - mae: 0.4583 - val_loss: 0.3632 - val_mse: 0.3632 - val_mae: 0.4623\n",
            "Epoch 285/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3509 - mse: 0.3509 - mae: 0.4566 - val_loss: 0.3748 - val_mse: 0.3748 - val_mae: 0.4694\n",
            "Epoch 286/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3528 - mse: 0.3528 - mae: 0.4597 - val_loss: 0.3772 - val_mse: 0.3772 - val_mae: 0.4733\n",
            "Epoch 287/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3612 - mse: 0.3612 - mae: 0.4705 - val_loss: 0.3695 - val_mse: 0.3695 - val_mae: 0.4682\n",
            "Epoch 288/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3460 - mse: 0.3460 - mae: 0.4565 - val_loss: 0.3740 - val_mse: 0.3740 - val_mae: 0.4695\n",
            "Epoch 289/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3514 - mse: 0.3514 - mae: 0.4539 - val_loss: 0.3799 - val_mse: 0.3799 - val_mae: 0.4744\n",
            "Epoch 290/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3537 - mse: 0.3537 - mae: 0.4586 - val_loss: 0.3789 - val_mse: 0.3789 - val_mae: 0.4742\n",
            "Epoch 291/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3488 - mse: 0.3488 - mae: 0.4533 - val_loss: 0.3757 - val_mse: 0.3757 - val_mae: 0.4715\n",
            "Epoch 292/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3560 - mse: 0.3560 - mae: 0.4637 - val_loss: 0.3821 - val_mse: 0.3821 - val_mae: 0.4762\n",
            "Epoch 293/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3603 - mse: 0.3603 - mae: 0.4562 - val_loss: 0.3741 - val_mse: 0.3741 - val_mae: 0.4715\n",
            "Epoch 294/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3501 - mse: 0.3501 - mae: 0.4514 - val_loss: 0.3806 - val_mse: 0.3806 - val_mae: 0.4741\n",
            "Epoch 295/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3405 - mse: 0.3405 - mae: 0.4565 - val_loss: 0.3706 - val_mse: 0.3706 - val_mae: 0.4661\n",
            "Epoch 296/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3624 - mse: 0.3624 - mae: 0.4689 - val_loss: 0.3699 - val_mse: 0.3699 - val_mae: 0.4693\n",
            "Epoch 297/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3643 - mse: 0.3643 - mae: 0.4702 - val_loss: 0.3858 - val_mse: 0.3858 - val_mae: 0.4813\n",
            "Epoch 298/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3531 - mse: 0.3531 - mae: 0.4726 - val_loss: 0.3729 - val_mse: 0.3729 - val_mae: 0.4703\n",
            "Epoch 299/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3648 - mse: 0.3648 - mae: 0.4662 - val_loss: 0.3820 - val_mse: 0.3820 - val_mae: 0.4769\n",
            "Epoch 300/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3564 - mse: 0.3564 - mae: 0.4677 - val_loss: 0.3638 - val_mse: 0.3638 - val_mae: 0.4636\n",
            "Epoch 301/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3561 - mse: 0.3561 - mae: 0.4620 - val_loss: 0.3725 - val_mse: 0.3725 - val_mae: 0.4711\n",
            "Epoch 302/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3583 - mse: 0.3583 - mae: 0.4669 - val_loss: 0.3647 - val_mse: 0.3647 - val_mae: 0.4635\n",
            "Epoch 303/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3502 - mse: 0.3502 - mae: 0.4642 - val_loss: 0.3647 - val_mse: 0.3647 - val_mae: 0.4633\n",
            "Epoch 304/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3399 - mse: 0.3399 - mae: 0.4507 - val_loss: 0.3704 - val_mse: 0.3704 - val_mae: 0.4692\n",
            "Epoch 305/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3553 - mse: 0.3553 - mae: 0.4640 - val_loss: 0.3710 - val_mse: 0.3710 - val_mae: 0.4681\n",
            "Epoch 306/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3561 - mse: 0.3561 - mae: 0.4558 - val_loss: 0.3705 - val_mse: 0.3705 - val_mae: 0.4685\n",
            "Epoch 307/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3398 - mse: 0.3398 - mae: 0.4529 - val_loss: 0.3692 - val_mse: 0.3692 - val_mae: 0.4667\n",
            "Epoch 308/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3322 - mse: 0.3322 - mae: 0.4534 - val_loss: 0.3749 - val_mse: 0.3749 - val_mae: 0.4713\n",
            "Epoch 309/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3543 - mse: 0.3543 - mae: 0.4575 - val_loss: 0.3663 - val_mse: 0.3663 - val_mae: 0.4663\n",
            "Epoch 310/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3385 - mse: 0.3385 - mae: 0.4501 - val_loss: 0.3704 - val_mse: 0.3704 - val_mae: 0.4689\n",
            "Epoch 311/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3685 - mse: 0.3685 - mae: 0.4669 - val_loss: 0.3643 - val_mse: 0.3643 - val_mae: 0.4642\n",
            "Epoch 312/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3545 - mse: 0.3545 - mae: 0.4628 - val_loss: 0.3717 - val_mse: 0.3717 - val_mae: 0.4721\n",
            "Epoch 313/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3435 - mse: 0.3435 - mae: 0.4518 - val_loss: 0.3679 - val_mse: 0.3679 - val_mae: 0.4661\n",
            "Epoch 314/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3353 - mse: 0.3353 - mae: 0.4474 - val_loss: 0.3713 - val_mse: 0.3713 - val_mae: 0.4704\n",
            "Epoch 315/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3575 - mse: 0.3575 - mae: 0.4628 - val_loss: 0.3716 - val_mse: 0.3716 - val_mae: 0.4714\n",
            "Epoch 316/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3562 - mse: 0.3562 - mae: 0.4635 - val_loss: 0.3615 - val_mse: 0.3615 - val_mae: 0.4665\n",
            "Epoch 317/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3502 - mse: 0.3502 - mae: 0.4585 - val_loss: 0.3670 - val_mse: 0.3670 - val_mae: 0.4695\n",
            "Epoch 318/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3458 - mse: 0.3458 - mae: 0.4592 - val_loss: 0.3677 - val_mse: 0.3677 - val_mae: 0.4676\n",
            "Epoch 319/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3440 - mse: 0.3440 - mae: 0.4526 - val_loss: 0.3732 - val_mse: 0.3732 - val_mae: 0.4717\n",
            "Epoch 320/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3454 - mse: 0.3454 - mae: 0.4626 - val_loss: 0.3668 - val_mse: 0.3668 - val_mae: 0.4675\n",
            "Epoch 321/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3422 - mse: 0.3422 - mae: 0.4584 - val_loss: 0.3737 - val_mse: 0.3737 - val_mae: 0.4723\n",
            "Epoch 322/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3594 - mse: 0.3594 - mae: 0.4604 - val_loss: 0.3690 - val_mse: 0.3690 - val_mae: 0.4692\n",
            "Epoch 323/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3367 - mse: 0.3367 - mae: 0.4510 - val_loss: 0.3728 - val_mse: 0.3728 - val_mae: 0.4718\n",
            "Epoch 324/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3479 - mse: 0.3479 - mae: 0.4572 - val_loss: 0.3708 - val_mse: 0.3708 - val_mae: 0.4696\n",
            "Epoch 325/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3569 - mse: 0.3569 - mae: 0.4634 - val_loss: 0.3748 - val_mse: 0.3748 - val_mae: 0.4711\n",
            "Epoch 326/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3530 - mse: 0.3530 - mae: 0.4634 - val_loss: 0.3708 - val_mse: 0.3708 - val_mae: 0.4703\n",
            "Epoch 327/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3550 - mse: 0.3550 - mae: 0.4594 - val_loss: 0.3770 - val_mse: 0.3770 - val_mae: 0.4745\n",
            "Epoch 328/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3370 - mse: 0.3370 - mae: 0.4563 - val_loss: 0.3734 - val_mse: 0.3734 - val_mae: 0.4730\n",
            "Epoch 329/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3384 - mse: 0.3384 - mae: 0.4519 - val_loss: 0.3710 - val_mse: 0.3710 - val_mae: 0.4703\n",
            "Epoch 330/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3317 - mse: 0.3317 - mae: 0.4496 - val_loss: 0.3707 - val_mse: 0.3707 - val_mae: 0.4707\n",
            "Epoch 331/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3567 - mse: 0.3567 - mae: 0.4640 - val_loss: 0.3645 - val_mse: 0.3645 - val_mae: 0.4662\n",
            "Epoch 332/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3396 - mse: 0.3396 - mae: 0.4460 - val_loss: 0.3746 - val_mse: 0.3746 - val_mae: 0.4711\n",
            "Epoch 333/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3372 - mse: 0.3372 - mae: 0.4512 - val_loss: 0.3693 - val_mse: 0.3693 - val_mae: 0.4701\n",
            "Epoch 334/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3341 - mse: 0.3341 - mae: 0.4426 - val_loss: 0.3655 - val_mse: 0.3655 - val_mae: 0.4685\n",
            "Epoch 335/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3459 - mse: 0.3459 - mae: 0.4533 - val_loss: 0.3722 - val_mse: 0.3722 - val_mae: 0.4752\n",
            "Epoch 336/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3410 - mse: 0.3410 - mae: 0.4547 - val_loss: 0.3711 - val_mse: 0.3711 - val_mae: 0.4731\n",
            "Epoch 337/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3500 - mse: 0.3500 - mae: 0.4567 - val_loss: 0.3684 - val_mse: 0.3684 - val_mae: 0.4701\n",
            "Epoch 338/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3316 - mse: 0.3316 - mae: 0.4477 - val_loss: 0.3631 - val_mse: 0.3631 - val_mae: 0.4658\n",
            "Epoch 339/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3295 - mse: 0.3295 - mae: 0.4505 - val_loss: 0.3694 - val_mse: 0.3694 - val_mae: 0.4707\n",
            "Epoch 340/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3550 - mse: 0.3550 - mae: 0.4622 - val_loss: 0.3771 - val_mse: 0.3771 - val_mae: 0.4769\n",
            "Epoch 341/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3396 - mse: 0.3396 - mae: 0.4549 - val_loss: 0.3666 - val_mse: 0.3666 - val_mae: 0.4672\n",
            "Epoch 342/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3574 - mse: 0.3574 - mae: 0.4603 - val_loss: 0.3676 - val_mse: 0.3676 - val_mae: 0.4686\n",
            "Epoch 343/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3408 - mse: 0.3408 - mae: 0.4537 - val_loss: 0.3730 - val_mse: 0.3730 - val_mae: 0.4745\n",
            "Epoch 344/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3500 - mse: 0.3500 - mae: 0.4652 - val_loss: 0.3698 - val_mse: 0.3698 - val_mae: 0.4706\n",
            "Epoch 345/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3364 - mse: 0.3364 - mae: 0.4497 - val_loss: 0.3787 - val_mse: 0.3787 - val_mae: 0.4767\n",
            "Epoch 346/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3432 - mse: 0.3432 - mae: 0.4611 - val_loss: 0.3709 - val_mse: 0.3709 - val_mae: 0.4707\n",
            "Epoch 347/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3434 - mse: 0.3434 - mae: 0.4522 - val_loss: 0.3593 - val_mse: 0.3593 - val_mae: 0.4664\n",
            "Epoch 348/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3401 - mse: 0.3401 - mae: 0.4474 - val_loss: 0.3664 - val_mse: 0.3664 - val_mae: 0.4683\n",
            "Epoch 349/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3320 - mse: 0.3320 - mae: 0.4405 - val_loss: 0.3741 - val_mse: 0.3741 - val_mae: 0.4739\n",
            "Epoch 350/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3460 - mse: 0.3460 - mae: 0.4498 - val_loss: 0.3677 - val_mse: 0.3677 - val_mae: 0.4703\n",
            "Epoch 351/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3516 - mse: 0.3516 - mae: 0.4572 - val_loss: 0.3660 - val_mse: 0.3660 - val_mae: 0.4691\n",
            "Epoch 352/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3283 - mse: 0.3283 - mae: 0.4454 - val_loss: 0.3636 - val_mse: 0.3636 - val_mae: 0.4676\n",
            "Epoch 353/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3372 - mse: 0.3372 - mae: 0.4502 - val_loss: 0.3832 - val_mse: 0.3832 - val_mae: 0.4824\n",
            "Epoch 354/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3424 - mse: 0.3424 - mae: 0.4457 - val_loss: 0.3700 - val_mse: 0.3700 - val_mae: 0.4712\n",
            "Epoch 355/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3317 - mse: 0.3317 - mae: 0.4496 - val_loss: 0.3639 - val_mse: 0.3639 - val_mae: 0.4648\n",
            "Epoch 356/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3393 - mse: 0.3393 - mae: 0.4470 - val_loss: 0.3619 - val_mse: 0.3619 - val_mae: 0.4636\n",
            "Epoch 357/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3397 - mse: 0.3397 - mae: 0.4566 - val_loss: 0.3723 - val_mse: 0.3723 - val_mae: 0.4715\n",
            "Epoch 358/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3386 - mse: 0.3386 - mae: 0.4472 - val_loss: 0.3842 - val_mse: 0.3842 - val_mae: 0.4794\n",
            "Epoch 359/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3362 - mse: 0.3362 - mae: 0.4560 - val_loss: 0.3648 - val_mse: 0.3648 - val_mae: 0.4680\n",
            "Epoch 360/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3580 - mse: 0.3580 - mae: 0.4598 - val_loss: 0.3607 - val_mse: 0.3607 - val_mae: 0.4657\n",
            "Epoch 361/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3376 - mse: 0.3376 - mae: 0.4525 - val_loss: 0.3715 - val_mse: 0.3715 - val_mae: 0.4749\n",
            "Epoch 362/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3397 - mse: 0.3397 - mae: 0.4500 - val_loss: 0.3663 - val_mse: 0.3663 - val_mae: 0.4692\n",
            "Epoch 363/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3463 - mse: 0.3463 - mae: 0.4578 - val_loss: 0.3579 - val_mse: 0.3579 - val_mae: 0.4624\n",
            "Epoch 364/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3344 - mse: 0.3344 - mae: 0.4496 - val_loss: 0.3611 - val_mse: 0.3611 - val_mae: 0.4660\n",
            "Epoch 365/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3377 - mse: 0.3377 - mae: 0.4493 - val_loss: 0.3709 - val_mse: 0.3709 - val_mae: 0.4734\n",
            "Epoch 366/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3417 - mse: 0.3417 - mae: 0.4560 - val_loss: 0.3644 - val_mse: 0.3644 - val_mae: 0.4683\n",
            "Epoch 367/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3514 - mse: 0.3514 - mae: 0.4558 - val_loss: 0.3848 - val_mse: 0.3848 - val_mae: 0.4787\n",
            "Epoch 368/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3330 - mse: 0.3330 - mae: 0.4467 - val_loss: 0.3601 - val_mse: 0.3601 - val_mae: 0.4633\n",
            "Epoch 369/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3523 - mse: 0.3523 - mae: 0.4537 - val_loss: 0.3642 - val_mse: 0.3642 - val_mae: 0.4679\n",
            "Epoch 370/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3432 - mse: 0.3432 - mae: 0.4548 - val_loss: 0.3718 - val_mse: 0.3718 - val_mae: 0.4708\n",
            "Epoch 371/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3299 - mse: 0.3299 - mae: 0.4486 - val_loss: 0.3722 - val_mse: 0.3722 - val_mae: 0.4715\n",
            "Epoch 372/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3340 - mse: 0.3340 - mae: 0.4475 - val_loss: 0.3717 - val_mse: 0.3717 - val_mae: 0.4715\n",
            "Epoch 373/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3289 - mse: 0.3289 - mae: 0.4523 - val_loss: 0.3634 - val_mse: 0.3634 - val_mae: 0.4678\n",
            "Epoch 374/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3362 - mse: 0.3362 - mae: 0.4422 - val_loss: 0.3684 - val_mse: 0.3684 - val_mae: 0.4694\n",
            "Epoch 375/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3429 - mse: 0.3429 - mae: 0.4561 - val_loss: 0.3650 - val_mse: 0.3650 - val_mae: 0.4645\n",
            "Epoch 376/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3302 - mse: 0.3302 - mae: 0.4459 - val_loss: 0.3631 - val_mse: 0.3631 - val_mae: 0.4636\n",
            "Epoch 377/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3521 - mse: 0.3521 - mae: 0.4531 - val_loss: 0.3784 - val_mse: 0.3784 - val_mae: 0.4814\n",
            "Epoch 378/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3499 - mse: 0.3499 - mae: 0.4628 - val_loss: 0.3632 - val_mse: 0.3632 - val_mae: 0.4676\n",
            "Epoch 379/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3398 - mse: 0.3398 - mae: 0.4556 - val_loss: 0.3644 - val_mse: 0.3644 - val_mae: 0.4701\n",
            "Epoch 380/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3427 - mse: 0.3427 - mae: 0.4527 - val_loss: 0.3695 - val_mse: 0.3695 - val_mae: 0.4714\n",
            "Epoch 381/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3230 - mse: 0.3230 - mae: 0.4454 - val_loss: 0.3661 - val_mse: 0.3661 - val_mae: 0.4729\n",
            "Epoch 382/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3169 - mse: 0.3169 - mae: 0.4364 - val_loss: 0.3634 - val_mse: 0.3634 - val_mae: 0.4670\n",
            "Epoch 383/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3446 - mse: 0.3446 - mae: 0.4564 - val_loss: 0.3682 - val_mse: 0.3682 - val_mae: 0.4684\n",
            "Epoch 384/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3404 - mse: 0.3404 - mae: 0.4525 - val_loss: 0.3701 - val_mse: 0.3701 - val_mae: 0.4725\n",
            "Epoch 385/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3319 - mse: 0.3319 - mae: 0.4457 - val_loss: 0.3640 - val_mse: 0.3640 - val_mae: 0.4688\n",
            "Epoch 386/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3443 - mse: 0.3443 - mae: 0.4521 - val_loss: 0.3606 - val_mse: 0.3606 - val_mae: 0.4646\n",
            "Epoch 387/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3245 - mse: 0.3245 - mae: 0.4404 - val_loss: 0.3577 - val_mse: 0.3577 - val_mae: 0.4630\n",
            "Epoch 388/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3306 - mse: 0.3306 - mae: 0.4400 - val_loss: 0.3718 - val_mse: 0.3718 - val_mae: 0.4709\n",
            "Epoch 389/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3318 - mse: 0.3318 - mae: 0.4469 - val_loss: 0.3740 - val_mse: 0.3740 - val_mae: 0.4754\n",
            "Epoch 390/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3374 - mse: 0.3374 - mae: 0.4522 - val_loss: 0.3652 - val_mse: 0.3652 - val_mae: 0.4684\n",
            "Epoch 391/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3388 - mse: 0.3388 - mae: 0.4518 - val_loss: 0.3637 - val_mse: 0.3637 - val_mae: 0.4658\n",
            "Epoch 392/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3282 - mse: 0.3282 - mae: 0.4408 - val_loss: 0.3695 - val_mse: 0.3695 - val_mae: 0.4703\n",
            "Epoch 393/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3469 - mse: 0.3469 - mae: 0.4540 - val_loss: 0.3759 - val_mse: 0.3759 - val_mae: 0.4729\n",
            "Epoch 394/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3298 - mse: 0.3298 - mae: 0.4461 - val_loss: 0.3661 - val_mse: 0.3661 - val_mae: 0.4690\n",
            "Epoch 395/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3248 - mse: 0.3248 - mae: 0.4459 - val_loss: 0.3652 - val_mse: 0.3652 - val_mae: 0.4660\n",
            "Epoch 396/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3346 - mse: 0.3346 - mae: 0.4475 - val_loss: 0.3648 - val_mse: 0.3648 - val_mae: 0.4696\n",
            "Epoch 397/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3367 - mse: 0.3367 - mae: 0.4493 - val_loss: 0.3976 - val_mse: 0.3976 - val_mae: 0.4875\n",
            "Epoch 398/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3281 - mse: 0.3281 - mae: 0.4417 - val_loss: 0.3653 - val_mse: 0.3653 - val_mae: 0.4663\n",
            "Epoch 399/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3242 - mse: 0.3242 - mae: 0.4442 - val_loss: 0.3629 - val_mse: 0.3629 - val_mae: 0.4673\n",
            "Epoch 400/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3374 - mse: 0.3374 - mae: 0.4466 - val_loss: 0.3723 - val_mse: 0.3723 - val_mae: 0.4733\n",
            "Epoch 401/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3377 - mse: 0.3377 - mae: 0.4585 - val_loss: 0.3662 - val_mse: 0.3662 - val_mae: 0.4730\n",
            "Epoch 402/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3402 - mse: 0.3402 - mae: 0.4539 - val_loss: 0.3635 - val_mse: 0.3635 - val_mae: 0.4682\n",
            "Epoch 403/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3204 - mse: 0.3204 - mae: 0.4380 - val_loss: 0.3704 - val_mse: 0.3704 - val_mae: 0.4655\n",
            "Epoch 404/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3348 - mse: 0.3348 - mae: 0.4511 - val_loss: 0.3655 - val_mse: 0.3655 - val_mae: 0.4698\n",
            "Epoch 405/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3291 - mse: 0.3291 - mae: 0.4426 - val_loss: 0.3640 - val_mse: 0.3640 - val_mae: 0.4671\n",
            "Epoch 406/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3384 - mse: 0.3384 - mae: 0.4527 - val_loss: 0.3692 - val_mse: 0.3692 - val_mae: 0.4729\n",
            "Epoch 407/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3331 - mse: 0.3331 - mae: 0.4479 - val_loss: 0.3632 - val_mse: 0.3632 - val_mae: 0.4636\n",
            "Epoch 408/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3139 - mse: 0.3139 - mae: 0.4375 - val_loss: 0.3678 - val_mse: 0.3678 - val_mae: 0.4679\n",
            "Epoch 409/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3273 - mse: 0.3273 - mae: 0.4421 - val_loss: 0.3656 - val_mse: 0.3656 - val_mae: 0.4648\n",
            "Epoch 410/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3443 - mse: 0.3443 - mae: 0.4502 - val_loss: 0.3741 - val_mse: 0.3741 - val_mae: 0.4694\n",
            "Epoch 411/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3249 - mse: 0.3249 - mae: 0.4483 - val_loss: 0.3683 - val_mse: 0.3683 - val_mae: 0.4686\n",
            "Epoch 412/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3284 - mse: 0.3284 - mae: 0.4463 - val_loss: 0.3646 - val_mse: 0.3646 - val_mae: 0.4652\n",
            "Epoch 413/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3169 - mse: 0.3169 - mae: 0.4338 - val_loss: 0.3760 - val_mse: 0.3760 - val_mae: 0.4747\n",
            "Epoch 414/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3245 - mse: 0.3245 - mae: 0.4378 - val_loss: 0.3636 - val_mse: 0.3636 - val_mae: 0.4646\n",
            "Epoch 415/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3291 - mse: 0.3291 - mae: 0.4394 - val_loss: 0.3692 - val_mse: 0.3692 - val_mae: 0.4709\n",
            "Epoch 416/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3206 - mse: 0.3206 - mae: 0.4365 - val_loss: 0.3685 - val_mse: 0.3685 - val_mae: 0.4689\n",
            "Epoch 417/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3353 - mse: 0.3353 - mae: 0.4478 - val_loss: 0.3641 - val_mse: 0.3641 - val_mae: 0.4659\n",
            "Epoch 418/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3397 - mse: 0.3397 - mae: 0.4551 - val_loss: 0.4006 - val_mse: 0.4006 - val_mae: 0.4910\n",
            "Epoch 419/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3265 - mse: 0.3265 - mae: 0.4487 - val_loss: 0.3681 - val_mse: 0.3681 - val_mae: 0.4669\n",
            "Epoch 420/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3234 - mse: 0.3234 - mae: 0.4425 - val_loss: 0.3654 - val_mse: 0.3654 - val_mae: 0.4667\n",
            "Epoch 421/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3242 - mse: 0.3242 - mae: 0.4420 - val_loss: 0.3651 - val_mse: 0.3651 - val_mae: 0.4671\n",
            "Epoch 422/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3140 - mse: 0.3140 - mae: 0.4320 - val_loss: 0.3693 - val_mse: 0.3693 - val_mae: 0.4697\n",
            "Epoch 423/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3465 - mse: 0.3465 - mae: 0.4607 - val_loss: 0.3678 - val_mse: 0.3678 - val_mae: 0.4684\n",
            "Epoch 424/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3230 - mse: 0.3230 - mae: 0.4419 - val_loss: 0.3646 - val_mse: 0.3646 - val_mae: 0.4662\n",
            "Epoch 425/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3327 - mse: 0.3327 - mae: 0.4458 - val_loss: 0.3662 - val_mse: 0.3662 - val_mae: 0.4662\n",
            "Epoch 426/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3232 - mse: 0.3232 - mae: 0.4376 - val_loss: 0.3645 - val_mse: 0.3645 - val_mae: 0.4676\n",
            "Epoch 427/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3182 - mse: 0.3182 - mae: 0.4379 - val_loss: 0.3742 - val_mse: 0.3742 - val_mae: 0.4734\n",
            "Epoch 428/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3224 - mse: 0.3224 - mae: 0.4334 - val_loss: 0.3653 - val_mse: 0.3653 - val_mae: 0.4701\n",
            "Epoch 429/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3305 - mse: 0.3305 - mae: 0.4475 - val_loss: 0.3693 - val_mse: 0.3693 - val_mae: 0.4700\n",
            "Epoch 430/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3183 - mse: 0.3183 - mae: 0.4420 - val_loss: 0.3623 - val_mse: 0.3623 - val_mae: 0.4644\n",
            "Epoch 431/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3265 - mse: 0.3265 - mae: 0.4423 - val_loss: 0.3643 - val_mse: 0.3643 - val_mae: 0.4684\n",
            "Epoch 432/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3182 - mse: 0.3182 - mae: 0.4346 - val_loss: 0.3670 - val_mse: 0.3670 - val_mae: 0.4715\n",
            "Epoch 433/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3209 - mse: 0.3209 - mae: 0.4343 - val_loss: 0.3703 - val_mse: 0.3703 - val_mae: 0.4730\n",
            "Epoch 434/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3176 - mse: 0.3176 - mae: 0.4396 - val_loss: 0.3692 - val_mse: 0.3692 - val_mae: 0.4695\n",
            "Epoch 435/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3127 - mse: 0.3127 - mae: 0.4336 - val_loss: 0.3826 - val_mse: 0.3826 - val_mae: 0.4781\n",
            "Epoch 436/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3300 - mse: 0.3300 - mae: 0.4419 - val_loss: 0.3715 - val_mse: 0.3715 - val_mae: 0.4705\n",
            "Epoch 437/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3394 - mse: 0.3394 - mae: 0.4591 - val_loss: 0.3715 - val_mse: 0.3715 - val_mae: 0.4751\n",
            "Epoch 438/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3097 - mse: 0.3097 - mae: 0.4281 - val_loss: 0.3856 - val_mse: 0.3856 - val_mae: 0.4772\n",
            "Epoch 439/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3255 - mse: 0.3255 - mae: 0.4402 - val_loss: 0.3680 - val_mse: 0.3680 - val_mae: 0.4672\n",
            "Epoch 440/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3078 - mse: 0.3078 - mae: 0.4299 - val_loss: 0.3742 - val_mse: 0.3742 - val_mae: 0.4696\n",
            "Epoch 441/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3282 - mse: 0.3282 - mae: 0.4450 - val_loss: 0.3659 - val_mse: 0.3659 - val_mae: 0.4669\n",
            "Epoch 442/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3121 - mse: 0.3121 - mae: 0.4377 - val_loss: 0.3691 - val_mse: 0.3691 - val_mae: 0.4657\n",
            "Epoch 443/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3194 - mse: 0.3194 - mae: 0.4367 - val_loss: 0.3777 - val_mse: 0.3777 - val_mae: 0.4733\n",
            "Epoch 444/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3307 - mse: 0.3307 - mae: 0.4473 - val_loss: 0.3673 - val_mse: 0.3673 - val_mae: 0.4674\n",
            "Epoch 445/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3255 - mse: 0.3255 - mae: 0.4401 - val_loss: 0.3727 - val_mse: 0.3727 - val_mae: 0.4729\n",
            "Epoch 446/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3108 - mse: 0.3108 - mae: 0.4312 - val_loss: 0.3660 - val_mse: 0.3660 - val_mae: 0.4672\n",
            "Epoch 447/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3136 - mse: 0.3136 - mae: 0.4410 - val_loss: 0.3713 - val_mse: 0.3713 - val_mae: 0.4723\n",
            "Epoch 448/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3340 - mse: 0.3340 - mae: 0.4433 - val_loss: 0.3684 - val_mse: 0.3684 - val_mae: 0.4700\n",
            "Epoch 449/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3320 - mse: 0.3320 - mae: 0.4452 - val_loss: 0.3714 - val_mse: 0.3714 - val_mae: 0.4711\n",
            "Epoch 450/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3288 - mse: 0.3288 - mae: 0.4425 - val_loss: 0.3695 - val_mse: 0.3695 - val_mae: 0.4707\n",
            "Epoch 451/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3167 - mse: 0.3167 - mae: 0.4403 - val_loss: 0.3672 - val_mse: 0.3672 - val_mae: 0.4666\n",
            "Epoch 452/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3383 - mse: 0.3383 - mae: 0.4483 - val_loss: 0.3728 - val_mse: 0.3728 - val_mae: 0.4699\n",
            "Epoch 453/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3222 - mse: 0.3222 - mae: 0.4434 - val_loss: 0.3791 - val_mse: 0.3791 - val_mae: 0.4793\n",
            "Epoch 454/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3189 - mse: 0.3189 - mae: 0.4332 - val_loss: 0.3780 - val_mse: 0.3780 - val_mae: 0.4732\n",
            "Epoch 455/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3142 - mse: 0.3142 - mae: 0.4335 - val_loss: 0.3719 - val_mse: 0.3719 - val_mae: 0.4681\n",
            "Epoch 456/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3313 - mse: 0.3313 - mae: 0.4415 - val_loss: 0.3755 - val_mse: 0.3755 - val_mae: 0.4693\n",
            "Epoch 457/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3255 - mse: 0.3255 - mae: 0.4437 - val_loss: 0.3735 - val_mse: 0.3735 - val_mae: 0.4694\n",
            "Epoch 458/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3166 - mse: 0.3166 - mae: 0.4349 - val_loss: 0.3640 - val_mse: 0.3640 - val_mae: 0.4643\n",
            "Epoch 459/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3148 - mse: 0.3148 - mae: 0.4353 - val_loss: 0.3707 - val_mse: 0.3707 - val_mae: 0.4687\n",
            "Epoch 460/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3227 - mse: 0.3227 - mae: 0.4379 - val_loss: 0.3757 - val_mse: 0.3757 - val_mae: 0.4741\n",
            "Epoch 461/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3311 - mse: 0.3311 - mae: 0.4444 - val_loss: 0.3797 - val_mse: 0.3797 - val_mae: 0.4738\n",
            "Epoch 462/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3124 - mse: 0.3124 - mae: 0.4329 - val_loss: 0.3793 - val_mse: 0.3793 - val_mae: 0.4829\n",
            "Epoch 463/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3027 - mse: 0.3027 - mae: 0.4198 - val_loss: 0.3813 - val_mse: 0.3813 - val_mae: 0.4743\n",
            "Epoch 464/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3325 - mse: 0.3325 - mae: 0.4501 - val_loss: 0.3706 - val_mse: 0.3706 - val_mae: 0.4704\n",
            "Epoch 465/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3267 - mse: 0.3267 - mae: 0.4497 - val_loss: 0.3723 - val_mse: 0.3723 - val_mae: 0.4724\n",
            "Epoch 466/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3229 - mse: 0.3229 - mae: 0.4399 - val_loss: 0.3909 - val_mse: 0.3909 - val_mae: 0.4808\n",
            "Epoch 467/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3251 - mse: 0.3251 - mae: 0.4395 - val_loss: 0.3719 - val_mse: 0.3719 - val_mae: 0.4735\n",
            "Epoch 468/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3104 - mse: 0.3104 - mae: 0.4285 - val_loss: 0.3670 - val_mse: 0.3670 - val_mae: 0.4652\n",
            "Epoch 469/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3315 - mse: 0.3315 - mae: 0.4431 - val_loss: 0.3679 - val_mse: 0.3679 - val_mae: 0.4672\n",
            "Epoch 470/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3217 - mse: 0.3217 - mae: 0.4420 - val_loss: 0.3773 - val_mse: 0.3773 - val_mae: 0.4768\n",
            "Epoch 471/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3189 - mse: 0.3189 - mae: 0.4378 - val_loss: 0.3663 - val_mse: 0.3663 - val_mae: 0.4657\n",
            "Epoch 472/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3147 - mse: 0.3147 - mae: 0.4407 - val_loss: 0.3747 - val_mse: 0.3747 - val_mae: 0.4728\n",
            "Epoch 473/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3160 - mse: 0.3160 - mae: 0.4375 - val_loss: 0.3648 - val_mse: 0.3648 - val_mae: 0.4645\n",
            "Epoch 474/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3138 - mse: 0.3138 - mae: 0.4296 - val_loss: 0.3693 - val_mse: 0.3693 - val_mae: 0.4640\n",
            "Epoch 475/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3384 - mse: 0.3384 - mae: 0.4533 - val_loss: 0.3736 - val_mse: 0.3736 - val_mae: 0.4697\n",
            "Epoch 476/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3165 - mse: 0.3165 - mae: 0.4328 - val_loss: 0.3716 - val_mse: 0.3716 - val_mae: 0.4671\n",
            "Epoch 477/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3265 - mse: 0.3265 - mae: 0.4484 - val_loss: 0.3656 - val_mse: 0.3656 - val_mae: 0.4663\n",
            "Epoch 478/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3269 - mse: 0.3269 - mae: 0.4454 - val_loss: 0.3631 - val_mse: 0.3631 - val_mae: 0.4646\n",
            "Epoch 479/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3026 - mse: 0.3026 - mae: 0.4266 - val_loss: 0.3673 - val_mse: 0.3673 - val_mae: 0.4701\n",
            "Epoch 480/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3296 - mse: 0.3296 - mae: 0.4537 - val_loss: 0.3701 - val_mse: 0.3701 - val_mae: 0.4754\n",
            "Epoch 481/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3141 - mse: 0.3141 - mae: 0.4399 - val_loss: 0.3645 - val_mse: 0.3645 - val_mae: 0.4700\n",
            "Epoch 482/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3418 - mse: 0.3418 - mae: 0.4535 - val_loss: 0.3643 - val_mse: 0.3643 - val_mae: 0.4632\n",
            "Epoch 483/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3215 - mse: 0.3215 - mae: 0.4363 - val_loss: 0.3650 - val_mse: 0.3650 - val_mae: 0.4675\n",
            "Epoch 484/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3196 - mse: 0.3196 - mae: 0.4404 - val_loss: 0.3876 - val_mse: 0.3876 - val_mae: 0.4816\n",
            "Epoch 485/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3296 - mse: 0.3296 - mae: 0.4518 - val_loss: 0.3677 - val_mse: 0.3677 - val_mae: 0.4685\n",
            "Epoch 486/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3153 - mse: 0.3153 - mae: 0.4374 - val_loss: 0.3681 - val_mse: 0.3681 - val_mae: 0.4723\n",
            "Epoch 487/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3186 - mse: 0.3186 - mae: 0.4414 - val_loss: 0.3779 - val_mse: 0.3779 - val_mae: 0.4723\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.4445 - mse: 0.4445 - mae: 0.5171\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 13ms/step - loss: 23.5896 - mse: 23.5896 - mae: 4.7331 - val_loss: 20.8684 - val_mse: 20.8684 - val_mae: 4.4447\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 17.9879 - mse: 17.9879 - mae: 4.0700 - val_loss: 15.7621 - val_mse: 15.7621 - val_mae: 3.8047\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 13.2416 - mse: 13.2416 - mae: 3.4246 - val_loss: 11.5477 - val_mse: 11.5477 - val_mae: 3.1821\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 9.6132 - mse: 9.6132 - mae: 2.8646 - val_loss: 8.2929 - val_mse: 8.2929 - val_mae: 2.6117\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 6.9121 - mse: 6.9121 - mae: 2.3427 - val_loss: 6.0445 - val_mse: 6.0445 - val_mae: 2.1498\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5.2571 - mse: 5.2571 - mae: 1.9960 - val_loss: 4.6265 - val_mse: 4.6265 - val_mae: 1.8406\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 4.1765 - mse: 4.1765 - mae: 1.7266 - val_loss: 3.8275 - val_mse: 3.8275 - val_mae: 1.6303\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 3.3992 - mse: 3.3992 - mae: 1.5034 - val_loss: 3.3272 - val_mse: 3.3272 - val_mae: 1.4805\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 3.1328 - mse: 3.1328 - mae: 1.4388 - val_loss: 3.0043 - val_mse: 3.0043 - val_mae: 1.3840\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.6866 - mse: 2.6866 - mae: 1.3197 - val_loss: 2.8006 - val_mse: 2.8006 - val_mae: 1.3218\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 2.5050 - mse: 2.5050 - mae: 1.2675 - val_loss: 2.6323 - val_mse: 2.6323 - val_mae: 1.2746\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.4558 - mse: 2.4558 - mae: 1.2482 - val_loss: 2.4908 - val_mse: 2.4908 - val_mae: 1.2327\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.3869 - mse: 2.3869 - mae: 1.2150 - val_loss: 2.3824 - val_mse: 2.3824 - val_mae: 1.2011\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 2.2231 - mse: 2.2231 - mae: 1.1923 - val_loss: 2.3016 - val_mse: 2.3016 - val_mae: 1.1772\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 2.0679 - mse: 2.0679 - mae: 1.1463 - val_loss: 2.2467 - val_mse: 2.2467 - val_mae: 1.1608\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.0635 - mse: 2.0635 - mae: 1.1341 - val_loss: 2.1809 - val_mse: 2.1809 - val_mae: 1.1438\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.9184 - mse: 1.9184 - mae: 1.0939 - val_loss: 2.1359 - val_mse: 2.1359 - val_mae: 1.1293\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.8645 - mse: 1.8645 - mae: 1.0779 - val_loss: 2.0888 - val_mse: 2.0888 - val_mae: 1.1171\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.8671 - mse: 1.8671 - mae: 1.0879 - val_loss: 2.0481 - val_mse: 2.0481 - val_mae: 1.1117\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.7189 - mse: 1.7189 - mae: 1.0456 - val_loss: 2.0181 - val_mse: 2.0181 - val_mae: 1.1033\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.8377 - mse: 1.8377 - mae: 1.0842 - val_loss: 1.9827 - val_mse: 1.9827 - val_mae: 1.0863\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.7643 - mse: 1.7643 - mae: 1.0405 - val_loss: 1.9522 - val_mse: 1.9522 - val_mae: 1.0797\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.6983 - mse: 1.6983 - mae: 1.0347 - val_loss: 1.9274 - val_mse: 1.9274 - val_mae: 1.0727\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.7158 - mse: 1.7158 - mae: 1.0357 - val_loss: 1.9030 - val_mse: 1.9030 - val_mae: 1.0598\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.7607 - mse: 1.7607 - mae: 1.0622 - val_loss: 1.8829 - val_mse: 1.8829 - val_mae: 1.0569\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.6072 - mse: 1.6072 - mae: 1.0148 - val_loss: 1.8421 - val_mse: 1.8421 - val_mae: 1.0414\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.6505 - mse: 1.6505 - mae: 1.0107 - val_loss: 1.8087 - val_mse: 1.8087 - val_mae: 1.0300\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.5742 - mse: 1.5742 - mae: 0.9876 - val_loss: 1.7915 - val_mse: 1.7915 - val_mae: 1.0282\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.4658 - mse: 1.4658 - mae: 0.9595 - val_loss: 1.7596 - val_mse: 1.7596 - val_mae: 1.0156\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.4459 - mse: 1.4459 - mae: 0.9530 - val_loss: 1.7286 - val_mse: 1.7286 - val_mae: 1.0012\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.5042 - mse: 1.5042 - mae: 0.9728 - val_loss: 1.7063 - val_mse: 1.7063 - val_mae: 0.9994\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.4748 - mse: 1.4748 - mae: 0.9573 - val_loss: 1.6746 - val_mse: 1.6746 - val_mae: 0.9878\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.4108 - mse: 1.4108 - mae: 0.9420 - val_loss: 1.6421 - val_mse: 1.6421 - val_mae: 0.9813\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.3813 - mse: 1.3813 - mae: 0.9406 - val_loss: 1.6180 - val_mse: 1.6180 - val_mae: 0.9747\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.3234 - mse: 1.3234 - mae: 0.9150 - val_loss: 1.5946 - val_mse: 1.5946 - val_mae: 0.9665\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.2965 - mse: 1.2965 - mae: 0.9143 - val_loss: 1.5741 - val_mse: 1.5741 - val_mae: 0.9565\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.3565 - mse: 1.3565 - mae: 0.9294 - val_loss: 1.5512 - val_mse: 1.5512 - val_mae: 0.9519\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.2929 - mse: 1.2929 - mae: 0.9051 - val_loss: 1.5246 - val_mse: 1.5246 - val_mae: 0.9425\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2867 - mse: 1.2867 - mae: 0.8990 - val_loss: 1.4983 - val_mse: 1.4983 - val_mae: 0.9317\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.2350 - mse: 1.2350 - mae: 0.8834 - val_loss: 1.4737 - val_mse: 1.4737 - val_mae: 0.9228\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.2098 - mse: 1.2098 - mae: 0.8660 - val_loss: 1.4570 - val_mse: 1.4570 - val_mae: 0.9211\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.1608 - mse: 1.1608 - mae: 0.8460 - val_loss: 1.4337 - val_mse: 1.4337 - val_mae: 0.9119\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.2227 - mse: 1.2227 - mae: 0.8734 - val_loss: 1.4143 - val_mse: 1.4143 - val_mae: 0.9053\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.1859 - mse: 1.1859 - mae: 0.8542 - val_loss: 1.3891 - val_mse: 1.3891 - val_mae: 0.8944\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1279 - mse: 1.1279 - mae: 0.8394 - val_loss: 1.3649 - val_mse: 1.3649 - val_mae: 0.8890\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.0845 - mse: 1.0845 - mae: 0.8265 - val_loss: 1.3508 - val_mse: 1.3508 - val_mae: 0.8839\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1217 - mse: 1.1217 - mae: 0.8409 - val_loss: 1.3369 - val_mse: 1.3369 - val_mae: 0.8782\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0785 - mse: 1.0785 - mae: 0.8229 - val_loss: 1.3125 - val_mse: 1.3125 - val_mae: 0.8677\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0645 - mse: 1.0645 - mae: 0.8123 - val_loss: 1.2885 - val_mse: 1.2885 - val_mae: 0.8611\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0234 - mse: 1.0234 - mae: 0.8031 - val_loss: 1.2681 - val_mse: 1.2681 - val_mae: 0.8524\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0019 - mse: 1.0019 - mae: 0.7979 - val_loss: 1.2504 - val_mse: 1.2504 - val_mae: 0.8428\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.9879 - mse: 0.9879 - mae: 0.7884 - val_loss: 1.2271 - val_mse: 1.2271 - val_mae: 0.8413\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.9571 - mse: 0.9571 - mae: 0.7730 - val_loss: 1.2025 - val_mse: 1.2025 - val_mae: 0.8324\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.9427 - mse: 0.9427 - mae: 0.7607 - val_loss: 1.1819 - val_mse: 1.1819 - val_mae: 0.8205\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.9158 - mse: 0.9158 - mae: 0.7573 - val_loss: 1.1568 - val_mse: 1.1568 - val_mae: 0.8141\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.9070 - mse: 0.9070 - mae: 0.7471 - val_loss: 1.1507 - val_mse: 1.1507 - val_mae: 0.8104\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8798 - mse: 0.8798 - mae: 0.7357 - val_loss: 1.1469 - val_mse: 1.1469 - val_mae: 0.8059\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8983 - mse: 0.8983 - mae: 0.7470 - val_loss: 1.1236 - val_mse: 1.1236 - val_mae: 0.7991\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8255 - mse: 0.8255 - mae: 0.7066 - val_loss: 1.0970 - val_mse: 1.0970 - val_mae: 0.7885\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8209 - mse: 0.8209 - mae: 0.7156 - val_loss: 1.0804 - val_mse: 1.0804 - val_mae: 0.7788\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8259 - mse: 0.8259 - mae: 0.7174 - val_loss: 1.0658 - val_mse: 1.0658 - val_mae: 0.7713\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8270 - mse: 0.8270 - mae: 0.7169 - val_loss: 1.0475 - val_mse: 1.0475 - val_mae: 0.7698\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.8321 - mse: 0.8321 - mae: 0.7134 - val_loss: 1.0290 - val_mse: 1.0290 - val_mae: 0.7616\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7803 - mse: 0.7803 - mae: 0.6879 - val_loss: 1.0232 - val_mse: 1.0232 - val_mae: 0.7610\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7728 - mse: 0.7728 - mae: 0.7001 - val_loss: 1.0060 - val_mse: 1.0060 - val_mae: 0.7538\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7643 - mse: 0.7643 - mae: 0.6946 - val_loss: 0.9892 - val_mse: 0.9892 - val_mae: 0.7441\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7878 - mse: 0.7878 - mae: 0.6951 - val_loss: 0.9755 - val_mse: 0.9755 - val_mae: 0.7404\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7713 - mse: 0.7713 - mae: 0.7033 - val_loss: 0.9536 - val_mse: 0.9536 - val_mae: 0.7305\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7437 - mse: 0.7437 - mae: 0.6853 - val_loss: 0.9355 - val_mse: 0.9355 - val_mae: 0.7245\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7088 - mse: 0.7088 - mae: 0.6564 - val_loss: 0.9294 - val_mse: 0.9294 - val_mae: 0.7184\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7464 - mse: 0.7464 - mae: 0.6757 - val_loss: 0.9104 - val_mse: 0.9104 - val_mae: 0.7193\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7052 - mse: 0.7052 - mae: 0.6595 - val_loss: 0.9113 - val_mse: 0.9113 - val_mae: 0.7121\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6994 - mse: 0.6994 - mae: 0.6582 - val_loss: 0.8759 - val_mse: 0.8759 - val_mae: 0.7008\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6962 - mse: 0.6962 - mae: 0.6531 - val_loss: 0.8777 - val_mse: 0.8777 - val_mae: 0.6992\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6686 - mse: 0.6686 - mae: 0.6408 - val_loss: 0.8499 - val_mse: 0.8499 - val_mae: 0.6917\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7013 - mse: 0.7013 - mae: 0.6573 - val_loss: 0.8456 - val_mse: 0.8456 - val_mae: 0.6827\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6694 - mse: 0.6694 - mae: 0.6360 - val_loss: 0.8366 - val_mse: 0.8366 - val_mae: 0.6827\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6928 - mse: 0.6928 - mae: 0.6468 - val_loss: 0.8141 - val_mse: 0.8141 - val_mae: 0.6777\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6232 - mse: 0.6232 - mae: 0.6207 - val_loss: 0.8119 - val_mse: 0.8119 - val_mae: 0.6708\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6343 - mse: 0.6343 - mae: 0.6190 - val_loss: 0.7952 - val_mse: 0.7952 - val_mae: 0.6665\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6276 - mse: 0.6276 - mae: 0.6118 - val_loss: 0.7814 - val_mse: 0.7814 - val_mae: 0.6607\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6221 - mse: 0.6221 - mae: 0.6185 - val_loss: 0.7785 - val_mse: 0.7785 - val_mae: 0.6567\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5969 - mse: 0.5969 - mae: 0.5973 - val_loss: 0.7611 - val_mse: 0.7611 - val_mae: 0.6516\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6015 - mse: 0.6015 - mae: 0.6012 - val_loss: 0.7650 - val_mse: 0.7650 - val_mae: 0.6511\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6145 - mse: 0.6145 - mae: 0.6168 - val_loss: 0.7471 - val_mse: 0.7471 - val_mae: 0.6453\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5817 - mse: 0.5817 - mae: 0.5891 - val_loss: 0.7401 - val_mse: 0.7401 - val_mae: 0.6396\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5824 - mse: 0.5824 - mae: 0.5981 - val_loss: 0.7336 - val_mse: 0.7336 - val_mae: 0.6382\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5935 - mse: 0.5935 - mae: 0.6005 - val_loss: 0.7203 - val_mse: 0.7203 - val_mae: 0.6332\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5875 - mse: 0.5875 - mae: 0.5996 - val_loss: 0.7002 - val_mse: 0.7002 - val_mae: 0.6215\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5533 - mse: 0.5533 - mae: 0.5830 - val_loss: 0.7059 - val_mse: 0.7059 - val_mae: 0.6254\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5847 - mse: 0.5847 - mae: 0.5926 - val_loss: 0.6908 - val_mse: 0.6908 - val_mae: 0.6176\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5616 - mse: 0.5616 - mae: 0.5828 - val_loss: 0.6781 - val_mse: 0.6781 - val_mae: 0.6097\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5459 - mse: 0.5459 - mae: 0.5861 - val_loss: 0.6710 - val_mse: 0.6710 - val_mae: 0.6085\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5436 - mse: 0.5436 - mae: 0.5776 - val_loss: 0.6594 - val_mse: 0.6594 - val_mae: 0.6060\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5138 - mse: 0.5138 - mae: 0.5637 - val_loss: 0.6594 - val_mse: 0.6594 - val_mae: 0.6052\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5339 - mse: 0.5339 - mae: 0.5696 - val_loss: 0.6486 - val_mse: 0.6486 - val_mae: 0.5961\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5208 - mse: 0.5208 - mae: 0.5552 - val_loss: 0.6441 - val_mse: 0.6441 - val_mae: 0.5941\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5291 - mse: 0.5291 - mae: 0.5613 - val_loss: 0.6477 - val_mse: 0.6477 - val_mae: 0.5991\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5317 - mse: 0.5317 - mae: 0.5702 - val_loss: 0.6379 - val_mse: 0.6379 - val_mae: 0.5931\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5498 - mse: 0.5498 - mae: 0.5850 - val_loss: 0.6252 - val_mse: 0.6252 - val_mae: 0.5886\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5116 - mse: 0.5116 - mae: 0.5569 - val_loss: 0.6201 - val_mse: 0.6201 - val_mae: 0.5818\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5173 - mse: 0.5173 - mae: 0.5606 - val_loss: 0.6231 - val_mse: 0.6231 - val_mae: 0.5866\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5254 - mse: 0.5254 - mae: 0.5641 - val_loss: 0.5975 - val_mse: 0.5975 - val_mae: 0.5757\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4817 - mse: 0.4817 - mae: 0.5396 - val_loss: 0.6006 - val_mse: 0.6006 - val_mae: 0.5759\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5003 - mse: 0.5003 - mae: 0.5525 - val_loss: 0.5968 - val_mse: 0.5968 - val_mae: 0.5755\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4868 - mse: 0.4868 - mae: 0.5369 - val_loss: 0.5945 - val_mse: 0.5945 - val_mae: 0.5742\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5024 - mse: 0.5024 - mae: 0.5473 - val_loss: 0.5882 - val_mse: 0.5882 - val_mae: 0.5693\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4902 - mse: 0.4902 - mae: 0.5389 - val_loss: 0.5782 - val_mse: 0.5782 - val_mae: 0.5668\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4802 - mse: 0.4802 - mae: 0.5369 - val_loss: 0.5779 - val_mse: 0.5779 - val_mae: 0.5657\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4743 - mse: 0.4743 - mae: 0.5372 - val_loss: 0.5680 - val_mse: 0.5680 - val_mae: 0.5558\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4872 - mse: 0.4872 - mae: 0.5322 - val_loss: 0.5544 - val_mse: 0.5544 - val_mae: 0.5549\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4755 - mse: 0.4755 - mae: 0.5392 - val_loss: 0.5586 - val_mse: 0.5586 - val_mae: 0.5560\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4625 - mse: 0.4625 - mae: 0.5334 - val_loss: 0.5588 - val_mse: 0.5588 - val_mae: 0.5548\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4635 - mse: 0.4635 - mae: 0.5263 - val_loss: 0.5550 - val_mse: 0.5550 - val_mae: 0.5534\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4798 - mse: 0.4798 - mae: 0.5383 - val_loss: 0.5477 - val_mse: 0.5477 - val_mae: 0.5443\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4737 - mse: 0.4737 - mae: 0.5388 - val_loss: 0.5428 - val_mse: 0.5428 - val_mae: 0.5484\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4623 - mse: 0.4623 - mae: 0.5293 - val_loss: 0.5366 - val_mse: 0.5366 - val_mae: 0.5426\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4553 - mse: 0.4553 - mae: 0.5259 - val_loss: 0.5329 - val_mse: 0.5329 - val_mae: 0.5411\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4477 - mse: 0.4477 - mae: 0.5278 - val_loss: 0.5366 - val_mse: 0.5366 - val_mae: 0.5407\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4472 - mse: 0.4472 - mae: 0.5224 - val_loss: 0.5340 - val_mse: 0.5340 - val_mae: 0.5395\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4690 - mse: 0.4690 - mae: 0.5375 - val_loss: 0.5254 - val_mse: 0.5254 - val_mae: 0.5377\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4325 - mse: 0.4325 - mae: 0.5140 - val_loss: 0.5316 - val_mse: 0.5316 - val_mae: 0.5408\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4474 - mse: 0.4474 - mae: 0.5257 - val_loss: 0.5225 - val_mse: 0.5225 - val_mae: 0.5376\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4519 - mse: 0.4519 - mae: 0.5250 - val_loss: 0.5089 - val_mse: 0.5089 - val_mae: 0.5294\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4378 - mse: 0.4378 - mae: 0.5124 - val_loss: 0.5180 - val_mse: 0.5180 - val_mae: 0.5345\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4453 - mse: 0.4453 - mae: 0.5218 - val_loss: 0.5095 - val_mse: 0.5095 - val_mae: 0.5367\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4653 - mse: 0.4653 - mae: 0.5305 - val_loss: 0.5010 - val_mse: 0.5010 - val_mae: 0.5258\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4414 - mse: 0.4414 - mae: 0.5173 - val_loss: 0.5041 - val_mse: 0.5041 - val_mae: 0.5250\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4363 - mse: 0.4363 - mae: 0.5096 - val_loss: 0.5077 - val_mse: 0.5077 - val_mae: 0.5306\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4397 - mse: 0.4397 - mae: 0.5140 - val_loss: 0.5007 - val_mse: 0.5007 - val_mae: 0.5263\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4376 - mse: 0.4376 - mae: 0.5117 - val_loss: 0.4987 - val_mse: 0.4987 - val_mae: 0.5265\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4366 - mse: 0.4366 - mae: 0.5118 - val_loss: 0.5017 - val_mse: 0.5017 - val_mae: 0.5263\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4459 - mse: 0.4459 - mae: 0.5167 - val_loss: 0.4958 - val_mse: 0.4958 - val_mae: 0.5265\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4262 - mse: 0.4262 - mae: 0.5151 - val_loss: 0.4839 - val_mse: 0.4839 - val_mae: 0.5193\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4370 - mse: 0.4370 - mae: 0.5185 - val_loss: 0.4847 - val_mse: 0.4847 - val_mae: 0.5214\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4478 - mse: 0.4478 - mae: 0.5199 - val_loss: 0.4815 - val_mse: 0.4815 - val_mae: 0.5189\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4450 - mse: 0.4450 - mae: 0.5227 - val_loss: 0.4766 - val_mse: 0.4766 - val_mae: 0.5187\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4288 - mse: 0.4288 - mae: 0.5076 - val_loss: 0.4820 - val_mse: 0.4820 - val_mae: 0.5228\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4345 - mse: 0.4345 - mae: 0.5091 - val_loss: 0.4892 - val_mse: 0.4892 - val_mae: 0.5225\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4137 - mse: 0.4137 - mae: 0.4976 - val_loss: 0.4729 - val_mse: 0.4729 - val_mae: 0.5142\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4264 - mse: 0.4264 - mae: 0.5098 - val_loss: 0.4708 - val_mse: 0.4708 - val_mae: 0.5146\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4237 - mse: 0.4237 - mae: 0.5056 - val_loss: 0.4700 - val_mse: 0.4700 - val_mae: 0.5166\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4243 - mse: 0.4243 - mae: 0.5071 - val_loss: 0.4934 - val_mse: 0.4934 - val_mae: 0.5256\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4195 - mse: 0.4195 - mae: 0.5043 - val_loss: 0.4741 - val_mse: 0.4741 - val_mae: 0.5150\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4150 - mse: 0.4150 - mae: 0.5015 - val_loss: 0.4735 - val_mse: 0.4735 - val_mae: 0.5170\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4193 - mse: 0.4193 - mae: 0.5091 - val_loss: 0.4604 - val_mse: 0.4604 - val_mae: 0.5113\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4112 - mse: 0.4112 - mae: 0.4973 - val_loss: 0.4543 - val_mse: 0.4543 - val_mae: 0.5086\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4062 - mse: 0.4062 - mae: 0.4991 - val_loss: 0.4648 - val_mse: 0.4648 - val_mae: 0.5144\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4270 - mse: 0.4270 - mae: 0.5128 - val_loss: 0.4647 - val_mse: 0.4647 - val_mae: 0.5121\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4260 - mse: 0.4260 - mae: 0.5046 - val_loss: 0.4616 - val_mse: 0.4616 - val_mae: 0.5112\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4174 - mse: 0.4174 - mae: 0.5042 - val_loss: 0.4560 - val_mse: 0.4560 - val_mae: 0.5102\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4332 - mse: 0.4332 - mae: 0.5183 - val_loss: 0.4475 - val_mse: 0.4475 - val_mae: 0.5083\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4158 - mse: 0.4158 - mae: 0.5004 - val_loss: 0.4651 - val_mse: 0.4651 - val_mae: 0.5141\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4112 - mse: 0.4112 - mae: 0.4978 - val_loss: 0.4464 - val_mse: 0.4464 - val_mae: 0.5103\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4274 - mse: 0.4274 - mae: 0.5114 - val_loss: 0.4716 - val_mse: 0.4716 - val_mae: 0.5203\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4236 - mse: 0.4236 - mae: 0.5120 - val_loss: 0.4596 - val_mse: 0.4596 - val_mae: 0.5144\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4105 - mse: 0.4105 - mae: 0.4951 - val_loss: 0.4467 - val_mse: 0.4467 - val_mae: 0.5058\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4026 - mse: 0.4026 - mae: 0.4947 - val_loss: 0.4466 - val_mse: 0.4466 - val_mae: 0.5051\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4212 - mse: 0.4212 - mae: 0.5020 - val_loss: 0.4425 - val_mse: 0.4425 - val_mae: 0.5031\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4219 - mse: 0.4219 - mae: 0.5035 - val_loss: 0.4516 - val_mse: 0.4516 - val_mae: 0.5109\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4096 - mse: 0.4096 - mae: 0.4946 - val_loss: 0.4450 - val_mse: 0.4450 - val_mae: 0.5035\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4144 - mse: 0.4144 - mae: 0.5039 - val_loss: 0.4404 - val_mse: 0.4404 - val_mae: 0.5017\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4174 - mse: 0.4174 - mae: 0.4975 - val_loss: 0.4474 - val_mse: 0.4474 - val_mae: 0.5105\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4083 - mse: 0.4083 - mae: 0.4927 - val_loss: 0.4355 - val_mse: 0.4355 - val_mae: 0.4993\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4107 - mse: 0.4107 - mae: 0.5007 - val_loss: 0.4514 - val_mse: 0.4514 - val_mae: 0.5072\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4297 - mse: 0.4297 - mae: 0.5109 - val_loss: 0.4329 - val_mse: 0.4329 - val_mae: 0.4994\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3923 - mse: 0.3923 - mae: 0.4926 - val_loss: 0.4340 - val_mse: 0.4340 - val_mae: 0.5054\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4133 - mse: 0.4133 - mae: 0.5008 - val_loss: 0.4396 - val_mse: 0.4396 - val_mae: 0.5050\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4146 - mse: 0.4146 - mae: 0.5016 - val_loss: 0.4452 - val_mse: 0.4452 - val_mae: 0.5057\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3974 - mse: 0.3974 - mae: 0.4850 - val_loss: 0.4467 - val_mse: 0.4467 - val_mae: 0.5088\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4016 - mse: 0.4016 - mae: 0.4933 - val_loss: 0.4433 - val_mse: 0.4433 - val_mae: 0.5095\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4009 - mse: 0.4009 - mae: 0.4960 - val_loss: 0.4254 - val_mse: 0.4254 - val_mae: 0.4975\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3970 - mse: 0.3970 - mae: 0.4929 - val_loss: 0.4349 - val_mse: 0.4349 - val_mae: 0.4997\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3996 - mse: 0.3996 - mae: 0.4852 - val_loss: 0.4257 - val_mse: 0.4257 - val_mae: 0.4956\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4075 - mse: 0.4075 - mae: 0.4985 - val_loss: 0.4323 - val_mse: 0.4323 - val_mae: 0.5045\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4042 - mse: 0.4042 - mae: 0.4982 - val_loss: 0.4308 - val_mse: 0.4308 - val_mae: 0.4992\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4100 - mse: 0.4100 - mae: 0.4930 - val_loss: 0.4315 - val_mse: 0.4315 - val_mae: 0.5003\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3976 - mse: 0.3976 - mae: 0.4934 - val_loss: 0.4250 - val_mse: 0.4250 - val_mae: 0.4989\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3875 - mse: 0.3875 - mae: 0.4820 - val_loss: 0.4364 - val_mse: 0.4364 - val_mae: 0.5026\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3858 - mse: 0.3858 - mae: 0.4816 - val_loss: 0.4325 - val_mse: 0.4325 - val_mae: 0.4996\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4241 - mse: 0.4241 - mae: 0.5117 - val_loss: 0.4325 - val_mse: 0.4325 - val_mae: 0.5013\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3856 - mse: 0.3856 - mae: 0.4805 - val_loss: 0.4340 - val_mse: 0.4340 - val_mae: 0.5018\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4012 - mse: 0.4012 - mae: 0.4902 - val_loss: 0.4262 - val_mse: 0.4262 - val_mae: 0.4995\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4007 - mse: 0.4007 - mae: 0.4943 - val_loss: 0.4348 - val_mse: 0.4348 - val_mae: 0.5038\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3910 - mse: 0.3910 - mae: 0.4878 - val_loss: 0.4186 - val_mse: 0.4186 - val_mae: 0.4950\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4011 - mse: 0.4011 - mae: 0.4941 - val_loss: 0.4211 - val_mse: 0.4211 - val_mae: 0.4957\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3982 - mse: 0.3982 - mae: 0.4902 - val_loss: 0.4239 - val_mse: 0.4239 - val_mae: 0.4992\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3900 - mse: 0.3900 - mae: 0.4870 - val_loss: 0.4122 - val_mse: 0.4122 - val_mae: 0.4899\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3922 - mse: 0.3922 - mae: 0.4876 - val_loss: 0.4337 - val_mse: 0.4337 - val_mae: 0.5023\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4112 - mse: 0.4112 - mae: 0.5064 - val_loss: 0.4163 - val_mse: 0.4163 - val_mae: 0.4933\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3928 - mse: 0.3928 - mae: 0.4830 - val_loss: 0.4243 - val_mse: 0.4243 - val_mae: 0.4987\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4004 - mse: 0.4004 - mae: 0.4950 - val_loss: 0.4187 - val_mse: 0.4187 - val_mae: 0.4964\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3984 - mse: 0.3984 - mae: 0.4911 - val_loss: 0.4228 - val_mse: 0.4228 - val_mae: 0.4953\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4042 - mse: 0.4042 - mae: 0.4887 - val_loss: 0.4203 - val_mse: 0.4203 - val_mae: 0.4953\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3838 - mse: 0.3838 - mae: 0.4818 - val_loss: 0.4162 - val_mse: 0.4162 - val_mae: 0.4941\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3774 - mse: 0.3774 - mae: 0.4806 - val_loss: 0.4173 - val_mse: 0.4173 - val_mae: 0.4948\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3971 - mse: 0.3971 - mae: 0.4901 - val_loss: 0.4188 - val_mse: 0.4188 - val_mae: 0.4926\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3951 - mse: 0.3951 - mae: 0.4859 - val_loss: 0.4206 - val_mse: 0.4206 - val_mae: 0.4956\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3884 - mse: 0.3884 - mae: 0.4891 - val_loss: 0.4243 - val_mse: 0.4243 - val_mae: 0.4971\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3813 - mse: 0.3813 - mae: 0.4836 - val_loss: 0.4166 - val_mse: 0.4166 - val_mae: 0.4913\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4160 - mse: 0.4160 - mae: 0.5035 - val_loss: 0.4096 - val_mse: 0.4096 - val_mae: 0.4887\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3936 - mse: 0.3936 - mae: 0.4927 - val_loss: 0.4213 - val_mse: 0.4213 - val_mae: 0.4972\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3928 - mse: 0.3928 - mae: 0.4867 - val_loss: 0.4105 - val_mse: 0.4105 - val_mae: 0.4900\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3789 - mse: 0.3789 - mae: 0.4802 - val_loss: 0.4168 - val_mse: 0.4168 - val_mae: 0.4923\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3869 - mse: 0.3869 - mae: 0.4857 - val_loss: 0.4208 - val_mse: 0.4208 - val_mae: 0.4938\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3802 - mse: 0.3802 - mae: 0.4743 - val_loss: 0.4213 - val_mse: 0.4213 - val_mae: 0.4959\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3782 - mse: 0.3782 - mae: 0.4828 - val_loss: 0.4103 - val_mse: 0.4103 - val_mae: 0.4888\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3817 - mse: 0.3817 - mae: 0.4763 - val_loss: 0.4243 - val_mse: 0.4243 - val_mae: 0.4960\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3853 - mse: 0.3853 - mae: 0.4884 - val_loss: 0.4142 - val_mse: 0.4142 - val_mae: 0.4910\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3999 - mse: 0.3999 - mae: 0.4920 - val_loss: 0.4078 - val_mse: 0.4078 - val_mae: 0.4908\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4026 - mse: 0.4026 - mae: 0.4957 - val_loss: 0.4162 - val_mse: 0.4162 - val_mae: 0.4938\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3987 - mse: 0.3987 - mae: 0.4986 - val_loss: 0.4122 - val_mse: 0.4122 - val_mae: 0.4922\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3944 - mse: 0.3944 - mae: 0.4899 - val_loss: 0.4047 - val_mse: 0.4047 - val_mae: 0.4873\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3821 - mse: 0.3821 - mae: 0.4796 - val_loss: 0.4043 - val_mse: 0.4043 - val_mae: 0.4859\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3821 - mse: 0.3821 - mae: 0.4796 - val_loss: 0.4273 - val_mse: 0.4273 - val_mae: 0.5018\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3881 - mse: 0.3881 - mae: 0.4884 - val_loss: 0.3987 - val_mse: 0.3987 - val_mae: 0.4832\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4057 - mse: 0.4057 - mae: 0.4934 - val_loss: 0.4056 - val_mse: 0.4056 - val_mae: 0.4862\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3948 - mse: 0.3948 - mae: 0.4854 - val_loss: 0.4031 - val_mse: 0.4031 - val_mae: 0.4861\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3875 - mse: 0.3875 - mae: 0.4821 - val_loss: 0.4136 - val_mse: 0.4136 - val_mae: 0.4910\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3921 - mse: 0.3921 - mae: 0.4899 - val_loss: 0.4039 - val_mse: 0.4039 - val_mae: 0.4869\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3914 - mse: 0.3914 - mae: 0.4852 - val_loss: 0.4185 - val_mse: 0.4185 - val_mae: 0.4911\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3827 - mse: 0.3827 - mae: 0.4875 - val_loss: 0.4007 - val_mse: 0.4007 - val_mae: 0.4838\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3841 - mse: 0.3841 - mae: 0.4787 - val_loss: 0.4021 - val_mse: 0.4021 - val_mae: 0.4847\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3723 - mse: 0.3723 - mae: 0.4732 - val_loss: 0.4018 - val_mse: 0.4018 - val_mae: 0.4841\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3797 - mse: 0.3797 - mae: 0.4838 - val_loss: 0.4045 - val_mse: 0.4045 - val_mae: 0.4847\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3992 - mse: 0.3992 - mae: 0.4873 - val_loss: 0.4172 - val_mse: 0.4172 - val_mae: 0.4934\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4040 - mse: 0.4040 - mae: 0.4976 - val_loss: 0.4010 - val_mse: 0.4010 - val_mae: 0.4858\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3958 - mse: 0.3958 - mae: 0.4910 - val_loss: 0.4073 - val_mse: 0.4073 - val_mae: 0.4914\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3916 - mse: 0.3916 - mae: 0.4847 - val_loss: 0.4094 - val_mse: 0.4094 - val_mae: 0.4914\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3789 - mse: 0.3789 - mae: 0.4772 - val_loss: 0.4017 - val_mse: 0.4017 - val_mae: 0.4854\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3814 - mse: 0.3814 - mae: 0.4838 - val_loss: 0.4034 - val_mse: 0.4034 - val_mae: 0.4827\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3778 - mse: 0.3778 - mae: 0.4768 - val_loss: 0.4089 - val_mse: 0.4089 - val_mae: 0.4882\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3795 - mse: 0.3795 - mae: 0.4781 - val_loss: 0.4018 - val_mse: 0.4018 - val_mae: 0.4843\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3818 - mse: 0.3818 - mae: 0.4868 - val_loss: 0.4001 - val_mse: 0.4001 - val_mae: 0.4845\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3874 - mse: 0.3874 - mae: 0.4815 - val_loss: 0.4051 - val_mse: 0.4051 - val_mae: 0.4865\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3817 - mse: 0.3817 - mae: 0.4853 - val_loss: 0.3892 - val_mse: 0.3892 - val_mae: 0.4786\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3893 - mse: 0.3893 - mae: 0.4856 - val_loss: 0.3987 - val_mse: 0.3987 - val_mae: 0.4824\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3723 - mse: 0.3723 - mae: 0.4701 - val_loss: 0.3970 - val_mse: 0.3970 - val_mae: 0.4815\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3765 - mse: 0.3765 - mae: 0.4807 - val_loss: 0.3869 - val_mse: 0.3869 - val_mae: 0.4778\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4062 - mse: 0.4062 - mae: 0.4976 - val_loss: 0.4036 - val_mse: 0.4036 - val_mae: 0.4863\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3744 - mse: 0.3744 - mae: 0.4726 - val_loss: 0.4022 - val_mse: 0.4022 - val_mae: 0.4867\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3873 - mse: 0.3873 - mae: 0.4869 - val_loss: 0.3922 - val_mse: 0.3922 - val_mae: 0.4797\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3884 - mse: 0.3884 - mae: 0.4845 - val_loss: 0.4025 - val_mse: 0.4025 - val_mae: 0.4874\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3821 - mse: 0.3821 - mae: 0.4843 - val_loss: 0.3902 - val_mse: 0.3902 - val_mae: 0.4808\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3872 - mse: 0.3872 - mae: 0.4859 - val_loss: 0.3891 - val_mse: 0.3891 - val_mae: 0.4778\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3824 - mse: 0.3824 - mae: 0.4755 - val_loss: 0.4005 - val_mse: 0.4005 - val_mae: 0.4836\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3840 - mse: 0.3840 - mae: 0.4829 - val_loss: 0.3837 - val_mse: 0.3837 - val_mae: 0.4724\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3841 - mse: 0.3841 - mae: 0.4765 - val_loss: 0.3892 - val_mse: 0.3892 - val_mae: 0.4769\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3842 - mse: 0.3842 - mae: 0.4838 - val_loss: 0.3857 - val_mse: 0.3857 - val_mae: 0.4762\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3609 - mse: 0.3609 - mae: 0.4699 - val_loss: 0.3894 - val_mse: 0.3894 - val_mae: 0.4755\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3831 - mse: 0.3831 - mae: 0.4854 - val_loss: 0.3878 - val_mse: 0.3878 - val_mae: 0.4765\n",
            "Epoch 252/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3852 - mse: 0.3852 - mae: 0.4850 - val_loss: 0.4190 - val_mse: 0.4190 - val_mae: 0.4952\n",
            "Epoch 253/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3815 - mse: 0.3815 - mae: 0.4786 - val_loss: 0.3879 - val_mse: 0.3879 - val_mae: 0.4756\n",
            "Epoch 254/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3692 - mse: 0.3692 - mae: 0.4739 - val_loss: 0.3886 - val_mse: 0.3886 - val_mae: 0.4795\n",
            "Epoch 255/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3551 - mse: 0.3551 - mae: 0.4589 - val_loss: 0.4055 - val_mse: 0.4055 - val_mae: 0.4880\n",
            "Epoch 256/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3649 - mse: 0.3649 - mae: 0.4698 - val_loss: 0.3853 - val_mse: 0.3853 - val_mae: 0.4750\n",
            "Epoch 257/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3851 - mse: 0.3851 - mae: 0.4820 - val_loss: 0.4053 - val_mse: 0.4053 - val_mae: 0.4874\n",
            "Epoch 258/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3918 - mse: 0.3918 - mae: 0.4855 - val_loss: 0.3899 - val_mse: 0.3899 - val_mae: 0.4794\n",
            "Epoch 259/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3785 - mse: 0.3785 - mae: 0.4817 - val_loss: 0.4034 - val_mse: 0.4034 - val_mae: 0.4855\n",
            "Epoch 260/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3798 - mse: 0.3798 - mae: 0.4788 - val_loss: 0.3866 - val_mse: 0.3866 - val_mae: 0.4773\n",
            "Epoch 261/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3785 - mse: 0.3785 - mae: 0.4792 - val_loss: 0.3813 - val_mse: 0.3813 - val_mae: 0.4735\n",
            "Epoch 262/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3738 - mse: 0.3738 - mae: 0.4754 - val_loss: 0.3931 - val_mse: 0.3931 - val_mae: 0.4800\n",
            "Epoch 263/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3756 - mse: 0.3756 - mae: 0.4737 - val_loss: 0.3814 - val_mse: 0.3814 - val_mae: 0.4738\n",
            "Epoch 264/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3695 - mse: 0.3695 - mae: 0.4680 - val_loss: 0.3791 - val_mse: 0.3791 - val_mae: 0.4692\n",
            "Epoch 265/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3672 - mse: 0.3672 - mae: 0.4682 - val_loss: 0.3959 - val_mse: 0.3959 - val_mae: 0.4836\n",
            "Epoch 266/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3817 - mse: 0.3817 - mae: 0.4827 - val_loss: 0.3858 - val_mse: 0.3858 - val_mae: 0.4756\n",
            "Epoch 267/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3624 - mse: 0.3624 - mae: 0.4713 - val_loss: 0.3790 - val_mse: 0.3790 - val_mae: 0.4683\n",
            "Epoch 268/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3718 - mse: 0.3718 - mae: 0.4735 - val_loss: 0.3779 - val_mse: 0.3779 - val_mae: 0.4685\n",
            "Epoch 269/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3705 - mse: 0.3705 - mae: 0.4682 - val_loss: 0.3907 - val_mse: 0.3907 - val_mae: 0.4785\n",
            "Epoch 270/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3692 - mse: 0.3692 - mae: 0.4746 - val_loss: 0.3871 - val_mse: 0.3871 - val_mae: 0.4777\n",
            "Epoch 271/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3604 - mse: 0.3604 - mae: 0.4693 - val_loss: 0.3825 - val_mse: 0.3825 - val_mae: 0.4719\n",
            "Epoch 272/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3725 - mse: 0.3725 - mae: 0.4715 - val_loss: 0.3939 - val_mse: 0.3939 - val_mae: 0.4801\n",
            "Epoch 273/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3747 - mse: 0.3747 - mae: 0.4770 - val_loss: 0.3822 - val_mse: 0.3822 - val_mae: 0.4725\n",
            "Epoch 274/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3822 - mse: 0.3822 - mae: 0.4767 - val_loss: 0.3793 - val_mse: 0.3793 - val_mae: 0.4691\n",
            "Epoch 275/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3774 - mse: 0.3774 - mae: 0.4809 - val_loss: 0.3785 - val_mse: 0.3785 - val_mae: 0.4725\n",
            "Epoch 276/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3760 - mse: 0.3760 - mae: 0.4779 - val_loss: 0.3830 - val_mse: 0.3830 - val_mae: 0.4733\n",
            "Epoch 277/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3797 - mse: 0.3797 - mae: 0.4858 - val_loss: 0.3794 - val_mse: 0.3794 - val_mae: 0.4674\n",
            "Epoch 278/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3603 - mse: 0.3603 - mae: 0.4660 - val_loss: 0.3844 - val_mse: 0.3844 - val_mae: 0.4726\n",
            "Epoch 279/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3586 - mse: 0.3586 - mae: 0.4651 - val_loss: 0.3882 - val_mse: 0.3882 - val_mae: 0.4733\n",
            "Epoch 280/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3612 - mse: 0.3612 - mae: 0.4674 - val_loss: 0.3793 - val_mse: 0.3793 - val_mae: 0.4695\n",
            "Epoch 281/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3705 - mse: 0.3705 - mae: 0.4695 - val_loss: 0.3896 - val_mse: 0.3896 - val_mae: 0.4752\n",
            "Epoch 282/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3594 - mse: 0.3594 - mae: 0.4656 - val_loss: 0.3842 - val_mse: 0.3842 - val_mae: 0.4743\n",
            "Epoch 283/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3684 - mse: 0.3684 - mae: 0.4765 - val_loss: 0.3791 - val_mse: 0.3791 - val_mae: 0.4713\n",
            "Epoch 284/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3801 - mse: 0.3801 - mae: 0.4815 - val_loss: 0.3793 - val_mse: 0.3793 - val_mae: 0.4681\n",
            "Epoch 285/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3731 - mse: 0.3731 - mae: 0.4706 - val_loss: 0.3880 - val_mse: 0.3880 - val_mae: 0.4764\n",
            "Epoch 286/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3737 - mse: 0.3737 - mae: 0.4726 - val_loss: 0.3795 - val_mse: 0.3795 - val_mae: 0.4694\n",
            "Epoch 287/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3667 - mse: 0.3667 - mae: 0.4715 - val_loss: 0.3982 - val_mse: 0.3982 - val_mae: 0.4804\n",
            "Epoch 288/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3602 - mse: 0.3602 - mae: 0.4668 - val_loss: 0.3820 - val_mse: 0.3820 - val_mae: 0.4705\n",
            "Epoch 289/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3547 - mse: 0.3547 - mae: 0.4619 - val_loss: 0.3820 - val_mse: 0.3820 - val_mae: 0.4711\n",
            "Epoch 290/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3813 - mse: 0.3813 - mae: 0.4771 - val_loss: 0.3818 - val_mse: 0.3818 - val_mae: 0.4723\n",
            "Epoch 291/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3570 - mse: 0.3570 - mae: 0.4743 - val_loss: 0.3780 - val_mse: 0.3780 - val_mae: 0.4693\n",
            "Epoch 292/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3561 - mse: 0.3561 - mae: 0.4586 - val_loss: 0.4044 - val_mse: 0.4044 - val_mae: 0.4885\n",
            "Epoch 293/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3673 - mse: 0.3673 - mae: 0.4714 - val_loss: 0.3794 - val_mse: 0.3794 - val_mae: 0.4683\n",
            "Epoch 294/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3577 - mse: 0.3577 - mae: 0.4609 - val_loss: 0.3817 - val_mse: 0.3817 - val_mae: 0.4714\n",
            "Epoch 295/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3634 - mse: 0.3634 - mae: 0.4708 - val_loss: 0.3932 - val_mse: 0.3932 - val_mae: 0.4792\n",
            "Epoch 296/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3504 - mse: 0.3504 - mae: 0.4656 - val_loss: 0.3851 - val_mse: 0.3851 - val_mae: 0.4741\n",
            "Epoch 297/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3633 - mse: 0.3633 - mae: 0.4728 - val_loss: 0.3931 - val_mse: 0.3931 - val_mae: 0.4794\n",
            "Epoch 298/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3620 - mse: 0.3620 - mae: 0.4677 - val_loss: 0.3807 - val_mse: 0.3807 - val_mae: 0.4687\n",
            "Epoch 299/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3671 - mse: 0.3671 - mae: 0.4700 - val_loss: 0.3802 - val_mse: 0.3802 - val_mae: 0.4705\n",
            "Epoch 300/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3573 - mse: 0.3573 - mae: 0.4668 - val_loss: 0.3779 - val_mse: 0.3779 - val_mae: 0.4680\n",
            "Epoch 301/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3638 - mse: 0.3638 - mae: 0.4675 - val_loss: 0.3785 - val_mse: 0.3785 - val_mae: 0.4697\n",
            "Epoch 302/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3772 - mse: 0.3772 - mae: 0.4769 - val_loss: 0.3843 - val_mse: 0.3843 - val_mae: 0.4750\n",
            "Epoch 303/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3717 - mse: 0.3717 - mae: 0.4727 - val_loss: 0.3757 - val_mse: 0.3757 - val_mae: 0.4681\n",
            "Epoch 304/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3605 - mse: 0.3605 - mae: 0.4774 - val_loss: 0.3757 - val_mse: 0.3757 - val_mae: 0.4669\n",
            "Epoch 305/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3885 - mse: 0.3885 - mae: 0.4784 - val_loss: 0.3957 - val_mse: 0.3957 - val_mae: 0.4817\n",
            "Epoch 306/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3678 - mse: 0.3678 - mae: 0.4674 - val_loss: 0.3725 - val_mse: 0.3725 - val_mae: 0.4643\n",
            "Epoch 307/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3786 - mse: 0.3786 - mae: 0.4828 - val_loss: 0.3671 - val_mse: 0.3671 - val_mae: 0.4606\n",
            "Epoch 308/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3654 - mse: 0.3654 - mae: 0.4671 - val_loss: 0.3780 - val_mse: 0.3780 - val_mae: 0.4675\n",
            "Epoch 309/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3680 - mse: 0.3680 - mae: 0.4646 - val_loss: 0.3701 - val_mse: 0.3701 - val_mae: 0.4637\n",
            "Epoch 310/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3790 - mse: 0.3790 - mae: 0.4767 - val_loss: 0.3944 - val_mse: 0.3944 - val_mae: 0.4793\n",
            "Epoch 311/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3689 - mse: 0.3689 - mae: 0.4701 - val_loss: 0.3808 - val_mse: 0.3808 - val_mae: 0.4714\n",
            "Epoch 312/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3557 - mse: 0.3557 - mae: 0.4580 - val_loss: 0.3763 - val_mse: 0.3763 - val_mae: 0.4672\n",
            "Epoch 313/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3672 - mse: 0.3672 - mae: 0.4703 - val_loss: 0.3792 - val_mse: 0.3792 - val_mae: 0.4719\n",
            "Epoch 314/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3686 - mse: 0.3686 - mae: 0.4737 - val_loss: 0.3701 - val_mse: 0.3701 - val_mae: 0.4636\n",
            "Epoch 315/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3575 - mse: 0.3575 - mae: 0.4627 - val_loss: 0.3818 - val_mse: 0.3818 - val_mae: 0.4711\n",
            "Epoch 316/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3505 - mse: 0.3505 - mae: 0.4628 - val_loss: 0.3815 - val_mse: 0.3815 - val_mae: 0.4712\n",
            "Epoch 317/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3642 - mse: 0.3642 - mae: 0.4746 - val_loss: 0.3869 - val_mse: 0.3869 - val_mae: 0.4749\n",
            "Epoch 318/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3768 - mse: 0.3768 - mae: 0.4789 - val_loss: 0.3791 - val_mse: 0.3791 - val_mae: 0.4680\n",
            "Epoch 319/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3617 - mse: 0.3617 - mae: 0.4703 - val_loss: 0.3796 - val_mse: 0.3796 - val_mae: 0.4706\n",
            "Epoch 320/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3610 - mse: 0.3610 - mae: 0.4716 - val_loss: 0.3686 - val_mse: 0.3686 - val_mae: 0.4616\n",
            "Epoch 321/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3627 - mse: 0.3627 - mae: 0.4663 - val_loss: 0.3937 - val_mse: 0.3937 - val_mae: 0.4801\n",
            "Epoch 322/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3720 - mse: 0.3720 - mae: 0.4791 - val_loss: 0.3756 - val_mse: 0.3756 - val_mae: 0.4668\n",
            "Epoch 323/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3650 - mse: 0.3650 - mae: 0.4656 - val_loss: 0.3812 - val_mse: 0.3812 - val_mae: 0.4690\n",
            "Epoch 324/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3629 - mse: 0.3629 - mae: 0.4664 - val_loss: 0.3726 - val_mse: 0.3726 - val_mae: 0.4629\n",
            "Epoch 325/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3546 - mse: 0.3546 - mae: 0.4571 - val_loss: 0.3887 - val_mse: 0.3887 - val_mae: 0.4768\n",
            "Epoch 326/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3526 - mse: 0.3526 - mae: 0.4595 - val_loss: 0.3877 - val_mse: 0.3877 - val_mae: 0.4784\n",
            "Epoch 327/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3645 - mse: 0.3645 - mae: 0.4729 - val_loss: 0.3711 - val_mse: 0.3711 - val_mae: 0.4627\n",
            "Epoch 328/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3674 - mse: 0.3674 - mae: 0.4739 - val_loss: 0.3866 - val_mse: 0.3866 - val_mae: 0.4766\n",
            "Epoch 329/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3523 - mse: 0.3523 - mae: 0.4631 - val_loss: 0.3842 - val_mse: 0.3842 - val_mae: 0.4745\n",
            "Epoch 330/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3651 - mse: 0.3651 - mae: 0.4716 - val_loss: 0.3713 - val_mse: 0.3713 - val_mae: 0.4616\n",
            "Epoch 331/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3655 - mse: 0.3655 - mae: 0.4693 - val_loss: 0.3776 - val_mse: 0.3776 - val_mae: 0.4703\n",
            "Epoch 332/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3642 - mse: 0.3642 - mae: 0.4729 - val_loss: 0.3787 - val_mse: 0.3787 - val_mae: 0.4716\n",
            "Epoch 333/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3583 - mse: 0.3583 - mae: 0.4598 - val_loss: 0.3757 - val_mse: 0.3757 - val_mae: 0.4684\n",
            "Epoch 334/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3565 - mse: 0.3565 - mae: 0.4632 - val_loss: 0.3816 - val_mse: 0.3816 - val_mae: 0.4729\n",
            "Epoch 335/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3638 - mse: 0.3638 - mae: 0.4655 - val_loss: 0.3747 - val_mse: 0.3747 - val_mae: 0.4670\n",
            "Epoch 336/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3602 - mse: 0.3602 - mae: 0.4703 - val_loss: 0.3696 - val_mse: 0.3696 - val_mae: 0.4631\n",
            "Epoch 337/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3452 - mse: 0.3452 - mae: 0.4594 - val_loss: 0.3742 - val_mse: 0.3742 - val_mae: 0.4700\n",
            "Epoch 338/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3433 - mse: 0.3433 - mae: 0.4590 - val_loss: 0.3846 - val_mse: 0.3846 - val_mae: 0.4772\n",
            "Epoch 339/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3605 - mse: 0.3605 - mae: 0.4658 - val_loss: 0.3718 - val_mse: 0.3718 - val_mae: 0.4652\n",
            "Epoch 340/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3529 - mse: 0.3529 - mae: 0.4536 - val_loss: 0.3885 - val_mse: 0.3885 - val_mae: 0.4774\n",
            "Epoch 341/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3591 - mse: 0.3591 - mae: 0.4692 - val_loss: 0.3684 - val_mse: 0.3684 - val_mae: 0.4615\n",
            "Epoch 342/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3684 - mse: 0.3684 - mae: 0.4667 - val_loss: 0.3802 - val_mse: 0.3802 - val_mae: 0.4716\n",
            "Epoch 343/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3678 - mse: 0.3678 - mae: 0.4685 - val_loss: 0.3844 - val_mse: 0.3844 - val_mae: 0.4768\n",
            "Epoch 344/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3527 - mse: 0.3527 - mae: 0.4647 - val_loss: 0.3746 - val_mse: 0.3746 - val_mae: 0.4656\n",
            "Epoch 345/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3493 - mse: 0.3493 - mae: 0.4574 - val_loss: 0.3746 - val_mse: 0.3746 - val_mae: 0.4662\n",
            "Epoch 346/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3628 - mse: 0.3628 - mae: 0.4711 - val_loss: 0.3691 - val_mse: 0.3691 - val_mae: 0.4645\n",
            "Epoch 347/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3617 - mse: 0.3617 - mae: 0.4676 - val_loss: 0.3851 - val_mse: 0.3851 - val_mae: 0.4756\n",
            "Epoch 348/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3651 - mse: 0.3651 - mae: 0.4750 - val_loss: 0.3638 - val_mse: 0.3638 - val_mae: 0.4586\n",
            "Epoch 349/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3594 - mse: 0.3594 - mae: 0.4701 - val_loss: 0.3709 - val_mse: 0.3709 - val_mae: 0.4629\n",
            "Epoch 350/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3553 - mse: 0.3553 - mae: 0.4625 - val_loss: 0.3750 - val_mse: 0.3750 - val_mae: 0.4666\n",
            "Epoch 351/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3483 - mse: 0.3483 - mae: 0.4613 - val_loss: 0.3614 - val_mse: 0.3614 - val_mae: 0.4548\n",
            "Epoch 352/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3804 - mse: 0.3804 - mae: 0.4744 - val_loss: 0.3756 - val_mse: 0.3756 - val_mae: 0.4670\n",
            "Epoch 353/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3533 - mse: 0.3533 - mae: 0.4665 - val_loss: 0.3651 - val_mse: 0.3651 - val_mae: 0.4584\n",
            "Epoch 354/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3541 - mse: 0.3541 - mae: 0.4596 - val_loss: 0.3690 - val_mse: 0.3690 - val_mae: 0.4631\n",
            "Epoch 355/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3600 - mse: 0.3600 - mae: 0.4599 - val_loss: 0.3652 - val_mse: 0.3652 - val_mae: 0.4587\n",
            "Epoch 356/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3716 - mse: 0.3716 - mae: 0.4736 - val_loss: 0.4022 - val_mse: 0.4022 - val_mae: 0.4804\n",
            "Epoch 357/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3585 - mse: 0.3585 - mae: 0.4682 - val_loss: 0.3665 - val_mse: 0.3665 - val_mae: 0.4576\n",
            "Epoch 358/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3412 - mse: 0.3412 - mae: 0.4487 - val_loss: 0.3831 - val_mse: 0.3831 - val_mae: 0.4699\n",
            "Epoch 359/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3451 - mse: 0.3451 - mae: 0.4546 - val_loss: 0.3661 - val_mse: 0.3661 - val_mae: 0.4582\n",
            "Epoch 360/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3474 - mse: 0.3474 - mae: 0.4524 - val_loss: 0.3703 - val_mse: 0.3703 - val_mae: 0.4610\n",
            "Epoch 361/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3552 - mse: 0.3552 - mae: 0.4634 - val_loss: 0.3627 - val_mse: 0.3627 - val_mae: 0.4576\n",
            "Epoch 362/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3652 - mse: 0.3652 - mae: 0.4704 - val_loss: 0.3653 - val_mse: 0.3653 - val_mae: 0.4618\n",
            "Epoch 363/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3536 - mse: 0.3536 - mae: 0.4570 - val_loss: 0.3710 - val_mse: 0.3710 - val_mae: 0.4581\n",
            "Epoch 364/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3438 - mse: 0.3438 - mae: 0.4572 - val_loss: 0.3684 - val_mse: 0.3684 - val_mae: 0.4628\n",
            "Epoch 365/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3446 - mse: 0.3446 - mae: 0.4548 - val_loss: 0.3732 - val_mse: 0.3732 - val_mae: 0.4638\n",
            "Epoch 366/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3452 - mse: 0.3452 - mae: 0.4558 - val_loss: 0.3672 - val_mse: 0.3672 - val_mae: 0.4611\n",
            "Epoch 367/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3495 - mse: 0.3495 - mae: 0.4586 - val_loss: 0.3843 - val_mse: 0.3843 - val_mae: 0.4699\n",
            "Epoch 368/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3405 - mse: 0.3405 - mae: 0.4498 - val_loss: 0.3742 - val_mse: 0.3742 - val_mae: 0.4644\n",
            "Epoch 369/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3538 - mse: 0.3538 - mae: 0.4590 - val_loss: 0.3707 - val_mse: 0.3707 - val_mae: 0.4592\n",
            "Epoch 370/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3657 - mse: 0.3657 - mae: 0.4721 - val_loss: 0.3764 - val_mse: 0.3764 - val_mae: 0.4658\n",
            "Epoch 371/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3503 - mse: 0.3503 - mae: 0.4641 - val_loss: 0.3779 - val_mse: 0.3779 - val_mae: 0.4638\n",
            "Epoch 372/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3447 - mse: 0.3447 - mae: 0.4589 - val_loss: 0.3724 - val_mse: 0.3724 - val_mae: 0.4628\n",
            "Epoch 373/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3651 - mse: 0.3651 - mae: 0.4730 - val_loss: 0.3745 - val_mse: 0.3745 - val_mae: 0.4651\n",
            "Epoch 374/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3509 - mse: 0.3509 - mae: 0.4564 - val_loss: 0.3796 - val_mse: 0.3796 - val_mae: 0.4655\n",
            "Epoch 375/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3382 - mse: 0.3382 - mae: 0.4522 - val_loss: 0.3691 - val_mse: 0.3691 - val_mae: 0.4606\n",
            "Epoch 376/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3632 - mse: 0.3632 - mae: 0.4705 - val_loss: 0.3784 - val_mse: 0.3784 - val_mae: 0.4656\n",
            "Epoch 377/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3498 - mse: 0.3498 - mae: 0.4558 - val_loss: 0.3741 - val_mse: 0.3741 - val_mae: 0.4642\n",
            "Epoch 378/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3459 - mse: 0.3459 - mae: 0.4572 - val_loss: 0.3674 - val_mse: 0.3674 - val_mae: 0.4578\n",
            "Epoch 379/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3574 - mse: 0.3574 - mae: 0.4707 - val_loss: 0.3721 - val_mse: 0.3721 - val_mae: 0.4627\n",
            "Epoch 380/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3529 - mse: 0.3529 - mae: 0.4576 - val_loss: 0.3685 - val_mse: 0.3685 - val_mae: 0.4586\n",
            "Epoch 381/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3402 - mse: 0.3402 - mae: 0.4484 - val_loss: 0.3820 - val_mse: 0.3820 - val_mae: 0.4690\n",
            "Epoch 382/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3470 - mse: 0.3470 - mae: 0.4558 - val_loss: 0.3694 - val_mse: 0.3694 - val_mae: 0.4594\n",
            "Epoch 383/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3345 - mse: 0.3345 - mae: 0.4427 - val_loss: 0.3764 - val_mse: 0.3764 - val_mae: 0.4655\n",
            "Epoch 384/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3469 - mse: 0.3469 - mae: 0.4610 - val_loss: 0.3769 - val_mse: 0.3769 - val_mae: 0.4644\n",
            "Epoch 385/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3526 - mse: 0.3526 - mae: 0.4648 - val_loss: 0.3644 - val_mse: 0.3644 - val_mae: 0.4567\n",
            "Epoch 386/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3458 - mse: 0.3458 - mae: 0.4556 - val_loss: 0.3778 - val_mse: 0.3778 - val_mae: 0.4657\n",
            "Epoch 387/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3638 - mse: 0.3638 - mae: 0.4742 - val_loss: 0.3734 - val_mse: 0.3734 - val_mae: 0.4624\n",
            "Epoch 388/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3491 - mse: 0.3491 - mae: 0.4666 - val_loss: 0.3698 - val_mse: 0.3698 - val_mae: 0.4621\n",
            "Epoch 389/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3513 - mse: 0.3513 - mae: 0.4563 - val_loss: 0.3744 - val_mse: 0.3744 - val_mae: 0.4638\n",
            "Epoch 390/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3394 - mse: 0.3394 - mae: 0.4521 - val_loss: 0.3665 - val_mse: 0.3665 - val_mae: 0.4573\n",
            "Epoch 391/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3505 - mse: 0.3505 - mae: 0.4625 - val_loss: 0.3755 - val_mse: 0.3755 - val_mae: 0.4655\n",
            "Epoch 392/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3504 - mse: 0.3504 - mae: 0.4569 - val_loss: 0.3628 - val_mse: 0.3628 - val_mae: 0.4539\n",
            "Epoch 393/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3361 - mse: 0.3361 - mae: 0.4507 - val_loss: 0.3681 - val_mse: 0.3681 - val_mae: 0.4582\n",
            "Epoch 394/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3663 - mse: 0.3663 - mae: 0.4729 - val_loss: 0.3799 - val_mse: 0.3799 - val_mae: 0.4683\n",
            "Epoch 395/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3662 - mse: 0.3662 - mae: 0.4706 - val_loss: 0.3781 - val_mse: 0.3781 - val_mae: 0.4664\n",
            "Epoch 396/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3378 - mse: 0.3378 - mae: 0.4507 - val_loss: 0.3678 - val_mse: 0.3678 - val_mae: 0.4588\n",
            "Epoch 397/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3445 - mse: 0.3445 - mae: 0.4606 - val_loss: 0.3638 - val_mse: 0.3638 - val_mae: 0.4561\n",
            "Epoch 398/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3504 - mse: 0.3504 - mae: 0.4552 - val_loss: 0.3599 - val_mse: 0.3599 - val_mae: 0.4523\n",
            "Epoch 399/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3495 - mse: 0.3495 - mae: 0.4579 - val_loss: 0.3713 - val_mse: 0.3713 - val_mae: 0.4628\n",
            "Epoch 400/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3458 - mse: 0.3458 - mae: 0.4569 - val_loss: 0.3748 - val_mse: 0.3748 - val_mae: 0.4655\n",
            "Epoch 401/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3577 - mse: 0.3577 - mae: 0.4693 - val_loss: 0.3693 - val_mse: 0.3693 - val_mae: 0.4612\n",
            "Epoch 402/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3443 - mse: 0.3443 - mae: 0.4527 - val_loss: 0.3677 - val_mse: 0.3677 - val_mae: 0.4594\n",
            "Epoch 403/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3484 - mse: 0.3484 - mae: 0.4572 - val_loss: 0.3727 - val_mse: 0.3727 - val_mae: 0.4642\n",
            "Epoch 404/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3530 - mse: 0.3530 - mae: 0.4657 - val_loss: 0.3673 - val_mse: 0.3673 - val_mae: 0.4573\n",
            "Epoch 405/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3457 - mse: 0.3457 - mae: 0.4522 - val_loss: 0.3660 - val_mse: 0.3660 - val_mae: 0.4576\n",
            "Epoch 406/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3370 - mse: 0.3370 - mae: 0.4501 - val_loss: 0.3637 - val_mse: 0.3637 - val_mae: 0.4563\n",
            "Epoch 407/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3516 - mse: 0.3516 - mae: 0.4614 - val_loss: 0.3622 - val_mse: 0.3622 - val_mae: 0.4577\n",
            "Epoch 408/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3427 - mse: 0.3427 - mae: 0.4546 - val_loss: 0.3735 - val_mse: 0.3735 - val_mae: 0.4623\n",
            "Epoch 409/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3386 - mse: 0.3386 - mae: 0.4601 - val_loss: 0.3581 - val_mse: 0.3581 - val_mae: 0.4523\n",
            "Epoch 410/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3539 - mse: 0.3539 - mae: 0.4523 - val_loss: 0.3703 - val_mse: 0.3703 - val_mae: 0.4588\n",
            "Epoch 411/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3526 - mse: 0.3526 - mae: 0.4603 - val_loss: 0.3929 - val_mse: 0.3929 - val_mae: 0.4763\n",
            "Epoch 412/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3405 - mse: 0.3405 - mae: 0.4512 - val_loss: 0.3619 - val_mse: 0.3619 - val_mae: 0.4547\n",
            "Epoch 413/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3422 - mse: 0.3422 - mae: 0.4523 - val_loss: 0.3783 - val_mse: 0.3783 - val_mae: 0.4672\n",
            "Epoch 414/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3399 - mse: 0.3399 - mae: 0.4569 - val_loss: 0.3672 - val_mse: 0.3672 - val_mae: 0.4594\n",
            "Epoch 415/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3483 - mse: 0.3483 - mae: 0.4575 - val_loss: 0.3678 - val_mse: 0.3678 - val_mae: 0.4618\n",
            "Epoch 416/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3725 - mse: 0.3725 - mae: 0.4769 - val_loss: 0.3643 - val_mse: 0.3643 - val_mae: 0.4582\n",
            "Epoch 417/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3510 - mse: 0.3510 - mae: 0.4572 - val_loss: 0.3863 - val_mse: 0.3863 - val_mae: 0.4702\n",
            "Epoch 418/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3555 - mse: 0.3555 - mae: 0.4650 - val_loss: 0.3813 - val_mse: 0.3813 - val_mae: 0.4671\n",
            "Epoch 419/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3406 - mse: 0.3406 - mae: 0.4531 - val_loss: 0.3629 - val_mse: 0.3629 - val_mae: 0.4550\n",
            "Epoch 420/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3496 - mse: 0.3496 - mae: 0.4584 - val_loss: 0.3720 - val_mse: 0.3720 - val_mae: 0.4622\n",
            "Epoch 421/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3535 - mse: 0.3535 - mae: 0.4592 - val_loss: 0.3682 - val_mse: 0.3682 - val_mae: 0.4606\n",
            "Epoch 422/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3591 - mse: 0.3591 - mae: 0.4680 - val_loss: 0.3740 - val_mse: 0.3740 - val_mae: 0.4657\n",
            "Epoch 423/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3246 - mse: 0.3246 - mae: 0.4420 - val_loss: 0.3792 - val_mse: 0.3792 - val_mae: 0.4713\n",
            "Epoch 424/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3445 - mse: 0.3445 - mae: 0.4567 - val_loss: 0.3610 - val_mse: 0.3610 - val_mae: 0.4550\n",
            "Epoch 425/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3445 - mse: 0.3445 - mae: 0.4639 - val_loss: 0.3620 - val_mse: 0.3620 - val_mae: 0.4548\n",
            "Epoch 426/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3386 - mse: 0.3386 - mae: 0.4518 - val_loss: 0.3706 - val_mse: 0.3706 - val_mae: 0.4611\n",
            "Epoch 427/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3371 - mse: 0.3371 - mae: 0.4446 - val_loss: 0.3738 - val_mse: 0.3738 - val_mae: 0.4630\n",
            "Epoch 428/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3499 - mse: 0.3499 - mae: 0.4665 - val_loss: 0.3598 - val_mse: 0.3598 - val_mae: 0.4559\n",
            "Epoch 429/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3424 - mse: 0.3424 - mae: 0.4531 - val_loss: 0.3771 - val_mse: 0.3771 - val_mae: 0.4646\n",
            "Epoch 430/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3521 - mse: 0.3521 - mae: 0.4525 - val_loss: 0.3785 - val_mse: 0.3785 - val_mae: 0.4651\n",
            "Epoch 431/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3469 - mse: 0.3469 - mae: 0.4647 - val_loss: 0.3752 - val_mse: 0.3752 - val_mae: 0.4648\n",
            "Epoch 432/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3416 - mse: 0.3416 - mae: 0.4499 - val_loss: 0.3544 - val_mse: 0.3544 - val_mae: 0.4486\n",
            "Epoch 433/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3487 - mse: 0.3487 - mae: 0.4579 - val_loss: 0.3639 - val_mse: 0.3639 - val_mae: 0.4595\n",
            "Epoch 434/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3446 - mse: 0.3446 - mae: 0.4534 - val_loss: 0.3711 - val_mse: 0.3711 - val_mae: 0.4623\n",
            "Epoch 435/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3464 - mse: 0.3464 - mae: 0.4616 - val_loss: 0.3685 - val_mse: 0.3685 - val_mae: 0.4605\n",
            "Epoch 436/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3324 - mse: 0.3324 - mae: 0.4461 - val_loss: 0.3834 - val_mse: 0.3834 - val_mae: 0.4699\n",
            "Epoch 437/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3666 - mse: 0.3666 - mae: 0.4756 - val_loss: 0.3603 - val_mse: 0.3603 - val_mae: 0.4532\n",
            "Epoch 438/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3492 - mse: 0.3492 - mae: 0.4503 - val_loss: 0.3637 - val_mse: 0.3637 - val_mae: 0.4555\n",
            "Epoch 439/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3386 - mse: 0.3386 - mae: 0.4536 - val_loss: 0.3757 - val_mse: 0.3757 - val_mae: 0.4646\n",
            "Epoch 440/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3476 - mse: 0.3476 - mae: 0.4596 - val_loss: 0.3586 - val_mse: 0.3586 - val_mae: 0.4504\n",
            "Epoch 441/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3356 - mse: 0.3356 - mae: 0.4460 - val_loss: 0.3721 - val_mse: 0.3721 - val_mae: 0.4595\n",
            "Epoch 442/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3416 - mse: 0.3416 - mae: 0.4549 - val_loss: 0.3652 - val_mse: 0.3652 - val_mae: 0.4572\n",
            "Epoch 443/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3462 - mse: 0.3462 - mae: 0.4547 - val_loss: 0.3608 - val_mse: 0.3608 - val_mae: 0.4509\n",
            "Epoch 444/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3504 - mse: 0.3504 - mae: 0.4566 - val_loss: 0.3759 - val_mse: 0.3759 - val_mae: 0.4645\n",
            "Epoch 445/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3429 - mse: 0.3429 - mae: 0.4536 - val_loss: 0.3659 - val_mse: 0.3659 - val_mae: 0.4582\n",
            "Epoch 446/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3482 - mse: 0.3482 - mae: 0.4619 - val_loss: 0.3619 - val_mse: 0.3619 - val_mae: 0.4567\n",
            "Epoch 447/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3452 - mse: 0.3452 - mae: 0.4527 - val_loss: 0.3661 - val_mse: 0.3661 - val_mae: 0.4559\n",
            "Epoch 448/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3413 - mse: 0.3413 - mae: 0.4539 - val_loss: 0.3680 - val_mse: 0.3680 - val_mae: 0.4605\n",
            "Epoch 449/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3316 - mse: 0.3316 - mae: 0.4413 - val_loss: 0.3619 - val_mse: 0.3619 - val_mae: 0.4558\n",
            "Epoch 450/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3423 - mse: 0.3423 - mae: 0.4591 - val_loss: 0.3664 - val_mse: 0.3664 - val_mae: 0.4582\n",
            "Epoch 451/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3487 - mse: 0.3487 - mae: 0.4585 - val_loss: 0.3859 - val_mse: 0.3859 - val_mae: 0.4701\n",
            "Epoch 452/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3544 - mse: 0.3544 - mae: 0.4590 - val_loss: 0.3633 - val_mse: 0.3633 - val_mae: 0.4553\n",
            "Epoch 453/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3399 - mse: 0.3399 - mae: 0.4493 - val_loss: 0.3727 - val_mse: 0.3727 - val_mae: 0.4617\n",
            "Epoch 454/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3495 - mse: 0.3495 - mae: 0.4580 - val_loss: 0.3605 - val_mse: 0.3605 - val_mae: 0.4519\n",
            "Epoch 455/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3277 - mse: 0.3277 - mae: 0.4351 - val_loss: 0.3658 - val_mse: 0.3658 - val_mae: 0.4584\n",
            "Epoch 456/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3378 - mse: 0.3378 - mae: 0.4475 - val_loss: 0.3736 - val_mse: 0.3736 - val_mae: 0.4642\n",
            "Epoch 457/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3321 - mse: 0.3321 - mae: 0.4492 - val_loss: 0.3631 - val_mse: 0.3631 - val_mae: 0.4548\n",
            "Epoch 458/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3337 - mse: 0.3337 - mae: 0.4435 - val_loss: 0.3591 - val_mse: 0.3591 - val_mae: 0.4537\n",
            "Epoch 459/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3531 - mse: 0.3531 - mae: 0.4624 - val_loss: 0.3783 - val_mse: 0.3783 - val_mae: 0.4643\n",
            "Epoch 460/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3361 - mse: 0.3361 - mae: 0.4513 - val_loss: 0.3655 - val_mse: 0.3655 - val_mae: 0.4582\n",
            "Epoch 461/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3253 - mse: 0.3253 - mae: 0.4408 - val_loss: 0.3750 - val_mse: 0.3750 - val_mae: 0.4616\n",
            "Epoch 462/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3295 - mse: 0.3295 - mae: 0.4469 - val_loss: 0.3645 - val_mse: 0.3645 - val_mae: 0.4570\n",
            "Epoch 463/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3296 - mse: 0.3296 - mae: 0.4388 - val_loss: 0.3867 - val_mse: 0.3867 - val_mae: 0.4689\n",
            "Epoch 464/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3489 - mse: 0.3489 - mae: 0.4623 - val_loss: 0.3616 - val_mse: 0.3616 - val_mae: 0.4538\n",
            "Epoch 465/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3346 - mse: 0.3346 - mae: 0.4487 - val_loss: 0.3783 - val_mse: 0.3783 - val_mae: 0.4689\n",
            "Epoch 466/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3495 - mse: 0.3495 - mae: 0.4577 - val_loss: 0.3822 - val_mse: 0.3822 - val_mae: 0.4650\n",
            "Epoch 467/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3515 - mse: 0.3515 - mae: 0.4627 - val_loss: 0.3751 - val_mse: 0.3751 - val_mae: 0.4667\n",
            "Epoch 468/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3384 - mse: 0.3384 - mae: 0.4518 - val_loss: 0.3724 - val_mse: 0.3724 - val_mae: 0.4614\n",
            "Epoch 469/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3383 - mse: 0.3383 - mae: 0.4480 - val_loss: 0.3816 - val_mse: 0.3816 - val_mae: 0.4711\n",
            "Epoch 470/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3470 - mse: 0.3470 - mae: 0.4532 - val_loss: 0.3695 - val_mse: 0.3695 - val_mae: 0.4601\n",
            "Epoch 471/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3452 - mse: 0.3452 - mae: 0.4575 - val_loss: 0.3731 - val_mse: 0.3731 - val_mae: 0.4643\n",
            "Epoch 472/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3416 - mse: 0.3416 - mae: 0.4592 - val_loss: 0.3676 - val_mse: 0.3676 - val_mae: 0.4562\n",
            "Epoch 473/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3451 - mse: 0.3451 - mae: 0.4565 - val_loss: 0.3745 - val_mse: 0.3745 - val_mae: 0.4645\n",
            "Epoch 474/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3434 - mse: 0.3434 - mae: 0.4596 - val_loss: 0.3623 - val_mse: 0.3623 - val_mae: 0.4570\n",
            "Epoch 475/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3477 - mse: 0.3477 - mae: 0.4596 - val_loss: 0.3777 - val_mse: 0.3777 - val_mae: 0.4643\n",
            "Epoch 476/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3468 - mse: 0.3468 - mae: 0.4532 - val_loss: 0.3648 - val_mse: 0.3648 - val_mae: 0.4580\n",
            "Epoch 477/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3538 - mse: 0.3538 - mae: 0.4535 - val_loss: 0.3655 - val_mse: 0.3655 - val_mae: 0.4579\n",
            "Epoch 478/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3376 - mse: 0.3376 - mae: 0.4505 - val_loss: 0.3708 - val_mse: 0.3708 - val_mae: 0.4565\n",
            "Epoch 479/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3371 - mse: 0.3371 - mae: 0.4448 - val_loss: 0.3762 - val_mse: 0.3762 - val_mae: 0.4593\n",
            "Epoch 480/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3345 - mse: 0.3345 - mae: 0.4507 - val_loss: 0.3653 - val_mse: 0.3653 - val_mae: 0.4563\n",
            "Epoch 481/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3370 - mse: 0.3370 - mae: 0.4491 - val_loss: 0.3675 - val_mse: 0.3675 - val_mae: 0.4563\n",
            "Epoch 482/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3497 - mse: 0.3497 - mae: 0.4571 - val_loss: 0.3721 - val_mse: 0.3721 - val_mae: 0.4628\n",
            "Epoch 483/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3299 - mse: 0.3299 - mae: 0.4418 - val_loss: 0.3770 - val_mse: 0.3770 - val_mae: 0.4650\n",
            "Epoch 484/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3324 - mse: 0.3324 - mae: 0.4512 - val_loss: 0.3669 - val_mse: 0.3669 - val_mae: 0.4566\n",
            "Epoch 485/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3247 - mse: 0.3247 - mae: 0.4404 - val_loss: 0.3674 - val_mse: 0.3674 - val_mae: 0.4570\n",
            "Epoch 486/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3405 - mse: 0.3405 - mae: 0.4565 - val_loss: 0.3638 - val_mse: 0.3638 - val_mae: 0.4498\n",
            "Epoch 487/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3381 - mse: 0.3381 - mae: 0.4547 - val_loss: 0.3671 - val_mse: 0.3671 - val_mae: 0.4557\n",
            "Epoch 488/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3335 - mse: 0.3335 - mae: 0.4467 - val_loss: 0.3706 - val_mse: 0.3706 - val_mae: 0.4595\n",
            "Epoch 489/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3309 - mse: 0.3309 - mae: 0.4472 - val_loss: 0.3664 - val_mse: 0.3664 - val_mae: 0.4551\n",
            "Epoch 490/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3544 - mse: 0.3544 - mae: 0.4573 - val_loss: 0.3682 - val_mse: 0.3682 - val_mae: 0.4613\n",
            "Epoch 491/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3348 - mse: 0.3348 - mae: 0.4541 - val_loss: 0.3636 - val_mse: 0.3636 - val_mae: 0.4573\n",
            "Epoch 492/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3493 - mse: 0.3493 - mae: 0.4612 - val_loss: 0.3703 - val_mse: 0.3703 - val_mae: 0.4603\n",
            "Epoch 493/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3371 - mse: 0.3371 - mae: 0.4473 - val_loss: 0.3677 - val_mse: 0.3677 - val_mae: 0.4575\n",
            "Epoch 494/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3285 - mse: 0.3285 - mae: 0.4405 - val_loss: 0.3750 - val_mse: 0.3750 - val_mae: 0.4648\n",
            "Epoch 495/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3369 - mse: 0.3369 - mae: 0.4467 - val_loss: 0.3784 - val_mse: 0.3784 - val_mae: 0.4619\n",
            "Epoch 496/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3319 - mse: 0.3319 - mae: 0.4508 - val_loss: 0.3628 - val_mse: 0.3628 - val_mae: 0.4578\n",
            "Epoch 497/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3394 - mse: 0.3394 - mae: 0.4530 - val_loss: 0.3750 - val_mse: 0.3750 - val_mae: 0.4662\n",
            "Epoch 498/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3444 - mse: 0.3444 - mae: 0.4472 - val_loss: 0.3674 - val_mse: 0.3674 - val_mae: 0.4582\n",
            "Epoch 499/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3393 - mse: 0.3393 - mae: 0.4518 - val_loss: 0.3766 - val_mse: 0.3766 - val_mae: 0.4665\n",
            "Epoch 500/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3232 - mse: 0.3232 - mae: 0.4420 - val_loss: 0.3681 - val_mse: 0.3681 - val_mae: 0.4582\n",
            "Epoch 501/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3261 - mse: 0.3261 - mae: 0.4356 - val_loss: 0.3765 - val_mse: 0.3765 - val_mae: 0.4634\n",
            "Epoch 502/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3253 - mse: 0.3253 - mae: 0.4369 - val_loss: 0.3648 - val_mse: 0.3648 - val_mae: 0.4569\n",
            "Epoch 503/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3258 - mse: 0.3258 - mae: 0.4409 - val_loss: 0.3690 - val_mse: 0.3690 - val_mae: 0.4599\n",
            "Epoch 504/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3340 - mse: 0.3340 - mae: 0.4417 - val_loss: 0.3686 - val_mse: 0.3686 - val_mae: 0.4577\n",
            "Epoch 505/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3269 - mse: 0.3269 - mae: 0.4441 - val_loss: 0.3745 - val_mse: 0.3745 - val_mae: 0.4628\n",
            "Epoch 506/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3226 - mse: 0.3226 - mae: 0.4367 - val_loss: 0.3650 - val_mse: 0.3650 - val_mae: 0.4555\n",
            "Epoch 507/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3122 - mse: 0.3122 - mae: 0.4437 - val_loss: 0.3666 - val_mse: 0.3666 - val_mae: 0.4573\n",
            "Epoch 508/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3273 - mse: 0.3273 - mae: 0.4462 - val_loss: 0.3736 - val_mse: 0.3736 - val_mae: 0.4612\n",
            "Epoch 509/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3301 - mse: 0.3301 - mae: 0.4436 - val_loss: 0.3758 - val_mse: 0.3758 - val_mae: 0.4659\n",
            "Epoch 510/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3423 - mse: 0.3423 - mae: 0.4535 - val_loss: 0.3623 - val_mse: 0.3623 - val_mae: 0.4552\n",
            "Epoch 511/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3372 - mse: 0.3372 - mae: 0.4511 - val_loss: 0.3841 - val_mse: 0.3841 - val_mae: 0.4708\n",
            "Epoch 512/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3396 - mse: 0.3396 - mae: 0.4546 - val_loss: 0.3629 - val_mse: 0.3629 - val_mae: 0.4546\n",
            "Epoch 513/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3345 - mse: 0.3345 - mae: 0.4504 - val_loss: 0.3648 - val_mse: 0.3648 - val_mae: 0.4583\n",
            "Epoch 514/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3218 - mse: 0.3218 - mae: 0.4453 - val_loss: 0.3686 - val_mse: 0.3686 - val_mae: 0.4624\n",
            "Epoch 515/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3436 - mse: 0.3436 - mae: 0.4483 - val_loss: 0.3699 - val_mse: 0.3699 - val_mae: 0.4598\n",
            "Epoch 516/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3355 - mse: 0.3355 - mae: 0.4461 - val_loss: 0.3644 - val_mse: 0.3644 - val_mae: 0.4563\n",
            "Epoch 517/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3237 - mse: 0.3237 - mae: 0.4394 - val_loss: 0.3777 - val_mse: 0.3777 - val_mae: 0.4646\n",
            "Epoch 518/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3408 - mse: 0.3408 - mae: 0.4577 - val_loss: 0.3681 - val_mse: 0.3681 - val_mae: 0.4610\n",
            "Epoch 519/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3324 - mse: 0.3324 - mae: 0.4447 - val_loss: 0.3688 - val_mse: 0.3688 - val_mae: 0.4606\n",
            "Epoch 520/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3315 - mse: 0.3315 - mae: 0.4512 - val_loss: 0.3705 - val_mse: 0.3705 - val_mae: 0.4618\n",
            "Epoch 521/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3253 - mse: 0.3253 - mae: 0.4415 - val_loss: 0.3659 - val_mse: 0.3659 - val_mae: 0.4582\n",
            "Epoch 522/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3383 - mse: 0.3383 - mae: 0.4521 - val_loss: 0.3834 - val_mse: 0.3834 - val_mae: 0.4666\n",
            "Epoch 523/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3452 - mse: 0.3452 - mae: 0.4534 - val_loss: 0.3670 - val_mse: 0.3670 - val_mae: 0.4562\n",
            "Epoch 524/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3387 - mse: 0.3387 - mae: 0.4546 - val_loss: 0.3777 - val_mse: 0.3777 - val_mae: 0.4706\n",
            "Epoch 525/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3182 - mse: 0.3182 - mae: 0.4351 - val_loss: 0.3745 - val_mse: 0.3745 - val_mae: 0.4588\n",
            "Epoch 526/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3366 - mse: 0.3366 - mae: 0.4479 - val_loss: 0.3655 - val_mse: 0.3655 - val_mae: 0.4547\n",
            "Epoch 527/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3423 - mse: 0.3423 - mae: 0.4541 - val_loss: 0.3741 - val_mse: 0.3741 - val_mae: 0.4658\n",
            "Epoch 528/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3375 - mse: 0.3375 - mae: 0.4524 - val_loss: 0.3927 - val_mse: 0.3927 - val_mae: 0.4698\n",
            "Epoch 529/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3357 - mse: 0.3357 - mae: 0.4501 - val_loss: 0.3696 - val_mse: 0.3696 - val_mae: 0.4567\n",
            "Epoch 530/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3185 - mse: 0.3185 - mae: 0.4404 - val_loss: 0.3750 - val_mse: 0.3750 - val_mae: 0.4603\n",
            "Epoch 531/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3280 - mse: 0.3280 - mae: 0.4444 - val_loss: 0.3720 - val_mse: 0.3720 - val_mae: 0.4611\n",
            "Epoch 532/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3373 - mse: 0.3373 - mae: 0.4493 - val_loss: 0.3918 - val_mse: 0.3918 - val_mae: 0.4705\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.5326 - mse: 0.5326 - mae: 0.5544\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 13ms/step - loss: 40.6460 - mse: 40.6460 - mae: 6.1684 - val_loss: 36.1944 - val_mse: 36.1944 - val_mae: 5.8011\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 32.1416 - mse: 32.1416 - mae: 5.4687 - val_loss: 28.7459 - val_mse: 28.7459 - val_mae: 5.1504\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 25.0174 - mse: 25.0174 - mae: 4.7803 - val_loss: 22.5673 - val_mse: 22.5673 - val_mae: 4.5343\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 19.4927 - mse: 19.4927 - mae: 4.1668 - val_loss: 17.4906 - val_mse: 17.4906 - val_mae: 3.9375\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 14.8126 - mse: 14.8126 - mae: 3.5743 - val_loss: 13.2514 - val_mse: 13.2514 - val_mae: 3.3775\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 11.0262 - mse: 11.0262 - mae: 3.0189 - val_loss: 9.8935 - val_mse: 9.8935 - val_mae: 2.8645\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 8.2657 - mse: 8.2657 - mae: 2.5446 - val_loss: 7.3449 - val_mse: 7.3449 - val_mae: 2.4227\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 6.0649 - mse: 6.0649 - mae: 2.1250 - val_loss: 5.4981 - val_mse: 5.4981 - val_mae: 2.0542\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 4.7672 - mse: 4.7672 - mae: 1.8545 - val_loss: 4.2571 - val_mse: 4.2571 - val_mae: 1.7702\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 3.8367 - mse: 3.8367 - mae: 1.6223 - val_loss: 3.4138 - val_mse: 3.4138 - val_mae: 1.5380\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 3.3493 - mse: 3.3493 - mae: 1.4755 - val_loss: 2.8691 - val_mse: 2.8691 - val_mae: 1.3848\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.8397 - mse: 2.8397 - mae: 1.3681 - val_loss: 2.5199 - val_mse: 2.5199 - val_mae: 1.2867\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.6684 - mse: 2.6684 - mae: 1.3059 - val_loss: 2.2834 - val_mse: 2.2834 - val_mae: 1.2173\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.4937 - mse: 2.4937 - mae: 1.2615 - val_loss: 2.1374 - val_mse: 2.1374 - val_mae: 1.1739\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.4327 - mse: 2.4327 - mae: 1.2337 - val_loss: 2.0026 - val_mse: 2.0026 - val_mae: 1.1304\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 2.2844 - mse: 2.2844 - mae: 1.1921 - val_loss: 1.9052 - val_mse: 1.9052 - val_mae: 1.0994\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.1194 - mse: 2.1194 - mae: 1.1731 - val_loss: 1.8096 - val_mse: 1.8096 - val_mae: 1.0633\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 2.1634 - mse: 2.1634 - mae: 1.1736 - val_loss: 1.7471 - val_mse: 1.7471 - val_mae: 1.0393\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.9372 - mse: 1.9372 - mae: 1.1097 - val_loss: 1.6816 - val_mse: 1.6816 - val_mae: 1.0217\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.0324 - mse: 2.0324 - mae: 1.1280 - val_loss: 1.6189 - val_mse: 1.6189 - val_mae: 1.0013\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.7796 - mse: 1.7796 - mae: 1.0538 - val_loss: 1.5671 - val_mse: 1.5671 - val_mae: 0.9790\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.7550 - mse: 1.7550 - mae: 1.0613 - val_loss: 1.5277 - val_mse: 1.5277 - val_mae: 0.9652\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.7470 - mse: 1.7470 - mae: 1.0449 - val_loss: 1.4846 - val_mse: 1.4846 - val_mae: 0.9487\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.7164 - mse: 1.7164 - mae: 1.0432 - val_loss: 1.4315 - val_mse: 1.4315 - val_mae: 0.9301\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.6533 - mse: 1.6533 - mae: 1.0143 - val_loss: 1.4000 - val_mse: 1.4000 - val_mae: 0.9149\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.5918 - mse: 1.5918 - mae: 0.9963 - val_loss: 1.3569 - val_mse: 1.3569 - val_mae: 0.8976\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.6235 - mse: 1.6235 - mae: 1.0102 - val_loss: 1.3191 - val_mse: 1.3191 - val_mae: 0.8874\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.5262 - mse: 1.5262 - mae: 0.9638 - val_loss: 1.2761 - val_mse: 1.2761 - val_mae: 0.8724\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.4991 - mse: 1.4991 - mae: 0.9719 - val_loss: 1.2500 - val_mse: 1.2500 - val_mae: 0.8631\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.4696 - mse: 1.4696 - mae: 0.9672 - val_loss: 1.2271 - val_mse: 1.2271 - val_mae: 0.8539\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.4096 - mse: 1.4096 - mae: 0.9393 - val_loss: 1.1947 - val_mse: 1.1947 - val_mae: 0.8439\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.3202 - mse: 1.3202 - mae: 0.9125 - val_loss: 1.1623 - val_mse: 1.1623 - val_mae: 0.8323\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.3944 - mse: 1.3944 - mae: 0.9367 - val_loss: 1.1332 - val_mse: 1.1332 - val_mae: 0.8218\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.3086 - mse: 1.3086 - mae: 0.8959 - val_loss: 1.1028 - val_mse: 1.1028 - val_mae: 0.8069\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.3200 - mse: 1.3200 - mae: 0.9151 - val_loss: 1.0781 - val_mse: 1.0781 - val_mae: 0.7988\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1855 - mse: 1.1855 - mae: 0.8816 - val_loss: 1.0510 - val_mse: 1.0510 - val_mae: 0.7884\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.2134 - mse: 1.2134 - mae: 0.8713 - val_loss: 1.0223 - val_mse: 1.0223 - val_mae: 0.7757\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2596 - mse: 1.2596 - mae: 0.8996 - val_loss: 1.0064 - val_mse: 1.0064 - val_mae: 0.7687\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1254 - mse: 1.1254 - mae: 0.8373 - val_loss: 0.9940 - val_mse: 0.9940 - val_mae: 0.7651\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1627 - mse: 1.1627 - mae: 0.8561 - val_loss: 0.9615 - val_mse: 0.9615 - val_mae: 0.7511\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1600 - mse: 1.1600 - mae: 0.8634 - val_loss: 0.9460 - val_mse: 0.9460 - val_mae: 0.7462\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1057 - mse: 1.1057 - mae: 0.8385 - val_loss: 0.9178 - val_mse: 0.9178 - val_mae: 0.7335\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0481 - mse: 1.0481 - mae: 0.7962 - val_loss: 0.9143 - val_mse: 0.9143 - val_mae: 0.7299\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0480 - mse: 1.0480 - mae: 0.8170 - val_loss: 0.8958 - val_mse: 0.8958 - val_mae: 0.7246\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0336 - mse: 1.0336 - mae: 0.8176 - val_loss: 0.8688 - val_mse: 0.8688 - val_mae: 0.7121\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.0435 - mse: 1.0435 - mae: 0.8069 - val_loss: 0.8465 - val_mse: 0.8465 - val_mae: 0.7051\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0325 - mse: 1.0325 - mae: 0.7899 - val_loss: 0.8416 - val_mse: 0.8416 - val_mae: 0.7051\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0191 - mse: 1.0191 - mae: 0.8096 - val_loss: 0.8270 - val_mse: 0.8270 - val_mae: 0.6989\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0166 - mse: 1.0166 - mae: 0.7978 - val_loss: 0.8165 - val_mse: 0.8165 - val_mae: 0.6928\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.9378 - mse: 0.9378 - mae: 0.7768 - val_loss: 0.7974 - val_mse: 0.7974 - val_mae: 0.6845\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.9975 - mse: 0.9975 - mae: 0.7992 - val_loss: 0.7922 - val_mse: 0.7922 - val_mae: 0.6858\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.9253 - mse: 0.9253 - mae: 0.7664 - val_loss: 0.7857 - val_mse: 0.7857 - val_mae: 0.6821\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.9139 - mse: 0.9139 - mae: 0.7575 - val_loss: 0.7720 - val_mse: 0.7720 - val_mae: 0.6758\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8753 - mse: 0.8753 - mae: 0.7388 - val_loss: 0.7520 - val_mse: 0.7520 - val_mae: 0.6712\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.8546 - mse: 0.8546 - mae: 0.7424 - val_loss: 0.7316 - val_mse: 0.7316 - val_mae: 0.6660\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8783 - mse: 0.8783 - mae: 0.7405 - val_loss: 0.7269 - val_mse: 0.7269 - val_mae: 0.6648\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.9129 - mse: 0.9129 - mae: 0.7651 - val_loss: 0.7312 - val_mse: 0.7312 - val_mae: 0.6634\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.9187 - mse: 0.9187 - mae: 0.7674 - val_loss: 0.7226 - val_mse: 0.7226 - val_mae: 0.6604\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.8319 - mse: 0.8319 - mae: 0.7174 - val_loss: 0.6967 - val_mse: 0.6967 - val_mae: 0.6505\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8292 - mse: 0.8292 - mae: 0.7226 - val_loss: 0.6959 - val_mse: 0.6959 - val_mae: 0.6513\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8340 - mse: 0.8340 - mae: 0.7361 - val_loss: 0.6848 - val_mse: 0.6848 - val_mae: 0.6454\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7746 - mse: 0.7746 - mae: 0.7032 - val_loss: 0.6853 - val_mse: 0.6853 - val_mae: 0.6465\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7731 - mse: 0.7731 - mae: 0.6950 - val_loss: 0.6752 - val_mse: 0.6752 - val_mae: 0.6434\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8249 - mse: 0.8249 - mae: 0.7206 - val_loss: 0.6568 - val_mse: 0.6568 - val_mae: 0.6329\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7682 - mse: 0.7682 - mae: 0.7012 - val_loss: 0.6554 - val_mse: 0.6554 - val_mae: 0.6325\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.8085 - mse: 0.8085 - mae: 0.7184 - val_loss: 0.6698 - val_mse: 0.6698 - val_mae: 0.6429\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7393 - mse: 0.7393 - mae: 0.6876 - val_loss: 0.6513 - val_mse: 0.6513 - val_mae: 0.6304\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7743 - mse: 0.7743 - mae: 0.6954 - val_loss: 0.6479 - val_mse: 0.6479 - val_mae: 0.6297\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7498 - mse: 0.7498 - mae: 0.6965 - val_loss: 0.6466 - val_mse: 0.6466 - val_mae: 0.6280\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7322 - mse: 0.7322 - mae: 0.6736 - val_loss: 0.6401 - val_mse: 0.6401 - val_mae: 0.6280\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7083 - mse: 0.7083 - mae: 0.6669 - val_loss: 0.6287 - val_mse: 0.6287 - val_mae: 0.6210\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7246 - mse: 0.7246 - mae: 0.6837 - val_loss: 0.6223 - val_mse: 0.6223 - val_mae: 0.6199\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.7262 - mse: 0.7262 - mae: 0.6799 - val_loss: 0.6049 - val_mse: 0.6049 - val_mae: 0.6091\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7372 - mse: 0.7372 - mae: 0.6838 - val_loss: 0.6115 - val_mse: 0.6115 - val_mae: 0.6120\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.6607 - mse: 0.6607 - mae: 0.6389 - val_loss: 0.5961 - val_mse: 0.5961 - val_mae: 0.6041\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6949 - mse: 0.6949 - mae: 0.6644 - val_loss: 0.6028 - val_mse: 0.6028 - val_mae: 0.6052\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6647 - mse: 0.6647 - mae: 0.6389 - val_loss: 0.5957 - val_mse: 0.5957 - val_mae: 0.6052\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6763 - mse: 0.6763 - mae: 0.6638 - val_loss: 0.5887 - val_mse: 0.5887 - val_mae: 0.5993\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7012 - mse: 0.7012 - mae: 0.6690 - val_loss: 0.5761 - val_mse: 0.5761 - val_mae: 0.5936\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6445 - mse: 0.6445 - mae: 0.6439 - val_loss: 0.5804 - val_mse: 0.5804 - val_mae: 0.5955\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6714 - mse: 0.6714 - mae: 0.6469 - val_loss: 0.5832 - val_mse: 0.5832 - val_mae: 0.5964\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6726 - mse: 0.6726 - mae: 0.6571 - val_loss: 0.5752 - val_mse: 0.5752 - val_mae: 0.5957\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6299 - mse: 0.6299 - mae: 0.6389 - val_loss: 0.5583 - val_mse: 0.5583 - val_mae: 0.5831\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6561 - mse: 0.6561 - mae: 0.6450 - val_loss: 0.5636 - val_mse: 0.5636 - val_mae: 0.5856\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6242 - mse: 0.6242 - mae: 0.6270 - val_loss: 0.5478 - val_mse: 0.5478 - val_mae: 0.5796\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6280 - mse: 0.6280 - mae: 0.6328 - val_loss: 0.5476 - val_mse: 0.5476 - val_mae: 0.5789\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6400 - mse: 0.6400 - mae: 0.6293 - val_loss: 0.5457 - val_mse: 0.5457 - val_mae: 0.5800\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6308 - mse: 0.6308 - mae: 0.6239 - val_loss: 0.5530 - val_mse: 0.5530 - val_mae: 0.5842\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5975 - mse: 0.5975 - mae: 0.6058 - val_loss: 0.5525 - val_mse: 0.5525 - val_mae: 0.5818\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6211 - mse: 0.6211 - mae: 0.6307 - val_loss: 0.5413 - val_mse: 0.5413 - val_mae: 0.5751\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6121 - mse: 0.6121 - mae: 0.6202 - val_loss: 0.5444 - val_mse: 0.5444 - val_mae: 0.5769\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5997 - mse: 0.5997 - mae: 0.6109 - val_loss: 0.5483 - val_mse: 0.5483 - val_mae: 0.5779\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6023 - mse: 0.6023 - mae: 0.6196 - val_loss: 0.5311 - val_mse: 0.5311 - val_mae: 0.5681\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6060 - mse: 0.6060 - mae: 0.6215 - val_loss: 0.5389 - val_mse: 0.5389 - val_mae: 0.5737\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6138 - mse: 0.6138 - mae: 0.6244 - val_loss: 0.5292 - val_mse: 0.5292 - val_mae: 0.5661\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.5866 - mse: 0.5866 - mae: 0.6082 - val_loss: 0.5306 - val_mse: 0.5306 - val_mae: 0.5697\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5970 - mse: 0.5970 - mae: 0.6155 - val_loss: 0.5348 - val_mse: 0.5348 - val_mae: 0.5724\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5567 - mse: 0.5567 - mae: 0.5927 - val_loss: 0.5265 - val_mse: 0.5265 - val_mae: 0.5689\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5610 - mse: 0.5610 - mae: 0.5909 - val_loss: 0.5277 - val_mse: 0.5277 - val_mae: 0.5649\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5731 - mse: 0.5731 - mae: 0.6036 - val_loss: 0.5169 - val_mse: 0.5169 - val_mae: 0.5603\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5667 - mse: 0.5667 - mae: 0.5942 - val_loss: 0.5197 - val_mse: 0.5197 - val_mae: 0.5644\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5288 - mse: 0.5288 - mae: 0.5769 - val_loss: 0.5247 - val_mse: 0.5247 - val_mae: 0.5646\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5352 - mse: 0.5352 - mae: 0.5770 - val_loss: 0.5257 - val_mse: 0.5257 - val_mae: 0.5640\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5579 - mse: 0.5579 - mae: 0.5935 - val_loss: 0.5231 - val_mse: 0.5231 - val_mae: 0.5636\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5673 - mse: 0.5673 - mae: 0.5910 - val_loss: 0.5120 - val_mse: 0.5120 - val_mae: 0.5603\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5703 - mse: 0.5703 - mae: 0.6031 - val_loss: 0.5117 - val_mse: 0.5117 - val_mae: 0.5586\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5481 - mse: 0.5481 - mae: 0.5911 - val_loss: 0.5190 - val_mse: 0.5190 - val_mae: 0.5638\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5289 - mse: 0.5289 - mae: 0.5713 - val_loss: 0.5121 - val_mse: 0.5121 - val_mae: 0.5587\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5398 - mse: 0.5398 - mae: 0.5748 - val_loss: 0.5195 - val_mse: 0.5195 - val_mae: 0.5616\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5316 - mse: 0.5316 - mae: 0.5749 - val_loss: 0.5121 - val_mse: 0.5121 - val_mae: 0.5588\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5424 - mse: 0.5424 - mae: 0.5860 - val_loss: 0.5015 - val_mse: 0.5015 - val_mae: 0.5520\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.5214 - mse: 0.5214 - mae: 0.5734 - val_loss: 0.5027 - val_mse: 0.5027 - val_mae: 0.5525\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5270 - mse: 0.5270 - mae: 0.5771 - val_loss: 0.5174 - val_mse: 0.5174 - val_mae: 0.5565\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5073 - mse: 0.5073 - mae: 0.5674 - val_loss: 0.5051 - val_mse: 0.5051 - val_mae: 0.5490\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5478 - mse: 0.5478 - mae: 0.5776 - val_loss: 0.4979 - val_mse: 0.4979 - val_mae: 0.5509\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5316 - mse: 0.5316 - mae: 0.5839 - val_loss: 0.5004 - val_mse: 0.5004 - val_mae: 0.5531\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5231 - mse: 0.5231 - mae: 0.5735 - val_loss: 0.4979 - val_mse: 0.4979 - val_mae: 0.5492\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5014 - mse: 0.5014 - mae: 0.5601 - val_loss: 0.4930 - val_mse: 0.4930 - val_mae: 0.5460\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5146 - mse: 0.5146 - mae: 0.5662 - val_loss: 0.4816 - val_mse: 0.4816 - val_mae: 0.5388\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5011 - mse: 0.5011 - mae: 0.5625 - val_loss: 0.4859 - val_mse: 0.4859 - val_mae: 0.5414\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4973 - mse: 0.4973 - mae: 0.5605 - val_loss: 0.4960 - val_mse: 0.4960 - val_mae: 0.5472\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5174 - mse: 0.5174 - mae: 0.5696 - val_loss: 0.4867 - val_mse: 0.4867 - val_mae: 0.5427\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4957 - mse: 0.4957 - mae: 0.5531 - val_loss: 0.4861 - val_mse: 0.4861 - val_mae: 0.5462\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4861 - mse: 0.4861 - mae: 0.5558 - val_loss: 0.4909 - val_mse: 0.4909 - val_mae: 0.5465\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4961 - mse: 0.4961 - mae: 0.5548 - val_loss: 0.4775 - val_mse: 0.4775 - val_mae: 0.5420\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5065 - mse: 0.5065 - mae: 0.5578 - val_loss: 0.4800 - val_mse: 0.4800 - val_mae: 0.5386\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4961 - mse: 0.4961 - mae: 0.5557 - val_loss: 0.4901 - val_mse: 0.4901 - val_mae: 0.5467\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4969 - mse: 0.4969 - mae: 0.5586 - val_loss: 0.4772 - val_mse: 0.4772 - val_mae: 0.5424\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4823 - mse: 0.4823 - mae: 0.5443 - val_loss: 0.4815 - val_mse: 0.4815 - val_mae: 0.5356\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4892 - mse: 0.4892 - mae: 0.5497 - val_loss: 0.4769 - val_mse: 0.4769 - val_mae: 0.5375\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4858 - mse: 0.4858 - mae: 0.5516 - val_loss: 0.4731 - val_mse: 0.4731 - val_mae: 0.5343\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5165 - mse: 0.5165 - mae: 0.5708 - val_loss: 0.4753 - val_mse: 0.4753 - val_mae: 0.5339\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4607 - mse: 0.4607 - mae: 0.5292 - val_loss: 0.4764 - val_mse: 0.4764 - val_mae: 0.5406\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4564 - mse: 0.4564 - mae: 0.5334 - val_loss: 0.4698 - val_mse: 0.4698 - val_mae: 0.5342\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4757 - mse: 0.4757 - mae: 0.5446 - val_loss: 0.4724 - val_mse: 0.4724 - val_mae: 0.5339\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4750 - mse: 0.4750 - mae: 0.5513 - val_loss: 0.4717 - val_mse: 0.4717 - val_mae: 0.5307\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4924 - mse: 0.4924 - mae: 0.5534 - val_loss: 0.4777 - val_mse: 0.4777 - val_mae: 0.5483\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4603 - mse: 0.4603 - mae: 0.5407 - val_loss: 0.4612 - val_mse: 0.4612 - val_mae: 0.5275\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4753 - mse: 0.4753 - mae: 0.5451 - val_loss: 0.4742 - val_mse: 0.4742 - val_mae: 0.5392\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4649 - mse: 0.4649 - mae: 0.5396 - val_loss: 0.4681 - val_mse: 0.4681 - val_mae: 0.5305\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4759 - mse: 0.4759 - mae: 0.5374 - val_loss: 0.4616 - val_mse: 0.4616 - val_mae: 0.5280\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4878 - mse: 0.4878 - mae: 0.5591 - val_loss: 0.4633 - val_mse: 0.4633 - val_mae: 0.5256\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4889 - mse: 0.4889 - mae: 0.5500 - val_loss: 0.4762 - val_mse: 0.4762 - val_mae: 0.5388\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4758 - mse: 0.4758 - mae: 0.5502 - val_loss: 0.4746 - val_mse: 0.4746 - val_mae: 0.5414\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4704 - mse: 0.4704 - mae: 0.5432 - val_loss: 0.4540 - val_mse: 0.4540 - val_mae: 0.5189\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4512 - mse: 0.4512 - mae: 0.5340 - val_loss: 0.4651 - val_mse: 0.4651 - val_mae: 0.5284\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4683 - mse: 0.4683 - mae: 0.5405 - val_loss: 0.4488 - val_mse: 0.4488 - val_mae: 0.5175\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4431 - mse: 0.4431 - mae: 0.5276 - val_loss: 0.4574 - val_mse: 0.4574 - val_mae: 0.5261\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4628 - mse: 0.4628 - mae: 0.5359 - val_loss: 0.4751 - val_mse: 0.4751 - val_mae: 0.5307\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4498 - mse: 0.4498 - mae: 0.5303 - val_loss: 0.4565 - val_mse: 0.4565 - val_mae: 0.5204\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4584 - mse: 0.4584 - mae: 0.5313 - val_loss: 0.4744 - val_mse: 0.4744 - val_mae: 0.5352\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4439 - mse: 0.4439 - mae: 0.5274 - val_loss: 0.4544 - val_mse: 0.4544 - val_mae: 0.5213\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4362 - mse: 0.4362 - mae: 0.5243 - val_loss: 0.4507 - val_mse: 0.4507 - val_mae: 0.5189\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4323 - mse: 0.4323 - mae: 0.5138 - val_loss: 0.4645 - val_mse: 0.4645 - val_mae: 0.5231\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4606 - mse: 0.4606 - mae: 0.5270 - val_loss: 0.4759 - val_mse: 0.4759 - val_mae: 0.5380\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4354 - mse: 0.4354 - mae: 0.5280 - val_loss: 0.4582 - val_mse: 0.4582 - val_mae: 0.5270\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4535 - mse: 0.4535 - mae: 0.5319 - val_loss: 0.4671 - val_mse: 0.4671 - val_mae: 0.5336\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4564 - mse: 0.4564 - mae: 0.5253 - val_loss: 0.4493 - val_mse: 0.4493 - val_mae: 0.5108\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4508 - mse: 0.4508 - mae: 0.5293 - val_loss: 0.4561 - val_mse: 0.4561 - val_mae: 0.5256\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4517 - mse: 0.4517 - mae: 0.5310 - val_loss: 0.4526 - val_mse: 0.4526 - val_mae: 0.5160\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4364 - mse: 0.4364 - mae: 0.5240 - val_loss: 0.4551 - val_mse: 0.4551 - val_mae: 0.5173\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4356 - mse: 0.4356 - mae: 0.5182 - val_loss: 0.4523 - val_mse: 0.4523 - val_mae: 0.5212\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4408 - mse: 0.4408 - mae: 0.5218 - val_loss: 0.4656 - val_mse: 0.4656 - val_mae: 0.5298\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4249 - mse: 0.4249 - mae: 0.5095 - val_loss: 0.4615 - val_mse: 0.4615 - val_mae: 0.5249\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4391 - mse: 0.4391 - mae: 0.5148 - val_loss: 0.4526 - val_mse: 0.4526 - val_mae: 0.5223\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4391 - mse: 0.4391 - mae: 0.5225 - val_loss: 0.4551 - val_mse: 0.4551 - val_mae: 0.5198\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4377 - mse: 0.4377 - mae: 0.5243 - val_loss: 0.4546 - val_mse: 0.4546 - val_mae: 0.5176\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4359 - mse: 0.4359 - mae: 0.5240 - val_loss: 0.4497 - val_mse: 0.4497 - val_mae: 0.5240\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4402 - mse: 0.4402 - mae: 0.5252 - val_loss: 0.4500 - val_mse: 0.4500 - val_mae: 0.5137\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4365 - mse: 0.4365 - mae: 0.5225 - val_loss: 0.4544 - val_mse: 0.4544 - val_mae: 0.5207\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4216 - mse: 0.4216 - mae: 0.5220 - val_loss: 0.4436 - val_mse: 0.4436 - val_mae: 0.5118\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4378 - mse: 0.4378 - mae: 0.5255 - val_loss: 0.4495 - val_mse: 0.4495 - val_mae: 0.5122\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4408 - mse: 0.4408 - mae: 0.5241 - val_loss: 0.4677 - val_mse: 0.4677 - val_mae: 0.5348\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4474 - mse: 0.4474 - mae: 0.5280 - val_loss: 0.4455 - val_mse: 0.4455 - val_mae: 0.5098\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4455 - mse: 0.4455 - mae: 0.5193 - val_loss: 0.4464 - val_mse: 0.4464 - val_mae: 0.5170\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4180 - mse: 0.4180 - mae: 0.5096 - val_loss: 0.4432 - val_mse: 0.4432 - val_mae: 0.5075\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4299 - mse: 0.4299 - mae: 0.5194 - val_loss: 0.4487 - val_mse: 0.4487 - val_mae: 0.5157\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4180 - mse: 0.4180 - mae: 0.5095 - val_loss: 0.4477 - val_mse: 0.4477 - val_mae: 0.5169\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4198 - mse: 0.4198 - mae: 0.5100 - val_loss: 0.4526 - val_mse: 0.4526 - val_mae: 0.5160\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4222 - mse: 0.4222 - mae: 0.5068 - val_loss: 0.4463 - val_mse: 0.4463 - val_mae: 0.5205\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4097 - mse: 0.4097 - mae: 0.5079 - val_loss: 0.4495 - val_mse: 0.4495 - val_mae: 0.5147\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4187 - mse: 0.4187 - mae: 0.5055 - val_loss: 0.4592 - val_mse: 0.4592 - val_mae: 0.5228\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4442 - mse: 0.4442 - mae: 0.5218 - val_loss: 0.4492 - val_mse: 0.4492 - val_mae: 0.5115\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4159 - mse: 0.4159 - mae: 0.5080 - val_loss: 0.4462 - val_mse: 0.4462 - val_mae: 0.5137\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4076 - mse: 0.4076 - mae: 0.5037 - val_loss: 0.4530 - val_mse: 0.4530 - val_mae: 0.5198\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4112 - mse: 0.4112 - mae: 0.5023 - val_loss: 0.4458 - val_mse: 0.4458 - val_mae: 0.5104\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4329 - mse: 0.4329 - mae: 0.5235 - val_loss: 0.4500 - val_mse: 0.4500 - val_mae: 0.5226\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4114 - mse: 0.4114 - mae: 0.5072 - val_loss: 0.4505 - val_mse: 0.4505 - val_mae: 0.5141\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4253 - mse: 0.4253 - mae: 0.5104 - val_loss: 0.4480 - val_mse: 0.4480 - val_mae: 0.5163\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4240 - mse: 0.4240 - mae: 0.5062 - val_loss: 0.4365 - val_mse: 0.4365 - val_mae: 0.5075\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4150 - mse: 0.4150 - mae: 0.5031 - val_loss: 0.4498 - val_mse: 0.4498 - val_mae: 0.5210\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4226 - mse: 0.4226 - mae: 0.5159 - val_loss: 0.4440 - val_mse: 0.4440 - val_mae: 0.5120\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4057 - mse: 0.4057 - mae: 0.5016 - val_loss: 0.4435 - val_mse: 0.4435 - val_mae: 0.5069\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4214 - mse: 0.4214 - mae: 0.5167 - val_loss: 0.4458 - val_mse: 0.4458 - val_mae: 0.5100\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4149 - mse: 0.4149 - mae: 0.5049 - val_loss: 0.4512 - val_mse: 0.4512 - val_mae: 0.5213\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4207 - mse: 0.4207 - mae: 0.5083 - val_loss: 0.4426 - val_mse: 0.4426 - val_mae: 0.5150\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4259 - mse: 0.4259 - mae: 0.5206 - val_loss: 0.4388 - val_mse: 0.4388 - val_mae: 0.5133\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4261 - mse: 0.4261 - mae: 0.5151 - val_loss: 0.4378 - val_mse: 0.4378 - val_mae: 0.5063\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3984 - mse: 0.3984 - mae: 0.4976 - val_loss: 0.4376 - val_mse: 0.4376 - val_mae: 0.5097\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4045 - mse: 0.4045 - mae: 0.5017 - val_loss: 0.4605 - val_mse: 0.4605 - val_mae: 0.5223\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4104 - mse: 0.4104 - mae: 0.5073 - val_loss: 0.4424 - val_mse: 0.4424 - val_mae: 0.5125\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4090 - mse: 0.4090 - mae: 0.4988 - val_loss: 0.4438 - val_mse: 0.4438 - val_mae: 0.5133\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.3974 - mse: 0.3974 - mae: 0.4988 - val_loss: 0.4285 - val_mse: 0.4285 - val_mae: 0.5025\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.4081 - mse: 0.4081 - mae: 0.5015 - val_loss: 0.4355 - val_mse: 0.4355 - val_mae: 0.5097\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.4214 - mse: 0.4214 - mae: 0.5090 - val_loss: 0.4374 - val_mse: 0.4374 - val_mae: 0.5105\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.4018 - mse: 0.4018 - mae: 0.5009 - val_loss: 0.4378 - val_mse: 0.4378 - val_mae: 0.5080\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.4243 - mse: 0.4243 - mae: 0.5142 - val_loss: 0.4384 - val_mse: 0.4384 - val_mae: 0.5046\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.4210 - mse: 0.4210 - mae: 0.5064 - val_loss: 0.4491 - val_mse: 0.4491 - val_mae: 0.5214\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.4029 - mse: 0.4029 - mae: 0.4978 - val_loss: 0.4439 - val_mse: 0.4439 - val_mae: 0.5104\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.4194 - mse: 0.4194 - mae: 0.5063 - val_loss: 0.4388 - val_mse: 0.4388 - val_mae: 0.5060\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.4163 - mse: 0.4163 - mae: 0.5030 - val_loss: 0.4436 - val_mse: 0.4436 - val_mae: 0.5172\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4292 - mse: 0.4292 - mae: 0.5169 - val_loss: 0.4423 - val_mse: 0.4423 - val_mae: 0.5094\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4175 - mse: 0.4175 - mae: 0.5012 - val_loss: 0.4361 - val_mse: 0.4361 - val_mae: 0.5069\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4085 - mse: 0.4085 - mae: 0.5090 - val_loss: 0.4322 - val_mse: 0.4322 - val_mae: 0.5060\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4045 - mse: 0.4045 - mae: 0.4989 - val_loss: 0.4354 - val_mse: 0.4354 - val_mae: 0.5046\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4084 - mse: 0.4084 - mae: 0.4992 - val_loss: 0.4408 - val_mse: 0.4408 - val_mae: 0.5096\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4121 - mse: 0.4121 - mae: 0.5094 - val_loss: 0.4248 - val_mse: 0.4248 - val_mae: 0.5017\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3827 - mse: 0.3827 - mae: 0.4852 - val_loss: 0.4320 - val_mse: 0.4320 - val_mae: 0.5073\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3919 - mse: 0.3919 - mae: 0.4902 - val_loss: 0.4380 - val_mse: 0.4380 - val_mae: 0.5045\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3863 - mse: 0.3863 - mae: 0.4880 - val_loss: 0.4330 - val_mse: 0.4330 - val_mae: 0.5060\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4194 - mse: 0.4194 - mae: 0.5049 - val_loss: 0.4441 - val_mse: 0.4441 - val_mae: 0.5167\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4078 - mse: 0.4078 - mae: 0.5104 - val_loss: 0.4380 - val_mse: 0.4380 - val_mae: 0.5114\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4033 - mse: 0.4033 - mae: 0.5010 - val_loss: 0.4337 - val_mse: 0.4337 - val_mae: 0.5034\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3980 - mse: 0.3980 - mae: 0.4938 - val_loss: 0.4441 - val_mse: 0.4441 - val_mae: 0.5119\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4090 - mse: 0.4090 - mae: 0.5037 - val_loss: 0.4223 - val_mse: 0.4223 - val_mae: 0.5040\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3902 - mse: 0.3902 - mae: 0.4866 - val_loss: 0.4358 - val_mse: 0.4358 - val_mae: 0.5035\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4158 - mse: 0.4158 - mae: 0.5056 - val_loss: 0.4371 - val_mse: 0.4371 - val_mae: 0.5118\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3959 - mse: 0.3959 - mae: 0.4927 - val_loss: 0.4403 - val_mse: 0.4403 - val_mae: 0.5117\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3937 - mse: 0.3937 - mae: 0.4958 - val_loss: 0.4409 - val_mse: 0.4409 - val_mae: 0.5110\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4082 - mse: 0.4082 - mae: 0.5013 - val_loss: 0.4291 - val_mse: 0.4291 - val_mae: 0.5044\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3795 - mse: 0.3795 - mae: 0.4807 - val_loss: 0.4427 - val_mse: 0.4427 - val_mae: 0.5188\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4145 - mse: 0.4145 - mae: 0.5124 - val_loss: 0.4257 - val_mse: 0.4257 - val_mae: 0.5043\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3860 - mse: 0.3860 - mae: 0.4879 - val_loss: 0.4294 - val_mse: 0.4294 - val_mae: 0.5074\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3955 - mse: 0.3955 - mae: 0.4979 - val_loss: 0.4297 - val_mse: 0.4297 - val_mae: 0.5017\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3790 - mse: 0.3790 - mae: 0.4875 - val_loss: 0.4262 - val_mse: 0.4262 - val_mae: 0.5049\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4107 - mse: 0.4107 - mae: 0.5012 - val_loss: 0.4304 - val_mse: 0.4304 - val_mae: 0.5050\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4076 - mse: 0.4076 - mae: 0.5032 - val_loss: 0.4293 - val_mse: 0.4293 - val_mae: 0.5060\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3844 - mse: 0.3844 - mae: 0.4892 - val_loss: 0.4211 - val_mse: 0.4211 - val_mae: 0.4973\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3868 - mse: 0.3868 - mae: 0.4881 - val_loss: 0.4373 - val_mse: 0.4373 - val_mae: 0.5126\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4038 - mse: 0.4038 - mae: 0.5035 - val_loss: 0.4455 - val_mse: 0.4455 - val_mae: 0.5149\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4060 - mse: 0.4060 - mae: 0.5024 - val_loss: 0.4221 - val_mse: 0.4221 - val_mae: 0.4973\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3860 - mse: 0.3860 - mae: 0.4908 - val_loss: 0.4343 - val_mse: 0.4343 - val_mae: 0.5041\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3939 - mse: 0.3939 - mae: 0.4901 - val_loss: 0.4342 - val_mse: 0.4342 - val_mae: 0.5064\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3973 - mse: 0.3973 - mae: 0.4966 - val_loss: 0.4267 - val_mse: 0.4267 - val_mae: 0.5024\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3957 - mse: 0.3957 - mae: 0.4869 - val_loss: 0.4416 - val_mse: 0.4416 - val_mae: 0.5179\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4107 - mse: 0.4107 - mae: 0.5103 - val_loss: 0.4226 - val_mse: 0.4226 - val_mae: 0.5011\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3907 - mse: 0.3907 - mae: 0.4937 - val_loss: 0.4293 - val_mse: 0.4293 - val_mae: 0.5028\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3837 - mse: 0.3837 - mae: 0.4906 - val_loss: 0.4243 - val_mse: 0.4243 - val_mae: 0.5012\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4042 - mse: 0.4042 - mae: 0.4992 - val_loss: 0.4326 - val_mse: 0.4326 - val_mae: 0.5056\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.3732 - mse: 0.3732 - mae: 0.4828 - val_loss: 0.4198 - val_mse: 0.4198 - val_mae: 0.4982\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.3936 - mse: 0.3936 - mae: 0.4915 - val_loss: 0.4375 - val_mse: 0.4375 - val_mae: 0.5105\n",
            "Epoch 252/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.4007 - mse: 0.4007 - mae: 0.5087 - val_loss: 0.4249 - val_mse: 0.4249 - val_mae: 0.5014\n",
            "Epoch 253/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.4118 - mse: 0.4118 - mae: 0.5104 - val_loss: 0.4237 - val_mse: 0.4237 - val_mae: 0.5036\n",
            "Epoch 254/1000\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.4083 - mse: 0.4083 - mae: 0.4980 - val_loss: 0.4344 - val_mse: 0.4344 - val_mae: 0.5081\n",
            "Epoch 255/1000\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.3945 - mse: 0.3945 - mae: 0.4961 - val_loss: 0.4224 - val_mse: 0.4224 - val_mae: 0.4947\n",
            "Epoch 256/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3808 - mse: 0.3808 - mae: 0.4922 - val_loss: 0.4243 - val_mse: 0.4243 - val_mae: 0.4982\n",
            "Epoch 257/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3810 - mse: 0.3810 - mae: 0.4841 - val_loss: 0.4309 - val_mse: 0.4309 - val_mae: 0.5035\n",
            "Epoch 258/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3925 - mse: 0.3925 - mae: 0.4929 - val_loss: 0.4119 - val_mse: 0.4119 - val_mae: 0.4938\n",
            "Epoch 259/1000\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.4026 - mse: 0.4026 - mae: 0.4991 - val_loss: 0.4563 - val_mse: 0.4563 - val_mae: 0.5236\n",
            "Epoch 260/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.3893 - mse: 0.3893 - mae: 0.4906 - val_loss: 0.4178 - val_mse: 0.4178 - val_mae: 0.4938\n",
            "Epoch 261/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.3953 - mse: 0.3953 - mae: 0.4995 - val_loss: 0.4356 - val_mse: 0.4356 - val_mae: 0.5098\n",
            "Epoch 262/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.4010 - mse: 0.4010 - mae: 0.4906 - val_loss: 0.4377 - val_mse: 0.4377 - val_mae: 0.5059\n",
            "Epoch 263/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.3922 - mse: 0.3922 - mae: 0.4932 - val_loss: 0.4195 - val_mse: 0.4195 - val_mae: 0.4974\n",
            "Epoch 264/1000\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.3751 - mse: 0.3751 - mae: 0.4809 - val_loss: 0.4333 - val_mse: 0.4333 - val_mae: 0.5060\n",
            "Epoch 265/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.3842 - mse: 0.3842 - mae: 0.4878 - val_loss: 0.4273 - val_mse: 0.4273 - val_mae: 0.5007\n",
            "Epoch 266/1000\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.3800 - mse: 0.3800 - mae: 0.4895 - val_loss: 0.4247 - val_mse: 0.4247 - val_mae: 0.5032\n",
            "Epoch 267/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3890 - mse: 0.3890 - mae: 0.4846 - val_loss: 0.4166 - val_mse: 0.4166 - val_mae: 0.4925\n",
            "Epoch 268/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3969 - mse: 0.3969 - mae: 0.4942 - val_loss: 0.4223 - val_mse: 0.4223 - val_mae: 0.4978\n",
            "Epoch 269/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3802 - mse: 0.3802 - mae: 0.4846 - val_loss: 0.4260 - val_mse: 0.4260 - val_mae: 0.5007\n",
            "Epoch 270/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3867 - mse: 0.3867 - mae: 0.4862 - val_loss: 0.4314 - val_mse: 0.4314 - val_mae: 0.5061\n",
            "Epoch 271/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3893 - mse: 0.3893 - mae: 0.4911 - val_loss: 0.4174 - val_mse: 0.4174 - val_mae: 0.4985\n",
            "Epoch 272/1000\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.3906 - mse: 0.3906 - mae: 0.4922 - val_loss: 0.4229 - val_mse: 0.4229 - val_mae: 0.4977\n",
            "Epoch 273/1000\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.3853 - mse: 0.3853 - mae: 0.4938 - val_loss: 0.4263 - val_mse: 0.4263 - val_mae: 0.4989\n",
            "Epoch 274/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.3849 - mse: 0.3849 - mae: 0.4884 - val_loss: 0.4333 - val_mse: 0.4333 - val_mae: 0.5041\n",
            "Epoch 275/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.3738 - mse: 0.3738 - mae: 0.4851 - val_loss: 0.4205 - val_mse: 0.4205 - val_mae: 0.4966\n",
            "Epoch 276/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3936 - mse: 0.3936 - mae: 0.4923 - val_loss: 0.4309 - val_mse: 0.4309 - val_mae: 0.5014\n",
            "Epoch 277/1000\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.3809 - mse: 0.3809 - mae: 0.4852 - val_loss: 0.4221 - val_mse: 0.4221 - val_mae: 0.5034\n",
            "Epoch 278/1000\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.3753 - mse: 0.3753 - mae: 0.4840 - val_loss: 0.4267 - val_mse: 0.4267 - val_mae: 0.4992\n",
            "Epoch 279/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.3718 - mse: 0.3718 - mae: 0.4738 - val_loss: 0.4290 - val_mse: 0.4290 - val_mae: 0.5038\n",
            "Epoch 280/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3723 - mse: 0.3723 - mae: 0.4796 - val_loss: 0.4193 - val_mse: 0.4193 - val_mae: 0.4972\n",
            "Epoch 281/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3799 - mse: 0.3799 - mae: 0.4862 - val_loss: 0.4158 - val_mse: 0.4158 - val_mae: 0.4970\n",
            "Epoch 282/1000\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3821 - mse: 0.3821 - mae: 0.4868 - val_loss: 0.4225 - val_mse: 0.4225 - val_mae: 0.4982\n",
            "Epoch 283/1000\n",
            "22/22 [==============================] - 0s 23ms/step - loss: 0.3825 - mse: 0.3825 - mae: 0.4868 - val_loss: 0.4120 - val_mse: 0.4120 - val_mae: 0.4927\n",
            "Epoch 284/1000\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3830 - mse: 0.3830 - mae: 0.4787 - val_loss: 0.4367 - val_mse: 0.4367 - val_mae: 0.5131\n",
            "Epoch 285/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3823 - mse: 0.3823 - mae: 0.4845 - val_loss: 0.4182 - val_mse: 0.4182 - val_mae: 0.4925\n",
            "Epoch 286/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.3899 - mse: 0.3899 - mae: 0.4861 - val_loss: 0.4249 - val_mse: 0.4249 - val_mae: 0.5029\n",
            "Epoch 287/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.3853 - mse: 0.3853 - mae: 0.4960 - val_loss: 0.4209 - val_mse: 0.4209 - val_mae: 0.5002\n",
            "Epoch 288/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.3769 - mse: 0.3769 - mae: 0.4791 - val_loss: 0.4107 - val_mse: 0.4107 - val_mae: 0.4909\n",
            "Epoch 289/1000\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.3937 - mse: 0.3937 - mae: 0.4903 - val_loss: 0.4220 - val_mse: 0.4220 - val_mae: 0.5005\n",
            "Epoch 290/1000\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.3906 - mse: 0.3906 - mae: 0.4973 - val_loss: 0.4141 - val_mse: 0.4141 - val_mae: 0.5006\n",
            "Epoch 291/1000\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3972 - mse: 0.3972 - mae: 0.4974 - val_loss: 0.4195 - val_mse: 0.4195 - val_mae: 0.4942\n",
            "Epoch 292/1000\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.3672 - mse: 0.3672 - mae: 0.4782 - val_loss: 0.4255 - val_mse: 0.4255 - val_mae: 0.5009\n",
            "Epoch 293/1000\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.3549 - mse: 0.3549 - mae: 0.4677 - val_loss: 0.4318 - val_mse: 0.4318 - val_mae: 0.5029\n",
            "Epoch 294/1000\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.3891 - mse: 0.3891 - mae: 0.4872 - val_loss: 0.4275 - val_mse: 0.4275 - val_mae: 0.4996\n",
            "Epoch 295/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.4071 - mse: 0.4071 - mae: 0.4944 - val_loss: 0.4194 - val_mse: 0.4194 - val_mae: 0.4967\n",
            "Epoch 296/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3831 - mse: 0.3831 - mae: 0.4888 - val_loss: 0.4247 - val_mse: 0.4247 - val_mae: 0.5041\n",
            "Epoch 297/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3777 - mse: 0.3777 - mae: 0.4841 - val_loss: 0.4151 - val_mse: 0.4151 - val_mae: 0.4916\n",
            "Epoch 298/1000\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.3754 - mse: 0.3754 - mae: 0.4761 - val_loss: 0.4236 - val_mse: 0.4236 - val_mae: 0.4989\n",
            "Epoch 299/1000\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.3758 - mse: 0.3758 - mae: 0.4787 - val_loss: 0.4210 - val_mse: 0.4210 - val_mae: 0.4973\n",
            "Epoch 300/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3653 - mse: 0.3653 - mae: 0.4782 - val_loss: 0.4290 - val_mse: 0.4290 - val_mae: 0.5000\n",
            "Epoch 301/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3740 - mse: 0.3740 - mae: 0.4794 - val_loss: 0.4172 - val_mse: 0.4172 - val_mae: 0.4948\n",
            "Epoch 302/1000\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.3830 - mse: 0.3830 - mae: 0.4829 - val_loss: 0.4199 - val_mse: 0.4199 - val_mae: 0.4994\n",
            "Epoch 303/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3709 - mse: 0.3709 - mae: 0.4822 - val_loss: 0.4214 - val_mse: 0.4214 - val_mae: 0.5007\n",
            "Epoch 304/1000\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.3857 - mse: 0.3857 - mae: 0.4883 - val_loss: 0.4139 - val_mse: 0.4139 - val_mae: 0.4984\n",
            "Epoch 305/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3741 - mse: 0.3741 - mae: 0.4793 - val_loss: 0.4257 - val_mse: 0.4257 - val_mae: 0.5034\n",
            "Epoch 306/1000\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3872 - mse: 0.3872 - mae: 0.4851 - val_loss: 0.4197 - val_mse: 0.4197 - val_mae: 0.5003\n",
            "Epoch 307/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3792 - mse: 0.3792 - mae: 0.4831 - val_loss: 0.4149 - val_mse: 0.4149 - val_mae: 0.4938\n",
            "Epoch 308/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.3838 - mse: 0.3838 - mae: 0.4843 - val_loss: 0.4437 - val_mse: 0.4437 - val_mae: 0.5198\n",
            "Epoch 309/1000\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.3672 - mse: 0.3672 - mae: 0.4750 - val_loss: 0.4123 - val_mse: 0.4123 - val_mae: 0.4931\n",
            "Epoch 310/1000\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.3866 - mse: 0.3866 - mae: 0.4866 - val_loss: 0.4297 - val_mse: 0.4297 - val_mae: 0.5049\n",
            "Epoch 311/1000\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.3800 - mse: 0.3800 - mae: 0.4878 - val_loss: 0.4100 - val_mse: 0.4100 - val_mae: 0.4939\n",
            "Epoch 312/1000\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.3829 - mse: 0.3829 - mae: 0.4770 - val_loss: 0.4310 - val_mse: 0.4310 - val_mae: 0.5053\n",
            "Epoch 313/1000\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.3696 - mse: 0.3696 - mae: 0.4755 - val_loss: 0.4302 - val_mse: 0.4302 - val_mae: 0.5004\n",
            "Epoch 314/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3930 - mse: 0.3930 - mae: 0.4931 - val_loss: 0.4183 - val_mse: 0.4183 - val_mae: 0.5020\n",
            "Epoch 315/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3600 - mse: 0.3600 - mae: 0.4731 - val_loss: 0.4214 - val_mse: 0.4214 - val_mae: 0.4969\n",
            "Epoch 316/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3836 - mse: 0.3836 - mae: 0.4884 - val_loss: 0.4163 - val_mse: 0.4163 - val_mae: 0.4957\n",
            "Epoch 317/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3996 - mse: 0.3996 - mae: 0.5062 - val_loss: 0.4178 - val_mse: 0.4178 - val_mae: 0.4953\n",
            "Epoch 318/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3661 - mse: 0.3661 - mae: 0.4763 - val_loss: 0.4252 - val_mse: 0.4252 - val_mae: 0.5046\n",
            "Epoch 319/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3782 - mse: 0.3782 - mae: 0.4860 - val_loss: 0.4162 - val_mse: 0.4162 - val_mae: 0.4920\n",
            "Epoch 320/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3701 - mse: 0.3701 - mae: 0.4787 - val_loss: 0.4251 - val_mse: 0.4251 - val_mae: 0.5067\n",
            "Epoch 321/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3564 - mse: 0.3564 - mae: 0.4690 - val_loss: 0.4157 - val_mse: 0.4157 - val_mae: 0.4926\n",
            "Epoch 322/1000\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.3834 - mse: 0.3834 - mae: 0.4813 - val_loss: 0.4265 - val_mse: 0.4265 - val_mae: 0.5015\n",
            "Epoch 323/1000\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.3611 - mse: 0.3611 - mae: 0.4768 - val_loss: 0.4201 - val_mse: 0.4201 - val_mae: 0.4957\n",
            "Epoch 324/1000\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.3782 - mse: 0.3782 - mae: 0.4796 - val_loss: 0.4142 - val_mse: 0.4142 - val_mae: 0.4986\n",
            "Epoch 325/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3821 - mse: 0.3821 - mae: 0.4779 - val_loss: 0.4174 - val_mse: 0.4174 - val_mae: 0.4981\n",
            "Epoch 326/1000\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.3865 - mse: 0.3865 - mae: 0.4847 - val_loss: 0.4199 - val_mse: 0.4199 - val_mae: 0.4982\n",
            "Epoch 327/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3768 - mse: 0.3768 - mae: 0.4827 - val_loss: 0.4192 - val_mse: 0.4192 - val_mae: 0.4940\n",
            "Epoch 328/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.3608 - mse: 0.3608 - mae: 0.4677 - val_loss: 0.4188 - val_mse: 0.4188 - val_mae: 0.4993\n",
            "Epoch 329/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.3809 - mse: 0.3809 - mae: 0.4877 - val_loss: 0.4236 - val_mse: 0.4236 - val_mae: 0.4972\n",
            "Epoch 330/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3798 - mse: 0.3798 - mae: 0.4795 - val_loss: 0.4125 - val_mse: 0.4125 - val_mae: 0.4933\n",
            "Epoch 331/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3733 - mse: 0.3733 - mae: 0.4767 - val_loss: 0.4165 - val_mse: 0.4165 - val_mae: 0.4942\n",
            "Epoch 332/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3684 - mse: 0.3684 - mae: 0.4790 - val_loss: 0.4272 - val_mse: 0.4272 - val_mae: 0.5051\n",
            "Epoch 333/1000\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.3654 - mse: 0.3654 - mae: 0.4723 - val_loss: 0.4186 - val_mse: 0.4186 - val_mae: 0.4980\n",
            "Epoch 334/1000\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.3673 - mse: 0.3673 - mae: 0.4765 - val_loss: 0.4131 - val_mse: 0.4131 - val_mae: 0.4947\n",
            "Epoch 335/1000\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.3659 - mse: 0.3659 - mae: 0.4717 - val_loss: 0.4240 - val_mse: 0.4240 - val_mae: 0.4991\n",
            "Epoch 336/1000\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3677 - mse: 0.3677 - mae: 0.4775 - val_loss: 0.4210 - val_mse: 0.4210 - val_mae: 0.5010\n",
            "Epoch 337/1000\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.3812 - mse: 0.3812 - mae: 0.4819 - val_loss: 0.4268 - val_mse: 0.4268 - val_mae: 0.5005\n",
            "Epoch 338/1000\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.3880 - mse: 0.3880 - mae: 0.4934 - val_loss: 0.4161 - val_mse: 0.4161 - val_mae: 0.4948\n",
            "Epoch 339/1000\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3676 - mse: 0.3676 - mae: 0.4714 - val_loss: 0.4307 - val_mse: 0.4307 - val_mae: 0.5087\n",
            "Epoch 340/1000\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.3924 - mse: 0.3924 - mae: 0.4945 - val_loss: 0.4088 - val_mse: 0.4088 - val_mae: 0.4933\n",
            "Epoch 341/1000\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.3621 - mse: 0.3621 - mae: 0.4718 - val_loss: 0.4151 - val_mse: 0.4151 - val_mae: 0.4962\n",
            "Epoch 342/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3621 - mse: 0.3621 - mae: 0.4723 - val_loss: 0.4173 - val_mse: 0.4173 - val_mae: 0.4946\n",
            "Epoch 343/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3866 - mse: 0.3866 - mae: 0.4813 - val_loss: 0.4283 - val_mse: 0.4283 - val_mae: 0.5030\n",
            "Epoch 344/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3627 - mse: 0.3627 - mae: 0.4725 - val_loss: 0.4199 - val_mse: 0.4199 - val_mae: 0.4964\n",
            "Epoch 345/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3779 - mse: 0.3779 - mae: 0.4790 - val_loss: 0.4154 - val_mse: 0.4154 - val_mae: 0.4945\n",
            "Epoch 346/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3555 - mse: 0.3555 - mae: 0.4715 - val_loss: 0.4187 - val_mse: 0.4187 - val_mae: 0.4977\n",
            "Epoch 347/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3660 - mse: 0.3660 - mae: 0.4724 - val_loss: 0.4283 - val_mse: 0.4283 - val_mae: 0.4994\n",
            "Epoch 348/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3608 - mse: 0.3608 - mae: 0.4662 - val_loss: 0.4178 - val_mse: 0.4178 - val_mae: 0.4934\n",
            "Epoch 349/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3550 - mse: 0.3550 - mae: 0.4654 - val_loss: 0.4150 - val_mse: 0.4150 - val_mae: 0.4916\n",
            "Epoch 350/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3730 - mse: 0.3730 - mae: 0.4791 - val_loss: 0.4119 - val_mse: 0.4119 - val_mae: 0.4899\n",
            "Epoch 351/1000\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.3642 - mse: 0.3642 - mae: 0.4741 - val_loss: 0.4274 - val_mse: 0.4274 - val_mae: 0.4998\n",
            "Epoch 352/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.3592 - mse: 0.3592 - mae: 0.4690 - val_loss: 0.4318 - val_mse: 0.4318 - val_mae: 0.5078\n",
            "Epoch 353/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.3703 - mse: 0.3703 - mae: 0.4791 - val_loss: 0.4118 - val_mse: 0.4118 - val_mae: 0.4891\n",
            "Epoch 354/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3625 - mse: 0.3625 - mae: 0.4677 - val_loss: 0.4342 - val_mse: 0.4342 - val_mae: 0.5140\n",
            "Epoch 355/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3800 - mse: 0.3800 - mae: 0.4909 - val_loss: 0.4174 - val_mse: 0.4174 - val_mae: 0.4953\n",
            "Epoch 356/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3744 - mse: 0.3744 - mae: 0.4837 - val_loss: 0.4220 - val_mse: 0.4220 - val_mae: 0.4989\n",
            "Epoch 357/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3446 - mse: 0.3446 - mae: 0.4620 - val_loss: 0.4210 - val_mse: 0.4210 - val_mae: 0.4978\n",
            "Epoch 358/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3660 - mse: 0.3660 - mae: 0.4764 - val_loss: 0.4227 - val_mse: 0.4227 - val_mae: 0.4981\n",
            "Epoch 359/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3771 - mse: 0.3771 - mae: 0.4732 - val_loss: 0.4151 - val_mse: 0.4151 - val_mae: 0.4915\n",
            "Epoch 360/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3768 - mse: 0.3768 - mae: 0.4805 - val_loss: 0.4146 - val_mse: 0.4146 - val_mae: 0.4939\n",
            "Epoch 361/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3552 - mse: 0.3552 - mae: 0.4684 - val_loss: 0.4193 - val_mse: 0.4193 - val_mae: 0.4969\n",
            "Epoch 362/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3716 - mse: 0.3716 - mae: 0.4810 - val_loss: 0.4215 - val_mse: 0.4215 - val_mae: 0.4949\n",
            "Epoch 363/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.3799 - mse: 0.3799 - mae: 0.4821 - val_loss: 0.4176 - val_mse: 0.4176 - val_mae: 0.4957\n",
            "Epoch 364/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.3575 - mse: 0.3575 - mae: 0.4705 - val_loss: 0.4186 - val_mse: 0.4186 - val_mae: 0.4948\n",
            "Epoch 365/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3705 - mse: 0.3705 - mae: 0.4768 - val_loss: 0.4209 - val_mse: 0.4209 - val_mae: 0.4946\n",
            "Epoch 366/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3681 - mse: 0.3681 - mae: 0.4785 - val_loss: 0.4160 - val_mse: 0.4160 - val_mae: 0.4906\n",
            "Epoch 367/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3632 - mse: 0.3632 - mae: 0.4739 - val_loss: 0.4496 - val_mse: 0.4496 - val_mae: 0.5192\n",
            "Epoch 368/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.3859 - mse: 0.3859 - mae: 0.4808 - val_loss: 0.4120 - val_mse: 0.4120 - val_mae: 0.4904\n",
            "Epoch 369/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3657 - mse: 0.3657 - mae: 0.4806 - val_loss: 0.4135 - val_mse: 0.4135 - val_mae: 0.4917\n",
            "Epoch 370/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3801 - mse: 0.3801 - mae: 0.4755 - val_loss: 0.4099 - val_mse: 0.4099 - val_mae: 0.4887\n",
            "Epoch 371/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3581 - mse: 0.3581 - mae: 0.4689 - val_loss: 0.4243 - val_mse: 0.4243 - val_mae: 0.5019\n",
            "Epoch 372/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3673 - mse: 0.3673 - mae: 0.4752 - val_loss: 0.4164 - val_mse: 0.4164 - val_mae: 0.4938\n",
            "Epoch 373/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3738 - mse: 0.3738 - mae: 0.4778 - val_loss: 0.4139 - val_mse: 0.4139 - val_mae: 0.4905\n",
            "Epoch 374/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3598 - mse: 0.3598 - mae: 0.4627 - val_loss: 0.4194 - val_mse: 0.4194 - val_mae: 0.4969\n",
            "Epoch 375/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3608 - mse: 0.3608 - mae: 0.4653 - val_loss: 0.4130 - val_mse: 0.4130 - val_mae: 0.4911\n",
            "Epoch 376/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3544 - mse: 0.3544 - mae: 0.4672 - val_loss: 0.4200 - val_mse: 0.4200 - val_mae: 0.4988\n",
            "Epoch 377/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3549 - mse: 0.3549 - mae: 0.4622 - val_loss: 0.4276 - val_mse: 0.4276 - val_mae: 0.5038\n",
            "Epoch 378/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3543 - mse: 0.3543 - mae: 0.4630 - val_loss: 0.4128 - val_mse: 0.4128 - val_mae: 0.4933\n",
            "Epoch 379/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3630 - mse: 0.3630 - mae: 0.4706 - val_loss: 0.4099 - val_mse: 0.4099 - val_mae: 0.4930\n",
            "Epoch 380/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3628 - mse: 0.3628 - mae: 0.4693 - val_loss: 0.4341 - val_mse: 0.4341 - val_mae: 0.5075\n",
            "Epoch 381/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3550 - mse: 0.3550 - mae: 0.4679 - val_loss: 0.4130 - val_mse: 0.4130 - val_mae: 0.4900\n",
            "Epoch 382/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3446 - mse: 0.3446 - mae: 0.4626 - val_loss: 0.4174 - val_mse: 0.4174 - val_mae: 0.4987\n",
            "Epoch 383/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3666 - mse: 0.3666 - mae: 0.4729 - val_loss: 0.4136 - val_mse: 0.4136 - val_mae: 0.4961\n",
            "Epoch 384/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3769 - mse: 0.3769 - mae: 0.4819 - val_loss: 0.4043 - val_mse: 0.4043 - val_mae: 0.4858\n",
            "Epoch 385/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3752 - mse: 0.3752 - mae: 0.4741 - val_loss: 0.4343 - val_mse: 0.4343 - val_mae: 0.5075\n",
            "Epoch 386/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3607 - mse: 0.3607 - mae: 0.4752 - val_loss: 0.4150 - val_mse: 0.4150 - val_mae: 0.4933\n",
            "Epoch 387/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3568 - mse: 0.3568 - mae: 0.4591 - val_loss: 0.4317 - val_mse: 0.4317 - val_mae: 0.5078\n",
            "Epoch 388/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3581 - mse: 0.3581 - mae: 0.4666 - val_loss: 0.4235 - val_mse: 0.4235 - val_mae: 0.5022\n",
            "Epoch 389/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3708 - mse: 0.3708 - mae: 0.4737 - val_loss: 0.4120 - val_mse: 0.4120 - val_mae: 0.4936\n",
            "Epoch 390/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3585 - mse: 0.3585 - mae: 0.4736 - val_loss: 0.4074 - val_mse: 0.4074 - val_mae: 0.4853\n",
            "Epoch 391/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3577 - mse: 0.3577 - mae: 0.4660 - val_loss: 0.4158 - val_mse: 0.4158 - val_mae: 0.4963\n",
            "Epoch 392/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3500 - mse: 0.3500 - mae: 0.4633 - val_loss: 0.4178 - val_mse: 0.4178 - val_mae: 0.4964\n",
            "Epoch 393/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3576 - mse: 0.3576 - mae: 0.4699 - val_loss: 0.4218 - val_mse: 0.4218 - val_mae: 0.4970\n",
            "Epoch 394/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3585 - mse: 0.3585 - mae: 0.4637 - val_loss: 0.4156 - val_mse: 0.4156 - val_mae: 0.4919\n",
            "Epoch 395/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3458 - mse: 0.3458 - mae: 0.4568 - val_loss: 0.4211 - val_mse: 0.4211 - val_mae: 0.4988\n",
            "Epoch 396/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3647 - mse: 0.3647 - mae: 0.4729 - val_loss: 0.4200 - val_mse: 0.4200 - val_mae: 0.4954\n",
            "Epoch 397/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3713 - mse: 0.3713 - mae: 0.4707 - val_loss: 0.4148 - val_mse: 0.4148 - val_mae: 0.4950\n",
            "Epoch 398/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3653 - mse: 0.3653 - mae: 0.4742 - val_loss: 0.4129 - val_mse: 0.4129 - val_mae: 0.4983\n",
            "Epoch 399/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3516 - mse: 0.3516 - mae: 0.4661 - val_loss: 0.4256 - val_mse: 0.4256 - val_mae: 0.5002\n",
            "Epoch 400/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3467 - mse: 0.3467 - mae: 0.4574 - val_loss: 0.4027 - val_mse: 0.4027 - val_mae: 0.4906\n",
            "Epoch 401/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3616 - mse: 0.3616 - mae: 0.4730 - val_loss: 0.4217 - val_mse: 0.4217 - val_mae: 0.5024\n",
            "Epoch 402/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3628 - mse: 0.3628 - mae: 0.4666 - val_loss: 0.4207 - val_mse: 0.4207 - val_mae: 0.5036\n",
            "Epoch 403/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3581 - mse: 0.3581 - mae: 0.4657 - val_loss: 0.4131 - val_mse: 0.4131 - val_mae: 0.4943\n",
            "Epoch 404/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3583 - mse: 0.3583 - mae: 0.4697 - val_loss: 0.4288 - val_mse: 0.4288 - val_mae: 0.5045\n",
            "Epoch 405/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.3713 - mse: 0.3713 - mae: 0.4754 - val_loss: 0.4103 - val_mse: 0.4103 - val_mae: 0.4925\n",
            "Epoch 406/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3779 - mse: 0.3779 - mae: 0.4785 - val_loss: 0.4229 - val_mse: 0.4229 - val_mae: 0.5001\n",
            "Epoch 407/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3667 - mse: 0.3667 - mae: 0.4707 - val_loss: 0.4189 - val_mse: 0.4189 - val_mae: 0.4978\n",
            "Epoch 408/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3555 - mse: 0.3555 - mae: 0.4634 - val_loss: 0.4185 - val_mse: 0.4185 - val_mae: 0.4927\n",
            "Epoch 409/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3610 - mse: 0.3610 - mae: 0.4731 - val_loss: 0.4075 - val_mse: 0.4075 - val_mae: 0.4956\n",
            "Epoch 410/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3423 - mse: 0.3423 - mae: 0.4562 - val_loss: 0.4115 - val_mse: 0.4115 - val_mae: 0.4919\n",
            "Epoch 411/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3500 - mse: 0.3500 - mae: 0.4655 - val_loss: 0.4106 - val_mse: 0.4106 - val_mae: 0.4897\n",
            "Epoch 412/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3447 - mse: 0.3447 - mae: 0.4667 - val_loss: 0.4313 - val_mse: 0.4313 - val_mae: 0.5042\n",
            "Epoch 413/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3546 - mse: 0.3546 - mae: 0.4613 - val_loss: 0.4096 - val_mse: 0.4096 - val_mae: 0.4924\n",
            "Epoch 414/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3645 - mse: 0.3645 - mae: 0.4744 - val_loss: 0.4123 - val_mse: 0.4123 - val_mae: 0.4901\n",
            "Epoch 415/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3511 - mse: 0.3511 - mae: 0.4576 - val_loss: 0.4178 - val_mse: 0.4178 - val_mae: 0.4941\n",
            "Epoch 416/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3616 - mse: 0.3616 - mae: 0.4720 - val_loss: 0.4111 - val_mse: 0.4111 - val_mae: 0.4894\n",
            "Epoch 417/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3638 - mse: 0.3638 - mae: 0.4733 - val_loss: 0.4117 - val_mse: 0.4117 - val_mae: 0.4919\n",
            "Epoch 418/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3555 - mse: 0.3555 - mae: 0.4675 - val_loss: 0.4101 - val_mse: 0.4101 - val_mae: 0.4889\n",
            "Epoch 419/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3689 - mse: 0.3689 - mae: 0.4738 - val_loss: 0.4109 - val_mse: 0.4109 - val_mae: 0.4897\n",
            "Epoch 420/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3387 - mse: 0.3387 - mae: 0.4558 - val_loss: 0.4115 - val_mse: 0.4115 - val_mae: 0.4952\n",
            "Epoch 421/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3670 - mse: 0.3670 - mae: 0.4678 - val_loss: 0.4191 - val_mse: 0.4191 - val_mae: 0.4908\n",
            "Epoch 422/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3474 - mse: 0.3474 - mae: 0.4636 - val_loss: 0.4088 - val_mse: 0.4088 - val_mae: 0.4876\n",
            "Epoch 423/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3682 - mse: 0.3682 - mae: 0.4710 - val_loss: 0.4155 - val_mse: 0.4155 - val_mae: 0.4973\n",
            "Epoch 424/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3544 - mse: 0.3544 - mae: 0.4677 - val_loss: 0.4162 - val_mse: 0.4162 - val_mae: 0.4968\n",
            "Epoch 425/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3629 - mse: 0.3629 - mae: 0.4754 - val_loss: 0.4074 - val_mse: 0.4074 - val_mae: 0.4868\n",
            "Epoch 426/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3497 - mse: 0.3497 - mae: 0.4602 - val_loss: 0.4141 - val_mse: 0.4141 - val_mae: 0.4967\n",
            "Epoch 427/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3556 - mse: 0.3556 - mae: 0.4637 - val_loss: 0.4103 - val_mse: 0.4103 - val_mae: 0.4930\n",
            "Epoch 428/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3663 - mse: 0.3663 - mae: 0.4664 - val_loss: 0.4153 - val_mse: 0.4153 - val_mae: 0.4928\n",
            "Epoch 429/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.3422 - mse: 0.3422 - mae: 0.4558 - val_loss: 0.4227 - val_mse: 0.4227 - val_mae: 0.5009\n",
            "Epoch 430/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3488 - mse: 0.3488 - mae: 0.4682 - val_loss: 0.4124 - val_mse: 0.4124 - val_mae: 0.4903\n",
            "Epoch 431/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3536 - mse: 0.3536 - mae: 0.4632 - val_loss: 0.4174 - val_mse: 0.4174 - val_mae: 0.4995\n",
            "Epoch 432/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3465 - mse: 0.3465 - mae: 0.4584 - val_loss: 0.4178 - val_mse: 0.4178 - val_mae: 0.4941\n",
            "Epoch 433/1000\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.3627 - mse: 0.3627 - mae: 0.4700 - val_loss: 0.4076 - val_mse: 0.4076 - val_mae: 0.4865\n",
            "Epoch 434/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3513 - mse: 0.3513 - mae: 0.4625 - val_loss: 0.4113 - val_mse: 0.4113 - val_mae: 0.4879\n",
            "Epoch 435/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3579 - mse: 0.3579 - mae: 0.4594 - val_loss: 0.4166 - val_mse: 0.4166 - val_mae: 0.4934\n",
            "Epoch 436/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.3399 - mse: 0.3399 - mae: 0.4522 - val_loss: 0.4150 - val_mse: 0.4150 - val_mae: 0.4990\n",
            "Epoch 437/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.3441 - mse: 0.3441 - mae: 0.4584 - val_loss: 0.4148 - val_mse: 0.4148 - val_mae: 0.4942\n",
            "Epoch 438/1000\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.3595 - mse: 0.3595 - mae: 0.4693 - val_loss: 0.4090 - val_mse: 0.4090 - val_mae: 0.4924\n",
            "Epoch 439/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3504 - mse: 0.3504 - mae: 0.4606 - val_loss: 0.4185 - val_mse: 0.4185 - val_mae: 0.4980\n",
            "Epoch 440/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3708 - mse: 0.3708 - mae: 0.4753 - val_loss: 0.4107 - val_mse: 0.4107 - val_mae: 0.4941\n",
            "Epoch 441/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3600 - mse: 0.3600 - mae: 0.4698 - val_loss: 0.4187 - val_mse: 0.4187 - val_mae: 0.4952\n",
            "Epoch 442/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3363 - mse: 0.3363 - mae: 0.4568 - val_loss: 0.4119 - val_mse: 0.4119 - val_mae: 0.4904\n",
            "Epoch 443/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3356 - mse: 0.3356 - mae: 0.4545 - val_loss: 0.4100 - val_mse: 0.4100 - val_mae: 0.4914\n",
            "Epoch 444/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3415 - mse: 0.3415 - mae: 0.4578 - val_loss: 0.4109 - val_mse: 0.4109 - val_mae: 0.4945\n",
            "Epoch 445/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3393 - mse: 0.3393 - mae: 0.4573 - val_loss: 0.4187 - val_mse: 0.4187 - val_mae: 0.4974\n",
            "Epoch 446/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3630 - mse: 0.3630 - mae: 0.4726 - val_loss: 0.4126 - val_mse: 0.4126 - val_mae: 0.4942\n",
            "Epoch 447/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3495 - mse: 0.3495 - mae: 0.4657 - val_loss: 0.4178 - val_mse: 0.4178 - val_mae: 0.4911\n",
            "Epoch 448/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3455 - mse: 0.3455 - mae: 0.4617 - val_loss: 0.4180 - val_mse: 0.4180 - val_mae: 0.4950\n",
            "Epoch 449/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3346 - mse: 0.3346 - mae: 0.4542 - val_loss: 0.4118 - val_mse: 0.4118 - val_mae: 0.4916\n",
            "Epoch 450/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3568 - mse: 0.3568 - mae: 0.4714 - val_loss: 0.4212 - val_mse: 0.4212 - val_mae: 0.5007\n",
            "Epoch 451/1000\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.3551 - mse: 0.3551 - mae: 0.4743 - val_loss: 0.4080 - val_mse: 0.4080 - val_mae: 0.4875\n",
            "Epoch 452/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3560 - mse: 0.3560 - mae: 0.4659 - val_loss: 0.4267 - val_mse: 0.4267 - val_mae: 0.5024\n",
            "Epoch 453/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3470 - mse: 0.3470 - mae: 0.4603 - val_loss: 0.4128 - val_mse: 0.4128 - val_mae: 0.4900\n",
            "Epoch 454/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3494 - mse: 0.3494 - mae: 0.4677 - val_loss: 0.4170 - val_mse: 0.4170 - val_mae: 0.4972\n",
            "Epoch 455/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3448 - mse: 0.3448 - mae: 0.4566 - val_loss: 0.4191 - val_mse: 0.4191 - val_mae: 0.4969\n",
            "Epoch 456/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3483 - mse: 0.3483 - mae: 0.4645 - val_loss: 0.4182 - val_mse: 0.4182 - val_mae: 0.4963\n",
            "Epoch 457/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3457 - mse: 0.3457 - mae: 0.4563 - val_loss: 0.4168 - val_mse: 0.4168 - val_mae: 0.4999\n",
            "Epoch 458/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3404 - mse: 0.3404 - mae: 0.4633 - val_loss: 0.4108 - val_mse: 0.4108 - val_mae: 0.4926\n",
            "Epoch 459/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3442 - mse: 0.3442 - mae: 0.4611 - val_loss: 0.4083 - val_mse: 0.4083 - val_mae: 0.4853\n",
            "Epoch 460/1000\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.3655 - mse: 0.3655 - mae: 0.4690 - val_loss: 0.4255 - val_mse: 0.4255 - val_mae: 0.5037\n",
            "Epoch 461/1000\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.3560 - mse: 0.3560 - mae: 0.4607 - val_loss: 0.4141 - val_mse: 0.4141 - val_mae: 0.4927\n",
            "Epoch 462/1000\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3402 - mse: 0.3402 - mae: 0.4561 - val_loss: 0.4363 - val_mse: 0.4363 - val_mae: 0.5074\n",
            "Epoch 463/1000\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.3357 - mse: 0.3357 - mae: 0.4515 - val_loss: 0.4037 - val_mse: 0.4037 - val_mae: 0.4835\n",
            "Epoch 464/1000\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.3584 - mse: 0.3584 - mae: 0.4640 - val_loss: 0.4319 - val_mse: 0.4319 - val_mae: 0.5073\n",
            "Epoch 465/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.3324 - mse: 0.3324 - mae: 0.4474 - val_loss: 0.4139 - val_mse: 0.4139 - val_mae: 0.4894\n",
            "Epoch 466/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3471 - mse: 0.3471 - mae: 0.4609 - val_loss: 0.4129 - val_mse: 0.4129 - val_mae: 0.4871\n",
            "Epoch 467/1000\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.3385 - mse: 0.3385 - mae: 0.4541 - val_loss: 0.4157 - val_mse: 0.4157 - val_mae: 0.4943\n",
            "Epoch 468/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3518 - mse: 0.3518 - mae: 0.4702 - val_loss: 0.4174 - val_mse: 0.4174 - val_mae: 0.4918\n",
            "Epoch 469/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3660 - mse: 0.3660 - mae: 0.4774 - val_loss: 0.4110 - val_mse: 0.4110 - val_mae: 0.4926\n",
            "Epoch 470/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3438 - mse: 0.3438 - mae: 0.4574 - val_loss: 0.4186 - val_mse: 0.4186 - val_mae: 0.4998\n",
            "Epoch 471/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3444 - mse: 0.3444 - mae: 0.4594 - val_loss: 0.4176 - val_mse: 0.4176 - val_mae: 0.4992\n",
            "Epoch 472/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3305 - mse: 0.3305 - mae: 0.4520 - val_loss: 0.4182 - val_mse: 0.4182 - val_mae: 0.4945\n",
            "Epoch 473/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3491 - mse: 0.3491 - mae: 0.4638 - val_loss: 0.4201 - val_mse: 0.4201 - val_mae: 0.4978\n",
            "Epoch 474/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3416 - mse: 0.3416 - mae: 0.4602 - val_loss: 0.4105 - val_mse: 0.4105 - val_mae: 0.4917\n",
            "Epoch 475/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3328 - mse: 0.3328 - mae: 0.4525 - val_loss: 0.4139 - val_mse: 0.4139 - val_mae: 0.4923\n",
            "Epoch 476/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3384 - mse: 0.3384 - mae: 0.4521 - val_loss: 0.4086 - val_mse: 0.4086 - val_mae: 0.4881\n",
            "Epoch 477/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3598 - mse: 0.3598 - mae: 0.4649 - val_loss: 0.4286 - val_mse: 0.4286 - val_mae: 0.5082\n",
            "Epoch 478/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3396 - mse: 0.3396 - mae: 0.4565 - val_loss: 0.4101 - val_mse: 0.4101 - val_mae: 0.4902\n",
            "Epoch 479/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3421 - mse: 0.3421 - mae: 0.4553 - val_loss: 0.4133 - val_mse: 0.4133 - val_mae: 0.4929\n",
            "Epoch 480/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3354 - mse: 0.3354 - mae: 0.4473 - val_loss: 0.4171 - val_mse: 0.4171 - val_mae: 0.4966\n",
            "Epoch 481/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3644 - mse: 0.3644 - mae: 0.4663 - val_loss: 0.4130 - val_mse: 0.4130 - val_mae: 0.4956\n",
            "Epoch 482/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3327 - mse: 0.3327 - mae: 0.4526 - val_loss: 0.4165 - val_mse: 0.4165 - val_mae: 0.4927\n",
            "Epoch 483/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3272 - mse: 0.3272 - mae: 0.4476 - val_loss: 0.4140 - val_mse: 0.4140 - val_mae: 0.4933\n",
            "Epoch 484/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3447 - mse: 0.3447 - mae: 0.4525 - val_loss: 0.4131 - val_mse: 0.4131 - val_mae: 0.4926\n",
            "Epoch 485/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3345 - mse: 0.3345 - mae: 0.4511 - val_loss: 0.4176 - val_mse: 0.4176 - val_mae: 0.4904\n",
            "Epoch 486/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3443 - mse: 0.3443 - mae: 0.4590 - val_loss: 0.4235 - val_mse: 0.4235 - val_mae: 0.5007\n",
            "Epoch 487/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3540 - mse: 0.3540 - mae: 0.4602 - val_loss: 0.4145 - val_mse: 0.4145 - val_mae: 0.4952\n",
            "Epoch 488/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3383 - mse: 0.3383 - mae: 0.4526 - val_loss: 0.4091 - val_mse: 0.4091 - val_mae: 0.4909\n",
            "Epoch 489/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3433 - mse: 0.3433 - mae: 0.4584 - val_loss: 0.4155 - val_mse: 0.4155 - val_mae: 0.4970\n",
            "Epoch 490/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3343 - mse: 0.3343 - mae: 0.4511 - val_loss: 0.4315 - val_mse: 0.4315 - val_mae: 0.5073\n",
            "Epoch 491/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3569 - mse: 0.3569 - mae: 0.4650 - val_loss: 0.4124 - val_mse: 0.4124 - val_mae: 0.4912\n",
            "Epoch 492/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3414 - mse: 0.3414 - mae: 0.4522 - val_loss: 0.4109 - val_mse: 0.4109 - val_mae: 0.4876\n",
            "Epoch 493/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3449 - mse: 0.3449 - mae: 0.4521 - val_loss: 0.4258 - val_mse: 0.4258 - val_mae: 0.5053\n",
            "Epoch 494/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3546 - mse: 0.3546 - mae: 0.4676 - val_loss: 0.4114 - val_mse: 0.4114 - val_mae: 0.4897\n",
            "Epoch 495/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3373 - mse: 0.3373 - mae: 0.4550 - val_loss: 0.4213 - val_mse: 0.4213 - val_mae: 0.5024\n",
            "Epoch 496/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3573 - mse: 0.3573 - mae: 0.4677 - val_loss: 0.4124 - val_mse: 0.4124 - val_mae: 0.4910\n",
            "Epoch 497/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3562 - mse: 0.3562 - mae: 0.4675 - val_loss: 0.4130 - val_mse: 0.4130 - val_mae: 0.4895\n",
            "Epoch 498/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3478 - mse: 0.3478 - mae: 0.4605 - val_loss: 0.4157 - val_mse: 0.4157 - val_mae: 0.4930\n",
            "Epoch 499/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3690 - mse: 0.3690 - mae: 0.4769 - val_loss: 0.4150 - val_mse: 0.4150 - val_mae: 0.4956\n",
            "Epoch 500/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3373 - mse: 0.3373 - mae: 0.4520 - val_loss: 0.4194 - val_mse: 0.4194 - val_mae: 0.4953\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.4602 - mse: 0.4602 - mae: 0.4933\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 15ms/step - loss: 29.4017 - mse: 29.4017 - mae: 5.2571 - val_loss: 17.7334 - val_mse: 17.7334 - val_mae: 4.0215\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 12.0404 - mse: 12.0404 - mae: 3.2040 - val_loss: 7.4846 - val_mse: 7.4846 - val_mae: 2.4203\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5.7625 - mse: 5.7625 - mae: 2.0339 - val_loss: 4.9246 - val_mse: 4.9246 - val_mae: 1.8071\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 4.0500 - mse: 4.0500 - mae: 1.5974 - val_loss: 3.8962 - val_mse: 3.8962 - val_mae: 1.5932\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 3.1171 - mse: 3.1171 - mae: 1.3891 - val_loss: 3.3157 - val_mse: 3.3157 - val_mae: 1.4829\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.8483 - mse: 2.8483 - mae: 1.3425 - val_loss: 2.9637 - val_mse: 2.9637 - val_mae: 1.4094\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.6156 - mse: 2.6156 - mae: 1.2592 - val_loss: 2.7445 - val_mse: 2.7445 - val_mae: 1.3592\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.3831 - mse: 2.3831 - mae: 1.2125 - val_loss: 2.5621 - val_mse: 2.5621 - val_mae: 1.3086\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.1324 - mse: 2.1324 - mae: 1.1573 - val_loss: 2.4186 - val_mse: 2.4186 - val_mae: 1.2670\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.1249 - mse: 2.1249 - mae: 1.1401 - val_loss: 2.2618 - val_mse: 2.2618 - val_mae: 1.2201\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.8749 - mse: 1.8749 - mae: 1.0593 - val_loss: 2.1654 - val_mse: 2.1654 - val_mae: 1.1870\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.7527 - mse: 1.7527 - mae: 1.0435 - val_loss: 2.0874 - val_mse: 2.0874 - val_mae: 1.1586\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.8209 - mse: 1.8209 - mae: 1.0591 - val_loss: 1.9899 - val_mse: 1.9899 - val_mae: 1.1357\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.6072 - mse: 1.6072 - mae: 0.9809 - val_loss: 1.9142 - val_mse: 1.9142 - val_mae: 1.1065\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.5563 - mse: 1.5563 - mae: 0.9904 - val_loss: 1.8426 - val_mse: 1.8426 - val_mae: 1.0818\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.5227 - mse: 1.5227 - mae: 0.9824 - val_loss: 1.7727 - val_mse: 1.7727 - val_mae: 1.0700\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.3952 - mse: 1.3952 - mae: 0.9218 - val_loss: 1.6948 - val_mse: 1.6948 - val_mae: 1.0422\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.3588 - mse: 1.3588 - mae: 0.9216 - val_loss: 1.6173 - val_mse: 1.6173 - val_mae: 1.0187\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2963 - mse: 1.2963 - mae: 0.8777 - val_loss: 1.5876 - val_mse: 1.5876 - val_mae: 1.0040\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2609 - mse: 1.2609 - mae: 0.8665 - val_loss: 1.5080 - val_mse: 1.5080 - val_mae: 0.9828\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1507 - mse: 1.1507 - mae: 0.8373 - val_loss: 1.4469 - val_mse: 1.4469 - val_mae: 0.9605\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1717 - mse: 1.1717 - mae: 0.8495 - val_loss: 1.4095 - val_mse: 1.4095 - val_mae: 0.9433\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1388 - mse: 1.1388 - mae: 0.8296 - val_loss: 1.3548 - val_mse: 1.3548 - val_mae: 0.9264\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.1448 - mse: 1.1448 - mae: 0.8331 - val_loss: 1.2962 - val_mse: 1.2962 - val_mae: 0.9067\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0790 - mse: 1.0790 - mae: 0.8024 - val_loss: 1.2484 - val_mse: 1.2484 - val_mae: 0.8880\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.9692 - mse: 0.9692 - mae: 0.7684 - val_loss: 1.2060 - val_mse: 1.2060 - val_mae: 0.8785\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0278 - mse: 1.0278 - mae: 0.7844 - val_loss: 1.1771 - val_mse: 1.1771 - val_mae: 0.8595\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9723 - mse: 0.9723 - mae: 0.7745 - val_loss: 1.1397 - val_mse: 1.1397 - val_mae: 0.8507\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8995 - mse: 0.8995 - mae: 0.7404 - val_loss: 1.1036 - val_mse: 1.1036 - val_mae: 0.8327\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9048 - mse: 0.9048 - mae: 0.7343 - val_loss: 1.0650 - val_mse: 1.0650 - val_mae: 0.8260\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.9089 - mse: 0.9089 - mae: 0.7335 - val_loss: 1.0259 - val_mse: 1.0259 - val_mae: 0.7979\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8534 - mse: 0.8534 - mae: 0.7167 - val_loss: 1.0140 - val_mse: 1.0140 - val_mae: 0.8086\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8577 - mse: 0.8577 - mae: 0.7201 - val_loss: 0.9719 - val_mse: 0.9719 - val_mae: 0.7862\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8469 - mse: 0.8469 - mae: 0.7189 - val_loss: 0.9440 - val_mse: 0.9440 - val_mae: 0.7768\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8085 - mse: 0.8085 - mae: 0.6937 - val_loss: 0.9051 - val_mse: 0.9051 - val_mae: 0.7475\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7737 - mse: 0.7737 - mae: 0.6832 - val_loss: 0.8879 - val_mse: 0.8879 - val_mae: 0.7541\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7384 - mse: 0.7384 - mae: 0.6643 - val_loss: 0.8590 - val_mse: 0.8590 - val_mae: 0.7402\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7347 - mse: 0.7347 - mae: 0.6711 - val_loss: 0.8319 - val_mse: 0.8319 - val_mae: 0.7134\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7231 - mse: 0.7231 - mae: 0.6612 - val_loss: 0.8075 - val_mse: 0.8075 - val_mae: 0.7195\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.6667 - mse: 0.6667 - mae: 0.6293 - val_loss: 0.7784 - val_mse: 0.7784 - val_mae: 0.7015\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6619 - mse: 0.6619 - mae: 0.6251 - val_loss: 0.7655 - val_mse: 0.7655 - val_mae: 0.6892\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.6605 - mse: 0.6605 - mae: 0.6290 - val_loss: 0.7348 - val_mse: 0.7348 - val_mae: 0.6706\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6902 - mse: 0.6902 - mae: 0.6448 - val_loss: 0.7917 - val_mse: 0.7917 - val_mae: 0.7266\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.6639 - mse: 0.6639 - mae: 0.6368 - val_loss: 0.7210 - val_mse: 0.7210 - val_mae: 0.6694\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5965 - mse: 0.5965 - mae: 0.5903 - val_loss: 0.7146 - val_mse: 0.7146 - val_mae: 0.6565\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.5890 - mse: 0.5890 - mae: 0.5894 - val_loss: 0.6886 - val_mse: 0.6886 - val_mae: 0.6587\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5981 - mse: 0.5981 - mae: 0.5937 - val_loss: 0.6716 - val_mse: 0.6716 - val_mae: 0.6525\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.5987 - mse: 0.5987 - mae: 0.5983 - val_loss: 0.6670 - val_mse: 0.6670 - val_mae: 0.6344\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6009 - mse: 0.6009 - mae: 0.5991 - val_loss: 0.6477 - val_mse: 0.6477 - val_mae: 0.6316\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5742 - mse: 0.5742 - mae: 0.5929 - val_loss: 0.6368 - val_mse: 0.6368 - val_mae: 0.6335\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5585 - mse: 0.5585 - mae: 0.5852 - val_loss: 0.6215 - val_mse: 0.6215 - val_mae: 0.6203\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.5545 - mse: 0.5545 - mae: 0.5771 - val_loss: 0.6032 - val_mse: 0.6032 - val_mae: 0.6100\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5797 - mse: 0.5797 - mae: 0.5894 - val_loss: 0.5962 - val_mse: 0.5962 - val_mae: 0.6033\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.5555 - mse: 0.5555 - mae: 0.5805 - val_loss: 0.5945 - val_mse: 0.5945 - val_mae: 0.6163\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.5069 - mse: 0.5069 - mae: 0.5595 - val_loss: 0.5715 - val_mse: 0.5715 - val_mae: 0.5904\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.4955 - mse: 0.4955 - mae: 0.5441 - val_loss: 0.5700 - val_mse: 0.5700 - val_mae: 0.5866\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4983 - mse: 0.4983 - mae: 0.5518 - val_loss: 0.5655 - val_mse: 0.5655 - val_mae: 0.5807\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4898 - mse: 0.4898 - mae: 0.5421 - val_loss: 0.5460 - val_mse: 0.5460 - val_mae: 0.5763\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.4728 - mse: 0.4728 - mae: 0.5245 - val_loss: 0.5602 - val_mse: 0.5602 - val_mae: 0.5916\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.4857 - mse: 0.4857 - mae: 0.5453 - val_loss: 0.5262 - val_mse: 0.5262 - val_mae: 0.5704\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.4771 - mse: 0.4771 - mae: 0.5347 - val_loss: 0.5278 - val_mse: 0.5278 - val_mae: 0.5651\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4522 - mse: 0.4522 - mae: 0.5197 - val_loss: 0.5521 - val_mse: 0.5521 - val_mae: 0.5911\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4585 - mse: 0.4585 - mae: 0.5286 - val_loss: 0.5358 - val_mse: 0.5358 - val_mae: 0.5664\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4605 - mse: 0.4605 - mae: 0.5232 - val_loss: 0.5107 - val_mse: 0.5107 - val_mae: 0.5604\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4461 - mse: 0.4461 - mae: 0.5202 - val_loss: 0.5042 - val_mse: 0.5042 - val_mae: 0.5493\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4592 - mse: 0.4592 - mae: 0.5290 - val_loss: 0.5276 - val_mse: 0.5276 - val_mae: 0.5582\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4452 - mse: 0.4452 - mae: 0.5141 - val_loss: 0.4923 - val_mse: 0.4923 - val_mae: 0.5468\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4327 - mse: 0.4327 - mae: 0.5097 - val_loss: 0.4834 - val_mse: 0.4834 - val_mae: 0.5392\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4311 - mse: 0.4311 - mae: 0.5201 - val_loss: 0.4739 - val_mse: 0.4739 - val_mae: 0.5407\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4452 - mse: 0.4452 - mae: 0.5124 - val_loss: 0.4718 - val_mse: 0.4718 - val_mae: 0.5359\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4140 - mse: 0.4140 - mae: 0.5004 - val_loss: 0.4676 - val_mse: 0.4676 - val_mae: 0.5343\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4123 - mse: 0.4123 - mae: 0.4925 - val_loss: 0.4670 - val_mse: 0.4670 - val_mae: 0.5287\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4076 - mse: 0.4076 - mae: 0.4867 - val_loss: 0.4688 - val_mse: 0.4688 - val_mae: 0.5343\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4190 - mse: 0.4190 - mae: 0.5054 - val_loss: 0.4684 - val_mse: 0.4684 - val_mae: 0.5341\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4106 - mse: 0.4106 - mae: 0.4924 - val_loss: 0.4628 - val_mse: 0.4628 - val_mae: 0.5322\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3743 - mse: 0.3743 - mae: 0.4732 - val_loss: 0.4530 - val_mse: 0.4530 - val_mae: 0.5251\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3949 - mse: 0.3949 - mae: 0.4922 - val_loss: 0.4530 - val_mse: 0.4530 - val_mae: 0.5239\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3890 - mse: 0.3890 - mae: 0.4802 - val_loss: 0.4452 - val_mse: 0.4452 - val_mae: 0.5222\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3939 - mse: 0.3939 - mae: 0.4810 - val_loss: 0.4579 - val_mse: 0.4579 - val_mae: 0.5316\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3879 - mse: 0.3879 - mae: 0.4855 - val_loss: 0.4496 - val_mse: 0.4496 - val_mae: 0.5259\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3845 - mse: 0.3845 - mae: 0.4796 - val_loss: 0.4499 - val_mse: 0.4499 - val_mae: 0.5257\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3816 - mse: 0.3816 - mae: 0.4720 - val_loss: 0.4417 - val_mse: 0.4417 - val_mae: 0.5191\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3914 - mse: 0.3914 - mae: 0.4792 - val_loss: 0.4409 - val_mse: 0.4409 - val_mae: 0.5231\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3771 - mse: 0.3771 - mae: 0.4806 - val_loss: 0.4333 - val_mse: 0.4333 - val_mae: 0.5179\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3640 - mse: 0.3640 - mae: 0.4582 - val_loss: 0.4482 - val_mse: 0.4482 - val_mae: 0.5281\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3819 - mse: 0.3819 - mae: 0.4766 - val_loss: 0.4336 - val_mse: 0.4336 - val_mae: 0.5184\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3745 - mse: 0.3745 - mae: 0.4748 - val_loss: 0.4483 - val_mse: 0.4483 - val_mae: 0.5255\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3575 - mse: 0.3575 - mae: 0.4623 - val_loss: 0.4325 - val_mse: 0.4325 - val_mae: 0.5153\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3762 - mse: 0.3762 - mae: 0.4757 - val_loss: 0.4513 - val_mse: 0.4513 - val_mae: 0.5293\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3780 - mse: 0.3780 - mae: 0.4768 - val_loss: 0.4760 - val_mse: 0.4760 - val_mae: 0.5409\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3826 - mse: 0.3826 - mae: 0.4890 - val_loss: 0.4213 - val_mse: 0.4213 - val_mae: 0.5054\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3648 - mse: 0.3648 - mae: 0.4696 - val_loss: 0.4320 - val_mse: 0.4320 - val_mae: 0.5135\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3489 - mse: 0.3489 - mae: 0.4599 - val_loss: 0.4342 - val_mse: 0.4342 - val_mae: 0.5124\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3607 - mse: 0.3607 - mae: 0.4676 - val_loss: 0.4228 - val_mse: 0.4228 - val_mae: 0.5067\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3698 - mse: 0.3698 - mae: 0.4706 - val_loss: 0.4190 - val_mse: 0.4190 - val_mae: 0.5011\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3655 - mse: 0.3655 - mae: 0.4701 - val_loss: 0.4300 - val_mse: 0.4300 - val_mae: 0.5156\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3615 - mse: 0.3615 - mae: 0.4640 - val_loss: 0.4097 - val_mse: 0.4097 - val_mae: 0.4988\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3494 - mse: 0.3494 - mae: 0.4527 - val_loss: 0.4276 - val_mse: 0.4276 - val_mae: 0.5098\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3454 - mse: 0.3454 - mae: 0.4575 - val_loss: 0.4090 - val_mse: 0.4090 - val_mae: 0.5022\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3430 - mse: 0.3430 - mae: 0.4562 - val_loss: 0.4047 - val_mse: 0.4047 - val_mae: 0.4974\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3533 - mse: 0.3533 - mae: 0.4624 - val_loss: 0.4060 - val_mse: 0.4060 - val_mae: 0.5022\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3623 - mse: 0.3623 - mae: 0.4658 - val_loss: 0.4140 - val_mse: 0.4140 - val_mae: 0.5057\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3292 - mse: 0.3292 - mae: 0.4483 - val_loss: 0.4123 - val_mse: 0.4123 - val_mae: 0.5036\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3388 - mse: 0.3388 - mae: 0.4499 - val_loss: 0.4036 - val_mse: 0.4036 - val_mae: 0.4980\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3402 - mse: 0.3402 - mae: 0.4537 - val_loss: 0.4112 - val_mse: 0.4112 - val_mae: 0.5019\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3449 - mse: 0.3449 - mae: 0.4482 - val_loss: 0.4218 - val_mse: 0.4218 - val_mae: 0.5080\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3342 - mse: 0.3342 - mae: 0.4457 - val_loss: 0.4134 - val_mse: 0.4134 - val_mae: 0.5044\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3494 - mse: 0.3494 - mae: 0.4567 - val_loss: 0.3994 - val_mse: 0.3994 - val_mae: 0.4930\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3380 - mse: 0.3380 - mae: 0.4487 - val_loss: 0.4044 - val_mse: 0.4044 - val_mae: 0.4969\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3193 - mse: 0.3193 - mae: 0.4320 - val_loss: 0.3942 - val_mse: 0.3942 - val_mae: 0.4897\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3158 - mse: 0.3158 - mae: 0.4358 - val_loss: 0.4015 - val_mse: 0.4015 - val_mae: 0.4984\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3197 - mse: 0.3197 - mae: 0.4446 - val_loss: 0.4267 - val_mse: 0.4267 - val_mae: 0.5089\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3458 - mse: 0.3458 - mae: 0.4542 - val_loss: 0.4158 - val_mse: 0.4158 - val_mae: 0.5073\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3487 - mse: 0.3487 - mae: 0.4630 - val_loss: 0.4028 - val_mse: 0.4028 - val_mae: 0.4973\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3310 - mse: 0.3310 - mae: 0.4479 - val_loss: 0.4094 - val_mse: 0.4094 - val_mae: 0.5021\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3214 - mse: 0.3214 - mae: 0.4355 - val_loss: 0.4069 - val_mse: 0.4069 - val_mae: 0.5012\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3358 - mse: 0.3358 - mae: 0.4444 - val_loss: 0.3941 - val_mse: 0.3941 - val_mae: 0.4906\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3082 - mse: 0.3082 - mae: 0.4332 - val_loss: 0.4002 - val_mse: 0.4002 - val_mae: 0.4970\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3071 - mse: 0.3071 - mae: 0.4273 - val_loss: 0.4109 - val_mse: 0.4109 - val_mae: 0.5080\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3367 - mse: 0.3367 - mae: 0.4567 - val_loss: 0.4041 - val_mse: 0.4041 - val_mae: 0.5030\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3082 - mse: 0.3082 - mae: 0.4283 - val_loss: 0.4202 - val_mse: 0.4202 - val_mae: 0.5025\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3174 - mse: 0.3174 - mae: 0.4311 - val_loss: 0.4131 - val_mse: 0.4131 - val_mae: 0.5067\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3175 - mse: 0.3175 - mae: 0.4348 - val_loss: 0.4041 - val_mse: 0.4041 - val_mae: 0.5006\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3280 - mse: 0.3280 - mae: 0.4412 - val_loss: 0.4029 - val_mse: 0.4029 - val_mae: 0.4971\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3324 - mse: 0.3324 - mae: 0.4484 - val_loss: 0.4098 - val_mse: 0.4098 - val_mae: 0.5027\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3054 - mse: 0.3054 - mae: 0.4316 - val_loss: 0.3952 - val_mse: 0.3952 - val_mae: 0.4895\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3018 - mse: 0.3018 - mae: 0.4308 - val_loss: 0.3928 - val_mse: 0.3928 - val_mae: 0.4931\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2981 - mse: 0.2981 - mae: 0.4240 - val_loss: 0.4065 - val_mse: 0.4065 - val_mae: 0.4952\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3220 - mse: 0.3220 - mae: 0.4464 - val_loss: 0.4074 - val_mse: 0.4074 - val_mae: 0.4983\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3253 - mse: 0.3253 - mae: 0.4497 - val_loss: 0.4210 - val_mse: 0.4210 - val_mae: 0.5018\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3191 - mse: 0.3191 - mae: 0.4388 - val_loss: 0.4029 - val_mse: 0.4029 - val_mae: 0.5080\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3061 - mse: 0.3061 - mae: 0.4312 - val_loss: 0.4127 - val_mse: 0.4127 - val_mae: 0.4971\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3089 - mse: 0.3089 - mae: 0.4355 - val_loss: 0.4003 - val_mse: 0.4003 - val_mae: 0.5011\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3145 - mse: 0.3145 - mae: 0.4380 - val_loss: 0.3937 - val_mse: 0.3937 - val_mae: 0.4938\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2887 - mse: 0.2887 - mae: 0.4203 - val_loss: 0.3960 - val_mse: 0.3960 - val_mae: 0.4906\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3082 - mse: 0.3082 - mae: 0.4386 - val_loss: 0.3912 - val_mse: 0.3912 - val_mae: 0.4819\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2864 - mse: 0.2864 - mae: 0.4099 - val_loss: 0.3878 - val_mse: 0.3878 - val_mae: 0.4884\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2960 - mse: 0.2960 - mae: 0.4240 - val_loss: 0.4134 - val_mse: 0.4134 - val_mae: 0.5054\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2887 - mse: 0.2887 - mae: 0.4178 - val_loss: 0.3826 - val_mse: 0.3826 - val_mae: 0.4842\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3072 - mse: 0.3072 - mae: 0.4324 - val_loss: 0.3975 - val_mse: 0.3975 - val_mae: 0.4895\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2865 - mse: 0.2865 - mae: 0.4163 - val_loss: 0.3848 - val_mse: 0.3848 - val_mae: 0.4813\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3018 - mse: 0.3018 - mae: 0.4238 - val_loss: 0.3776 - val_mse: 0.3776 - val_mae: 0.4878\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2945 - mse: 0.2945 - mae: 0.4197 - val_loss: 0.3931 - val_mse: 0.3931 - val_mae: 0.4895\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2923 - mse: 0.2923 - mae: 0.4205 - val_loss: 0.4279 - val_mse: 0.4279 - val_mae: 0.5157\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3275 - mse: 0.3275 - mae: 0.4428 - val_loss: 0.3880 - val_mse: 0.3880 - val_mae: 0.4865\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2993 - mse: 0.2993 - mae: 0.4270 - val_loss: 0.3862 - val_mse: 0.3862 - val_mae: 0.4893\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2997 - mse: 0.2997 - mae: 0.4289 - val_loss: 0.4110 - val_mse: 0.4110 - val_mae: 0.5054\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3010 - mse: 0.3010 - mae: 0.4288 - val_loss: 0.3903 - val_mse: 0.3903 - val_mae: 0.4925\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3224 - mse: 0.3224 - mae: 0.4403 - val_loss: 0.3837 - val_mse: 0.3837 - val_mae: 0.4899\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2931 - mse: 0.2931 - mae: 0.4188 - val_loss: 0.3950 - val_mse: 0.3950 - val_mae: 0.4965\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3066 - mse: 0.3066 - mae: 0.4302 - val_loss: 0.3857 - val_mse: 0.3857 - val_mae: 0.4851\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2847 - mse: 0.2847 - mae: 0.4161 - val_loss: 0.3896 - val_mse: 0.3896 - val_mae: 0.4901\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2893 - mse: 0.2893 - mae: 0.4179 - val_loss: 0.3894 - val_mse: 0.3894 - val_mae: 0.4973\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3110 - mse: 0.3110 - mae: 0.4332 - val_loss: 0.3970 - val_mse: 0.3970 - val_mae: 0.5008\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2894 - mse: 0.2894 - mae: 0.4146 - val_loss: 0.3745 - val_mse: 0.3745 - val_mae: 0.4753\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2897 - mse: 0.2897 - mae: 0.4196 - val_loss: 0.3842 - val_mse: 0.3842 - val_mae: 0.4857\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3111 - mse: 0.3111 - mae: 0.4378 - val_loss: 0.3842 - val_mse: 0.3842 - val_mae: 0.4918\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2745 - mse: 0.2745 - mae: 0.4056 - val_loss: 0.3861 - val_mse: 0.3861 - val_mae: 0.4915\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2829 - mse: 0.2829 - mae: 0.4123 - val_loss: 0.3830 - val_mse: 0.3830 - val_mae: 0.4847\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2956 - mse: 0.2956 - mae: 0.4292 - val_loss: 0.4190 - val_mse: 0.4190 - val_mae: 0.5046\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2822 - mse: 0.2822 - mae: 0.4158 - val_loss: 0.3958 - val_mse: 0.3958 - val_mae: 0.4849\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2837 - mse: 0.2837 - mae: 0.4115 - val_loss: 0.3871 - val_mse: 0.3871 - val_mae: 0.4893\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.2977 - mse: 0.2977 - mae: 0.4200 - val_loss: 0.3940 - val_mse: 0.3940 - val_mae: 0.5032\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.3006 - mse: 0.3006 - mae: 0.4286 - val_loss: 0.3962 - val_mse: 0.3962 - val_mae: 0.4934\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2983 - mse: 0.2983 - mae: 0.4193 - val_loss: 0.3953 - val_mse: 0.3953 - val_mae: 0.4847\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2979 - mse: 0.2979 - mae: 0.4230 - val_loss: 0.3996 - val_mse: 0.3996 - val_mae: 0.4917\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2830 - mse: 0.2830 - mae: 0.4170 - val_loss: 0.3917 - val_mse: 0.3917 - val_mae: 0.4944\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2748 - mse: 0.2748 - mae: 0.4066 - val_loss: 0.3915 - val_mse: 0.3915 - val_mae: 0.4902\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2780 - mse: 0.2780 - mae: 0.4091 - val_loss: 0.3889 - val_mse: 0.3889 - val_mae: 0.4895\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3037 - mse: 0.3037 - mae: 0.4305 - val_loss: 0.3865 - val_mse: 0.3865 - val_mae: 0.4798\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.3302 - mse: 0.3302 - mae: 0.4457 - val_loss: 0.4042 - val_mse: 0.4042 - val_mae: 0.5068\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.2736 - mse: 0.2736 - mae: 0.4043 - val_loss: 0.3999 - val_mse: 0.3999 - val_mae: 0.4964\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2833 - mse: 0.2833 - mae: 0.4129 - val_loss: 0.4144 - val_mse: 0.4144 - val_mae: 0.5068\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2987 - mse: 0.2987 - mae: 0.4271 - val_loss: 0.3926 - val_mse: 0.3926 - val_mae: 0.4909\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.2874 - mse: 0.2874 - mae: 0.4157 - val_loss: 0.4043 - val_mse: 0.4043 - val_mae: 0.5014\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3116 - mse: 0.3116 - mae: 0.4273 - val_loss: 0.3854 - val_mse: 0.3854 - val_mae: 0.4821\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.2747 - mse: 0.2747 - mae: 0.4071 - val_loss: 0.3793 - val_mse: 0.3793 - val_mae: 0.4844\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.2825 - mse: 0.2825 - mae: 0.4162 - val_loss: 0.3922 - val_mse: 0.3922 - val_mae: 0.4952\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2969 - mse: 0.2969 - mae: 0.4230 - val_loss: 0.3814 - val_mse: 0.3814 - val_mae: 0.4896\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.2826 - mse: 0.2826 - mae: 0.4131 - val_loss: 0.3843 - val_mse: 0.3843 - val_mae: 0.4815\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.2617 - mse: 0.2617 - mae: 0.4001 - val_loss: 0.3867 - val_mse: 0.3867 - val_mae: 0.4857\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.2681 - mse: 0.2681 - mae: 0.3957 - val_loss: 0.3975 - val_mse: 0.3975 - val_mae: 0.4899\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.2843 - mse: 0.2843 - mae: 0.4121 - val_loss: 0.3840 - val_mse: 0.3840 - val_mae: 0.4820\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.2776 - mse: 0.2776 - mae: 0.4038 - val_loss: 0.3862 - val_mse: 0.3862 - val_mae: 0.4918\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.2823 - mse: 0.2823 - mae: 0.4146 - val_loss: 0.3901 - val_mse: 0.3901 - val_mae: 0.4872\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.2834 - mse: 0.2834 - mae: 0.4156 - val_loss: 0.3863 - val_mse: 0.3863 - val_mae: 0.4832\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.2892 - mse: 0.2892 - mae: 0.4184 - val_loss: 0.3805 - val_mse: 0.3805 - val_mae: 0.4808\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2701 - mse: 0.2701 - mae: 0.4059 - val_loss: 0.3873 - val_mse: 0.3873 - val_mae: 0.4752\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2945 - mse: 0.2945 - mae: 0.4219 - val_loss: 0.3912 - val_mse: 0.3912 - val_mae: 0.4942\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2679 - mse: 0.2679 - mae: 0.4037 - val_loss: 0.3829 - val_mse: 0.3829 - val_mae: 0.4850\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2869 - mse: 0.2869 - mae: 0.4166 - val_loss: 0.3865 - val_mse: 0.3865 - val_mae: 0.4944\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2843 - mse: 0.2843 - mae: 0.4119 - val_loss: 0.3811 - val_mse: 0.3811 - val_mae: 0.4826\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2824 - mse: 0.2824 - mae: 0.4114 - val_loss: 0.3983 - val_mse: 0.3983 - val_mae: 0.4964\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2873 - mse: 0.2873 - mae: 0.4123 - val_loss: 0.3918 - val_mse: 0.3918 - val_mae: 0.4964\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2736 - mse: 0.2736 - mae: 0.4134 - val_loss: 0.3998 - val_mse: 0.3998 - val_mae: 0.4832\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2994 - mse: 0.2994 - mae: 0.4221 - val_loss: 0.3852 - val_mse: 0.3852 - val_mae: 0.4854\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2579 - mse: 0.2579 - mae: 0.3940 - val_loss: 0.3838 - val_mse: 0.3838 - val_mae: 0.4782\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2725 - mse: 0.2725 - mae: 0.4048 - val_loss: 0.4006 - val_mse: 0.4006 - val_mae: 0.4964\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2576 - mse: 0.2576 - mae: 0.3951 - val_loss: 0.3837 - val_mse: 0.3837 - val_mae: 0.4845\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2546 - mse: 0.2546 - mae: 0.3887 - val_loss: 0.3826 - val_mse: 0.3826 - val_mae: 0.4885\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2615 - mse: 0.2615 - mae: 0.3916 - val_loss: 0.3939 - val_mse: 0.3939 - val_mae: 0.4851\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2615 - mse: 0.2615 - mae: 0.3915 - val_loss: 0.3944 - val_mse: 0.3944 - val_mae: 0.4870\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2565 - mse: 0.2565 - mae: 0.3885 - val_loss: 0.4082 - val_mse: 0.4082 - val_mae: 0.5084\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2903 - mse: 0.2903 - mae: 0.4192 - val_loss: 0.3920 - val_mse: 0.3920 - val_mae: 0.4894\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2998 - mse: 0.2998 - mae: 0.4269 - val_loss: 0.3940 - val_mse: 0.3940 - val_mae: 0.4907\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2736 - mse: 0.2736 - mae: 0.4095 - val_loss: 0.4014 - val_mse: 0.4014 - val_mae: 0.4986\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2728 - mse: 0.2728 - mae: 0.4012 - val_loss: 0.4091 - val_mse: 0.4091 - val_mae: 0.5123\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2546 - mse: 0.2546 - mae: 0.3941 - val_loss: 0.3901 - val_mse: 0.3901 - val_mae: 0.4872\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2558 - mse: 0.2558 - mae: 0.3988 - val_loss: 0.3983 - val_mse: 0.3983 - val_mae: 0.4884\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2759 - mse: 0.2759 - mae: 0.4155 - val_loss: 0.4102 - val_mse: 0.4102 - val_mae: 0.5002\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2757 - mse: 0.2757 - mae: 0.3964 - val_loss: 0.4147 - val_mse: 0.4147 - val_mae: 0.4982\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2643 - mse: 0.2643 - mae: 0.3966 - val_loss: 0.4054 - val_mse: 0.4054 - val_mae: 0.4868\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2904 - mse: 0.2904 - mae: 0.4284 - val_loss: 0.4390 - val_mse: 0.4390 - val_mae: 0.5158\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2704 - mse: 0.2704 - mae: 0.4072 - val_loss: 0.3968 - val_mse: 0.3968 - val_mae: 0.4889\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2574 - mse: 0.2574 - mae: 0.3881 - val_loss: 0.4058 - val_mse: 0.4058 - val_mae: 0.4926\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2681 - mse: 0.2681 - mae: 0.3957 - val_loss: 0.4192 - val_mse: 0.4192 - val_mae: 0.5096\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2706 - mse: 0.2706 - mae: 0.3992 - val_loss: 0.4048 - val_mse: 0.4048 - val_mae: 0.5022\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.2564 - mse: 0.2564 - mae: 0.3977 - val_loss: 0.4004 - val_mse: 0.4004 - val_mae: 0.4918\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.2692 - mse: 0.2692 - mae: 0.4004 - val_loss: 0.4048 - val_mse: 0.4048 - val_mae: 0.5050\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.2535 - mse: 0.2535 - mae: 0.3860 - val_loss: 0.3937 - val_mse: 0.3937 - val_mae: 0.4907\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.2597 - mse: 0.2597 - mae: 0.3958 - val_loss: 0.3837 - val_mse: 0.3837 - val_mae: 0.4857\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2778 - mse: 0.2778 - mae: 0.4005 - val_loss: 0.3851 - val_mse: 0.3851 - val_mae: 0.4947\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2730 - mse: 0.2730 - mae: 0.3998 - val_loss: 0.4314 - val_mse: 0.4314 - val_mae: 0.5170\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2647 - mse: 0.2647 - mae: 0.4044 - val_loss: 0.3878 - val_mse: 0.3878 - val_mae: 0.4813\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2737 - mse: 0.2737 - mae: 0.3969 - val_loss: 0.3960 - val_mse: 0.3960 - val_mae: 0.4906\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2617 - mse: 0.2617 - mae: 0.4046 - val_loss: 0.3961 - val_mse: 0.3961 - val_mae: 0.4983\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2513 - mse: 0.2513 - mae: 0.3914 - val_loss: 0.3842 - val_mse: 0.3842 - val_mae: 0.4921\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2383 - mse: 0.2383 - mae: 0.3744 - val_loss: 0.3944 - val_mse: 0.3944 - val_mae: 0.4878\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2527 - mse: 0.2527 - mae: 0.3928 - val_loss: 0.3860 - val_mse: 0.3860 - val_mae: 0.4918\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2686 - mse: 0.2686 - mae: 0.4062 - val_loss: 0.3887 - val_mse: 0.3887 - val_mae: 0.4882\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2560 - mse: 0.2560 - mae: 0.3939 - val_loss: 0.4071 - val_mse: 0.4071 - val_mae: 0.5053\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2357 - mse: 0.2357 - mae: 0.3776 - val_loss: 0.3964 - val_mse: 0.3964 - val_mae: 0.5031\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2546 - mse: 0.2546 - mae: 0.3917 - val_loss: 0.3988 - val_mse: 0.3988 - val_mae: 0.4960\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2495 - mse: 0.2495 - mae: 0.3901 - val_loss: 0.4156 - val_mse: 0.4156 - val_mae: 0.5146\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2745 - mse: 0.2745 - mae: 0.4100 - val_loss: 0.3948 - val_mse: 0.3948 - val_mae: 0.4999\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2476 - mse: 0.2476 - mae: 0.3833 - val_loss: 0.4008 - val_mse: 0.4008 - val_mae: 0.5129\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2424 - mse: 0.2424 - mae: 0.3841 - val_loss: 0.4014 - val_mse: 0.4014 - val_mae: 0.4996\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2495 - mse: 0.2495 - mae: 0.3917 - val_loss: 0.3964 - val_mse: 0.3964 - val_mae: 0.4919\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2589 - mse: 0.2589 - mae: 0.3917 - val_loss: 0.3943 - val_mse: 0.3943 - val_mae: 0.5001\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2671 - mse: 0.2671 - mae: 0.3999 - val_loss: 0.4079 - val_mse: 0.4079 - val_mae: 0.5071\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2408 - mse: 0.2408 - mae: 0.3831 - val_loss: 0.4035 - val_mse: 0.4035 - val_mae: 0.4963\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2414 - mse: 0.2414 - mae: 0.3748 - val_loss: 0.4233 - val_mse: 0.4233 - val_mae: 0.5187\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2531 - mse: 0.2531 - mae: 0.3836 - val_loss: 0.4111 - val_mse: 0.4111 - val_mae: 0.5078\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2611 - mse: 0.2611 - mae: 0.3901 - val_loss: 0.4053 - val_mse: 0.4053 - val_mae: 0.5009\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2371 - mse: 0.2371 - mae: 0.3833 - val_loss: 0.4325 - val_mse: 0.4325 - val_mae: 0.5062\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2501 - mse: 0.2501 - mae: 0.3816 - val_loss: 0.4072 - val_mse: 0.4072 - val_mae: 0.5011\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2510 - mse: 0.2510 - mae: 0.3902 - val_loss: 0.3977 - val_mse: 0.3977 - val_mae: 0.4959\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2417 - mse: 0.2417 - mae: 0.3842 - val_loss: 0.3995 - val_mse: 0.3995 - val_mae: 0.5000\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2329 - mse: 0.2329 - mae: 0.3818 - val_loss: 0.3981 - val_mse: 0.3981 - val_mae: 0.4966\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2519 - mse: 0.2519 - mae: 0.3878 - val_loss: 0.3908 - val_mse: 0.3908 - val_mae: 0.4936\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2494 - mse: 0.2494 - mae: 0.3800 - val_loss: 0.4068 - val_mse: 0.4068 - val_mae: 0.4900\n",
            "Epoch 252/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2499 - mse: 0.2499 - mae: 0.3913 - val_loss: 0.4036 - val_mse: 0.4036 - val_mae: 0.4993\n",
            "Epoch 253/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2555 - mse: 0.2555 - mae: 0.3893 - val_loss: 0.3924 - val_mse: 0.3924 - val_mae: 0.4874\n",
            "Epoch 254/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2623 - mse: 0.2623 - mae: 0.3994 - val_loss: 0.3999 - val_mse: 0.3999 - val_mae: 0.4823\n",
            "Epoch 255/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2506 - mse: 0.2506 - mae: 0.3850 - val_loss: 0.4031 - val_mse: 0.4031 - val_mae: 0.4912\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.4995 - mse: 0.4995 - mae: 0.5308\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 20ms/step - loss: 19.9950 - mse: 19.9950 - mae: 4.1914 - val_loss: 11.4016 - val_mse: 11.4016 - val_mae: 3.0584\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 8.2709 - mse: 8.2709 - mae: 2.4948 - val_loss: 5.7016 - val_mse: 5.7016 - val_mae: 1.9798\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5.2555 - mse: 5.2555 - mae: 1.8698 - val_loss: 4.8904 - val_mse: 4.8904 - val_mae: 1.7954\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 4.2277 - mse: 4.2277 - mae: 1.6855 - val_loss: 4.1796 - val_mse: 4.1796 - val_mae: 1.6486\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 3.4337 - mse: 3.4337 - mae: 1.4959 - val_loss: 3.6214 - val_mse: 3.6214 - val_mae: 1.5261\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 3.0508 - mse: 3.0508 - mae: 1.4148 - val_loss: 3.2188 - val_mse: 3.2188 - val_mae: 1.4431\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.6427 - mse: 2.6427 - mae: 1.3034 - val_loss: 2.9257 - val_mse: 2.9257 - val_mae: 1.3790\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.4850 - mse: 2.4850 - mae: 1.2599 - val_loss: 2.7084 - val_mse: 2.7084 - val_mae: 1.3348\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.2246 - mse: 2.2246 - mae: 1.1873 - val_loss: 2.5600 - val_mse: 2.5600 - val_mae: 1.2983\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.9416 - mse: 1.9416 - mae: 1.1138 - val_loss: 2.4243 - val_mse: 2.4243 - val_mae: 1.2623\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.8738 - mse: 1.8738 - mae: 1.1151 - val_loss: 2.3552 - val_mse: 2.3552 - val_mae: 1.2291\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.8697 - mse: 1.8697 - mae: 1.0979 - val_loss: 2.2943 - val_mse: 2.2943 - val_mae: 1.1999\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.7023 - mse: 1.7023 - mae: 1.0267 - val_loss: 2.1863 - val_mse: 2.1863 - val_mae: 1.1650\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.5697 - mse: 1.5697 - mae: 0.9976 - val_loss: 2.0622 - val_mse: 2.0622 - val_mae: 1.1321\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.5327 - mse: 1.5327 - mae: 0.9894 - val_loss: 2.0344 - val_mse: 2.0344 - val_mae: 1.1135\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.5204 - mse: 1.5204 - mae: 0.9848 - val_loss: 1.9472 - val_mse: 1.9472 - val_mae: 1.0913\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.4165 - mse: 1.4165 - mae: 0.9620 - val_loss: 1.8728 - val_mse: 1.8728 - val_mae: 1.0623\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.4401 - mse: 1.4401 - mae: 0.9336 - val_loss: 1.8352 - val_mse: 1.8352 - val_mae: 1.0451\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.3823 - mse: 1.3823 - mae: 0.9285 - val_loss: 1.7722 - val_mse: 1.7722 - val_mae: 1.0272\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.2360 - mse: 1.2360 - mae: 0.8878 - val_loss: 1.7102 - val_mse: 1.7102 - val_mae: 1.0011\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2115 - mse: 1.2115 - mae: 0.8648 - val_loss: 1.6998 - val_mse: 1.6998 - val_mae: 0.9935\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2316 - mse: 1.2316 - mae: 0.8729 - val_loss: 1.6119 - val_mse: 1.6119 - val_mae: 0.9673\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1335 - mse: 1.1335 - mae: 0.8469 - val_loss: 1.5845 - val_mse: 1.5845 - val_mae: 0.9527\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1630 - mse: 1.1630 - mae: 0.8471 - val_loss: 1.5125 - val_mse: 1.5125 - val_mae: 0.9332\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0347 - mse: 1.0347 - mae: 0.8131 - val_loss: 1.5099 - val_mse: 1.5099 - val_mae: 0.9160\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0021 - mse: 1.0021 - mae: 0.7920 - val_loss: 1.4133 - val_mse: 1.4133 - val_mae: 0.9009\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9481 - mse: 0.9481 - mae: 0.7641 - val_loss: 1.3331 - val_mse: 1.3331 - val_mae: 0.8803\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.9144 - mse: 0.9144 - mae: 0.7621 - val_loss: 1.3138 - val_mse: 1.3138 - val_mae: 0.8603\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9571 - mse: 0.9571 - mae: 0.7699 - val_loss: 1.2518 - val_mse: 1.2518 - val_mae: 0.8497\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8369 - mse: 0.8369 - mae: 0.7169 - val_loss: 1.2162 - val_mse: 1.2162 - val_mae: 0.8314\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8305 - mse: 0.8305 - mae: 0.7206 - val_loss: 1.1850 - val_mse: 1.1850 - val_mae: 0.8158\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8559 - mse: 0.8559 - mae: 0.7228 - val_loss: 1.1583 - val_mse: 1.1583 - val_mae: 0.8077\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8018 - mse: 0.8018 - mae: 0.7097 - val_loss: 1.1206 - val_mse: 1.1206 - val_mae: 0.8017\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7914 - mse: 0.7914 - mae: 0.6948 - val_loss: 1.1082 - val_mse: 1.1082 - val_mae: 0.7795\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7976 - mse: 0.7976 - mae: 0.7022 - val_loss: 1.0259 - val_mse: 1.0259 - val_mae: 0.7642\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7465 - mse: 0.7465 - mae: 0.6921 - val_loss: 1.0013 - val_mse: 1.0013 - val_mae: 0.7598\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7553 - mse: 0.7553 - mae: 0.6798 - val_loss: 0.9623 - val_mse: 0.9623 - val_mae: 0.7338\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7191 - mse: 0.7191 - mae: 0.6707 - val_loss: 0.9578 - val_mse: 0.9578 - val_mae: 0.7475\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6623 - mse: 0.6623 - mae: 0.6435 - val_loss: 0.9463 - val_mse: 0.9463 - val_mae: 0.7203\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6793 - mse: 0.6793 - mae: 0.6421 - val_loss: 0.9222 - val_mse: 0.9222 - val_mae: 0.7261\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6356 - mse: 0.6356 - mae: 0.6324 - val_loss: 0.8852 - val_mse: 0.8852 - val_mae: 0.7079\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6385 - mse: 0.6385 - mae: 0.6262 - val_loss: 0.8581 - val_mse: 0.8581 - val_mae: 0.6984\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6088 - mse: 0.6088 - mae: 0.6098 - val_loss: 0.8120 - val_mse: 0.8120 - val_mae: 0.6799\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5783 - mse: 0.5783 - mae: 0.5995 - val_loss: 0.8099 - val_mse: 0.8099 - val_mae: 0.6770\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5875 - mse: 0.5875 - mae: 0.6003 - val_loss: 0.7872 - val_mse: 0.7872 - val_mae: 0.6732\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5795 - mse: 0.5795 - mae: 0.5989 - val_loss: 0.7827 - val_mse: 0.7827 - val_mae: 0.6598\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5538 - mse: 0.5538 - mae: 0.5877 - val_loss: 0.7330 - val_mse: 0.7330 - val_mae: 0.6494\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5702 - mse: 0.5702 - mae: 0.5998 - val_loss: 0.7566 - val_mse: 0.7566 - val_mae: 0.6491\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5507 - mse: 0.5507 - mae: 0.5795 - val_loss: 0.7279 - val_mse: 0.7279 - val_mae: 0.6523\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5102 - mse: 0.5102 - mae: 0.5533 - val_loss: 0.7076 - val_mse: 0.7076 - val_mae: 0.6341\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5038 - mse: 0.5038 - mae: 0.5504 - val_loss: 0.6854 - val_mse: 0.6854 - val_mae: 0.6308\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5465 - mse: 0.5465 - mae: 0.5772 - val_loss: 0.6727 - val_mse: 0.6727 - val_mae: 0.6228\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4896 - mse: 0.4896 - mae: 0.5485 - val_loss: 0.6601 - val_mse: 0.6601 - val_mae: 0.6176\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5055 - mse: 0.5055 - mae: 0.5537 - val_loss: 0.6598 - val_mse: 0.6598 - val_mae: 0.6083\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4991 - mse: 0.4991 - mae: 0.5478 - val_loss: 0.6351 - val_mse: 0.6351 - val_mae: 0.6126\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5153 - mse: 0.5153 - mae: 0.5649 - val_loss: 0.6148 - val_mse: 0.6148 - val_mae: 0.6060\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5085 - mse: 0.5085 - mae: 0.5583 - val_loss: 0.6693 - val_mse: 0.6693 - val_mae: 0.6170\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5230 - mse: 0.5230 - mae: 0.5743 - val_loss: 0.5867 - val_mse: 0.5867 - val_mae: 0.5937\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4614 - mse: 0.4614 - mae: 0.5284 - val_loss: 0.6761 - val_mse: 0.6761 - val_mae: 0.6164\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4624 - mse: 0.4624 - mae: 0.5286 - val_loss: 0.5887 - val_mse: 0.5887 - val_mae: 0.5919\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4594 - mse: 0.4594 - mae: 0.5337 - val_loss: 0.5786 - val_mse: 0.5786 - val_mae: 0.5868\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4463 - mse: 0.4463 - mae: 0.5251 - val_loss: 0.5746 - val_mse: 0.5746 - val_mae: 0.5816\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4480 - mse: 0.4480 - mae: 0.5206 - val_loss: 0.5755 - val_mse: 0.5755 - val_mae: 0.5838\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4553 - mse: 0.4553 - mae: 0.5346 - val_loss: 0.5751 - val_mse: 0.5751 - val_mae: 0.5759\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4476 - mse: 0.4476 - mae: 0.5185 - val_loss: 0.5609 - val_mse: 0.5609 - val_mae: 0.5758\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4438 - mse: 0.4438 - mae: 0.5171 - val_loss: 0.5198 - val_mse: 0.5198 - val_mae: 0.5584\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4234 - mse: 0.4234 - mae: 0.5086 - val_loss: 0.5274 - val_mse: 0.5274 - val_mae: 0.5626\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4051 - mse: 0.4051 - mae: 0.4938 - val_loss: 0.5420 - val_mse: 0.5420 - val_mae: 0.5677\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4182 - mse: 0.4182 - mae: 0.5052 - val_loss: 0.5544 - val_mse: 0.5544 - val_mae: 0.5619\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3813 - mse: 0.3813 - mae: 0.4779 - val_loss: 0.5404 - val_mse: 0.5404 - val_mae: 0.5665\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4203 - mse: 0.4203 - mae: 0.5059 - val_loss: 0.4855 - val_mse: 0.4855 - val_mae: 0.5391\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4154 - mse: 0.4154 - mae: 0.5098 - val_loss: 0.4903 - val_mse: 0.4903 - val_mae: 0.5479\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4245 - mse: 0.4245 - mae: 0.5134 - val_loss: 0.4802 - val_mse: 0.4802 - val_mae: 0.5392\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4127 - mse: 0.4127 - mae: 0.5094 - val_loss: 0.5054 - val_mse: 0.5054 - val_mae: 0.5495\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4072 - mse: 0.4072 - mae: 0.4936 - val_loss: 0.4735 - val_mse: 0.4735 - val_mae: 0.5359\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3872 - mse: 0.3872 - mae: 0.4879 - val_loss: 0.5477 - val_mse: 0.5477 - val_mae: 0.5646\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3963 - mse: 0.3963 - mae: 0.4850 - val_loss: 0.5401 - val_mse: 0.5401 - val_mae: 0.5641\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3908 - mse: 0.3908 - mae: 0.4943 - val_loss: 0.4840 - val_mse: 0.4840 - val_mae: 0.5311\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3829 - mse: 0.3829 - mae: 0.4843 - val_loss: 0.4840 - val_mse: 0.4840 - val_mae: 0.5405\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3858 - mse: 0.3858 - mae: 0.4833 - val_loss: 0.4799 - val_mse: 0.4799 - val_mae: 0.5313\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3952 - mse: 0.3952 - mae: 0.4867 - val_loss: 0.4764 - val_mse: 0.4764 - val_mae: 0.5319\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3683 - mse: 0.3683 - mae: 0.4772 - val_loss: 0.4661 - val_mse: 0.4661 - val_mae: 0.5330\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3912 - mse: 0.3912 - mae: 0.4916 - val_loss: 0.5055 - val_mse: 0.5055 - val_mae: 0.5409\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3840 - mse: 0.3840 - mae: 0.4837 - val_loss: 0.5054 - val_mse: 0.5054 - val_mae: 0.5458\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3766 - mse: 0.3766 - mae: 0.4805 - val_loss: 0.4981 - val_mse: 0.4981 - val_mae: 0.5386\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3934 - mse: 0.3934 - mae: 0.4888 - val_loss: 0.5488 - val_mse: 0.5488 - val_mae: 0.5611\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3827 - mse: 0.3827 - mae: 0.4794 - val_loss: 0.4530 - val_mse: 0.4530 - val_mae: 0.5201\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3770 - mse: 0.3770 - mae: 0.4669 - val_loss: 0.4492 - val_mse: 0.4492 - val_mae: 0.5136\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3705 - mse: 0.3705 - mae: 0.4762 - val_loss: 0.4532 - val_mse: 0.4532 - val_mae: 0.5194\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3722 - mse: 0.3722 - mae: 0.4724 - val_loss: 0.4465 - val_mse: 0.4465 - val_mae: 0.5201\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3672 - mse: 0.3672 - mae: 0.4777 - val_loss: 0.4815 - val_mse: 0.4815 - val_mae: 0.5245\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3487 - mse: 0.3487 - mae: 0.4629 - val_loss: 0.4734 - val_mse: 0.4734 - val_mae: 0.5243\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3661 - mse: 0.3661 - mae: 0.4691 - val_loss: 0.4461 - val_mse: 0.4461 - val_mae: 0.5139\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.3720 - mse: 0.3720 - mae: 0.4729 - val_loss: 0.4323 - val_mse: 0.4323 - val_mae: 0.5064\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3380 - mse: 0.3380 - mae: 0.4554 - val_loss: 0.5130 - val_mse: 0.5130 - val_mae: 0.5416\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3909 - mse: 0.3909 - mae: 0.4867 - val_loss: 0.4666 - val_mse: 0.4666 - val_mae: 0.5229\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3493 - mse: 0.3493 - mae: 0.4683 - val_loss: 0.4350 - val_mse: 0.4350 - val_mae: 0.5070\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3564 - mse: 0.3564 - mae: 0.4628 - val_loss: 0.4500 - val_mse: 0.4500 - val_mae: 0.5187\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3397 - mse: 0.3397 - mae: 0.4454 - val_loss: 0.4578 - val_mse: 0.4578 - val_mae: 0.5197\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3434 - mse: 0.3434 - mae: 0.4603 - val_loss: 0.4571 - val_mse: 0.4571 - val_mae: 0.5197\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3631 - mse: 0.3631 - mae: 0.4733 - val_loss: 0.4156 - val_mse: 0.4156 - val_mae: 0.4942\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3767 - mse: 0.3767 - mae: 0.4769 - val_loss: 0.4221 - val_mse: 0.4221 - val_mae: 0.5116\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3562 - mse: 0.3562 - mae: 0.4601 - val_loss: 0.4687 - val_mse: 0.4687 - val_mae: 0.5262\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3526 - mse: 0.3526 - mae: 0.4559 - val_loss: 0.4254 - val_mse: 0.4254 - val_mae: 0.5021\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3709 - mse: 0.3709 - mae: 0.4687 - val_loss: 0.4081 - val_mse: 0.4081 - val_mae: 0.5090\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3496 - mse: 0.3496 - mae: 0.4580 - val_loss: 0.3927 - val_mse: 0.3927 - val_mae: 0.4916\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3543 - mse: 0.3543 - mae: 0.4675 - val_loss: 0.4113 - val_mse: 0.4113 - val_mae: 0.5021\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3479 - mse: 0.3479 - mae: 0.4585 - val_loss: 0.4237 - val_mse: 0.4237 - val_mae: 0.5025\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3480 - mse: 0.3480 - mae: 0.4588 - val_loss: 0.4752 - val_mse: 0.4752 - val_mae: 0.5335\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3529 - mse: 0.3529 - mae: 0.4656 - val_loss: 0.4054 - val_mse: 0.4054 - val_mae: 0.4915\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3408 - mse: 0.3408 - mae: 0.4545 - val_loss: 0.4364 - val_mse: 0.4364 - val_mae: 0.5160\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3545 - mse: 0.3545 - mae: 0.4630 - val_loss: 0.3856 - val_mse: 0.3856 - val_mae: 0.4883\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3363 - mse: 0.3363 - mae: 0.4470 - val_loss: 0.4047 - val_mse: 0.4047 - val_mae: 0.4931\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3418 - mse: 0.3418 - mae: 0.4521 - val_loss: 0.4175 - val_mse: 0.4175 - val_mae: 0.4989\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3234 - mse: 0.3234 - mae: 0.4423 - val_loss: 0.3986 - val_mse: 0.3986 - val_mae: 0.4870\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3270 - mse: 0.3270 - mae: 0.4447 - val_loss: 0.3848 - val_mse: 0.3848 - val_mae: 0.4854\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3401 - mse: 0.3401 - mae: 0.4550 - val_loss: 0.3844 - val_mse: 0.3844 - val_mae: 0.4925\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3415 - mse: 0.3415 - mae: 0.4587 - val_loss: 0.3962 - val_mse: 0.3962 - val_mae: 0.4912\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3352 - mse: 0.3352 - mae: 0.4475 - val_loss: 0.3970 - val_mse: 0.3970 - val_mae: 0.4875\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3438 - mse: 0.3438 - mae: 0.4482 - val_loss: 0.3992 - val_mse: 0.3992 - val_mae: 0.4922\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3190 - mse: 0.3190 - mae: 0.4402 - val_loss: 0.3973 - val_mse: 0.3973 - val_mae: 0.4895\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3121 - mse: 0.3121 - mae: 0.4350 - val_loss: 0.3932 - val_mse: 0.3932 - val_mae: 0.4846\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3571 - mse: 0.3571 - mae: 0.4659 - val_loss: 0.4048 - val_mse: 0.4048 - val_mae: 0.4924\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3581 - mse: 0.3581 - mae: 0.4704 - val_loss: 0.4273 - val_mse: 0.4273 - val_mae: 0.5047\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3298 - mse: 0.3298 - mae: 0.4528 - val_loss: 0.3739 - val_mse: 0.3739 - val_mae: 0.4800\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3408 - mse: 0.3408 - mae: 0.4582 - val_loss: 0.3967 - val_mse: 0.3967 - val_mae: 0.4937\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3213 - mse: 0.3213 - mae: 0.4355 - val_loss: 0.4025 - val_mse: 0.4025 - val_mae: 0.4893\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3211 - mse: 0.3211 - mae: 0.4385 - val_loss: 0.3826 - val_mse: 0.3826 - val_mae: 0.4797\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3133 - mse: 0.3133 - mae: 0.4326 - val_loss: 0.3974 - val_mse: 0.3974 - val_mae: 0.4961\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3274 - mse: 0.3274 - mae: 0.4450 - val_loss: 0.4023 - val_mse: 0.4023 - val_mae: 0.4882\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3234 - mse: 0.3234 - mae: 0.4387 - val_loss: 0.3858 - val_mse: 0.3858 - val_mae: 0.4816\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3234 - mse: 0.3234 - mae: 0.4350 - val_loss: 0.3831 - val_mse: 0.3831 - val_mae: 0.4856\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2853 - mse: 0.2853 - mae: 0.4139 - val_loss: 0.3839 - val_mse: 0.3839 - val_mae: 0.4797\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3422 - mse: 0.3422 - mae: 0.4506 - val_loss: 0.3846 - val_mse: 0.3846 - val_mae: 0.4818\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3196 - mse: 0.3196 - mae: 0.4370 - val_loss: 0.3764 - val_mse: 0.3764 - val_mae: 0.4741\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3203 - mse: 0.3203 - mae: 0.4376 - val_loss: 0.3918 - val_mse: 0.3918 - val_mae: 0.4868\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3209 - mse: 0.3209 - mae: 0.4391 - val_loss: 0.3785 - val_mse: 0.3785 - val_mae: 0.4842\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3154 - mse: 0.3154 - mae: 0.4377 - val_loss: 0.3778 - val_mse: 0.3778 - val_mae: 0.4850\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3211 - mse: 0.3211 - mae: 0.4330 - val_loss: 0.3930 - val_mse: 0.3930 - val_mae: 0.4843\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3066 - mse: 0.3066 - mae: 0.4252 - val_loss: 0.3838 - val_mse: 0.3838 - val_mae: 0.4825\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2971 - mse: 0.2971 - mae: 0.4172 - val_loss: 0.3666 - val_mse: 0.3666 - val_mae: 0.4709\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3246 - mse: 0.3246 - mae: 0.4415 - val_loss: 0.3842 - val_mse: 0.3842 - val_mae: 0.4817\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3257 - mse: 0.3257 - mae: 0.4459 - val_loss: 0.3714 - val_mse: 0.3714 - val_mae: 0.4717\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3121 - mse: 0.3121 - mae: 0.4343 - val_loss: 0.3865 - val_mse: 0.3865 - val_mae: 0.4811\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3149 - mse: 0.3149 - mae: 0.4363 - val_loss: 0.3950 - val_mse: 0.3950 - val_mae: 0.4856\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3097 - mse: 0.3097 - mae: 0.4291 - val_loss: 0.4249 - val_mse: 0.4249 - val_mae: 0.4981\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3009 - mse: 0.3009 - mae: 0.4197 - val_loss: 0.4076 - val_mse: 0.4076 - val_mae: 0.4929\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2917 - mse: 0.2917 - mae: 0.4135 - val_loss: 0.3942 - val_mse: 0.3942 - val_mae: 0.4846\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3083 - mse: 0.3083 - mae: 0.4260 - val_loss: 0.3960 - val_mse: 0.3960 - val_mae: 0.4865\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3078 - mse: 0.3078 - mae: 0.4307 - val_loss: 0.3866 - val_mse: 0.3866 - val_mae: 0.4847\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3069 - mse: 0.3069 - mae: 0.4333 - val_loss: 0.3792 - val_mse: 0.3792 - val_mae: 0.4787\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3041 - mse: 0.3041 - mae: 0.4175 - val_loss: 0.3879 - val_mse: 0.3879 - val_mae: 0.4860\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3053 - mse: 0.3053 - mae: 0.4294 - val_loss: 0.4216 - val_mse: 0.4216 - val_mae: 0.5029\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3100 - mse: 0.3100 - mae: 0.4321 - val_loss: 0.3926 - val_mse: 0.3926 - val_mae: 0.4876\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2950 - mse: 0.2950 - mae: 0.4164 - val_loss: 0.3773 - val_mse: 0.3773 - val_mae: 0.4829\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3023 - mse: 0.3023 - mae: 0.4165 - val_loss: 0.3752 - val_mse: 0.3752 - val_mae: 0.4783\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2958 - mse: 0.2958 - mae: 0.4218 - val_loss: 0.3718 - val_mse: 0.3718 - val_mae: 0.4736\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3026 - mse: 0.3026 - mae: 0.4264 - val_loss: 0.3928 - val_mse: 0.3928 - val_mae: 0.4884\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3152 - mse: 0.3152 - mae: 0.4297 - val_loss: 0.3871 - val_mse: 0.3871 - val_mae: 0.4798\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3006 - mse: 0.3006 - mae: 0.4230 - val_loss: 0.3689 - val_mse: 0.3689 - val_mae: 0.4767\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3204 - mse: 0.3204 - mae: 0.4432 - val_loss: 0.3977 - val_mse: 0.3977 - val_mae: 0.4896\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2856 - mse: 0.2856 - mae: 0.4148 - val_loss: 0.3909 - val_mse: 0.3909 - val_mae: 0.4816\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2789 - mse: 0.2789 - mae: 0.4076 - val_loss: 0.3878 - val_mse: 0.3878 - val_mae: 0.4818\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2943 - mse: 0.2943 - mae: 0.4161 - val_loss: 0.3876 - val_mse: 0.3876 - val_mae: 0.4841\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2838 - mse: 0.2838 - mae: 0.4136 - val_loss: 0.3845 - val_mse: 0.3845 - val_mae: 0.4784\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2841 - mse: 0.2841 - mae: 0.4119 - val_loss: 0.3666 - val_mse: 0.3666 - val_mae: 0.4770\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3013 - mse: 0.3013 - mae: 0.4249 - val_loss: 0.3866 - val_mse: 0.3866 - val_mae: 0.4893\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3093 - mse: 0.3093 - mae: 0.4253 - val_loss: 0.3628 - val_mse: 0.3628 - val_mae: 0.4730\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2884 - mse: 0.2884 - mae: 0.4191 - val_loss: 0.3878 - val_mse: 0.3878 - val_mae: 0.4887\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2953 - mse: 0.2953 - mae: 0.4237 - val_loss: 0.3856 - val_mse: 0.3856 - val_mae: 0.4779\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2937 - mse: 0.2937 - mae: 0.4232 - val_loss: 0.3732 - val_mse: 0.3732 - val_mae: 0.4793\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2792 - mse: 0.2792 - mae: 0.4064 - val_loss: 0.3741 - val_mse: 0.3741 - val_mae: 0.4756\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3034 - mse: 0.3034 - mae: 0.4284 - val_loss: 0.3795 - val_mse: 0.3795 - val_mae: 0.4829\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2986 - mse: 0.2986 - mae: 0.4190 - val_loss: 0.3811 - val_mse: 0.3811 - val_mae: 0.4808\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2939 - mse: 0.2939 - mae: 0.4111 - val_loss: 0.4053 - val_mse: 0.4053 - val_mae: 0.4860\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3040 - mse: 0.3040 - mae: 0.4222 - val_loss: 0.3962 - val_mse: 0.3962 - val_mae: 0.4865\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2903 - mse: 0.2903 - mae: 0.4167 - val_loss: 0.3867 - val_mse: 0.3867 - val_mae: 0.4823\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2886 - mse: 0.2886 - mae: 0.4102 - val_loss: 0.3862 - val_mse: 0.3862 - val_mae: 0.4834\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2932 - mse: 0.2932 - mae: 0.4199 - val_loss: 0.3987 - val_mse: 0.3987 - val_mae: 0.4839\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2983 - mse: 0.2983 - mae: 0.4164 - val_loss: 0.4015 - val_mse: 0.4015 - val_mae: 0.4895\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2799 - mse: 0.2799 - mae: 0.4120 - val_loss: 0.3895 - val_mse: 0.3895 - val_mae: 0.4807\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2799 - mse: 0.2799 - mae: 0.4011 - val_loss: 0.3828 - val_mse: 0.3828 - val_mae: 0.4815\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3075 - mse: 0.3075 - mae: 0.4272 - val_loss: 0.4303 - val_mse: 0.4303 - val_mae: 0.4970\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3110 - mse: 0.3110 - mae: 0.4352 - val_loss: 0.4083 - val_mse: 0.4083 - val_mae: 0.4900\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2930 - mse: 0.2930 - mae: 0.4200 - val_loss: 0.3927 - val_mse: 0.3927 - val_mae: 0.4787\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2724 - mse: 0.2724 - mae: 0.4016 - val_loss: 0.3672 - val_mse: 0.3672 - val_mae: 0.4732\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2830 - mse: 0.2830 - mae: 0.4096 - val_loss: 0.3840 - val_mse: 0.3840 - val_mae: 0.4862\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2963 - mse: 0.2963 - mae: 0.4150 - val_loss: 0.3748 - val_mse: 0.3748 - val_mae: 0.4762\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2923 - mse: 0.2923 - mae: 0.4191 - val_loss: 0.3870 - val_mse: 0.3870 - val_mae: 0.4789\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2633 - mse: 0.2633 - mae: 0.3897 - val_loss: 0.3977 - val_mse: 0.3977 - val_mae: 0.4883\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2837 - mse: 0.2837 - mae: 0.4074 - val_loss: 0.3909 - val_mse: 0.3909 - val_mae: 0.4840\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2669 - mse: 0.2669 - mae: 0.3998 - val_loss: 0.3856 - val_mse: 0.3856 - val_mae: 0.4763\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2792 - mse: 0.2792 - mae: 0.4060 - val_loss: 0.4375 - val_mse: 0.4375 - val_mae: 0.4974\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2746 - mse: 0.2746 - mae: 0.4032 - val_loss: 0.4301 - val_mse: 0.4301 - val_mae: 0.4945\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2868 - mse: 0.2868 - mae: 0.4113 - val_loss: 0.4082 - val_mse: 0.4082 - val_mae: 0.4888\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2934 - mse: 0.2934 - mae: 0.4185 - val_loss: 0.4114 - val_mse: 0.4114 - val_mae: 0.4859\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2860 - mse: 0.2860 - mae: 0.4108 - val_loss: 0.4033 - val_mse: 0.4033 - val_mae: 0.4850\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2887 - mse: 0.2887 - mae: 0.4182 - val_loss: 0.4995 - val_mse: 0.4995 - val_mae: 0.5346\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2969 - mse: 0.2969 - mae: 0.4167 - val_loss: 0.4258 - val_mse: 0.4258 - val_mae: 0.4976\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2850 - mse: 0.2850 - mae: 0.4167 - val_loss: 0.4409 - val_mse: 0.4409 - val_mae: 0.5029\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2784 - mse: 0.2784 - mae: 0.4106 - val_loss: 0.3980 - val_mse: 0.3980 - val_mae: 0.4807\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2747 - mse: 0.2747 - mae: 0.4029 - val_loss: 0.3949 - val_mse: 0.3949 - val_mae: 0.4875\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2789 - mse: 0.2789 - mae: 0.4110 - val_loss: 0.3793 - val_mse: 0.3793 - val_mae: 0.4697\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2709 - mse: 0.2709 - mae: 0.4018 - val_loss: 0.4119 - val_mse: 0.4119 - val_mae: 0.4929\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2636 - mse: 0.2636 - mae: 0.4030 - val_loss: 0.3877 - val_mse: 0.3877 - val_mae: 0.4784\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2894 - mse: 0.2894 - mae: 0.4183 - val_loss: 0.3986 - val_mse: 0.3986 - val_mae: 0.4818\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2726 - mse: 0.2726 - mae: 0.4086 - val_loss: 0.3945 - val_mse: 0.3945 - val_mae: 0.4765\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2763 - mse: 0.2763 - mae: 0.4066 - val_loss: 0.3738 - val_mse: 0.3738 - val_mae: 0.4700\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2737 - mse: 0.2737 - mae: 0.3979 - val_loss: 0.3651 - val_mse: 0.3651 - val_mae: 0.4707\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2869 - mse: 0.2869 - mae: 0.4145 - val_loss: 0.3916 - val_mse: 0.3916 - val_mae: 0.4843\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2753 - mse: 0.2753 - mae: 0.4061 - val_loss: 0.3807 - val_mse: 0.3807 - val_mae: 0.4773\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2610 - mse: 0.2610 - mae: 0.3944 - val_loss: 0.3813 - val_mse: 0.3813 - val_mae: 0.4716\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2661 - mse: 0.2661 - mae: 0.3957 - val_loss: 0.4212 - val_mse: 0.4212 - val_mae: 0.4931\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2591 - mse: 0.2591 - mae: 0.3862 - val_loss: 0.3767 - val_mse: 0.3767 - val_mae: 0.4712\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2779 - mse: 0.2779 - mae: 0.4078 - val_loss: 0.4024 - val_mse: 0.4024 - val_mae: 0.4771\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2735 - mse: 0.2735 - mae: 0.4042 - val_loss: 0.3984 - val_mse: 0.3984 - val_mae: 0.4906\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2940 - mse: 0.2940 - mae: 0.4266 - val_loss: 0.3799 - val_mse: 0.3799 - val_mae: 0.4832\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2630 - mse: 0.2630 - mae: 0.3914 - val_loss: 0.3809 - val_mse: 0.3809 - val_mae: 0.4830\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2922 - mse: 0.2922 - mae: 0.4162 - val_loss: 0.3900 - val_mse: 0.3900 - val_mae: 0.4861\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2797 - mse: 0.2797 - mae: 0.4127 - val_loss: 0.3784 - val_mse: 0.3784 - val_mae: 0.4774\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2790 - mse: 0.2790 - mae: 0.4069 - val_loss: 0.3798 - val_mse: 0.3798 - val_mae: 0.4747\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3073 - mse: 0.3073 - mae: 0.4295 - val_loss: 0.3973 - val_mse: 0.3973 - val_mae: 0.4904\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2525 - mse: 0.2525 - mae: 0.3825 - val_loss: 0.3968 - val_mse: 0.3968 - val_mae: 0.4765\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2589 - mse: 0.2589 - mae: 0.3930 - val_loss: 0.3871 - val_mse: 0.3871 - val_mae: 0.4781\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2626 - mse: 0.2626 - mae: 0.3870 - val_loss: 0.3903 - val_mse: 0.3903 - val_mae: 0.4775\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2594 - mse: 0.2594 - mae: 0.3860 - val_loss: 0.4133 - val_mse: 0.4133 - val_mae: 0.5094\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2798 - mse: 0.2798 - mae: 0.4176 - val_loss: 0.3743 - val_mse: 0.3743 - val_mae: 0.4731\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2994 - mse: 0.2994 - mae: 0.4167 - val_loss: 0.3748 - val_mse: 0.3748 - val_mae: 0.4681\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2904 - mse: 0.2904 - mae: 0.4191 - val_loss: 0.4365 - val_mse: 0.4365 - val_mae: 0.5025\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2727 - mse: 0.2727 - mae: 0.4019 - val_loss: 0.3947 - val_mse: 0.3947 - val_mae: 0.4817\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2624 - mse: 0.2624 - mae: 0.3945 - val_loss: 0.3970 - val_mse: 0.3970 - val_mae: 0.4812\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2796 - mse: 0.2796 - mae: 0.4087 - val_loss: 0.3827 - val_mse: 0.3827 - val_mae: 0.4733\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2478 - mse: 0.2478 - mae: 0.3852 - val_loss: 0.3877 - val_mse: 0.3877 - val_mae: 0.4729\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2645 - mse: 0.2645 - mae: 0.3962 - val_loss: 0.3856 - val_mse: 0.3856 - val_mae: 0.4782\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2648 - mse: 0.2648 - mae: 0.3912 - val_loss: 0.3948 - val_mse: 0.3948 - val_mae: 0.4747\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2783 - mse: 0.2783 - mae: 0.4067 - val_loss: 0.3975 - val_mse: 0.3975 - val_mae: 0.4912\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2602 - mse: 0.2602 - mae: 0.3918 - val_loss: 0.3865 - val_mse: 0.3865 - val_mae: 0.4753\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2679 - mse: 0.2679 - mae: 0.3985 - val_loss: 0.3823 - val_mse: 0.3823 - val_mae: 0.4763\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2645 - mse: 0.2645 - mae: 0.3870 - val_loss: 0.3937 - val_mse: 0.3937 - val_mae: 0.4876\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2564 - mse: 0.2564 - mae: 0.3845 - val_loss: 0.3869 - val_mse: 0.3869 - val_mae: 0.4771\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2580 - mse: 0.2580 - mae: 0.3858 - val_loss: 0.3807 - val_mse: 0.3807 - val_mae: 0.4738\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2493 - mse: 0.2493 - mae: 0.3866 - val_loss: 0.4031 - val_mse: 0.4031 - val_mae: 0.4841\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2610 - mse: 0.2610 - mae: 0.4024 - val_loss: 0.4119 - val_mse: 0.4119 - val_mae: 0.4846\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2506 - mse: 0.2506 - mae: 0.3865 - val_loss: 0.3961 - val_mse: 0.3961 - val_mae: 0.4838\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2622 - mse: 0.2622 - mae: 0.3984 - val_loss: 0.4105 - val_mse: 0.4105 - val_mae: 0.4851\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2607 - mse: 0.2607 - mae: 0.3943 - val_loss: 0.4330 - val_mse: 0.4330 - val_mae: 0.5011\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2415 - mse: 0.2415 - mae: 0.3807 - val_loss: 0.4404 - val_mse: 0.4404 - val_mae: 0.5051\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2496 - mse: 0.2496 - mae: 0.3865 - val_loss: 0.3742 - val_mse: 0.3742 - val_mae: 0.4661\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2395 - mse: 0.2395 - mae: 0.3787 - val_loss: 0.4058 - val_mse: 0.4058 - val_mae: 0.4796\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2561 - mse: 0.2561 - mae: 0.3969 - val_loss: 0.4234 - val_mse: 0.4234 - val_mae: 0.4916\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2693 - mse: 0.2693 - mae: 0.3952 - val_loss: 0.3884 - val_mse: 0.3884 - val_mae: 0.4696\n",
            "Epoch 252/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2667 - mse: 0.2667 - mae: 0.4008 - val_loss: 0.4000 - val_mse: 0.4000 - val_mae: 0.4881\n",
            "Epoch 253/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2558 - mse: 0.2558 - mae: 0.3829 - val_loss: 0.4194 - val_mse: 0.4194 - val_mae: 0.4946\n",
            "Epoch 254/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2532 - mse: 0.2532 - mae: 0.3821 - val_loss: 0.4068 - val_mse: 0.4068 - val_mae: 0.4832\n",
            "Epoch 255/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2573 - mse: 0.2573 - mae: 0.3870 - val_loss: 0.4459 - val_mse: 0.4459 - val_mae: 0.5086\n",
            "Epoch 256/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2682 - mse: 0.2682 - mae: 0.4040 - val_loss: 0.3806 - val_mse: 0.3806 - val_mae: 0.4697\n",
            "Epoch 257/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2611 - mse: 0.2611 - mae: 0.3976 - val_loss: 0.3973 - val_mse: 0.3973 - val_mae: 0.4892\n",
            "Epoch 258/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2587 - mse: 0.2587 - mae: 0.3905 - val_loss: 0.4415 - val_mse: 0.4415 - val_mae: 0.5013\n",
            "Epoch 259/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2559 - mse: 0.2559 - mae: 0.3936 - val_loss: 0.3884 - val_mse: 0.3884 - val_mae: 0.4778\n",
            "Epoch 260/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2576 - mse: 0.2576 - mae: 0.3910 - val_loss: 0.4044 - val_mse: 0.4044 - val_mae: 0.4901\n",
            "Epoch 261/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2434 - mse: 0.2434 - mae: 0.3832 - val_loss: 0.3936 - val_mse: 0.3936 - val_mae: 0.4796\n",
            "Epoch 262/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2444 - mse: 0.2444 - mae: 0.3749 - val_loss: 0.3908 - val_mse: 0.3908 - val_mae: 0.4775\n",
            "Epoch 263/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2415 - mse: 0.2415 - mae: 0.3770 - val_loss: 0.4057 - val_mse: 0.4057 - val_mae: 0.4935\n",
            "Epoch 264/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2409 - mse: 0.2409 - mae: 0.3757 - val_loss: 0.4302 - val_mse: 0.4302 - val_mae: 0.4973\n",
            "Epoch 265/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2540 - mse: 0.2540 - mae: 0.3893 - val_loss: 0.4286 - val_mse: 0.4286 - val_mae: 0.5023\n",
            "Epoch 266/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2415 - mse: 0.2415 - mae: 0.3755 - val_loss: 0.4148 - val_mse: 0.4148 - val_mae: 0.4928\n",
            "Epoch 267/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2650 - mse: 0.2650 - mae: 0.3979 - val_loss: 0.4763 - val_mse: 0.4763 - val_mae: 0.5266\n",
            "Epoch 268/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2959 - mse: 0.2959 - mae: 0.4226 - val_loss: 0.4580 - val_mse: 0.4580 - val_mae: 0.5142\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.6271 - mse: 0.6271 - mae: 0.5830\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 16ms/step - loss: 32.6098 - mse: 32.6098 - mae: 5.5210 - val_loss: 21.4083 - val_mse: 21.4083 - val_mae: 4.3582\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 13.8473 - mse: 13.8473 - mae: 3.3760 - val_loss: 9.7873 - val_mse: 9.7873 - val_mae: 2.7921\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 6.1941 - mse: 6.1941 - mae: 2.0841 - val_loss: 5.9044 - val_mse: 5.9044 - val_mae: 1.9371\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 4.0281 - mse: 4.0281 - mae: 1.6398 - val_loss: 3.9389 - val_mse: 3.9389 - val_mae: 1.5705\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.9387 - mse: 2.9387 - mae: 1.3861 - val_loss: 3.0404 - val_mse: 3.0404 - val_mae: 1.3742\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.6201 - mse: 2.6201 - mae: 1.3137 - val_loss: 2.5499 - val_mse: 2.5499 - val_mae: 1.2554\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.2324 - mse: 2.2324 - mae: 1.1901 - val_loss: 2.2646 - val_mse: 2.2646 - val_mae: 1.1746\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.1394 - mse: 2.1394 - mae: 1.1703 - val_loss: 2.0692 - val_mse: 2.0692 - val_mae: 1.1272\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.9100 - mse: 1.9100 - mae: 1.1008 - val_loss: 1.9518 - val_mse: 1.9518 - val_mae: 1.0905\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.9601 - mse: 1.9601 - mae: 1.1253 - val_loss: 1.8318 - val_mse: 1.8318 - val_mae: 1.0622\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.6775 - mse: 1.6775 - mae: 1.0454 - val_loss: 1.8298 - val_mse: 1.8298 - val_mae: 1.0607\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.7420 - mse: 1.7420 - mae: 1.0575 - val_loss: 1.7124 - val_mse: 1.7124 - val_mae: 1.0285\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.6190 - mse: 1.6190 - mae: 1.0042 - val_loss: 1.5952 - val_mse: 1.5952 - val_mae: 0.9967\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.5476 - mse: 1.5476 - mae: 0.9835 - val_loss: 1.6149 - val_mse: 1.6149 - val_mae: 0.9956\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.6230 - mse: 1.6230 - mae: 1.0039 - val_loss: 1.5301 - val_mse: 1.5301 - val_mae: 0.9724\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.4863 - mse: 1.4863 - mae: 0.9642 - val_loss: 1.4999 - val_mse: 1.4999 - val_mae: 0.9606\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.4912 - mse: 1.4912 - mae: 0.9745 - val_loss: 1.4928 - val_mse: 1.4928 - val_mae: 0.9521\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.4426 - mse: 1.4426 - mae: 0.9479 - val_loss: 1.3814 - val_mse: 1.3814 - val_mae: 0.9243\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.3603 - mse: 1.3603 - mae: 0.9266 - val_loss: 1.3005 - val_mse: 1.3005 - val_mae: 0.8975\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.3868 - mse: 1.3868 - mae: 0.9241 - val_loss: 1.2667 - val_mse: 1.2667 - val_mae: 0.8828\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.3152 - mse: 1.3152 - mae: 0.8937 - val_loss: 1.2801 - val_mse: 1.2801 - val_mae: 0.8895\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.2941 - mse: 1.2941 - mae: 0.9008 - val_loss: 1.2316 - val_mse: 1.2316 - val_mae: 0.8640\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2721 - mse: 1.2721 - mae: 0.8843 - val_loss: 1.1658 - val_mse: 1.1658 - val_mae: 0.8481\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2137 - mse: 1.2137 - mae: 0.8665 - val_loss: 1.1365 - val_mse: 1.1365 - val_mae: 0.8241\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.2391 - mse: 1.2391 - mae: 0.8773 - val_loss: 1.1005 - val_mse: 1.1005 - val_mae: 0.8214\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1125 - mse: 1.1125 - mae: 0.8376 - val_loss: 1.1306 - val_mse: 1.1306 - val_mae: 0.8198\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1032 - mse: 1.1032 - mae: 0.8313 - val_loss: 1.0333 - val_mse: 1.0333 - val_mae: 0.7892\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0563 - mse: 1.0563 - mae: 0.8070 - val_loss: 1.0288 - val_mse: 1.0288 - val_mae: 0.7798\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0419 - mse: 1.0419 - mae: 0.8173 - val_loss: 1.0157 - val_mse: 1.0157 - val_mae: 0.7716\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9207 - mse: 0.9207 - mae: 0.7665 - val_loss: 0.9644 - val_mse: 0.9644 - val_mae: 0.7514\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.9558 - mse: 0.9558 - mae: 0.7708 - val_loss: 0.9448 - val_mse: 0.9448 - val_mae: 0.7519\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9652 - mse: 0.9652 - mae: 0.7762 - val_loss: 0.9554 - val_mse: 0.9554 - val_mae: 0.7378\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.9669 - mse: 0.9669 - mae: 0.7670 - val_loss: 0.8960 - val_mse: 0.8960 - val_mae: 0.7297\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.9026 - mse: 0.9026 - mae: 0.7465 - val_loss: 0.8815 - val_mse: 0.8815 - val_mae: 0.7205\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8324 - mse: 0.8324 - mae: 0.7206 - val_loss: 0.8561 - val_mse: 0.8561 - val_mae: 0.7039\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8632 - mse: 0.8632 - mae: 0.7400 - val_loss: 0.8624 - val_mse: 0.8624 - val_mae: 0.7184\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8274 - mse: 0.8274 - mae: 0.7021 - val_loss: 0.8118 - val_mse: 0.8118 - val_mae: 0.6823\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7842 - mse: 0.7842 - mae: 0.7027 - val_loss: 0.8052 - val_mse: 0.8052 - val_mae: 0.6890\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7651 - mse: 0.7651 - mae: 0.6813 - val_loss: 0.7826 - val_mse: 0.7826 - val_mae: 0.6773\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8046 - mse: 0.8046 - mae: 0.7114 - val_loss: 0.7720 - val_mse: 0.7720 - val_mae: 0.6738\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7730 - mse: 0.7730 - mae: 0.6950 - val_loss: 0.7954 - val_mse: 0.7954 - val_mae: 0.6696\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7305 - mse: 0.7305 - mae: 0.6681 - val_loss: 0.7313 - val_mse: 0.7313 - val_mae: 0.6533\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7126 - mse: 0.7126 - mae: 0.6605 - val_loss: 0.7376 - val_mse: 0.7376 - val_mae: 0.6482\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7508 - mse: 0.7508 - mae: 0.6842 - val_loss: 0.7436 - val_mse: 0.7436 - val_mae: 0.6635\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7542 - mse: 0.7542 - mae: 0.6891 - val_loss: 0.6833 - val_mse: 0.6833 - val_mae: 0.6271\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6967 - mse: 0.6967 - mae: 0.6614 - val_loss: 0.6930 - val_mse: 0.6930 - val_mae: 0.6339\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6660 - mse: 0.6660 - mae: 0.6359 - val_loss: 0.6696 - val_mse: 0.6696 - val_mae: 0.6244\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6534 - mse: 0.6534 - mae: 0.6335 - val_loss: 0.6561 - val_mse: 0.6561 - val_mae: 0.6158\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6631 - mse: 0.6631 - mae: 0.6299 - val_loss: 0.6291 - val_mse: 0.6291 - val_mae: 0.6004\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6071 - mse: 0.6071 - mae: 0.6147 - val_loss: 0.6378 - val_mse: 0.6378 - val_mae: 0.6055\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6299 - mse: 0.6299 - mae: 0.6247 - val_loss: 0.6351 - val_mse: 0.6351 - val_mae: 0.6066\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6231 - mse: 0.6231 - mae: 0.6266 - val_loss: 0.6421 - val_mse: 0.6421 - val_mae: 0.6093\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6357 - mse: 0.6357 - mae: 0.6183 - val_loss: 0.6187 - val_mse: 0.6187 - val_mae: 0.6002\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5768 - mse: 0.5768 - mae: 0.5927 - val_loss: 0.6119 - val_mse: 0.6119 - val_mae: 0.5954\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6182 - mse: 0.6182 - mae: 0.6151 - val_loss: 0.5760 - val_mse: 0.5760 - val_mae: 0.5759\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6064 - mse: 0.6064 - mae: 0.6050 - val_loss: 0.5845 - val_mse: 0.5845 - val_mae: 0.5835\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.5466 - mse: 0.5466 - mae: 0.5781 - val_loss: 0.5829 - val_mse: 0.5829 - val_mae: 0.5813\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5561 - mse: 0.5561 - mae: 0.5847 - val_loss: 0.5612 - val_mse: 0.5612 - val_mae: 0.5711\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5434 - mse: 0.5434 - mae: 0.5846 - val_loss: 0.6101 - val_mse: 0.6101 - val_mae: 0.5924\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5200 - mse: 0.5200 - mae: 0.5585 - val_loss: 0.5300 - val_mse: 0.5300 - val_mae: 0.5504\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5331 - mse: 0.5331 - mae: 0.5785 - val_loss: 0.5507 - val_mse: 0.5507 - val_mae: 0.5627\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5547 - mse: 0.5547 - mae: 0.5765 - val_loss: 0.5491 - val_mse: 0.5491 - val_mae: 0.5635\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5430 - mse: 0.5430 - mae: 0.5801 - val_loss: 0.5354 - val_mse: 0.5354 - val_mae: 0.5509\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5040 - mse: 0.5040 - mae: 0.5585 - val_loss: 0.5516 - val_mse: 0.5516 - val_mae: 0.5663\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5100 - mse: 0.5100 - mae: 0.5596 - val_loss: 0.5346 - val_mse: 0.5346 - val_mae: 0.5582\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4995 - mse: 0.4995 - mae: 0.5665 - val_loss: 0.5126 - val_mse: 0.5126 - val_mae: 0.5450\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4821 - mse: 0.4821 - mae: 0.5434 - val_loss: 0.5268 - val_mse: 0.5268 - val_mae: 0.5511\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4938 - mse: 0.4938 - mae: 0.5542 - val_loss: 0.5267 - val_mse: 0.5267 - val_mae: 0.5575\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4902 - mse: 0.4902 - mae: 0.5411 - val_loss: 0.5146 - val_mse: 0.5146 - val_mae: 0.5494\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4712 - mse: 0.4712 - mae: 0.5306 - val_loss: 0.5120 - val_mse: 0.5120 - val_mae: 0.5413\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4750 - mse: 0.4750 - mae: 0.5361 - val_loss: 0.5173 - val_mse: 0.5173 - val_mae: 0.5459\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4396 - mse: 0.4396 - mae: 0.5207 - val_loss: 0.5177 - val_mse: 0.5177 - val_mae: 0.5514\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4604 - mse: 0.4604 - mae: 0.5237 - val_loss: 0.4939 - val_mse: 0.4939 - val_mae: 0.5372\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4673 - mse: 0.4673 - mae: 0.5390 - val_loss: 0.5169 - val_mse: 0.5169 - val_mae: 0.5527\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4520 - mse: 0.4520 - mae: 0.5303 - val_loss: 0.5083 - val_mse: 0.5083 - val_mae: 0.5498\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4337 - mse: 0.4337 - mae: 0.5088 - val_loss: 0.4953 - val_mse: 0.4953 - val_mae: 0.5361\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4540 - mse: 0.4540 - mae: 0.5290 - val_loss: 0.5069 - val_mse: 0.5069 - val_mae: 0.5461\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4454 - mse: 0.4454 - mae: 0.5279 - val_loss: 0.5065 - val_mse: 0.5065 - val_mae: 0.5332\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4436 - mse: 0.4436 - mae: 0.5228 - val_loss: 0.5110 - val_mse: 0.5110 - val_mae: 0.5376\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4567 - mse: 0.4567 - mae: 0.5325 - val_loss: 0.4809 - val_mse: 0.4809 - val_mae: 0.5259\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4213 - mse: 0.4213 - mae: 0.5133 - val_loss: 0.4710 - val_mse: 0.4710 - val_mae: 0.5295\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4420 - mse: 0.4420 - mae: 0.5210 - val_loss: 0.4721 - val_mse: 0.4721 - val_mae: 0.5239\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4198 - mse: 0.4198 - mae: 0.5040 - val_loss: 0.4800 - val_mse: 0.4800 - val_mae: 0.5363\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4184 - mse: 0.4184 - mae: 0.5086 - val_loss: 0.4633 - val_mse: 0.4633 - val_mae: 0.5221\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4100 - mse: 0.4100 - mae: 0.4967 - val_loss: 0.4773 - val_mse: 0.4773 - val_mae: 0.5379\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4030 - mse: 0.4030 - mae: 0.4955 - val_loss: 0.4990 - val_mse: 0.4990 - val_mae: 0.5519\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4236 - mse: 0.4236 - mae: 0.5022 - val_loss: 0.4701 - val_mse: 0.4701 - val_mae: 0.5302\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4207 - mse: 0.4207 - mae: 0.5077 - val_loss: 0.4846 - val_mse: 0.4846 - val_mae: 0.5418\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4118 - mse: 0.4118 - mae: 0.5053 - val_loss: 0.4697 - val_mse: 0.4697 - val_mae: 0.5266\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4239 - mse: 0.4239 - mae: 0.5147 - val_loss: 0.4476 - val_mse: 0.4476 - val_mae: 0.5176\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3953 - mse: 0.3953 - mae: 0.4855 - val_loss: 0.4565 - val_mse: 0.4565 - val_mae: 0.5186\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3845 - mse: 0.3845 - mae: 0.4870 - val_loss: 0.4472 - val_mse: 0.4472 - val_mae: 0.5165\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3804 - mse: 0.3804 - mae: 0.4819 - val_loss: 0.4449 - val_mse: 0.4449 - val_mae: 0.5108\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3866 - mse: 0.3866 - mae: 0.4832 - val_loss: 0.4498 - val_mse: 0.4498 - val_mae: 0.5124\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4015 - mse: 0.4015 - mae: 0.4937 - val_loss: 0.4507 - val_mse: 0.4507 - val_mae: 0.5114\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3578 - mse: 0.3578 - mae: 0.4632 - val_loss: 0.4536 - val_mse: 0.4536 - val_mae: 0.5117\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3690 - mse: 0.3690 - mae: 0.4747 - val_loss: 0.4558 - val_mse: 0.4558 - val_mae: 0.5151\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3914 - mse: 0.3914 - mae: 0.4876 - val_loss: 0.4767 - val_mse: 0.4767 - val_mae: 0.5285\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4084 - mse: 0.4084 - mae: 0.5038 - val_loss: 0.4387 - val_mse: 0.4387 - val_mae: 0.5043\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.3910 - mse: 0.3910 - mae: 0.4830 - val_loss: 0.4529 - val_mse: 0.4529 - val_mae: 0.5201\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.3897 - mse: 0.3897 - mae: 0.4833 - val_loss: 0.4330 - val_mse: 0.4330 - val_mae: 0.5067\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.3651 - mse: 0.3651 - mae: 0.4758 - val_loss: 0.4384 - val_mse: 0.4384 - val_mae: 0.5104\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.3797 - mse: 0.3797 - mae: 0.4853 - val_loss: 0.4493 - val_mse: 0.4493 - val_mae: 0.5316\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.3756 - mse: 0.3756 - mae: 0.4838 - val_loss: 0.4365 - val_mse: 0.4365 - val_mae: 0.5226\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.3903 - mse: 0.3903 - mae: 0.4832 - val_loss: 0.4242 - val_mse: 0.4242 - val_mae: 0.5076\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.3642 - mse: 0.3642 - mae: 0.4624 - val_loss: 0.4239 - val_mse: 0.4239 - val_mae: 0.5085\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.3733 - mse: 0.3733 - mae: 0.4738 - val_loss: 0.4370 - val_mse: 0.4370 - val_mae: 0.5106\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.3642 - mse: 0.3642 - mae: 0.4628 - val_loss: 0.4300 - val_mse: 0.4300 - val_mae: 0.5046\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 23ms/step - loss: 0.3641 - mse: 0.3641 - mae: 0.4763 - val_loss: 0.4129 - val_mse: 0.4129 - val_mae: 0.5015\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3564 - mse: 0.3564 - mae: 0.4631 - val_loss: 0.4289 - val_mse: 0.4289 - val_mae: 0.5105\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3556 - mse: 0.3556 - mae: 0.4626 - val_loss: 0.4268 - val_mse: 0.4268 - val_mae: 0.5135\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3519 - mse: 0.3519 - mae: 0.4659 - val_loss: 0.4399 - val_mse: 0.4399 - val_mae: 0.5218\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3535 - mse: 0.3535 - mae: 0.4635 - val_loss: 0.4302 - val_mse: 0.4302 - val_mae: 0.5110\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3667 - mse: 0.3667 - mae: 0.4795 - val_loss: 0.4287 - val_mse: 0.4287 - val_mae: 0.5168\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3440 - mse: 0.3440 - mae: 0.4568 - val_loss: 0.4242 - val_mse: 0.4242 - val_mae: 0.5085\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3731 - mse: 0.3731 - mae: 0.4752 - val_loss: 0.4179 - val_mse: 0.4179 - val_mae: 0.5051\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3238 - mse: 0.3238 - mae: 0.4479 - val_loss: 0.4270 - val_mse: 0.4270 - val_mae: 0.5187\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3518 - mse: 0.3518 - mae: 0.4576 - val_loss: 0.4520 - val_mse: 0.4520 - val_mae: 0.5265\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3550 - mse: 0.3550 - mae: 0.4657 - val_loss: 0.4223 - val_mse: 0.4223 - val_mae: 0.5089\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3477 - mse: 0.3477 - mae: 0.4629 - val_loss: 0.4142 - val_mse: 0.4142 - val_mae: 0.5035\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3336 - mse: 0.3336 - mae: 0.4443 - val_loss: 0.4471 - val_mse: 0.4471 - val_mae: 0.5050\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3744 - mse: 0.3744 - mae: 0.4684 - val_loss: 0.4412 - val_mse: 0.4412 - val_mae: 0.5098\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3576 - mse: 0.3576 - mae: 0.4690 - val_loss: 0.4225 - val_mse: 0.4225 - val_mae: 0.5025\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3264 - mse: 0.3264 - mae: 0.4450 - val_loss: 0.4293 - val_mse: 0.4293 - val_mae: 0.5216\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3395 - mse: 0.3395 - mae: 0.4502 - val_loss: 0.4207 - val_mse: 0.4207 - val_mae: 0.5157\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3372 - mse: 0.3372 - mae: 0.4479 - val_loss: 0.4327 - val_mse: 0.4327 - val_mae: 0.5090\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3409 - mse: 0.3409 - mae: 0.4481 - val_loss: 0.4114 - val_mse: 0.4114 - val_mae: 0.5118\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3330 - mse: 0.3330 - mae: 0.4464 - val_loss: 0.4129 - val_mse: 0.4129 - val_mae: 0.5100\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3173 - mse: 0.3173 - mae: 0.4427 - val_loss: 0.4222 - val_mse: 0.4222 - val_mae: 0.5009\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3221 - mse: 0.3221 - mae: 0.4367 - val_loss: 0.4132 - val_mse: 0.4132 - val_mae: 0.4996\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3195 - mse: 0.3195 - mae: 0.4418 - val_loss: 0.3979 - val_mse: 0.3979 - val_mae: 0.4962\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3255 - mse: 0.3255 - mae: 0.4452 - val_loss: 0.4053 - val_mse: 0.4053 - val_mae: 0.5070\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3390 - mse: 0.3390 - mae: 0.4469 - val_loss: 0.4134 - val_mse: 0.4134 - val_mae: 0.5104\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3348 - mse: 0.3348 - mae: 0.4489 - val_loss: 0.4082 - val_mse: 0.4082 - val_mae: 0.4966\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3173 - mse: 0.3173 - mae: 0.4376 - val_loss: 0.4338 - val_mse: 0.4338 - val_mae: 0.5104\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3324 - mse: 0.3324 - mae: 0.4581 - val_loss: 0.4277 - val_mse: 0.4277 - val_mae: 0.4975\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3327 - mse: 0.3327 - mae: 0.4507 - val_loss: 0.4067 - val_mse: 0.4067 - val_mae: 0.4917\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3140 - mse: 0.3140 - mae: 0.4364 - val_loss: 0.3974 - val_mse: 0.3974 - val_mae: 0.4985\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3160 - mse: 0.3160 - mae: 0.4325 - val_loss: 0.4002 - val_mse: 0.4002 - val_mae: 0.4944\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3352 - mse: 0.3352 - mae: 0.4427 - val_loss: 0.4027 - val_mse: 0.4027 - val_mae: 0.4912\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3346 - mse: 0.3346 - mae: 0.4470 - val_loss: 0.3939 - val_mse: 0.3939 - val_mae: 0.4881\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3133 - mse: 0.3133 - mae: 0.4290 - val_loss: 0.4294 - val_mse: 0.4294 - val_mae: 0.5102\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3059 - mse: 0.3059 - mae: 0.4282 - val_loss: 0.4122 - val_mse: 0.4122 - val_mae: 0.5102\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3225 - mse: 0.3225 - mae: 0.4398 - val_loss: 0.4345 - val_mse: 0.4345 - val_mae: 0.5100\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3174 - mse: 0.3174 - mae: 0.4376 - val_loss: 0.3865 - val_mse: 0.3865 - val_mae: 0.4929\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3111 - mse: 0.3111 - mae: 0.4329 - val_loss: 0.4002 - val_mse: 0.4002 - val_mae: 0.4903\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3044 - mse: 0.3044 - mae: 0.4322 - val_loss: 0.3979 - val_mse: 0.3979 - val_mae: 0.5030\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3111 - mse: 0.3111 - mae: 0.4327 - val_loss: 0.4081 - val_mse: 0.4081 - val_mae: 0.5032\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3232 - mse: 0.3232 - mae: 0.4398 - val_loss: 0.3842 - val_mse: 0.3842 - val_mae: 0.4892\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3251 - mse: 0.3251 - mae: 0.4370 - val_loss: 0.4077 - val_mse: 0.4077 - val_mae: 0.4929\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3337 - mse: 0.3337 - mae: 0.4562 - val_loss: 0.3939 - val_mse: 0.3939 - val_mae: 0.4896\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3253 - mse: 0.3253 - mae: 0.4423 - val_loss: 0.3876 - val_mse: 0.3876 - val_mae: 0.4906\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2912 - mse: 0.2912 - mae: 0.4227 - val_loss: 0.3929 - val_mse: 0.3929 - val_mae: 0.4922\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2908 - mse: 0.2908 - mae: 0.4142 - val_loss: 0.4133 - val_mse: 0.4133 - val_mae: 0.4981\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3127 - mse: 0.3127 - mae: 0.4429 - val_loss: 0.3965 - val_mse: 0.3965 - val_mae: 0.5020\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3005 - mse: 0.3005 - mae: 0.4225 - val_loss: 0.3915 - val_mse: 0.3915 - val_mae: 0.4976\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2928 - mse: 0.2928 - mae: 0.4240 - val_loss: 0.3892 - val_mse: 0.3892 - val_mae: 0.4918\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3246 - mse: 0.3246 - mae: 0.4488 - val_loss: 0.3918 - val_mse: 0.3918 - val_mae: 0.4926\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2874 - mse: 0.2874 - mae: 0.4176 - val_loss: 0.4053 - val_mse: 0.4053 - val_mae: 0.4978\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3250 - mse: 0.3250 - mae: 0.4353 - val_loss: 0.3786 - val_mse: 0.3786 - val_mae: 0.4851\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3308 - mse: 0.3308 - mae: 0.4426 - val_loss: 0.3961 - val_mse: 0.3961 - val_mae: 0.4976\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3142 - mse: 0.3142 - mae: 0.4265 - val_loss: 0.3883 - val_mse: 0.3883 - val_mae: 0.4913\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3082 - mse: 0.3082 - mae: 0.4385 - val_loss: 0.3874 - val_mse: 0.3874 - val_mae: 0.4885\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2897 - mse: 0.2897 - mae: 0.4209 - val_loss: 0.3937 - val_mse: 0.3937 - val_mae: 0.4915\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2935 - mse: 0.2935 - mae: 0.4184 - val_loss: 0.3842 - val_mse: 0.3842 - val_mae: 0.4884\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3171 - mse: 0.3171 - mae: 0.4379 - val_loss: 0.4045 - val_mse: 0.4045 - val_mae: 0.4978\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2978 - mse: 0.2978 - mae: 0.4253 - val_loss: 0.4102 - val_mse: 0.4102 - val_mae: 0.5009\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2934 - mse: 0.2934 - mae: 0.4173 - val_loss: 0.4084 - val_mse: 0.4084 - val_mae: 0.5056\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3096 - mse: 0.3096 - mae: 0.4322 - val_loss: 0.4118 - val_mse: 0.4118 - val_mae: 0.4953\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3148 - mse: 0.3148 - mae: 0.4367 - val_loss: 0.3955 - val_mse: 0.3955 - val_mae: 0.4829\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3001 - mse: 0.3001 - mae: 0.4236 - val_loss: 0.4058 - val_mse: 0.4058 - val_mae: 0.4911\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2799 - mse: 0.2799 - mae: 0.4078 - val_loss: 0.4037 - val_mse: 0.4037 - val_mae: 0.4896\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3136 - mse: 0.3136 - mae: 0.4365 - val_loss: 0.3882 - val_mse: 0.3882 - val_mae: 0.4889\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2861 - mse: 0.2861 - mae: 0.4217 - val_loss: 0.3884 - val_mse: 0.3884 - val_mae: 0.4950\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2923 - mse: 0.2923 - mae: 0.4193 - val_loss: 0.3978 - val_mse: 0.3978 - val_mae: 0.4909\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2877 - mse: 0.2877 - mae: 0.4120 - val_loss: 0.4015 - val_mse: 0.4015 - val_mae: 0.4881\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3048 - mse: 0.3048 - mae: 0.4274 - val_loss: 0.4163 - val_mse: 0.4163 - val_mae: 0.5025\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3010 - mse: 0.3010 - mae: 0.4220 - val_loss: 0.3985 - val_mse: 0.3985 - val_mae: 0.5086\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2943 - mse: 0.2943 - mae: 0.4263 - val_loss: 0.4145 - val_mse: 0.4145 - val_mae: 0.5141\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2973 - mse: 0.2973 - mae: 0.4341 - val_loss: 0.4221 - val_mse: 0.4221 - val_mae: 0.5210\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2920 - mse: 0.2920 - mae: 0.4161 - val_loss: 0.4221 - val_mse: 0.4221 - val_mae: 0.5234\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2685 - mse: 0.2685 - mae: 0.4032 - val_loss: 0.4014 - val_mse: 0.4014 - val_mae: 0.5034\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2877 - mse: 0.2877 - mae: 0.4154 - val_loss: 0.3996 - val_mse: 0.3996 - val_mae: 0.4951\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2937 - mse: 0.2937 - mae: 0.4214 - val_loss: 0.4463 - val_mse: 0.4463 - val_mae: 0.5116\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3009 - mse: 0.3009 - mae: 0.4288 - val_loss: 0.4342 - val_mse: 0.4342 - val_mae: 0.5047\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3002 - mse: 0.3002 - mae: 0.4257 - val_loss: 0.3838 - val_mse: 0.3838 - val_mae: 0.4892\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3150 - mse: 0.3150 - mae: 0.4313 - val_loss: 0.4950 - val_mse: 0.4950 - val_mae: 0.5750\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2891 - mse: 0.2891 - mae: 0.4152 - val_loss: 0.4024 - val_mse: 0.4024 - val_mae: 0.4941\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2896 - mse: 0.2896 - mae: 0.4174 - val_loss: 0.4002 - val_mse: 0.4002 - val_mae: 0.5013\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2936 - mse: 0.2936 - mae: 0.4161 - val_loss: 0.4109 - val_mse: 0.4109 - val_mae: 0.4954\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2955 - mse: 0.2955 - mae: 0.4198 - val_loss: 0.4319 - val_mse: 0.4319 - val_mae: 0.5049\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2980 - mse: 0.2980 - mae: 0.4318 - val_loss: 0.3961 - val_mse: 0.3961 - val_mae: 0.4976\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2834 - mse: 0.2834 - mae: 0.4111 - val_loss: 0.4155 - val_mse: 0.4155 - val_mae: 0.5020\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3038 - mse: 0.3038 - mae: 0.4343 - val_loss: 0.3986 - val_mse: 0.3986 - val_mae: 0.4913\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2779 - mse: 0.2779 - mae: 0.4132 - val_loss: 0.4160 - val_mse: 0.4160 - val_mae: 0.5064\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2884 - mse: 0.2884 - mae: 0.4183 - val_loss: 0.4024 - val_mse: 0.4024 - val_mae: 0.5101\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3030 - mse: 0.3030 - mae: 0.4290 - val_loss: 0.4122 - val_mse: 0.4122 - val_mae: 0.4981\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2953 - mse: 0.2953 - mae: 0.4251 - val_loss: 0.3964 - val_mse: 0.3964 - val_mae: 0.4942\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2720 - mse: 0.2720 - mae: 0.4037 - val_loss: 0.4222 - val_mse: 0.4222 - val_mae: 0.5131\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2758 - mse: 0.2758 - mae: 0.4045 - val_loss: 0.3963 - val_mse: 0.3963 - val_mae: 0.4892\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2714 - mse: 0.2714 - mae: 0.4041 - val_loss: 0.3828 - val_mse: 0.3828 - val_mae: 0.4842\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2995 - mse: 0.2995 - mae: 0.4253 - val_loss: 0.3963 - val_mse: 0.3963 - val_mae: 0.4871\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2906 - mse: 0.2906 - mae: 0.4194 - val_loss: 0.4026 - val_mse: 0.4026 - val_mae: 0.4947\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2866 - mse: 0.2866 - mae: 0.4193 - val_loss: 0.4066 - val_mse: 0.4066 - val_mae: 0.4983\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2791 - mse: 0.2791 - mae: 0.4119 - val_loss: 0.4013 - val_mse: 0.4013 - val_mae: 0.5009\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2795 - mse: 0.2795 - mae: 0.4115 - val_loss: 0.3928 - val_mse: 0.3928 - val_mae: 0.4917\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2918 - mse: 0.2918 - mae: 0.4210 - val_loss: 0.4111 - val_mse: 0.4111 - val_mae: 0.5032\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3272 - mse: 0.3272 - mae: 0.4431 - val_loss: 0.3966 - val_mse: 0.3966 - val_mae: 0.4904\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2808 - mse: 0.2808 - mae: 0.4156 - val_loss: 0.4304 - val_mse: 0.4304 - val_mae: 0.5224\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2750 - mse: 0.2750 - mae: 0.4092 - val_loss: 0.3950 - val_mse: 0.3950 - val_mae: 0.4951\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2709 - mse: 0.2709 - mae: 0.4135 - val_loss: 0.3874 - val_mse: 0.3874 - val_mae: 0.4877\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2831 - mse: 0.2831 - mae: 0.4115 - val_loss: 0.3995 - val_mse: 0.3995 - val_mae: 0.4963\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2722 - mse: 0.2722 - mae: 0.4028 - val_loss: 0.4124 - val_mse: 0.4124 - val_mae: 0.5103\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2734 - mse: 0.2734 - mae: 0.4149 - val_loss: 0.4095 - val_mse: 0.4095 - val_mae: 0.5074\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2715 - mse: 0.2715 - mae: 0.4069 - val_loss: 0.3814 - val_mse: 0.3814 - val_mae: 0.4832\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2617 - mse: 0.2617 - mae: 0.3972 - val_loss: 0.3910 - val_mse: 0.3910 - val_mae: 0.4966\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2621 - mse: 0.2621 - mae: 0.3967 - val_loss: 0.3963 - val_mse: 0.3963 - val_mae: 0.4988\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2877 - mse: 0.2877 - mae: 0.4187 - val_loss: 0.4063 - val_mse: 0.4063 - val_mae: 0.5097\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2886 - mse: 0.2886 - mae: 0.4197 - val_loss: 0.4042 - val_mse: 0.4042 - val_mae: 0.5010\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2653 - mse: 0.2653 - mae: 0.4044 - val_loss: 0.3971 - val_mse: 0.3971 - val_mae: 0.4989\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2620 - mse: 0.2620 - mae: 0.3920 - val_loss: 0.3914 - val_mse: 0.3914 - val_mae: 0.4955\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2681 - mse: 0.2681 - mae: 0.4042 - val_loss: 0.4007 - val_mse: 0.4007 - val_mae: 0.5043\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2640 - mse: 0.2640 - mae: 0.3999 - val_loss: 0.4023 - val_mse: 0.4023 - val_mae: 0.4989\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2620 - mse: 0.2620 - mae: 0.3973 - val_loss: 0.3982 - val_mse: 0.3982 - val_mae: 0.5069\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2672 - mse: 0.2672 - mae: 0.3983 - val_loss: 0.3960 - val_mse: 0.3960 - val_mae: 0.4969\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2769 - mse: 0.2769 - mae: 0.4075 - val_loss: 0.3977 - val_mse: 0.3977 - val_mae: 0.5021\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2660 - mse: 0.2660 - mae: 0.3990 - val_loss: 0.4156 - val_mse: 0.4156 - val_mae: 0.5210\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2642 - mse: 0.2642 - mae: 0.4041 - val_loss: 0.4033 - val_mse: 0.4033 - val_mae: 0.5073\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2627 - mse: 0.2627 - mae: 0.4007 - val_loss: 0.3917 - val_mse: 0.3917 - val_mae: 0.5010\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2686 - mse: 0.2686 - mae: 0.4008 - val_loss: 0.3944 - val_mse: 0.3944 - val_mae: 0.4974\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2690 - mse: 0.2690 - mae: 0.3960 - val_loss: 0.3956 - val_mse: 0.3956 - val_mae: 0.4974\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2874 - mse: 0.2874 - mae: 0.4190 - val_loss: 0.3969 - val_mse: 0.3969 - val_mae: 0.4972\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2586 - mse: 0.2586 - mae: 0.4003 - val_loss: 0.4071 - val_mse: 0.4071 - val_mae: 0.5136\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2767 - mse: 0.2767 - mae: 0.4143 - val_loss: 0.3983 - val_mse: 0.3983 - val_mae: 0.4997\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2701 - mse: 0.2701 - mae: 0.4025 - val_loss: 0.4077 - val_mse: 0.4077 - val_mae: 0.5076\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2767 - mse: 0.2767 - mae: 0.4158 - val_loss: 0.3949 - val_mse: 0.3949 - val_mae: 0.4998\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2771 - mse: 0.2771 - mae: 0.4062 - val_loss: 0.3943 - val_mse: 0.3943 - val_mae: 0.5011\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2672 - mse: 0.2672 - mae: 0.3976 - val_loss: 0.4221 - val_mse: 0.4221 - val_mae: 0.5076\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2659 - mse: 0.2659 - mae: 0.3950 - val_loss: 0.4074 - val_mse: 0.4074 - val_mae: 0.5055\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2614 - mse: 0.2614 - mae: 0.3913 - val_loss: 0.3996 - val_mse: 0.3996 - val_mae: 0.4997\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2536 - mse: 0.2536 - mae: 0.3883 - val_loss: 0.4021 - val_mse: 0.4021 - val_mae: 0.4959\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2608 - mse: 0.2608 - mae: 0.3988 - val_loss: 0.4653 - val_mse: 0.4653 - val_mae: 0.5539\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2729 - mse: 0.2729 - mae: 0.4152 - val_loss: 0.4405 - val_mse: 0.4405 - val_mae: 0.5316\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2782 - mse: 0.2782 - mae: 0.4089 - val_loss: 0.3988 - val_mse: 0.3988 - val_mae: 0.5007\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2550 - mse: 0.2550 - mae: 0.3914 - val_loss: 0.4209 - val_mse: 0.4209 - val_mae: 0.5167\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2798 - mse: 0.2798 - mae: 0.4081 - val_loss: 0.4185 - val_mse: 0.4185 - val_mae: 0.5062\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2643 - mse: 0.2643 - mae: 0.4049 - val_loss: 0.3971 - val_mse: 0.3971 - val_mae: 0.4890\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2574 - mse: 0.2574 - mae: 0.3994 - val_loss: 0.3899 - val_mse: 0.3899 - val_mae: 0.4904\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2716 - mse: 0.2716 - mae: 0.4027 - val_loss: 0.4219 - val_mse: 0.4219 - val_mae: 0.5082\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2456 - mse: 0.2456 - mae: 0.3810 - val_loss: 0.3996 - val_mse: 0.3996 - val_mae: 0.4994\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2656 - mse: 0.2656 - mae: 0.4083 - val_loss: 0.4251 - val_mse: 0.4251 - val_mae: 0.4995\n",
            "Epoch 252/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2550 - mse: 0.2550 - mae: 0.3910 - val_loss: 0.4027 - val_mse: 0.4027 - val_mae: 0.4984\n",
            "Epoch 253/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2440 - mse: 0.2440 - mae: 0.3821 - val_loss: 0.4036 - val_mse: 0.4036 - val_mae: 0.5014\n",
            "Epoch 254/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2429 - mse: 0.2429 - mae: 0.3829 - val_loss: 0.3947 - val_mse: 0.3947 - val_mae: 0.4842\n",
            "Epoch 255/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2597 - mse: 0.2597 - mae: 0.3877 - val_loss: 0.4126 - val_mse: 0.4126 - val_mae: 0.4928\n",
            "Epoch 256/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2630 - mse: 0.2630 - mae: 0.3936 - val_loss: 0.4033 - val_mse: 0.4033 - val_mae: 0.5066\n",
            "Epoch 257/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2525 - mse: 0.2525 - mae: 0.3894 - val_loss: 0.4028 - val_mse: 0.4028 - val_mae: 0.4957\n",
            "Epoch 258/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2689 - mse: 0.2689 - mae: 0.4022 - val_loss: 0.4330 - val_mse: 0.4330 - val_mae: 0.5195\n",
            "Epoch 259/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2805 - mse: 0.2805 - mae: 0.4158 - val_loss: 0.4196 - val_mse: 0.4196 - val_mae: 0.5024\n",
            "Epoch 260/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2606 - mse: 0.2606 - mae: 0.3891 - val_loss: 0.4208 - val_mse: 0.4208 - val_mae: 0.5002\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.4540 - mse: 0.4540 - mae: 0.4966\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 16ms/step - loss: 26.1216 - mse: 26.1216 - mae: 4.8757 - val_loss: 15.4013 - val_mse: 15.4013 - val_mae: 3.6924\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 8.8191 - mse: 8.8191 - mae: 2.6363 - val_loss: 4.9918 - val_mse: 4.9918 - val_mae: 1.9025\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 4.0578 - mse: 4.0578 - mae: 1.6235 - val_loss: 3.7889 - val_mse: 3.7889 - val_mae: 1.5579\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 3.5349 - mse: 3.5349 - mae: 1.4968 - val_loss: 3.3662 - val_mse: 3.3662 - val_mae: 1.4762\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 3.0148 - mse: 3.0148 - mae: 1.3931 - val_loss: 3.0733 - val_mse: 3.0733 - val_mae: 1.4090\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.7391 - mse: 2.7391 - mae: 1.3161 - val_loss: 2.8881 - val_mse: 2.8881 - val_mae: 1.3570\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.6219 - mse: 2.6219 - mae: 1.2900 - val_loss: 2.7334 - val_mse: 2.7334 - val_mae: 1.3159\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.4154 - mse: 2.4154 - mae: 1.2423 - val_loss: 2.5638 - val_mse: 2.5638 - val_mae: 1.2782\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.1414 - mse: 2.1414 - mae: 1.1670 - val_loss: 2.4273 - val_mse: 2.4273 - val_mae: 1.2433\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 2.0968 - mse: 2.0968 - mae: 1.1571 - val_loss: 2.3239 - val_mse: 2.3239 - val_mae: 1.2239\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.9637 - mse: 1.9637 - mae: 1.1016 - val_loss: 2.1967 - val_mse: 2.1967 - val_mae: 1.1916\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.9122 - mse: 1.9122 - mae: 1.1031 - val_loss: 2.0935 - val_mse: 2.0935 - val_mae: 1.1659\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.7910 - mse: 1.7910 - mae: 1.0609 - val_loss: 1.9851 - val_mse: 1.9851 - val_mae: 1.1330\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.6571 - mse: 1.6571 - mae: 1.0292 - val_loss: 1.9184 - val_mse: 1.9184 - val_mae: 1.1165\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.6582 - mse: 1.6582 - mae: 1.0082 - val_loss: 1.8455 - val_mse: 1.8455 - val_mae: 1.0971\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.5830 - mse: 1.5830 - mae: 0.9885 - val_loss: 1.7619 - val_mse: 1.7619 - val_mae: 1.0727\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.4750 - mse: 1.4750 - mae: 0.9532 - val_loss: 1.6539 - val_mse: 1.6539 - val_mae: 1.0338\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.3931 - mse: 1.3931 - mae: 0.9251 - val_loss: 1.6062 - val_mse: 1.6062 - val_mae: 1.0228\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.3521 - mse: 1.3521 - mae: 0.9146 - val_loss: 1.5292 - val_mse: 1.5292 - val_mae: 0.9990\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2626 - mse: 1.2626 - mae: 0.8794 - val_loss: 1.4575 - val_mse: 1.4575 - val_mae: 0.9687\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2012 - mse: 1.2012 - mae: 0.8586 - val_loss: 1.3841 - val_mse: 1.3841 - val_mae: 0.9479\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1894 - mse: 1.1894 - mae: 0.8515 - val_loss: 1.3271 - val_mse: 1.3271 - val_mae: 0.9311\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1412 - mse: 1.1412 - mae: 0.8346 - val_loss: 1.2856 - val_mse: 1.2856 - val_mae: 0.9122\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1307 - mse: 1.1307 - mae: 0.8422 - val_loss: 1.2084 - val_mse: 1.2084 - val_mae: 0.8861\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9999 - mse: 0.9999 - mae: 0.7860 - val_loss: 1.1519 - val_mse: 1.1519 - val_mae: 0.8574\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0193 - mse: 1.0193 - mae: 0.7768 - val_loss: 1.1198 - val_mse: 1.1198 - val_mae: 0.8508\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9721 - mse: 0.9721 - mae: 0.7647 - val_loss: 1.0528 - val_mse: 1.0528 - val_mae: 0.8228\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9300 - mse: 0.9300 - mae: 0.7554 - val_loss: 1.0249 - val_mse: 1.0249 - val_mae: 0.8163\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8983 - mse: 0.8983 - mae: 0.7371 - val_loss: 1.0004 - val_mse: 1.0004 - val_mae: 0.7942\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8870 - mse: 0.8870 - mae: 0.7318 - val_loss: 0.9397 - val_mse: 0.9397 - val_mae: 0.7766\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8613 - mse: 0.8613 - mae: 0.7206 - val_loss: 0.8925 - val_mse: 0.8925 - val_mae: 0.7516\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.8187 - mse: 0.8187 - mae: 0.7002 - val_loss: 0.8651 - val_mse: 0.8651 - val_mae: 0.7442\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7715 - mse: 0.7715 - mae: 0.6768 - val_loss: 0.8319 - val_mse: 0.8319 - val_mae: 0.7198\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7599 - mse: 0.7599 - mae: 0.6771 - val_loss: 0.8127 - val_mse: 0.8127 - val_mae: 0.7111\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7368 - mse: 0.7368 - mae: 0.6708 - val_loss: 0.7762 - val_mse: 0.7762 - val_mae: 0.6970\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7445 - mse: 0.7445 - mae: 0.6796 - val_loss: 0.7556 - val_mse: 0.7556 - val_mae: 0.6852\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6913 - mse: 0.6913 - mae: 0.6485 - val_loss: 0.7358 - val_mse: 0.7358 - val_mae: 0.6763\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6940 - mse: 0.6940 - mae: 0.6492 - val_loss: 0.7197 - val_mse: 0.7197 - val_mae: 0.6602\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6329 - mse: 0.6329 - mae: 0.6119 - val_loss: 0.7008 - val_mse: 0.7008 - val_mae: 0.6583\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6221 - mse: 0.6221 - mae: 0.6147 - val_loss: 0.6619 - val_mse: 0.6619 - val_mae: 0.6355\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6008 - mse: 0.6008 - mae: 0.6053 - val_loss: 0.6649 - val_mse: 0.6649 - val_mae: 0.6462\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6005 - mse: 0.6005 - mae: 0.5988 - val_loss: 0.6507 - val_mse: 0.6507 - val_mae: 0.6368\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5967 - mse: 0.5967 - mae: 0.5976 - val_loss: 0.6215 - val_mse: 0.6215 - val_mae: 0.6138\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5720 - mse: 0.5720 - mae: 0.5782 - val_loss: 0.6137 - val_mse: 0.6137 - val_mae: 0.6106\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5281 - mse: 0.5281 - mae: 0.5682 - val_loss: 0.5957 - val_mse: 0.5957 - val_mae: 0.6037\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5740 - mse: 0.5740 - mae: 0.5791 - val_loss: 0.5902 - val_mse: 0.5902 - val_mae: 0.5975\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5556 - mse: 0.5556 - mae: 0.5827 - val_loss: 0.5718 - val_mse: 0.5718 - val_mae: 0.5979\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5127 - mse: 0.5127 - mae: 0.5515 - val_loss: 0.5615 - val_mse: 0.5615 - val_mae: 0.5890\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5158 - mse: 0.5158 - mae: 0.5612 - val_loss: 0.5575 - val_mse: 0.5575 - val_mae: 0.5925\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5003 - mse: 0.5003 - mae: 0.5420 - val_loss: 0.5582 - val_mse: 0.5582 - val_mae: 0.5960\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5115 - mse: 0.5115 - mae: 0.5535 - val_loss: 0.5771 - val_mse: 0.5771 - val_mae: 0.5780\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5073 - mse: 0.5073 - mae: 0.5512 - val_loss: 0.5422 - val_mse: 0.5422 - val_mae: 0.5671\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4862 - mse: 0.4862 - mae: 0.5434 - val_loss: 0.5316 - val_mse: 0.5316 - val_mae: 0.5813\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4593 - mse: 0.4593 - mae: 0.5302 - val_loss: 0.5209 - val_mse: 0.5209 - val_mae: 0.5650\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4864 - mse: 0.4864 - mae: 0.5452 - val_loss: 0.5094 - val_mse: 0.5094 - val_mae: 0.5650\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4589 - mse: 0.4589 - mae: 0.5229 - val_loss: 0.5204 - val_mse: 0.5204 - val_mae: 0.5676\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4794 - mse: 0.4794 - mae: 0.5265 - val_loss: 0.5067 - val_mse: 0.5067 - val_mae: 0.5551\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4582 - mse: 0.4582 - mae: 0.5195 - val_loss: 0.4936 - val_mse: 0.4936 - val_mae: 0.5515\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4482 - mse: 0.4482 - mae: 0.5239 - val_loss: 0.4901 - val_mse: 0.4901 - val_mae: 0.5524\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4401 - mse: 0.4401 - mae: 0.5124 - val_loss: 0.4836 - val_mse: 0.4836 - val_mae: 0.5462\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4241 - mse: 0.4241 - mae: 0.4967 - val_loss: 0.4876 - val_mse: 0.4876 - val_mae: 0.5428\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4696 - mse: 0.4696 - mae: 0.5240 - val_loss: 0.4974 - val_mse: 0.4974 - val_mae: 0.5460\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4272 - mse: 0.4272 - mae: 0.5040 - val_loss: 0.4854 - val_mse: 0.4854 - val_mae: 0.5414\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4096 - mse: 0.4096 - mae: 0.4941 - val_loss: 0.4770 - val_mse: 0.4770 - val_mae: 0.5516\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4161 - mse: 0.4161 - mae: 0.4965 - val_loss: 0.4749 - val_mse: 0.4749 - val_mae: 0.5408\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3936 - mse: 0.3936 - mae: 0.4853 - val_loss: 0.4637 - val_mse: 0.4637 - val_mae: 0.5342\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3745 - mse: 0.3745 - mae: 0.4721 - val_loss: 0.4850 - val_mse: 0.4850 - val_mae: 0.5407\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4396 - mse: 0.4396 - mae: 0.5086 - val_loss: 0.4963 - val_mse: 0.4963 - val_mae: 0.5433\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4186 - mse: 0.4186 - mae: 0.5051 - val_loss: 0.4512 - val_mse: 0.4512 - val_mae: 0.5267\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3874 - mse: 0.3874 - mae: 0.4745 - val_loss: 0.4584 - val_mse: 0.4584 - val_mae: 0.5265\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3933 - mse: 0.3933 - mae: 0.4771 - val_loss: 0.4486 - val_mse: 0.4486 - val_mae: 0.5249\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3911 - mse: 0.3911 - mae: 0.4771 - val_loss: 0.4580 - val_mse: 0.4580 - val_mae: 0.5335\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3934 - mse: 0.3934 - mae: 0.4790 - val_loss: 0.4610 - val_mse: 0.4610 - val_mae: 0.5368\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3788 - mse: 0.3788 - mae: 0.4698 - val_loss: 0.4459 - val_mse: 0.4459 - val_mae: 0.5186\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3991 - mse: 0.3991 - mae: 0.4793 - val_loss: 0.4519 - val_mse: 0.4519 - val_mae: 0.5183\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3618 - mse: 0.3618 - mae: 0.4615 - val_loss: 0.4456 - val_mse: 0.4456 - val_mae: 0.5176\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3956 - mse: 0.3956 - mae: 0.4820 - val_loss: 0.4430 - val_mse: 0.4430 - val_mae: 0.5240\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3907 - mse: 0.3907 - mae: 0.4783 - val_loss: 0.4337 - val_mse: 0.4337 - val_mae: 0.5188\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3690 - mse: 0.3690 - mae: 0.4674 - val_loss: 0.4361 - val_mse: 0.4361 - val_mae: 0.5138\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3559 - mse: 0.3559 - mae: 0.4627 - val_loss: 0.4274 - val_mse: 0.4274 - val_mae: 0.5159\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3761 - mse: 0.3761 - mae: 0.4759 - val_loss: 0.4319 - val_mse: 0.4319 - val_mae: 0.5117\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3599 - mse: 0.3599 - mae: 0.4626 - val_loss: 0.4429 - val_mse: 0.4429 - val_mae: 0.5164\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3884 - mse: 0.3884 - mae: 0.4834 - val_loss: 0.4233 - val_mse: 0.4233 - val_mae: 0.5086\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3551 - mse: 0.3551 - mae: 0.4652 - val_loss: 0.4706 - val_mse: 0.4706 - val_mae: 0.5448\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3614 - mse: 0.3614 - mae: 0.4566 - val_loss: 0.4296 - val_mse: 0.4296 - val_mae: 0.5111\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3453 - mse: 0.3453 - mae: 0.4534 - val_loss: 0.4360 - val_mse: 0.4360 - val_mae: 0.5125\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3711 - mse: 0.3711 - mae: 0.4729 - val_loss: 0.4327 - val_mse: 0.4327 - val_mae: 0.5111\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3573 - mse: 0.3573 - mae: 0.4628 - val_loss: 0.4205 - val_mse: 0.4205 - val_mae: 0.5075\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3651 - mse: 0.3651 - mae: 0.4714 - val_loss: 0.4384 - val_mse: 0.4384 - val_mae: 0.5155\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3585 - mse: 0.3585 - mae: 0.4605 - val_loss: 0.4263 - val_mse: 0.4263 - val_mae: 0.5067\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3308 - mse: 0.3308 - mae: 0.4515 - val_loss: 0.4261 - val_mse: 0.4261 - val_mae: 0.5078\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3460 - mse: 0.3460 - mae: 0.4594 - val_loss: 0.4339 - val_mse: 0.4339 - val_mae: 0.5157\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3592 - mse: 0.3592 - mae: 0.4600 - val_loss: 0.4298 - val_mse: 0.4298 - val_mae: 0.5131\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3231 - mse: 0.3231 - mae: 0.4369 - val_loss: 0.4281 - val_mse: 0.4281 - val_mae: 0.5126\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3507 - mse: 0.3507 - mae: 0.4604 - val_loss: 0.4253 - val_mse: 0.4253 - val_mae: 0.5106\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3326 - mse: 0.3326 - mae: 0.4464 - val_loss: 0.4335 - val_mse: 0.4335 - val_mae: 0.5113\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3609 - mse: 0.3609 - mae: 0.4590 - val_loss: 0.4227 - val_mse: 0.4227 - val_mae: 0.5167\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3439 - mse: 0.3439 - mae: 0.4579 - val_loss: 0.4339 - val_mse: 0.4339 - val_mae: 0.5088\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3542 - mse: 0.3542 - mae: 0.4573 - val_loss: 0.4257 - val_mse: 0.4257 - val_mae: 0.5072\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3679 - mse: 0.3679 - mae: 0.4554 - val_loss: 0.4201 - val_mse: 0.4201 - val_mae: 0.5109\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3461 - mse: 0.3461 - mae: 0.4578 - val_loss: 0.4183 - val_mse: 0.4183 - val_mae: 0.5105\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3188 - mse: 0.3188 - mae: 0.4336 - val_loss: 0.4128 - val_mse: 0.4128 - val_mae: 0.5020\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3141 - mse: 0.3141 - mae: 0.4329 - val_loss: 0.4222 - val_mse: 0.4222 - val_mae: 0.5065\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3524 - mse: 0.3524 - mae: 0.4583 - val_loss: 0.4502 - val_mse: 0.4502 - val_mae: 0.5362\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3336 - mse: 0.3336 - mae: 0.4513 - val_loss: 0.4137 - val_mse: 0.4137 - val_mae: 0.5022\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3325 - mse: 0.3325 - mae: 0.4431 - val_loss: 0.4201 - val_mse: 0.4201 - val_mae: 0.5070\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3436 - mse: 0.3436 - mae: 0.4497 - val_loss: 0.4517 - val_mse: 0.4517 - val_mae: 0.5188\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3294 - mse: 0.3294 - mae: 0.4444 - val_loss: 0.4235 - val_mse: 0.4235 - val_mae: 0.5073\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3166 - mse: 0.3166 - mae: 0.4383 - val_loss: 0.4186 - val_mse: 0.4186 - val_mae: 0.5036\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3412 - mse: 0.3412 - mae: 0.4510 - val_loss: 0.4180 - val_mse: 0.4180 - val_mae: 0.5095\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3407 - mse: 0.3407 - mae: 0.4530 - val_loss: 0.4148 - val_mse: 0.4148 - val_mae: 0.5113\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3087 - mse: 0.3087 - mae: 0.4256 - val_loss: 0.4184 - val_mse: 0.4184 - val_mae: 0.5044\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3340 - mse: 0.3340 - mae: 0.4435 - val_loss: 0.4094 - val_mse: 0.4094 - val_mae: 0.5036\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3330 - mse: 0.3330 - mae: 0.4494 - val_loss: 0.4372 - val_mse: 0.4372 - val_mae: 0.5257\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3419 - mse: 0.3419 - mae: 0.4463 - val_loss: 0.4145 - val_mse: 0.4145 - val_mae: 0.5069\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3025 - mse: 0.3025 - mae: 0.4312 - val_loss: 0.4172 - val_mse: 0.4172 - val_mae: 0.5065\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3142 - mse: 0.3142 - mae: 0.4395 - val_loss: 0.4139 - val_mse: 0.4139 - val_mae: 0.5020\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3046 - mse: 0.3046 - mae: 0.4274 - val_loss: 0.4174 - val_mse: 0.4174 - val_mae: 0.5090\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3318 - mse: 0.3318 - mae: 0.4405 - val_loss: 0.4262 - val_mse: 0.4262 - val_mae: 0.5187\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3161 - mse: 0.3161 - mae: 0.4230 - val_loss: 0.4112 - val_mse: 0.4112 - val_mae: 0.4989\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3087 - mse: 0.3087 - mae: 0.4335 - val_loss: 0.4096 - val_mse: 0.4096 - val_mae: 0.5004\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2942 - mse: 0.2942 - mae: 0.4183 - val_loss: 0.4162 - val_mse: 0.4162 - val_mae: 0.5028\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2995 - mse: 0.2995 - mae: 0.4154 - val_loss: 0.4239 - val_mse: 0.4239 - val_mae: 0.5078\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3168 - mse: 0.3168 - mae: 0.4379 - val_loss: 0.4191 - val_mse: 0.4191 - val_mae: 0.5091\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.3191 - mse: 0.3191 - mae: 0.4293 - val_loss: 0.4398 - val_mse: 0.4398 - val_mae: 0.5255\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3022 - mse: 0.3022 - mae: 0.4178 - val_loss: 0.4153 - val_mse: 0.4153 - val_mae: 0.5042\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3142 - mse: 0.3142 - mae: 0.4296 - val_loss: 0.4305 - val_mse: 0.4305 - val_mae: 0.5129\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3294 - mse: 0.3294 - mae: 0.4428 - val_loss: 0.4199 - val_mse: 0.4199 - val_mae: 0.5158\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2965 - mse: 0.2965 - mae: 0.4183 - val_loss: 0.4233 - val_mse: 0.4233 - val_mae: 0.5122\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2914 - mse: 0.2914 - mae: 0.4146 - val_loss: 0.4194 - val_mse: 0.4194 - val_mae: 0.5095\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3097 - mse: 0.3097 - mae: 0.4307 - val_loss: 0.4260 - val_mse: 0.4260 - val_mae: 0.5176\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3065 - mse: 0.3065 - mae: 0.4286 - val_loss: 0.4439 - val_mse: 0.4439 - val_mae: 0.5277\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.3218 - mse: 0.3218 - mae: 0.4350 - val_loss: 0.4211 - val_mse: 0.4211 - val_mae: 0.5166\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3276 - mse: 0.3276 - mae: 0.4405 - val_loss: 0.4248 - val_mse: 0.4248 - val_mae: 0.5107\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2897 - mse: 0.2897 - mae: 0.4099 - val_loss: 0.4381 - val_mse: 0.4381 - val_mae: 0.5282\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3017 - mse: 0.3017 - mae: 0.4276 - val_loss: 0.4255 - val_mse: 0.4255 - val_mae: 0.5177\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2919 - mse: 0.2919 - mae: 0.4218 - val_loss: 0.4391 - val_mse: 0.4391 - val_mae: 0.5193\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3015 - mse: 0.3015 - mae: 0.4201 - val_loss: 0.4274 - val_mse: 0.4274 - val_mae: 0.5100\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2950 - mse: 0.2950 - mae: 0.4144 - val_loss: 0.4357 - val_mse: 0.4357 - val_mae: 0.5213\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3016 - mse: 0.3016 - mae: 0.4205 - val_loss: 0.4506 - val_mse: 0.4506 - val_mae: 0.5313\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3132 - mse: 0.3132 - mae: 0.4256 - val_loss: 0.4216 - val_mse: 0.4216 - val_mae: 0.5103\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2892 - mse: 0.2892 - mae: 0.4157 - val_loss: 0.4213 - val_mse: 0.4213 - val_mae: 0.5129\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2741 - mse: 0.2741 - mae: 0.4025 - val_loss: 0.4263 - val_mse: 0.4263 - val_mae: 0.5114\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2901 - mse: 0.2901 - mae: 0.4250 - val_loss: 0.4256 - val_mse: 0.4256 - val_mae: 0.5131\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2993 - mse: 0.2993 - mae: 0.4287 - val_loss: 0.4241 - val_mse: 0.4241 - val_mae: 0.5094\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3076 - mse: 0.3076 - mae: 0.4255 - val_loss: 0.4132 - val_mse: 0.4132 - val_mae: 0.5095\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2971 - mse: 0.2971 - mae: 0.4270 - val_loss: 0.4202 - val_mse: 0.4202 - val_mae: 0.5069\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2960 - mse: 0.2960 - mae: 0.4192 - val_loss: 0.4309 - val_mse: 0.4309 - val_mae: 0.5129\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3040 - mse: 0.3040 - mae: 0.4268 - val_loss: 0.4162 - val_mse: 0.4162 - val_mae: 0.5125\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2964 - mse: 0.2964 - mae: 0.4235 - val_loss: 0.4274 - val_mse: 0.4274 - val_mae: 0.5165\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3074 - mse: 0.3074 - mae: 0.4327 - val_loss: 0.4303 - val_mse: 0.4303 - val_mae: 0.5108\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2818 - mse: 0.2818 - mae: 0.4022 - val_loss: 0.4252 - val_mse: 0.4252 - val_mae: 0.5111\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2815 - mse: 0.2815 - mae: 0.4057 - val_loss: 0.4361 - val_mse: 0.4361 - val_mae: 0.5199\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3224 - mse: 0.3224 - mae: 0.4451 - val_loss: 0.4264 - val_mse: 0.4264 - val_mae: 0.5128\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2829 - mse: 0.2829 - mae: 0.4050 - val_loss: 0.4380 - val_mse: 0.4380 - val_mae: 0.5187\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2906 - mse: 0.2906 - mae: 0.4175 - val_loss: 0.4556 - val_mse: 0.4556 - val_mae: 0.5332\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3136 - mse: 0.3136 - mae: 0.4259 - val_loss: 0.4160 - val_mse: 0.4160 - val_mae: 0.5103\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2762 - mse: 0.2762 - mae: 0.4023 - val_loss: 0.4218 - val_mse: 0.4218 - val_mae: 0.5090\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2864 - mse: 0.2864 - mae: 0.4082 - val_loss: 0.4267 - val_mse: 0.4267 - val_mae: 0.5127\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2889 - mse: 0.2889 - mae: 0.4105 - val_loss: 0.4335 - val_mse: 0.4335 - val_mae: 0.5197\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2702 - mse: 0.2702 - mae: 0.4061 - val_loss: 0.4333 - val_mse: 0.4333 - val_mae: 0.5171\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2757 - mse: 0.2757 - mae: 0.4043 - val_loss: 0.4268 - val_mse: 0.4268 - val_mae: 0.5137\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2833 - mse: 0.2833 - mae: 0.4086 - val_loss: 0.4321 - val_mse: 0.4321 - val_mae: 0.5177\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3047 - mse: 0.3047 - mae: 0.4226 - val_loss: 0.4226 - val_mse: 0.4226 - val_mae: 0.5186\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2741 - mse: 0.2741 - mae: 0.4071 - val_loss: 0.4304 - val_mse: 0.4304 - val_mae: 0.5116\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2801 - mse: 0.2801 - mae: 0.4088 - val_loss: 0.4149 - val_mse: 0.4149 - val_mae: 0.5062\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2600 - mse: 0.2600 - mae: 0.3933 - val_loss: 0.4104 - val_mse: 0.4104 - val_mae: 0.5039\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2856 - mse: 0.2856 - mae: 0.4143 - val_loss: 0.4636 - val_mse: 0.4636 - val_mae: 0.5339\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2992 - mse: 0.2992 - mae: 0.4224 - val_loss: 0.4220 - val_mse: 0.4220 - val_mae: 0.5135\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2683 - mse: 0.2683 - mae: 0.3987 - val_loss: 0.4110 - val_mse: 0.4110 - val_mae: 0.5013\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2938 - mse: 0.2938 - mae: 0.4186 - val_loss: 0.4350 - val_mse: 0.4350 - val_mae: 0.5187\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2831 - mse: 0.2831 - mae: 0.4060 - val_loss: 0.4140 - val_mse: 0.4140 - val_mae: 0.5086\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2919 - mse: 0.2919 - mae: 0.4128 - val_loss: 0.4446 - val_mse: 0.4446 - val_mae: 0.5275\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2850 - mse: 0.2850 - mae: 0.4198 - val_loss: 0.4225 - val_mse: 0.4225 - val_mae: 0.5111\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2787 - mse: 0.2787 - mae: 0.4107 - val_loss: 0.4135 - val_mse: 0.4135 - val_mae: 0.5077\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2909 - mse: 0.2909 - mae: 0.4170 - val_loss: 0.4137 - val_mse: 0.4137 - val_mae: 0.5107\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3033 - mse: 0.3033 - mae: 0.4238 - val_loss: 0.4111 - val_mse: 0.4111 - val_mae: 0.5018\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2970 - mse: 0.2970 - mae: 0.4197 - val_loss: 0.4239 - val_mse: 0.4239 - val_mae: 0.5186\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2905 - mse: 0.2905 - mae: 0.4134 - val_loss: 0.4290 - val_mse: 0.4290 - val_mae: 0.5229\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2496 - mse: 0.2496 - mae: 0.3877 - val_loss: 0.4297 - val_mse: 0.4297 - val_mae: 0.5125\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2823 - mse: 0.2823 - mae: 0.4060 - val_loss: 0.4307 - val_mse: 0.4307 - val_mae: 0.5195\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2734 - mse: 0.2734 - mae: 0.4071 - val_loss: 0.4305 - val_mse: 0.4305 - val_mae: 0.5192\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2580 - mse: 0.2580 - mae: 0.3869 - val_loss: 0.4287 - val_mse: 0.4287 - val_mae: 0.5210\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2862 - mse: 0.2862 - mae: 0.4153 - val_loss: 0.4213 - val_mse: 0.4213 - val_mae: 0.5116\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2836 - mse: 0.2836 - mae: 0.4076 - val_loss: 0.4277 - val_mse: 0.4277 - val_mae: 0.5164\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2824 - mse: 0.2824 - mae: 0.4031 - val_loss: 0.4202 - val_mse: 0.4202 - val_mae: 0.5123\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2846 - mse: 0.2846 - mae: 0.4150 - val_loss: 0.4267 - val_mse: 0.4267 - val_mae: 0.5146\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2653 - mse: 0.2653 - mae: 0.4032 - val_loss: 0.4381 - val_mse: 0.4381 - val_mae: 0.5232\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2818 - mse: 0.2818 - mae: 0.4111 - val_loss: 0.5270 - val_mse: 0.5270 - val_mae: 0.5685\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2957 - mse: 0.2957 - mae: 0.4256 - val_loss: 0.4214 - val_mse: 0.4214 - val_mae: 0.5130\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2727 - mse: 0.2727 - mae: 0.4041 - val_loss: 0.4252 - val_mse: 0.4252 - val_mae: 0.5170\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2882 - mse: 0.2882 - mae: 0.4078 - val_loss: 0.4221 - val_mse: 0.4221 - val_mae: 0.5158\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2979 - mse: 0.2979 - mae: 0.4291 - val_loss: 0.4368 - val_mse: 0.4368 - val_mae: 0.5149\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2887 - mse: 0.2887 - mae: 0.4178 - val_loss: 0.4312 - val_mse: 0.4312 - val_mae: 0.5199\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2639 - mse: 0.2639 - mae: 0.3983 - val_loss: 0.4407 - val_mse: 0.4407 - val_mae: 0.5215\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2658 - mse: 0.2658 - mae: 0.3901 - val_loss: 0.4341 - val_mse: 0.4341 - val_mae: 0.5180\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2792 - mse: 0.2792 - mae: 0.3998 - val_loss: 0.4289 - val_mse: 0.4289 - val_mae: 0.5144\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2798 - mse: 0.2798 - mae: 0.4145 - val_loss: 0.4424 - val_mse: 0.4424 - val_mae: 0.5197\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2701 - mse: 0.2701 - mae: 0.3977 - val_loss: 0.4226 - val_mse: 0.4226 - val_mae: 0.5163\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2588 - mse: 0.2588 - mae: 0.3821 - val_loss: 0.4278 - val_mse: 0.4278 - val_mae: 0.5221\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2931 - mse: 0.2931 - mae: 0.4110 - val_loss: 0.4815 - val_mse: 0.4815 - val_mae: 0.5515\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2973 - mse: 0.2973 - mae: 0.4319 - val_loss: 0.4418 - val_mse: 0.4418 - val_mae: 0.5209\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2910 - mse: 0.2910 - mae: 0.4151 - val_loss: 0.4589 - val_mse: 0.4589 - val_mae: 0.5410\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2976 - mse: 0.2976 - mae: 0.4167 - val_loss: 0.4353 - val_mse: 0.4353 - val_mae: 0.5277\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2902 - mse: 0.2902 - mae: 0.4211 - val_loss: 0.4286 - val_mse: 0.4286 - val_mae: 0.5171\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2757 - mse: 0.2757 - mae: 0.4036 - val_loss: 0.4161 - val_mse: 0.4161 - val_mae: 0.5095\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2585 - mse: 0.2585 - mae: 0.3884 - val_loss: 0.4218 - val_mse: 0.4218 - val_mae: 0.5179\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2441 - mse: 0.2441 - mae: 0.3846 - val_loss: 0.4239 - val_mse: 0.4239 - val_mae: 0.5187\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2606 - mse: 0.2606 - mae: 0.3979 - val_loss: 0.4359 - val_mse: 0.4359 - val_mae: 0.5185\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2626 - mse: 0.2626 - mae: 0.3917 - val_loss: 0.4288 - val_mse: 0.4288 - val_mae: 0.5173\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2718 - mse: 0.2718 - mae: 0.4037 - val_loss: 0.4552 - val_mse: 0.4552 - val_mae: 0.5317\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2883 - mse: 0.2883 - mae: 0.4164 - val_loss: 0.4322 - val_mse: 0.4322 - val_mae: 0.5233\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2554 - mse: 0.2554 - mae: 0.3913 - val_loss: 0.4276 - val_mse: 0.4276 - val_mae: 0.5247\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.5075 - mse: 0.5075 - mae: 0.5364\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 2s 14ms/step - loss: 19.8754 - mse: 19.8754 - mae: 4.2213 - val_loss: 10.7976 - val_mse: 10.7976 - val_mae: 3.0023\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 6.6465 - mse: 6.6465 - mae: 2.2359 - val_loss: 5.0482 - val_mse: 5.0482 - val_mae: 1.9153\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 3.9899 - mse: 3.9899 - mae: 1.6029 - val_loss: 3.6310 - val_mse: 3.6310 - val_mae: 1.5671\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.9032 - mse: 2.9032 - mae: 1.3655 - val_loss: 3.0231 - val_mse: 3.0231 - val_mae: 1.4183\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.4211 - mse: 2.4211 - mae: 1.2582 - val_loss: 2.6273 - val_mse: 2.6273 - val_mae: 1.3034\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.1405 - mse: 2.1405 - mae: 1.1595 - val_loss: 2.4184 - val_mse: 2.4184 - val_mae: 1.2464\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.0493 - mse: 2.0493 - mae: 1.1503 - val_loss: 2.2830 - val_mse: 2.2830 - val_mae: 1.2103\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.0448 - mse: 2.0448 - mae: 1.1412 - val_loss: 2.1515 - val_mse: 2.1515 - val_mae: 1.1755\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.9211 - mse: 1.9211 - mae: 1.1119 - val_loss: 2.0340 - val_mse: 2.0340 - val_mae: 1.1332\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.7078 - mse: 1.7078 - mae: 1.0370 - val_loss: 1.9081 - val_mse: 1.9081 - val_mae: 1.0922\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.6902 - mse: 1.6902 - mae: 1.0353 - val_loss: 1.8383 - val_mse: 1.8383 - val_mae: 1.0734\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.5164 - mse: 1.5164 - mae: 0.9852 - val_loss: 1.7406 - val_mse: 1.7406 - val_mae: 1.0438\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.5182 - mse: 1.5182 - mae: 0.9762 - val_loss: 1.6797 - val_mse: 1.6797 - val_mae: 1.0244\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.3791 - mse: 1.3791 - mae: 0.9324 - val_loss: 1.6053 - val_mse: 1.6053 - val_mae: 1.0085\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.3085 - mse: 1.3085 - mae: 0.8891 - val_loss: 1.5662 - val_mse: 1.5662 - val_mae: 0.9853\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.3825 - mse: 1.3825 - mae: 0.9275 - val_loss: 1.5463 - val_mse: 1.5463 - val_mae: 0.9772\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2453 - mse: 1.2453 - mae: 0.8854 - val_loss: 1.4846 - val_mse: 1.4846 - val_mae: 0.9586\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.1559 - mse: 1.1559 - mae: 0.8648 - val_loss: 1.4487 - val_mse: 1.4487 - val_mae: 0.9456\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1144 - mse: 1.1144 - mae: 0.8228 - val_loss: 1.3736 - val_mse: 1.3736 - val_mae: 0.9305\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0349 - mse: 1.0349 - mae: 0.8057 - val_loss: 1.3671 - val_mse: 1.3671 - val_mae: 0.9121\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0625 - mse: 1.0625 - mae: 0.8169 - val_loss: 1.3228 - val_mse: 1.3228 - val_mae: 0.9010\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0420 - mse: 1.0420 - mae: 0.8193 - val_loss: 1.2605 - val_mse: 1.2605 - val_mae: 0.8930\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9718 - mse: 0.9718 - mae: 0.7667 - val_loss: 1.2278 - val_mse: 1.2278 - val_mae: 0.8787\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9041 - mse: 0.9041 - mae: 0.7533 - val_loss: 1.1881 - val_mse: 1.1881 - val_mae: 0.8592\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9205 - mse: 0.9205 - mae: 0.7564 - val_loss: 1.1663 - val_mse: 1.1663 - val_mae: 0.8597\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9096 - mse: 0.9096 - mae: 0.7533 - val_loss: 1.1223 - val_mse: 1.1223 - val_mae: 0.8446\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9007 - mse: 0.9007 - mae: 0.7592 - val_loss: 1.0695 - val_mse: 1.0695 - val_mae: 0.8248\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8753 - mse: 0.8753 - mae: 0.7261 - val_loss: 1.0797 - val_mse: 1.0797 - val_mae: 0.8091\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7784 - mse: 0.7784 - mae: 0.7105 - val_loss: 1.0458 - val_mse: 1.0458 - val_mae: 0.8038\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7578 - mse: 0.7578 - mae: 0.6798 - val_loss: 1.0215 - val_mse: 1.0215 - val_mae: 0.7945\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8045 - mse: 0.8045 - mae: 0.7193 - val_loss: 0.9864 - val_mse: 0.9864 - val_mae: 0.7838\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7763 - mse: 0.7763 - mae: 0.6979 - val_loss: 0.9645 - val_mse: 0.9645 - val_mae: 0.7819\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7944 - mse: 0.7944 - mae: 0.7083 - val_loss: 0.9777 - val_mse: 0.9777 - val_mae: 0.7767\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7367 - mse: 0.7367 - mae: 0.6788 - val_loss: 0.9285 - val_mse: 0.9285 - val_mae: 0.7579\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6728 - mse: 0.6728 - mae: 0.6383 - val_loss: 0.9045 - val_mse: 0.9045 - val_mae: 0.7495\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6965 - mse: 0.6965 - mae: 0.6547 - val_loss: 0.8439 - val_mse: 0.8439 - val_mae: 0.7299\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7005 - mse: 0.7005 - mae: 0.6598 - val_loss: 0.8487 - val_mse: 0.8487 - val_mae: 0.7332\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6414 - mse: 0.6414 - mae: 0.6329 - val_loss: 0.8315 - val_mse: 0.8315 - val_mae: 0.7148\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6185 - mse: 0.6185 - mae: 0.6235 - val_loss: 0.8197 - val_mse: 0.8197 - val_mae: 0.7096\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6002 - mse: 0.6002 - mae: 0.6151 - val_loss: 0.8109 - val_mse: 0.8109 - val_mae: 0.7005\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5963 - mse: 0.5963 - mae: 0.6187 - val_loss: 0.7857 - val_mse: 0.7857 - val_mae: 0.6954\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6105 - mse: 0.6105 - mae: 0.6241 - val_loss: 0.7655 - val_mse: 0.7655 - val_mae: 0.6893\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5588 - mse: 0.5588 - mae: 0.5852 - val_loss: 0.7741 - val_mse: 0.7741 - val_mae: 0.6832\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5731 - mse: 0.5731 - mae: 0.5949 - val_loss: 0.7544 - val_mse: 0.7544 - val_mae: 0.6800\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5325 - mse: 0.5325 - mae: 0.5738 - val_loss: 0.7575 - val_mse: 0.7575 - val_mae: 0.6810\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5432 - mse: 0.5432 - mae: 0.5855 - val_loss: 0.7266 - val_mse: 0.7266 - val_mae: 0.6747\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5508 - mse: 0.5508 - mae: 0.5775 - val_loss: 0.7213 - val_mse: 0.7213 - val_mae: 0.6797\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5352 - mse: 0.5352 - mae: 0.5747 - val_loss: 0.6934 - val_mse: 0.6934 - val_mae: 0.6573\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5684 - mse: 0.5684 - mae: 0.5830 - val_loss: 0.7230 - val_mse: 0.7230 - val_mae: 0.6679\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5428 - mse: 0.5428 - mae: 0.5870 - val_loss: 0.6839 - val_mse: 0.6839 - val_mae: 0.6516\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5396 - mse: 0.5396 - mae: 0.5799 - val_loss: 0.6726 - val_mse: 0.6726 - val_mae: 0.6436\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5052 - mse: 0.5052 - mae: 0.5572 - val_loss: 0.6801 - val_mse: 0.6801 - val_mae: 0.6399\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5435 - mse: 0.5435 - mae: 0.5763 - val_loss: 0.6758 - val_mse: 0.6758 - val_mae: 0.6596\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4901 - mse: 0.4901 - mae: 0.5484 - val_loss: 0.6535 - val_mse: 0.6535 - val_mae: 0.6313\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5103 - mse: 0.5103 - mae: 0.5648 - val_loss: 0.6444 - val_mse: 0.6444 - val_mae: 0.6272\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5113 - mse: 0.5113 - mae: 0.5630 - val_loss: 0.6627 - val_mse: 0.6627 - val_mae: 0.6465\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4833 - mse: 0.4833 - mae: 0.5578 - val_loss: 0.6395 - val_mse: 0.6395 - val_mae: 0.6172\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4929 - mse: 0.4929 - mae: 0.5546 - val_loss: 0.6678 - val_mse: 0.6678 - val_mae: 0.6200\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5060 - mse: 0.5060 - mae: 0.5674 - val_loss: 0.6418 - val_mse: 0.6418 - val_mae: 0.6146\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4973 - mse: 0.4973 - mae: 0.5645 - val_loss: 0.6482 - val_mse: 0.6482 - val_mae: 0.6204\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4486 - mse: 0.4486 - mae: 0.5220 - val_loss: 0.6189 - val_mse: 0.6189 - val_mae: 0.6044\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4869 - mse: 0.4869 - mae: 0.5517 - val_loss: 0.6113 - val_mse: 0.6113 - val_mae: 0.6026\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4301 - mse: 0.4301 - mae: 0.5051 - val_loss: 0.6366 - val_mse: 0.6366 - val_mae: 0.6033\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4390 - mse: 0.4390 - mae: 0.5224 - val_loss: 0.5943 - val_mse: 0.5943 - val_mae: 0.5984\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4587 - mse: 0.4587 - mae: 0.5308 - val_loss: 0.5977 - val_mse: 0.5977 - val_mae: 0.6004\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4522 - mse: 0.4522 - mae: 0.5306 - val_loss: 0.5903 - val_mse: 0.5903 - val_mae: 0.6041\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4330 - mse: 0.4330 - mae: 0.5232 - val_loss: 0.6324 - val_mse: 0.6324 - val_mae: 0.5975\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4432 - mse: 0.4432 - mae: 0.5229 - val_loss: 0.6176 - val_mse: 0.6176 - val_mae: 0.5932\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4505 - mse: 0.4505 - mae: 0.5310 - val_loss: 0.5852 - val_mse: 0.5852 - val_mae: 0.5922\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4437 - mse: 0.4437 - mae: 0.5214 - val_loss: 0.5747 - val_mse: 0.5747 - val_mae: 0.5731\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4263 - mse: 0.4263 - mae: 0.5086 - val_loss: 0.5936 - val_mse: 0.5936 - val_mae: 0.5868\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4246 - mse: 0.4246 - mae: 0.5117 - val_loss: 0.5733 - val_mse: 0.5733 - val_mae: 0.5877\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4129 - mse: 0.4129 - mae: 0.5036 - val_loss: 0.5514 - val_mse: 0.5514 - val_mae: 0.5758\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4345 - mse: 0.4345 - mae: 0.5167 - val_loss: 0.5634 - val_mse: 0.5634 - val_mae: 0.5658\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4250 - mse: 0.4250 - mae: 0.5100 - val_loss: 0.5842 - val_mse: 0.5842 - val_mae: 0.5883\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4004 - mse: 0.4004 - mae: 0.4967 - val_loss: 0.5538 - val_mse: 0.5538 - val_mae: 0.5566\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4206 - mse: 0.4206 - mae: 0.5063 - val_loss: 0.5669 - val_mse: 0.5669 - val_mae: 0.5684\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4141 - mse: 0.4141 - mae: 0.5055 - val_loss: 0.5608 - val_mse: 0.5608 - val_mae: 0.5575\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3824 - mse: 0.3824 - mae: 0.4895 - val_loss: 0.5626 - val_mse: 0.5626 - val_mae: 0.5692\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3903 - mse: 0.3903 - mae: 0.4847 - val_loss: 0.5375 - val_mse: 0.5375 - val_mae: 0.5565\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3986 - mse: 0.3986 - mae: 0.4857 - val_loss: 0.5284 - val_mse: 0.5284 - val_mae: 0.5626\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3779 - mse: 0.3779 - mae: 0.4866 - val_loss: 0.5622 - val_mse: 0.5622 - val_mae: 0.5677\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3935 - mse: 0.3935 - mae: 0.4946 - val_loss: 0.5393 - val_mse: 0.5393 - val_mae: 0.5465\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3849 - mse: 0.3849 - mae: 0.4888 - val_loss: 0.5445 - val_mse: 0.5445 - val_mae: 0.5707\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4186 - mse: 0.4186 - mae: 0.5041 - val_loss: 0.5239 - val_mse: 0.5239 - val_mae: 0.5489\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3863 - mse: 0.3863 - mae: 0.4823 - val_loss: 0.5422 - val_mse: 0.5422 - val_mae: 0.5522\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3989 - mse: 0.3989 - mae: 0.4926 - val_loss: 0.5584 - val_mse: 0.5584 - val_mae: 0.5541\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4124 - mse: 0.4124 - mae: 0.5028 - val_loss: 0.5215 - val_mse: 0.5215 - val_mae: 0.5446\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3982 - mse: 0.3982 - mae: 0.4809 - val_loss: 0.5108 - val_mse: 0.5108 - val_mae: 0.5463\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3714 - mse: 0.3714 - mae: 0.4729 - val_loss: 0.5058 - val_mse: 0.5058 - val_mae: 0.5456\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3731 - mse: 0.3731 - mae: 0.4770 - val_loss: 0.5054 - val_mse: 0.5054 - val_mae: 0.5428\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4033 - mse: 0.4033 - mae: 0.4932 - val_loss: 0.5337 - val_mse: 0.5337 - val_mae: 0.5487\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3732 - mse: 0.3732 - mae: 0.4785 - val_loss: 0.5037 - val_mse: 0.5037 - val_mae: 0.5439\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3815 - mse: 0.3815 - mae: 0.4782 - val_loss: 0.5120 - val_mse: 0.5120 - val_mae: 0.5414\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3772 - mse: 0.3772 - mae: 0.4793 - val_loss: 0.5053 - val_mse: 0.5053 - val_mae: 0.5378\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3697 - mse: 0.3697 - mae: 0.4673 - val_loss: 0.5131 - val_mse: 0.5131 - val_mae: 0.5375\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3715 - mse: 0.3715 - mae: 0.4782 - val_loss: 0.4988 - val_mse: 0.4988 - val_mae: 0.5479\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3577 - mse: 0.3577 - mae: 0.4696 - val_loss: 0.5084 - val_mse: 0.5084 - val_mae: 0.5385\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3605 - mse: 0.3605 - mae: 0.4676 - val_loss: 0.5198 - val_mse: 0.5198 - val_mae: 0.5363\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3567 - mse: 0.3567 - mae: 0.4655 - val_loss: 0.4967 - val_mse: 0.4967 - val_mae: 0.5489\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3718 - mse: 0.3718 - mae: 0.4714 - val_loss: 0.5276 - val_mse: 0.5276 - val_mae: 0.5469\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3532 - mse: 0.3532 - mae: 0.4645 - val_loss: 0.4981 - val_mse: 0.4981 - val_mae: 0.5386\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3670 - mse: 0.3670 - mae: 0.4688 - val_loss: 0.4962 - val_mse: 0.4962 - val_mae: 0.5387\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3571 - mse: 0.3571 - mae: 0.4586 - val_loss: 0.5101 - val_mse: 0.5101 - val_mae: 0.5518\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3780 - mse: 0.3780 - mae: 0.4821 - val_loss: 0.5534 - val_mse: 0.5534 - val_mae: 0.5580\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3636 - mse: 0.3636 - mae: 0.4608 - val_loss: 0.4974 - val_mse: 0.4974 - val_mae: 0.5286\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3463 - mse: 0.3463 - mae: 0.4593 - val_loss: 0.5079 - val_mse: 0.5079 - val_mae: 0.5447\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3738 - mse: 0.3738 - mae: 0.4771 - val_loss: 0.5473 - val_mse: 0.5473 - val_mae: 0.5556\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3709 - mse: 0.3709 - mae: 0.4807 - val_loss: 0.4993 - val_mse: 0.4993 - val_mae: 0.5279\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3411 - mse: 0.3411 - mae: 0.4482 - val_loss: 0.4795 - val_mse: 0.4795 - val_mae: 0.5226\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3305 - mse: 0.3305 - mae: 0.4446 - val_loss: 0.4717 - val_mse: 0.4717 - val_mae: 0.5177\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3566 - mse: 0.3566 - mae: 0.4545 - val_loss: 0.4774 - val_mse: 0.4774 - val_mae: 0.5236\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3393 - mse: 0.3393 - mae: 0.4559 - val_loss: 0.4850 - val_mse: 0.4850 - val_mae: 0.5324\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3630 - mse: 0.3630 - mae: 0.4712 - val_loss: 0.4568 - val_mse: 0.4568 - val_mae: 0.5171\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3530 - mse: 0.3530 - mae: 0.4718 - val_loss: 0.4709 - val_mse: 0.4709 - val_mae: 0.5191\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3399 - mse: 0.3399 - mae: 0.4465 - val_loss: 0.4575 - val_mse: 0.4575 - val_mae: 0.5217\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3356 - mse: 0.3356 - mae: 0.4496 - val_loss: 0.4757 - val_mse: 0.4757 - val_mae: 0.5177\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3354 - mse: 0.3354 - mae: 0.4471 - val_loss: 0.4781 - val_mse: 0.4781 - val_mae: 0.5234\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3521 - mse: 0.3521 - mae: 0.4601 - val_loss: 0.4565 - val_mse: 0.4565 - val_mae: 0.5095\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3477 - mse: 0.3477 - mae: 0.4596 - val_loss: 0.4626 - val_mse: 0.4626 - val_mae: 0.5176\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3536 - mse: 0.3536 - mae: 0.4616 - val_loss: 0.5202 - val_mse: 0.5202 - val_mae: 0.5464\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3662 - mse: 0.3662 - mae: 0.4700 - val_loss: 0.5047 - val_mse: 0.5047 - val_mae: 0.5269\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3196 - mse: 0.3196 - mae: 0.4392 - val_loss: 0.4696 - val_mse: 0.4696 - val_mae: 0.5221\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3451 - mse: 0.3451 - mae: 0.4550 - val_loss: 0.4595 - val_mse: 0.4595 - val_mae: 0.5207\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3487 - mse: 0.3487 - mae: 0.4637 - val_loss: 0.4628 - val_mse: 0.4628 - val_mae: 0.5212\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3353 - mse: 0.3353 - mae: 0.4525 - val_loss: 0.4615 - val_mse: 0.4615 - val_mae: 0.5145\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3148 - mse: 0.3148 - mae: 0.4344 - val_loss: 0.4569 - val_mse: 0.4569 - val_mae: 0.5121\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3441 - mse: 0.3441 - mae: 0.4480 - val_loss: 0.4682 - val_mse: 0.4682 - val_mae: 0.5150\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3264 - mse: 0.3264 - mae: 0.4431 - val_loss: 0.4462 - val_mse: 0.4462 - val_mae: 0.4952\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3218 - mse: 0.3218 - mae: 0.4408 - val_loss: 0.4499 - val_mse: 0.4499 - val_mae: 0.5096\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3362 - mse: 0.3362 - mae: 0.4483 - val_loss: 0.4497 - val_mse: 0.4497 - val_mae: 0.5010\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3198 - mse: 0.3198 - mae: 0.4290 - val_loss: 0.4775 - val_mse: 0.4775 - val_mae: 0.5168\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3312 - mse: 0.3312 - mae: 0.4458 - val_loss: 0.4847 - val_mse: 0.4847 - val_mae: 0.5179\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3157 - mse: 0.3157 - mae: 0.4350 - val_loss: 0.4871 - val_mse: 0.4871 - val_mae: 0.5155\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3287 - mse: 0.3287 - mae: 0.4559 - val_loss: 0.4501 - val_mse: 0.4501 - val_mae: 0.5139\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3026 - mse: 0.3026 - mae: 0.4216 - val_loss: 0.4569 - val_mse: 0.4569 - val_mae: 0.5082\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3174 - mse: 0.3174 - mae: 0.4388 - val_loss: 0.4423 - val_mse: 0.4423 - val_mae: 0.5040\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3204 - mse: 0.3204 - mae: 0.4379 - val_loss: 0.4446 - val_mse: 0.4446 - val_mae: 0.5132\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3096 - mse: 0.3096 - mae: 0.4333 - val_loss: 0.4466 - val_mse: 0.4466 - val_mae: 0.5103\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3377 - mse: 0.3377 - mae: 0.4507 - val_loss: 0.4419 - val_mse: 0.4419 - val_mae: 0.4949\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3267 - mse: 0.3267 - mae: 0.4421 - val_loss: 0.4562 - val_mse: 0.4562 - val_mae: 0.5235\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3173 - mse: 0.3173 - mae: 0.4390 - val_loss: 0.4580 - val_mse: 0.4580 - val_mae: 0.5110\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3245 - mse: 0.3245 - mae: 0.4398 - val_loss: 0.4318 - val_mse: 0.4318 - val_mae: 0.4915\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3289 - mse: 0.3289 - mae: 0.4356 - val_loss: 0.4454 - val_mse: 0.4454 - val_mae: 0.4996\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3332 - mse: 0.3332 - mae: 0.4527 - val_loss: 0.4828 - val_mse: 0.4828 - val_mae: 0.5207\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2913 - mse: 0.2913 - mae: 0.4170 - val_loss: 0.5101 - val_mse: 0.5101 - val_mae: 0.5267\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3102 - mse: 0.3102 - mae: 0.4334 - val_loss: 0.4904 - val_mse: 0.4904 - val_mae: 0.5275\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3433 - mse: 0.3433 - mae: 0.4572 - val_loss: 0.4823 - val_mse: 0.4823 - val_mae: 0.5263\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3103 - mse: 0.3103 - mae: 0.4289 - val_loss: 0.4471 - val_mse: 0.4471 - val_mae: 0.5022\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3068 - mse: 0.3068 - mae: 0.4271 - val_loss: 0.4272 - val_mse: 0.4272 - val_mae: 0.4968\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2967 - mse: 0.2967 - mae: 0.4224 - val_loss: 0.4325 - val_mse: 0.4325 - val_mae: 0.4928\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2988 - mse: 0.2988 - mae: 0.4210 - val_loss: 0.4609 - val_mse: 0.4609 - val_mae: 0.5073\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3118 - mse: 0.3118 - mae: 0.4316 - val_loss: 0.4587 - val_mse: 0.4587 - val_mae: 0.5133\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2886 - mse: 0.2886 - mae: 0.4170 - val_loss: 0.4214 - val_mse: 0.4214 - val_mae: 0.4811\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2948 - mse: 0.2948 - mae: 0.4234 - val_loss: 0.4243 - val_mse: 0.4243 - val_mae: 0.4928\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3131 - mse: 0.3131 - mae: 0.4316 - val_loss: 0.4274 - val_mse: 0.4274 - val_mae: 0.4891\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3062 - mse: 0.3062 - mae: 0.4306 - val_loss: 0.4459 - val_mse: 0.4459 - val_mae: 0.5022\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3120 - mse: 0.3120 - mae: 0.4278 - val_loss: 0.4250 - val_mse: 0.4250 - val_mae: 0.4873\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2926 - mse: 0.2926 - mae: 0.4221 - val_loss: 0.4907 - val_mse: 0.4907 - val_mae: 0.5301\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2955 - mse: 0.2955 - mae: 0.4139 - val_loss: 0.4676 - val_mse: 0.4676 - val_mae: 0.5106\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2933 - mse: 0.2933 - mae: 0.4164 - val_loss: 0.4279 - val_mse: 0.4279 - val_mae: 0.4912\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3088 - mse: 0.3088 - mae: 0.4322 - val_loss: 0.4471 - val_mse: 0.4471 - val_mae: 0.5077\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2863 - mse: 0.2863 - mae: 0.4182 - val_loss: 0.4535 - val_mse: 0.4535 - val_mae: 0.5076\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3074 - mse: 0.3074 - mae: 0.4231 - val_loss: 0.4258 - val_mse: 0.4258 - val_mae: 0.5001\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3020 - mse: 0.3020 - mae: 0.4230 - val_loss: 0.4212 - val_mse: 0.4212 - val_mae: 0.4890\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2926 - mse: 0.2926 - mae: 0.4129 - val_loss: 0.4236 - val_mse: 0.4236 - val_mae: 0.4890\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2830 - mse: 0.2830 - mae: 0.4088 - val_loss: 0.5135 - val_mse: 0.5135 - val_mae: 0.5387\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2852 - mse: 0.2852 - mae: 0.4178 - val_loss: 0.4253 - val_mse: 0.4253 - val_mae: 0.4948\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3084 - mse: 0.3084 - mae: 0.4285 - val_loss: 0.4584 - val_mse: 0.4584 - val_mae: 0.5123\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3167 - mse: 0.3167 - mae: 0.4383 - val_loss: 0.4792 - val_mse: 0.4792 - val_mae: 0.5174\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2926 - mse: 0.2926 - mae: 0.4165 - val_loss: 0.4726 - val_mse: 0.4726 - val_mae: 0.5086\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2957 - mse: 0.2957 - mae: 0.4203 - val_loss: 0.4442 - val_mse: 0.4442 - val_mae: 0.5145\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2821 - mse: 0.2821 - mae: 0.4027 - val_loss: 0.4342 - val_mse: 0.4342 - val_mae: 0.4880\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3137 - mse: 0.3137 - mae: 0.4371 - val_loss: 0.5504 - val_mse: 0.5504 - val_mae: 0.5490\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2885 - mse: 0.2885 - mae: 0.4109 - val_loss: 0.4727 - val_mse: 0.4727 - val_mae: 0.5132\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2910 - mse: 0.2910 - mae: 0.4175 - val_loss: 0.4392 - val_mse: 0.4392 - val_mae: 0.5005\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3021 - mse: 0.3021 - mae: 0.4267 - val_loss: 0.4447 - val_mse: 0.4447 - val_mae: 0.4971\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2704 - mse: 0.2704 - mae: 0.3988 - val_loss: 0.4488 - val_mse: 0.4488 - val_mae: 0.5051\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2833 - mse: 0.2833 - mae: 0.4140 - val_loss: 0.4110 - val_mse: 0.4110 - val_mae: 0.4852\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2785 - mse: 0.2785 - mae: 0.4053 - val_loss: 0.4484 - val_mse: 0.4484 - val_mae: 0.4943\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2847 - mse: 0.2847 - mae: 0.4128 - val_loss: 0.4689 - val_mse: 0.4689 - val_mae: 0.5049\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2946 - mse: 0.2946 - mae: 0.4253 - val_loss: 0.4425 - val_mse: 0.4425 - val_mae: 0.5000\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2646 - mse: 0.2646 - mae: 0.3984 - val_loss: 0.4579 - val_mse: 0.4579 - val_mae: 0.5020\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2799 - mse: 0.2799 - mae: 0.4092 - val_loss: 0.4447 - val_mse: 0.4447 - val_mae: 0.4909\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2826 - mse: 0.2826 - mae: 0.4094 - val_loss: 0.5121 - val_mse: 0.5121 - val_mae: 0.5261\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2844 - mse: 0.2844 - mae: 0.4114 - val_loss: 0.4318 - val_mse: 0.4318 - val_mae: 0.4915\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2919 - mse: 0.2919 - mae: 0.4190 - val_loss: 0.4324 - val_mse: 0.4324 - val_mae: 0.4869\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2782 - mse: 0.2782 - mae: 0.4066 - val_loss: 0.4224 - val_mse: 0.4224 - val_mae: 0.4812\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2584 - mse: 0.2584 - mae: 0.3896 - val_loss: 0.4048 - val_mse: 0.4048 - val_mae: 0.4703\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2834 - mse: 0.2834 - mae: 0.4131 - val_loss: 0.4223 - val_mse: 0.4223 - val_mae: 0.4903\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2842 - mse: 0.2842 - mae: 0.4053 - val_loss: 0.4074 - val_mse: 0.4074 - val_mae: 0.4749\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2771 - mse: 0.2771 - mae: 0.4041 - val_loss: 0.4470 - val_mse: 0.4470 - val_mae: 0.4939\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3007 - mse: 0.3007 - mae: 0.4244 - val_loss: 0.5006 - val_mse: 0.5006 - val_mae: 0.5283\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2822 - mse: 0.2822 - mae: 0.4094 - val_loss: 0.4101 - val_mse: 0.4101 - val_mae: 0.4906\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2737 - mse: 0.2737 - mae: 0.4035 - val_loss: 0.4119 - val_mse: 0.4119 - val_mae: 0.4806\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2751 - mse: 0.2751 - mae: 0.4017 - val_loss: 0.4050 - val_mse: 0.4050 - val_mae: 0.4902\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2850 - mse: 0.2850 - mae: 0.4029 - val_loss: 0.4245 - val_mse: 0.4245 - val_mae: 0.4872\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2728 - mse: 0.2728 - mae: 0.4003 - val_loss: 0.4080 - val_mse: 0.4080 - val_mae: 0.4821\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2676 - mse: 0.2676 - mae: 0.3957 - val_loss: 0.4312 - val_mse: 0.4312 - val_mae: 0.4908\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2900 - mse: 0.2900 - mae: 0.4196 - val_loss: 0.4015 - val_mse: 0.4015 - val_mae: 0.4767\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2723 - mse: 0.2723 - mae: 0.3997 - val_loss: 0.4278 - val_mse: 0.4278 - val_mae: 0.5026\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2650 - mse: 0.2650 - mae: 0.3995 - val_loss: 0.4036 - val_mse: 0.4036 - val_mae: 0.4763\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2756 - mse: 0.2756 - mae: 0.4088 - val_loss: 0.4135 - val_mse: 0.4135 - val_mae: 0.4723\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2567 - mse: 0.2567 - mae: 0.3837 - val_loss: 0.4125 - val_mse: 0.4125 - val_mae: 0.4986\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2772 - mse: 0.2772 - mae: 0.4077 - val_loss: 0.4199 - val_mse: 0.4199 - val_mae: 0.4991\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2834 - mse: 0.2834 - mae: 0.4080 - val_loss: 0.4230 - val_mse: 0.4230 - val_mae: 0.5018\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2679 - mse: 0.2679 - mae: 0.4052 - val_loss: 0.4337 - val_mse: 0.4337 - val_mae: 0.4939\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2825 - mse: 0.2825 - mae: 0.4055 - val_loss: 0.4365 - val_mse: 0.4365 - val_mae: 0.5096\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2781 - mse: 0.2781 - mae: 0.4112 - val_loss: 0.4327 - val_mse: 0.4327 - val_mae: 0.4845\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2821 - mse: 0.2821 - mae: 0.4121 - val_loss: 0.4586 - val_mse: 0.4586 - val_mae: 0.5143\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2637 - mse: 0.2637 - mae: 0.3944 - val_loss: 0.4493 - val_mse: 0.4493 - val_mae: 0.4969\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2769 - mse: 0.2769 - mae: 0.4065 - val_loss: 0.4360 - val_mse: 0.4360 - val_mae: 0.5032\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2623 - mse: 0.2623 - mae: 0.3966 - val_loss: 0.4856 - val_mse: 0.4856 - val_mae: 0.5218\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2824 - mse: 0.2824 - mae: 0.4199 - val_loss: 0.4143 - val_mse: 0.4143 - val_mae: 0.4874\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2545 - mse: 0.2545 - mae: 0.3817 - val_loss: 0.4305 - val_mse: 0.4305 - val_mae: 0.4943\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2555 - mse: 0.2555 - mae: 0.3865 - val_loss: 0.4361 - val_mse: 0.4361 - val_mae: 0.4955\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2643 - mse: 0.2643 - mae: 0.3996 - val_loss: 0.4811 - val_mse: 0.4811 - val_mae: 0.5074\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2896 - mse: 0.2896 - mae: 0.4137 - val_loss: 0.4389 - val_mse: 0.4389 - val_mae: 0.4951\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2649 - mse: 0.2649 - mae: 0.3925 - val_loss: 0.4422 - val_mse: 0.4422 - val_mae: 0.5035\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2591 - mse: 0.2591 - mae: 0.3909 - val_loss: 0.4699 - val_mse: 0.4699 - val_mae: 0.5090\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2751 - mse: 0.2751 - mae: 0.4047 - val_loss: 0.4354 - val_mse: 0.4354 - val_mae: 0.4895\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2529 - mse: 0.2529 - mae: 0.3889 - val_loss: 0.4346 - val_mse: 0.4346 - val_mae: 0.4888\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2483 - mse: 0.2483 - mae: 0.3803 - val_loss: 0.4216 - val_mse: 0.4216 - val_mae: 0.4786\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2503 - mse: 0.2503 - mae: 0.3815 - val_loss: 0.4371 - val_mse: 0.4371 - val_mae: 0.4895\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2567 - mse: 0.2567 - mae: 0.3891 - val_loss: 0.4264 - val_mse: 0.4264 - val_mae: 0.4833\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2477 - mse: 0.2477 - mae: 0.3820 - val_loss: 0.4254 - val_mse: 0.4254 - val_mae: 0.4890\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2582 - mse: 0.2582 - mae: 0.3935 - val_loss: 0.4089 - val_mse: 0.4089 - val_mae: 0.4881\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2640 - mse: 0.2640 - mae: 0.3913 - val_loss: 0.4192 - val_mse: 0.4192 - val_mae: 0.4724\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2477 - mse: 0.2477 - mae: 0.3824 - val_loss: 0.4945 - val_mse: 0.4945 - val_mae: 0.5202\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3186 - mse: 0.3186 - mae: 0.4366 - val_loss: 0.4252 - val_mse: 0.4252 - val_mae: 0.4886\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2757 - mse: 0.2757 - mae: 0.4144 - val_loss: 0.4073 - val_mse: 0.4073 - val_mae: 0.4747\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2703 - mse: 0.2703 - mae: 0.4004 - val_loss: 0.4341 - val_mse: 0.4341 - val_mae: 0.4896\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2706 - mse: 0.2706 - mae: 0.3968 - val_loss: 0.4068 - val_mse: 0.4068 - val_mae: 0.4834\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2759 - mse: 0.2759 - mae: 0.4077 - val_loss: 0.4171 - val_mse: 0.4171 - val_mae: 0.4752\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2487 - mse: 0.2487 - mae: 0.3819 - val_loss: 0.4333 - val_mse: 0.4333 - val_mae: 0.4802\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2589 - mse: 0.2589 - mae: 0.3835 - val_loss: 0.4069 - val_mse: 0.4069 - val_mae: 0.4815\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2538 - mse: 0.2538 - mae: 0.3841 - val_loss: 0.4850 - val_mse: 0.4850 - val_mae: 0.5162\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2619 - mse: 0.2619 - mae: 0.3895 - val_loss: 0.4708 - val_mse: 0.4708 - val_mae: 0.5022\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2491 - mse: 0.2491 - mae: 0.3736 - val_loss: 0.4569 - val_mse: 0.4569 - val_mae: 0.5032\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2711 - mse: 0.2711 - mae: 0.4068 - val_loss: 0.4125 - val_mse: 0.4125 - val_mae: 0.4790\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2469 - mse: 0.2469 - mae: 0.3772 - val_loss: 0.4152 - val_mse: 0.4152 - val_mae: 0.4919\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2515 - mse: 0.2515 - mae: 0.3853 - val_loss: 0.4565 - val_mse: 0.4565 - val_mae: 0.4954\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2495 - mse: 0.2495 - mae: 0.3826 - val_loss: 0.4368 - val_mse: 0.4368 - val_mae: 0.4986\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2444 - mse: 0.2444 - mae: 0.3777 - val_loss: 0.4093 - val_mse: 0.4093 - val_mae: 0.4775\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2824 - mse: 0.2824 - mae: 0.4079 - val_loss: 0.4434 - val_mse: 0.4434 - val_mae: 0.5000\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2889 - mse: 0.2889 - mae: 0.4182 - val_loss: 0.4248 - val_mse: 0.4248 - val_mae: 0.4790\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2546 - mse: 0.2546 - mae: 0.3893 - val_loss: 0.4025 - val_mse: 0.4025 - val_mae: 0.4776\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2489 - mse: 0.2489 - mae: 0.3885 - val_loss: 0.4340 - val_mse: 0.4340 - val_mae: 0.4881\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2505 - mse: 0.2505 - mae: 0.3828 - val_loss: 0.4143 - val_mse: 0.4143 - val_mae: 0.4767\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2417 - mse: 0.2417 - mae: 0.3808 - val_loss: 0.4136 - val_mse: 0.4136 - val_mae: 0.4831\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2528 - mse: 0.2528 - mae: 0.3831 - val_loss: 0.4243 - val_mse: 0.4243 - val_mae: 0.4817\n",
            "Epoch 252/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2519 - mse: 0.2519 - mae: 0.3839 - val_loss: 0.4213 - val_mse: 0.4213 - val_mae: 0.4977\n",
            "Epoch 253/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2638 - mse: 0.2638 - mae: 0.3952 - val_loss: 0.4498 - val_mse: 0.4498 - val_mae: 0.4890\n",
            "Epoch 254/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2733 - mse: 0.2733 - mae: 0.4020 - val_loss: 0.4515 - val_mse: 0.4515 - val_mae: 0.5012\n",
            "Epoch 255/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2438 - mse: 0.2438 - mae: 0.3743 - val_loss: 0.4014 - val_mse: 0.4014 - val_mae: 0.4700\n",
            "Epoch 256/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2731 - mse: 0.2731 - mae: 0.3998 - val_loss: 0.4102 - val_mse: 0.4102 - val_mae: 0.4948\n",
            "Epoch 257/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2668 - mse: 0.2668 - mae: 0.3961 - val_loss: 0.4123 - val_mse: 0.4123 - val_mae: 0.4730\n",
            "Epoch 258/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2598 - mse: 0.2598 - mae: 0.3866 - val_loss: 0.4322 - val_mse: 0.4322 - val_mae: 0.4926\n",
            "Epoch 259/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2653 - mse: 0.2653 - mae: 0.3917 - val_loss: 0.4908 - val_mse: 0.4908 - val_mae: 0.5214\n",
            "Epoch 260/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2473 - mse: 0.2473 - mae: 0.3844 - val_loss: 0.4144 - val_mse: 0.4144 - val_mae: 0.4768\n",
            "Epoch 261/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2378 - mse: 0.2378 - mae: 0.3744 - val_loss: 0.4184 - val_mse: 0.4184 - val_mae: 0.4762\n",
            "Epoch 262/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2640 - mse: 0.2640 - mae: 0.3999 - val_loss: 0.3913 - val_mse: 0.3913 - val_mae: 0.4709\n",
            "Epoch 263/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2546 - mse: 0.2546 - mae: 0.3889 - val_loss: 0.4367 - val_mse: 0.4367 - val_mae: 0.4830\n",
            "Epoch 264/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2417 - mse: 0.2417 - mae: 0.3779 - val_loss: 0.4064 - val_mse: 0.4064 - val_mae: 0.4734\n",
            "Epoch 265/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2514 - mse: 0.2514 - mae: 0.3819 - val_loss: 0.4029 - val_mse: 0.4029 - val_mae: 0.4692\n",
            "Epoch 266/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2439 - mse: 0.2439 - mae: 0.3842 - val_loss: 0.4323 - val_mse: 0.4323 - val_mae: 0.4910\n",
            "Epoch 267/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2392 - mse: 0.2392 - mae: 0.3757 - val_loss: 0.4311 - val_mse: 0.4311 - val_mae: 0.4770\n",
            "Epoch 268/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2429 - mse: 0.2429 - mae: 0.3735 - val_loss: 0.4097 - val_mse: 0.4097 - val_mae: 0.4741\n",
            "Epoch 269/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2434 - mse: 0.2434 - mae: 0.3761 - val_loss: 0.4053 - val_mse: 0.4053 - val_mae: 0.4877\n",
            "Epoch 270/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2421 - mse: 0.2421 - mae: 0.3831 - val_loss: 0.4187 - val_mse: 0.4187 - val_mae: 0.4810\n",
            "Epoch 271/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2446 - mse: 0.2446 - mae: 0.3785 - val_loss: 0.3997 - val_mse: 0.3997 - val_mae: 0.4718\n",
            "Epoch 272/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2491 - mse: 0.2491 - mae: 0.3884 - val_loss: 0.3923 - val_mse: 0.3923 - val_mae: 0.4643\n",
            "Epoch 273/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2335 - mse: 0.2335 - mae: 0.3666 - val_loss: 0.4230 - val_mse: 0.4230 - val_mae: 0.4859\n",
            "Epoch 274/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2346 - mse: 0.2346 - mae: 0.3709 - val_loss: 0.4173 - val_mse: 0.4173 - val_mae: 0.4757\n",
            "Epoch 275/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2352 - mse: 0.2352 - mae: 0.3700 - val_loss: 0.4068 - val_mse: 0.4068 - val_mae: 0.4751\n",
            "Epoch 276/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2418 - mse: 0.2418 - mae: 0.3794 - val_loss: 0.4484 - val_mse: 0.4484 - val_mae: 0.4964\n",
            "Epoch 277/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2648 - mse: 0.2648 - mae: 0.3889 - val_loss: 0.4654 - val_mse: 0.4654 - val_mae: 0.5132\n",
            "Epoch 278/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2417 - mse: 0.2417 - mae: 0.3800 - val_loss: 0.4618 - val_mse: 0.4618 - val_mae: 0.5115\n",
            "Epoch 279/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2374 - mse: 0.2374 - mae: 0.3705 - val_loss: 0.4283 - val_mse: 0.4283 - val_mae: 0.4742\n",
            "Epoch 280/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2362 - mse: 0.2362 - mae: 0.3756 - val_loss: 0.4017 - val_mse: 0.4017 - val_mae: 0.4708\n",
            "Epoch 281/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2325 - mse: 0.2325 - mae: 0.3679 - val_loss: 0.4232 - val_mse: 0.4232 - val_mae: 0.4845\n",
            "Epoch 282/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2278 - mse: 0.2278 - mae: 0.3627 - val_loss: 0.4074 - val_mse: 0.4074 - val_mae: 0.4662\n",
            "Epoch 283/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2426 - mse: 0.2426 - mae: 0.3769 - val_loss: 0.4068 - val_mse: 0.4068 - val_mae: 0.4832\n",
            "Epoch 284/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2529 - mse: 0.2529 - mae: 0.3885 - val_loss: 0.4102 - val_mse: 0.4102 - val_mae: 0.4771\n",
            "Epoch 285/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2646 - mse: 0.2646 - mae: 0.3941 - val_loss: 0.4027 - val_mse: 0.4027 - val_mae: 0.4625\n",
            "Epoch 286/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2520 - mse: 0.2520 - mae: 0.3848 - val_loss: 0.5105 - val_mse: 0.5105 - val_mae: 0.5286\n",
            "Epoch 287/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2818 - mse: 0.2818 - mae: 0.4162 - val_loss: 0.3876 - val_mse: 0.3876 - val_mae: 0.4632\n",
            "Epoch 288/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2405 - mse: 0.2405 - mae: 0.3858 - val_loss: 0.4077 - val_mse: 0.4077 - val_mae: 0.4803\n",
            "Epoch 289/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2377 - mse: 0.2377 - mae: 0.3770 - val_loss: 0.4088 - val_mse: 0.4088 - val_mae: 0.4702\n",
            "Epoch 290/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2537 - mse: 0.2537 - mae: 0.3889 - val_loss: 0.4066 - val_mse: 0.4066 - val_mae: 0.4716\n",
            "Epoch 291/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2409 - mse: 0.2409 - mae: 0.3745 - val_loss: 0.3942 - val_mse: 0.3942 - val_mae: 0.4716\n",
            "Epoch 292/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2515 - mse: 0.2515 - mae: 0.3901 - val_loss: 0.4133 - val_mse: 0.4133 - val_mae: 0.4770\n",
            "Epoch 293/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2369 - mse: 0.2369 - mae: 0.3744 - val_loss: 0.4025 - val_mse: 0.4025 - val_mae: 0.4707\n",
            "Epoch 294/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2430 - mse: 0.2430 - mae: 0.3740 - val_loss: 0.4042 - val_mse: 0.4042 - val_mae: 0.4827\n",
            "Epoch 295/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2271 - mse: 0.2271 - mae: 0.3668 - val_loss: 0.4256 - val_mse: 0.4256 - val_mae: 0.4815\n",
            "Epoch 296/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2433 - mse: 0.2433 - mae: 0.3832 - val_loss: 0.4219 - val_mse: 0.4219 - val_mae: 0.4761\n",
            "Epoch 297/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2242 - mse: 0.2242 - mae: 0.3616 - val_loss: 0.4172 - val_mse: 0.4172 - val_mae: 0.4701\n",
            "Epoch 298/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2381 - mse: 0.2381 - mae: 0.3676 - val_loss: 0.4336 - val_mse: 0.4336 - val_mae: 0.4867\n",
            "Epoch 299/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2502 - mse: 0.2502 - mae: 0.3842 - val_loss: 0.4032 - val_mse: 0.4032 - val_mae: 0.4787\n",
            "Epoch 300/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2383 - mse: 0.2383 - mae: 0.3839 - val_loss: 0.4384 - val_mse: 0.4384 - val_mae: 0.4919\n",
            "Epoch 301/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2376 - mse: 0.2376 - mae: 0.3699 - val_loss: 0.4123 - val_mse: 0.4123 - val_mae: 0.4834\n",
            "Epoch 302/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2482 - mse: 0.2482 - mae: 0.3832 - val_loss: 0.4089 - val_mse: 0.4089 - val_mae: 0.4772\n",
            "Epoch 303/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2400 - mse: 0.2400 - mae: 0.3777 - val_loss: 0.4027 - val_mse: 0.4027 - val_mae: 0.4694\n",
            "Epoch 304/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2144 - mse: 0.2144 - mae: 0.3443 - val_loss: 0.4163 - val_mse: 0.4163 - val_mae: 0.4799\n",
            "Epoch 305/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2323 - mse: 0.2323 - mae: 0.3646 - val_loss: 0.4053 - val_mse: 0.4053 - val_mae: 0.4778\n",
            "Epoch 306/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2538 - mse: 0.2538 - mae: 0.3906 - val_loss: 0.4241 - val_mse: 0.4241 - val_mae: 0.4897\n",
            "Epoch 307/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2439 - mse: 0.2439 - mae: 0.3785 - val_loss: 0.4222 - val_mse: 0.4222 - val_mae: 0.4834\n",
            "Epoch 308/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2310 - mse: 0.2310 - mae: 0.3719 - val_loss: 0.4240 - val_mse: 0.4240 - val_mae: 0.4914\n",
            "Epoch 309/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2234 - mse: 0.2234 - mae: 0.3576 - val_loss: 0.4171 - val_mse: 0.4171 - val_mae: 0.4869\n",
            "Epoch 310/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2221 - mse: 0.2221 - mae: 0.3673 - val_loss: 0.3987 - val_mse: 0.3987 - val_mae: 0.4685\n",
            "Epoch 311/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2506 - mse: 0.2506 - mae: 0.3891 - val_loss: 0.4132 - val_mse: 0.4132 - val_mae: 0.4706\n",
            "Epoch 312/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2260 - mse: 0.2260 - mae: 0.3626 - val_loss: 0.4156 - val_mse: 0.4156 - val_mae: 0.4743\n",
            "Epoch 313/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2253 - mse: 0.2253 - mae: 0.3652 - val_loss: 0.4147 - val_mse: 0.4147 - val_mae: 0.4797\n",
            "Epoch 314/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2387 - mse: 0.2387 - mae: 0.3782 - val_loss: 0.4169 - val_mse: 0.4169 - val_mae: 0.4842\n",
            "Epoch 315/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2318 - mse: 0.2318 - mae: 0.3727 - val_loss: 0.4173 - val_mse: 0.4173 - val_mae: 0.4746\n",
            "Epoch 316/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2373 - mse: 0.2373 - mae: 0.3777 - val_loss: 0.4227 - val_mse: 0.4227 - val_mae: 0.4883\n",
            "Epoch 317/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2547 - mse: 0.2547 - mae: 0.3935 - val_loss: 0.4430 - val_mse: 0.4430 - val_mae: 0.4952\n",
            "Epoch 318/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2679 - mse: 0.2679 - mae: 0.4028 - val_loss: 0.3983 - val_mse: 0.3983 - val_mae: 0.4835\n",
            "Epoch 319/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2538 - mse: 0.2538 - mae: 0.3879 - val_loss: 0.3924 - val_mse: 0.3924 - val_mae: 0.4663\n",
            "Epoch 320/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2279 - mse: 0.2279 - mae: 0.3610 - val_loss: 0.4314 - val_mse: 0.4314 - val_mae: 0.4972\n",
            "Epoch 321/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2392 - mse: 0.2392 - mae: 0.3734 - val_loss: 0.4288 - val_mse: 0.4288 - val_mae: 0.4812\n",
            "Epoch 322/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2166 - mse: 0.2166 - mae: 0.3500 - val_loss: 0.4128 - val_mse: 0.4128 - val_mae: 0.4822\n",
            "Epoch 323/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2252 - mse: 0.2252 - mae: 0.3685 - val_loss: 0.4179 - val_mse: 0.4179 - val_mae: 0.4761\n",
            "Epoch 324/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2170 - mse: 0.2170 - mae: 0.3571 - val_loss: 0.4017 - val_mse: 0.4017 - val_mae: 0.4686\n",
            "Epoch 325/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2191 - mse: 0.2191 - mae: 0.3561 - val_loss: 0.3892 - val_mse: 0.3892 - val_mae: 0.4652\n",
            "Epoch 326/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2497 - mse: 0.2497 - mae: 0.3866 - val_loss: 0.4129 - val_mse: 0.4129 - val_mae: 0.4722\n",
            "Epoch 327/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2370 - mse: 0.2370 - mae: 0.3757 - val_loss: 0.4261 - val_mse: 0.4261 - val_mae: 0.4961\n",
            "Epoch 328/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2226 - mse: 0.2226 - mae: 0.3596 - val_loss: 0.3998 - val_mse: 0.3998 - val_mae: 0.4662\n",
            "Epoch 329/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2446 - mse: 0.2446 - mae: 0.3829 - val_loss: 0.4014 - val_mse: 0.4014 - val_mae: 0.4698\n",
            "Epoch 330/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2259 - mse: 0.2259 - mae: 0.3703 - val_loss: 0.4103 - val_mse: 0.4103 - val_mae: 0.4841\n",
            "Epoch 331/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2171 - mse: 0.2171 - mae: 0.3593 - val_loss: 0.4184 - val_mse: 0.4184 - val_mae: 0.4859\n",
            "Epoch 332/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2343 - mse: 0.2343 - mae: 0.3644 - val_loss: 0.4301 - val_mse: 0.4301 - val_mae: 0.4969\n",
            "Epoch 333/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2236 - mse: 0.2236 - mae: 0.3647 - val_loss: 0.4526 - val_mse: 0.4526 - val_mae: 0.4990\n",
            "Epoch 334/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2540 - mse: 0.2540 - mae: 0.3807 - val_loss: 0.3965 - val_mse: 0.3965 - val_mae: 0.4733\n",
            "Epoch 335/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2401 - mse: 0.2401 - mae: 0.3770 - val_loss: 0.4226 - val_mse: 0.4226 - val_mae: 0.4829\n",
            "Epoch 336/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2114 - mse: 0.2114 - mae: 0.3538 - val_loss: 0.4068 - val_mse: 0.4068 - val_mae: 0.4726\n",
            "Epoch 337/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2216 - mse: 0.2216 - mae: 0.3614 - val_loss: 0.3953 - val_mse: 0.3953 - val_mae: 0.4646\n",
            "Epoch 338/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2021 - mse: 0.2021 - mae: 0.3431 - val_loss: 0.4005 - val_mse: 0.4005 - val_mae: 0.4744\n",
            "Epoch 339/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2203 - mse: 0.2203 - mae: 0.3555 - val_loss: 0.4138 - val_mse: 0.4138 - val_mae: 0.4718\n",
            "Epoch 340/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2238 - mse: 0.2238 - mae: 0.3531 - val_loss: 0.4382 - val_mse: 0.4382 - val_mae: 0.4866\n",
            "Epoch 341/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2256 - mse: 0.2256 - mae: 0.3671 - val_loss: 0.4635 - val_mse: 0.4635 - val_mae: 0.5036\n",
            "Epoch 342/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2098 - mse: 0.2098 - mae: 0.3548 - val_loss: 0.4221 - val_mse: 0.4221 - val_mae: 0.4827\n",
            "Epoch 343/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2207 - mse: 0.2207 - mae: 0.3599 - val_loss: 0.4032 - val_mse: 0.4032 - val_mae: 0.4674\n",
            "Epoch 344/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2216 - mse: 0.2216 - mae: 0.3600 - val_loss: 0.4345 - val_mse: 0.4345 - val_mae: 0.4858\n",
            "Epoch 345/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2147 - mse: 0.2147 - mae: 0.3561 - val_loss: 0.3945 - val_mse: 0.3945 - val_mae: 0.4732\n",
            "Epoch 346/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2126 - mse: 0.2126 - mae: 0.3576 - val_loss: 0.3994 - val_mse: 0.3994 - val_mae: 0.4649\n",
            "Epoch 347/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2231 - mse: 0.2231 - mae: 0.3630 - val_loss: 0.3963 - val_mse: 0.3963 - val_mae: 0.4721\n",
            "Epoch 348/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2278 - mse: 0.2278 - mae: 0.3661 - val_loss: 0.4097 - val_mse: 0.4097 - val_mae: 0.4720\n",
            "Epoch 349/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2445 - mse: 0.2445 - mae: 0.3809 - val_loss: 0.3915 - val_mse: 0.3915 - val_mae: 0.4713\n",
            "Epoch 350/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2286 - mse: 0.2286 - mae: 0.3642 - val_loss: 0.4045 - val_mse: 0.4045 - val_mae: 0.4716\n",
            "Epoch 351/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2431 - mse: 0.2431 - mae: 0.3784 - val_loss: 0.4153 - val_mse: 0.4153 - val_mae: 0.4766\n",
            "Epoch 352/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2336 - mse: 0.2336 - mae: 0.3770 - val_loss: 0.4202 - val_mse: 0.4202 - val_mae: 0.4880\n",
            "Epoch 353/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2457 - mse: 0.2457 - mae: 0.3861 - val_loss: 0.4079 - val_mse: 0.4079 - val_mae: 0.4764\n",
            "Epoch 354/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2353 - mse: 0.2353 - mae: 0.3759 - val_loss: 0.4626 - val_mse: 0.4626 - val_mae: 0.5053\n",
            "Epoch 355/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2414 - mse: 0.2414 - mae: 0.3875 - val_loss: 0.4024 - val_mse: 0.4024 - val_mae: 0.4723\n",
            "Epoch 356/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2134 - mse: 0.2134 - mae: 0.3510 - val_loss: 0.4019 - val_mse: 0.4019 - val_mae: 0.4734\n",
            "Epoch 357/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2028 - mse: 0.2028 - mae: 0.3470 - val_loss: 0.4144 - val_mse: 0.4144 - val_mae: 0.4794\n",
            "Epoch 358/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2223 - mse: 0.2223 - mae: 0.3624 - val_loss: 0.4158 - val_mse: 0.4158 - val_mae: 0.4739\n",
            "Epoch 359/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2028 - mse: 0.2028 - mae: 0.3432 - val_loss: 0.4373 - val_mse: 0.4373 - val_mae: 0.4892\n",
            "Epoch 360/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2224 - mse: 0.2224 - mae: 0.3635 - val_loss: 0.4290 - val_mse: 0.4290 - val_mae: 0.4922\n",
            "Epoch 361/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2387 - mse: 0.2387 - mae: 0.3788 - val_loss: 0.4124 - val_mse: 0.4124 - val_mae: 0.4743\n",
            "Epoch 362/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2175 - mse: 0.2175 - mae: 0.3576 - val_loss: 0.4002 - val_mse: 0.4002 - val_mae: 0.4727\n",
            "Epoch 363/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2501 - mse: 0.2501 - mae: 0.3826 - val_loss: 0.4772 - val_mse: 0.4772 - val_mae: 0.5193\n",
            "Epoch 364/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2254 - mse: 0.2254 - mae: 0.3725 - val_loss: 0.4080 - val_mse: 0.4080 - val_mae: 0.4776\n",
            "Epoch 365/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2178 - mse: 0.2178 - mae: 0.3568 - val_loss: 0.4346 - val_mse: 0.4346 - val_mae: 0.4888\n",
            "Epoch 366/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2032 - mse: 0.2032 - mae: 0.3476 - val_loss: 0.4520 - val_mse: 0.4520 - val_mae: 0.5013\n",
            "Epoch 367/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2263 - mse: 0.2263 - mae: 0.3631 - val_loss: 0.4075 - val_mse: 0.4075 - val_mae: 0.4831\n",
            "Epoch 368/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2164 - mse: 0.2164 - mae: 0.3626 - val_loss: 0.4183 - val_mse: 0.4183 - val_mae: 0.4826\n",
            "Epoch 369/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2492 - mse: 0.2492 - mae: 0.3774 - val_loss: 0.4189 - val_mse: 0.4189 - val_mae: 0.4841\n",
            "Epoch 370/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2046 - mse: 0.2046 - mae: 0.3405 - val_loss: 0.4142 - val_mse: 0.4142 - val_mae: 0.4784\n",
            "Epoch 371/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2069 - mse: 0.2069 - mae: 0.3472 - val_loss: 0.4533 - val_mse: 0.4533 - val_mae: 0.5059\n",
            "Epoch 372/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2200 - mse: 0.2200 - mae: 0.3628 - val_loss: 0.4137 - val_mse: 0.4137 - val_mae: 0.4763\n",
            "Epoch 373/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2128 - mse: 0.2128 - mae: 0.3503 - val_loss: 0.4028 - val_mse: 0.4028 - val_mae: 0.4731\n",
            "Epoch 374/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2215 - mse: 0.2215 - mae: 0.3635 - val_loss: 0.4080 - val_mse: 0.4080 - val_mae: 0.4855\n",
            "Epoch 375/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2084 - mse: 0.2084 - mae: 0.3429 - val_loss: 0.4445 - val_mse: 0.4445 - val_mae: 0.5020\n",
            "Epoch 376/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2295 - mse: 0.2295 - mae: 0.3773 - val_loss: 0.4139 - val_mse: 0.4139 - val_mae: 0.4741\n",
            "Epoch 377/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2312 - mse: 0.2312 - mae: 0.3639 - val_loss: 0.4518 - val_mse: 0.4518 - val_mae: 0.5041\n",
            "Epoch 378/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2189 - mse: 0.2189 - mae: 0.3613 - val_loss: 0.4000 - val_mse: 0.4000 - val_mae: 0.4656\n",
            "Epoch 379/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2315 - mse: 0.2315 - mae: 0.3609 - val_loss: 0.4934 - val_mse: 0.4934 - val_mae: 0.5334\n",
            "Epoch 380/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2277 - mse: 0.2277 - mae: 0.3760 - val_loss: 0.4955 - val_mse: 0.4955 - val_mae: 0.5387\n",
            "Epoch 381/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2183 - mse: 0.2183 - mae: 0.3617 - val_loss: 0.4202 - val_mse: 0.4202 - val_mae: 0.4904\n",
            "Epoch 382/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2252 - mse: 0.2252 - mae: 0.3680 - val_loss: 0.4028 - val_mse: 0.4028 - val_mae: 0.4686\n",
            "Epoch 383/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2130 - mse: 0.2130 - mae: 0.3553 - val_loss: 0.4559 - val_mse: 0.4559 - val_mae: 0.5177\n",
            "Epoch 384/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2325 - mse: 0.2325 - mae: 0.3647 - val_loss: 0.4092 - val_mse: 0.4092 - val_mae: 0.4841\n",
            "Epoch 385/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2021 - mse: 0.2021 - mae: 0.3422 - val_loss: 0.4242 - val_mse: 0.4242 - val_mae: 0.4872\n",
            "Epoch 386/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2165 - mse: 0.2165 - mae: 0.3574 - val_loss: 0.4101 - val_mse: 0.4101 - val_mae: 0.4908\n",
            "Epoch 387/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2207 - mse: 0.2207 - mae: 0.3628 - val_loss: 0.4703 - val_mse: 0.4703 - val_mae: 0.5103\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.5849 - mse: 0.5849 - mae: 0.5632\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 15ms/step - loss: 12.6566 - mse: 12.6566 - mae: 3.2778 - val_loss: 7.2551 - val_mse: 7.2551 - val_mae: 2.2883\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 4.9676 - mse: 4.9676 - mae: 1.7740 - val_loss: 4.5222 - val_mse: 4.5222 - val_mae: 1.6514\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 3.4575 - mse: 3.4575 - mae: 1.4844 - val_loss: 3.4812 - val_mse: 3.4812 - val_mae: 1.4711\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.9407 - mse: 2.9407 - mae: 1.3875 - val_loss: 3.1069 - val_mse: 3.1069 - val_mae: 1.3580\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.6473 - mse: 2.6473 - mae: 1.3104 - val_loss: 2.9152 - val_mse: 2.9152 - val_mae: 1.3223\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.4899 - mse: 2.4899 - mae: 1.2579 - val_loss: 2.7177 - val_mse: 2.7177 - val_mae: 1.2697\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.3651 - mse: 2.3651 - mae: 1.2149 - val_loss: 2.5360 - val_mse: 2.5360 - val_mae: 1.2288\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.0934 - mse: 2.0934 - mae: 1.1629 - val_loss: 2.4128 - val_mse: 2.4128 - val_mae: 1.1941\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.0365 - mse: 2.0365 - mae: 1.1189 - val_loss: 2.3082 - val_mse: 2.3082 - val_mae: 1.1697\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.9202 - mse: 1.9202 - mae: 1.1035 - val_loss: 2.1471 - val_mse: 2.1471 - val_mae: 1.1255\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.9425 - mse: 1.9425 - mae: 1.1025 - val_loss: 2.0429 - val_mse: 2.0429 - val_mae: 1.0989\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.7792 - mse: 1.7792 - mae: 1.0694 - val_loss: 1.9547 - val_mse: 1.9547 - val_mae: 1.0754\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.6532 - mse: 1.6532 - mae: 1.0213 - val_loss: 1.8756 - val_mse: 1.8756 - val_mae: 1.0467\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.5176 - mse: 1.5176 - mae: 0.9851 - val_loss: 1.8062 - val_mse: 1.8062 - val_mae: 1.0251\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.4295 - mse: 1.4295 - mae: 0.9585 - val_loss: 1.7530 - val_mse: 1.7530 - val_mae: 1.0103\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.3875 - mse: 1.3875 - mae: 0.9419 - val_loss: 1.7090 - val_mse: 1.7090 - val_mae: 1.0006\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.4654 - mse: 1.4654 - mae: 0.9752 - val_loss: 1.6059 - val_mse: 1.6059 - val_mae: 0.9606\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.3866 - mse: 1.3866 - mae: 0.9456 - val_loss: 1.5954 - val_mse: 1.5954 - val_mae: 0.9519\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2934 - mse: 1.2934 - mae: 0.8956 - val_loss: 1.4418 - val_mse: 1.4418 - val_mae: 0.9136\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2133 - mse: 1.2133 - mae: 0.8709 - val_loss: 1.4223 - val_mse: 1.4223 - val_mae: 0.8974\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1653 - mse: 1.1653 - mae: 0.8562 - val_loss: 1.4322 - val_mse: 1.4322 - val_mae: 0.8999\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1383 - mse: 1.1383 - mae: 0.8502 - val_loss: 1.2898 - val_mse: 1.2898 - val_mae: 0.8500\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1053 - mse: 1.1053 - mae: 0.8343 - val_loss: 1.2314 - val_mse: 1.2314 - val_mae: 0.8315\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0339 - mse: 1.0339 - mae: 0.8001 - val_loss: 1.2761 - val_mse: 1.2761 - val_mae: 0.8358\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0050 - mse: 1.0050 - mae: 0.7883 - val_loss: 1.1628 - val_mse: 1.1628 - val_mae: 0.8023\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9724 - mse: 0.9724 - mae: 0.7709 - val_loss: 1.1331 - val_mse: 1.1331 - val_mae: 0.7882\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9679 - mse: 0.9679 - mae: 0.7770 - val_loss: 1.1508 - val_mse: 1.1508 - val_mae: 0.8004\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9678 - mse: 0.9678 - mae: 0.7780 - val_loss: 1.0782 - val_mse: 1.0782 - val_mae: 0.7718\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9241 - mse: 0.9241 - mae: 0.7523 - val_loss: 1.0274 - val_mse: 1.0274 - val_mae: 0.7612\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9247 - mse: 0.9247 - mae: 0.7616 - val_loss: 0.9716 - val_mse: 0.9716 - val_mae: 0.7501\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8745 - mse: 0.8745 - mae: 0.7455 - val_loss: 1.0083 - val_mse: 1.0083 - val_mae: 0.7327\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8654 - mse: 0.8654 - mae: 0.7283 - val_loss: 0.9052 - val_mse: 0.9052 - val_mae: 0.7305\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8408 - mse: 0.8408 - mae: 0.7247 - val_loss: 0.8588 - val_mse: 0.8588 - val_mae: 0.6943\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7981 - mse: 0.7981 - mae: 0.7002 - val_loss: 0.8808 - val_mse: 0.8808 - val_mae: 0.6824\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7284 - mse: 0.7284 - mae: 0.6651 - val_loss: 0.8336 - val_mse: 0.8336 - val_mae: 0.6767\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7169 - mse: 0.7169 - mae: 0.6649 - val_loss: 0.8424 - val_mse: 0.8424 - val_mae: 0.6670\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7038 - mse: 0.7038 - mae: 0.6518 - val_loss: 0.7390 - val_mse: 0.7390 - val_mae: 0.6493\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6984 - mse: 0.6984 - mae: 0.6532 - val_loss: 0.7596 - val_mse: 0.7596 - val_mae: 0.6537\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6877 - mse: 0.6877 - mae: 0.6507 - val_loss: 0.7664 - val_mse: 0.7664 - val_mae: 0.6474\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6720 - mse: 0.6720 - mae: 0.6394 - val_loss: 0.7295 - val_mse: 0.7295 - val_mae: 0.6429\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6533 - mse: 0.6533 - mae: 0.6289 - val_loss: 0.7183 - val_mse: 0.7183 - val_mae: 0.6363\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6402 - mse: 0.6402 - mae: 0.6283 - val_loss: 0.6949 - val_mse: 0.6949 - val_mae: 0.6281\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6095 - mse: 0.6095 - mae: 0.6081 - val_loss: 0.6403 - val_mse: 0.6403 - val_mae: 0.6104\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6396 - mse: 0.6396 - mae: 0.6231 - val_loss: 0.6504 - val_mse: 0.6504 - val_mae: 0.6033\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5717 - mse: 0.5717 - mae: 0.5882 - val_loss: 0.6200 - val_mse: 0.6200 - val_mae: 0.5917\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5642 - mse: 0.5642 - mae: 0.5823 - val_loss: 0.6555 - val_mse: 0.6555 - val_mae: 0.5995\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5651 - mse: 0.5651 - mae: 0.5897 - val_loss: 0.6935 - val_mse: 0.6935 - val_mae: 0.6104\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5812 - mse: 0.5812 - mae: 0.5988 - val_loss: 0.6414 - val_mse: 0.6414 - val_mae: 0.5916\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5767 - mse: 0.5767 - mae: 0.5812 - val_loss: 0.5791 - val_mse: 0.5791 - val_mae: 0.5681\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5418 - mse: 0.5418 - mae: 0.5689 - val_loss: 0.5825 - val_mse: 0.5825 - val_mae: 0.5652\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5515 - mse: 0.5515 - mae: 0.5738 - val_loss: 0.5593 - val_mse: 0.5593 - val_mae: 0.5547\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5462 - mse: 0.5462 - mae: 0.5848 - val_loss: 0.5649 - val_mse: 0.5649 - val_mae: 0.5580\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4895 - mse: 0.4895 - mae: 0.5416 - val_loss: 0.5267 - val_mse: 0.5267 - val_mae: 0.5464\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5081 - mse: 0.5081 - mae: 0.5567 - val_loss: 0.5216 - val_mse: 0.5216 - val_mae: 0.5412\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5182 - mse: 0.5182 - mae: 0.5638 - val_loss: 0.5169 - val_mse: 0.5169 - val_mae: 0.5392\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4921 - mse: 0.4921 - mae: 0.5411 - val_loss: 0.4911 - val_mse: 0.4911 - val_mae: 0.5321\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4650 - mse: 0.4650 - mae: 0.5327 - val_loss: 0.4901 - val_mse: 0.4901 - val_mae: 0.5357\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4840 - mse: 0.4840 - mae: 0.5353 - val_loss: 0.4644 - val_mse: 0.4644 - val_mae: 0.5242\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4799 - mse: 0.4799 - mae: 0.5425 - val_loss: 0.5107 - val_mse: 0.5107 - val_mae: 0.5425\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5078 - mse: 0.5078 - mae: 0.5582 - val_loss: 0.5583 - val_mse: 0.5583 - val_mae: 0.5644\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4342 - mse: 0.4342 - mae: 0.5119 - val_loss: 0.4889 - val_mse: 0.4889 - val_mae: 0.5312\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4427 - mse: 0.4427 - mae: 0.5259 - val_loss: 0.4773 - val_mse: 0.4773 - val_mae: 0.5304\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4648 - mse: 0.4648 - mae: 0.5209 - val_loss: 0.4548 - val_mse: 0.4548 - val_mae: 0.5253\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4245 - mse: 0.4245 - mae: 0.5101 - val_loss: 0.5182 - val_mse: 0.5182 - val_mae: 0.5412\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4523 - mse: 0.4523 - mae: 0.5249 - val_loss: 0.4793 - val_mse: 0.4793 - val_mae: 0.5397\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4481 - mse: 0.4481 - mae: 0.5256 - val_loss: 0.4797 - val_mse: 0.4797 - val_mae: 0.5274\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4532 - mse: 0.4532 - mae: 0.5181 - val_loss: 0.4981 - val_mse: 0.4981 - val_mae: 0.5304\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4565 - mse: 0.4565 - mae: 0.5251 - val_loss: 0.4847 - val_mse: 0.4847 - val_mae: 0.5334\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4246 - mse: 0.4246 - mae: 0.5054 - val_loss: 0.4822 - val_mse: 0.4822 - val_mae: 0.5209\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3926 - mse: 0.3926 - mae: 0.4848 - val_loss: 0.5129 - val_mse: 0.5129 - val_mae: 0.5320\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4201 - mse: 0.4201 - mae: 0.5032 - val_loss: 0.4820 - val_mse: 0.4820 - val_mae: 0.5286\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4105 - mse: 0.4105 - mae: 0.5011 - val_loss: 0.4949 - val_mse: 0.4949 - val_mae: 0.5336\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4173 - mse: 0.4173 - mae: 0.5004 - val_loss: 0.4758 - val_mse: 0.4758 - val_mae: 0.5257\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3958 - mse: 0.3958 - mae: 0.4907 - val_loss: 0.4619 - val_mse: 0.4619 - val_mae: 0.5165\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4092 - mse: 0.4092 - mae: 0.4958 - val_loss: 0.4779 - val_mse: 0.4779 - val_mae: 0.5264\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4073 - mse: 0.4073 - mae: 0.4989 - val_loss: 0.4459 - val_mse: 0.4459 - val_mae: 0.5068\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4196 - mse: 0.4196 - mae: 0.5002 - val_loss: 0.4795 - val_mse: 0.4795 - val_mae: 0.5253\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3985 - mse: 0.3985 - mae: 0.4870 - val_loss: 0.4849 - val_mse: 0.4849 - val_mae: 0.5329\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3879 - mse: 0.3879 - mae: 0.4867 - val_loss: 0.4739 - val_mse: 0.4739 - val_mae: 0.5139\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3913 - mse: 0.3913 - mae: 0.4935 - val_loss: 0.4682 - val_mse: 0.4682 - val_mae: 0.5118\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3967 - mse: 0.3967 - mae: 0.4906 - val_loss: 0.4876 - val_mse: 0.4876 - val_mae: 0.5271\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3914 - mse: 0.3914 - mae: 0.4825 - val_loss: 0.5295 - val_mse: 0.5295 - val_mae: 0.5427\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4017 - mse: 0.4017 - mae: 0.4930 - val_loss: 0.5058 - val_mse: 0.5058 - val_mae: 0.5321\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4031 - mse: 0.4031 - mae: 0.4970 - val_loss: 0.4722 - val_mse: 0.4722 - val_mae: 0.5143\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4179 - mse: 0.4179 - mae: 0.5036 - val_loss: 0.4696 - val_mse: 0.4696 - val_mae: 0.5132\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3915 - mse: 0.3915 - mae: 0.4892 - val_loss: 0.4949 - val_mse: 0.4949 - val_mae: 0.5286\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3635 - mse: 0.3635 - mae: 0.4663 - val_loss: 0.4563 - val_mse: 0.4563 - val_mae: 0.5202\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3813 - mse: 0.3813 - mae: 0.4827 - val_loss: 0.4505 - val_mse: 0.4505 - val_mae: 0.5086\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4004 - mse: 0.4004 - mae: 0.4870 - val_loss: 0.4558 - val_mse: 0.4558 - val_mae: 0.5107\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3623 - mse: 0.3623 - mae: 0.4776 - val_loss: 0.4496 - val_mse: 0.4496 - val_mae: 0.5047\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3698 - mse: 0.3698 - mae: 0.4679 - val_loss: 0.4843 - val_mse: 0.4843 - val_mae: 0.5143\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3447 - mse: 0.3447 - mae: 0.4415 - val_loss: 0.5175 - val_mse: 0.5175 - val_mae: 0.5356\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3882 - mse: 0.3882 - mae: 0.4849 - val_loss: 0.5405 - val_mse: 0.5405 - val_mae: 0.5497\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3692 - mse: 0.3692 - mae: 0.4743 - val_loss: 0.4629 - val_mse: 0.4629 - val_mae: 0.5042\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3547 - mse: 0.3547 - mae: 0.4621 - val_loss: 0.4744 - val_mse: 0.4744 - val_mae: 0.5099\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3953 - mse: 0.3953 - mae: 0.4807 - val_loss: 0.4822 - val_mse: 0.4822 - val_mae: 0.5158\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3674 - mse: 0.3674 - mae: 0.4710 - val_loss: 0.5039 - val_mse: 0.5039 - val_mae: 0.5275\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3963 - mse: 0.3963 - mae: 0.4865 - val_loss: 0.4559 - val_mse: 0.4559 - val_mae: 0.5049\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3602 - mse: 0.3602 - mae: 0.4728 - val_loss: 0.4621 - val_mse: 0.4621 - val_mae: 0.5100\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3705 - mse: 0.3705 - mae: 0.4624 - val_loss: 0.5001 - val_mse: 0.5001 - val_mae: 0.5293\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3620 - mse: 0.3620 - mae: 0.4635 - val_loss: 0.4611 - val_mse: 0.4611 - val_mae: 0.5129\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3613 - mse: 0.3613 - mae: 0.4678 - val_loss: 0.4776 - val_mse: 0.4776 - val_mae: 0.5115\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3614 - mse: 0.3614 - mae: 0.4679 - val_loss: 0.4634 - val_mse: 0.4634 - val_mae: 0.5120\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3412 - mse: 0.3412 - mae: 0.4532 - val_loss: 0.4920 - val_mse: 0.4920 - val_mae: 0.5284\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3674 - mse: 0.3674 - mae: 0.4647 - val_loss: 0.4473 - val_mse: 0.4473 - val_mae: 0.5077\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3365 - mse: 0.3365 - mae: 0.4548 - val_loss: 0.4762 - val_mse: 0.4762 - val_mae: 0.5191\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3871 - mse: 0.3871 - mae: 0.4830 - val_loss: 0.4354 - val_mse: 0.4354 - val_mae: 0.4964\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3362 - mse: 0.3362 - mae: 0.4559 - val_loss: 0.4304 - val_mse: 0.4304 - val_mae: 0.4923\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3421 - mse: 0.3421 - mae: 0.4549 - val_loss: 0.4867 - val_mse: 0.4867 - val_mae: 0.5282\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3331 - mse: 0.3331 - mae: 0.4569 - val_loss: 0.4544 - val_mse: 0.4544 - val_mae: 0.5009\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3556 - mse: 0.3556 - mae: 0.4627 - val_loss: 0.4652 - val_mse: 0.4652 - val_mae: 0.5093\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3370 - mse: 0.3370 - mae: 0.4517 - val_loss: 0.4731 - val_mse: 0.4731 - val_mae: 0.5135\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3425 - mse: 0.3425 - mae: 0.4662 - val_loss: 0.4466 - val_mse: 0.4466 - val_mae: 0.5017\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3306 - mse: 0.3306 - mae: 0.4466 - val_loss: 0.4527 - val_mse: 0.4527 - val_mae: 0.5024\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3159 - mse: 0.3159 - mae: 0.4319 - val_loss: 0.4537 - val_mse: 0.4537 - val_mae: 0.5064\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3531 - mse: 0.3531 - mae: 0.4601 - val_loss: 0.4582 - val_mse: 0.4582 - val_mae: 0.5065\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3313 - mse: 0.3313 - mae: 0.4380 - val_loss: 0.4692 - val_mse: 0.4692 - val_mae: 0.5259\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3575 - mse: 0.3575 - mae: 0.4668 - val_loss: 0.4843 - val_mse: 0.4843 - val_mae: 0.5298\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3233 - mse: 0.3233 - mae: 0.4395 - val_loss: 0.4379 - val_mse: 0.4379 - val_mae: 0.4966\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3392 - mse: 0.3392 - mae: 0.4522 - val_loss: 0.4219 - val_mse: 0.4219 - val_mae: 0.4856\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3300 - mse: 0.3300 - mae: 0.4470 - val_loss: 0.4449 - val_mse: 0.4449 - val_mae: 0.5001\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3561 - mse: 0.3561 - mae: 0.4606 - val_loss: 0.4560 - val_mse: 0.4560 - val_mae: 0.5171\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3279 - mse: 0.3279 - mae: 0.4469 - val_loss: 0.4281 - val_mse: 0.4281 - val_mae: 0.4921\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3348 - mse: 0.3348 - mae: 0.4502 - val_loss: 0.4274 - val_mse: 0.4274 - val_mae: 0.4970\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3302 - mse: 0.3302 - mae: 0.4432 - val_loss: 0.4670 - val_mse: 0.4670 - val_mae: 0.5237\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3128 - mse: 0.3128 - mae: 0.4375 - val_loss: 0.4258 - val_mse: 0.4258 - val_mae: 0.4918\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3302 - mse: 0.3302 - mae: 0.4473 - val_loss: 0.4513 - val_mse: 0.4513 - val_mae: 0.5095\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3564 - mse: 0.3564 - mae: 0.4614 - val_loss: 0.4428 - val_mse: 0.4428 - val_mae: 0.5032\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3561 - mse: 0.3561 - mae: 0.4627 - val_loss: 0.4302 - val_mse: 0.4302 - val_mae: 0.4941\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3104 - mse: 0.3104 - mae: 0.4315 - val_loss: 0.4412 - val_mse: 0.4412 - val_mae: 0.5026\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3178 - mse: 0.3178 - mae: 0.4416 - val_loss: 0.4885 - val_mse: 0.4885 - val_mae: 0.5324\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3186 - mse: 0.3186 - mae: 0.4375 - val_loss: 0.4460 - val_mse: 0.4460 - val_mae: 0.5037\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3288 - mse: 0.3288 - mae: 0.4427 - val_loss: 0.4322 - val_mse: 0.4322 - val_mae: 0.4961\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3206 - mse: 0.3206 - mae: 0.4438 - val_loss: 0.4343 - val_mse: 0.4343 - val_mae: 0.4990\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3153 - mse: 0.3153 - mae: 0.4393 - val_loss: 0.4317 - val_mse: 0.4317 - val_mae: 0.4965\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3223 - mse: 0.3223 - mae: 0.4481 - val_loss: 0.4381 - val_mse: 0.4381 - val_mae: 0.5006\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3066 - mse: 0.3066 - mae: 0.4281 - val_loss: 0.4556 - val_mse: 0.4556 - val_mae: 0.5213\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3220 - mse: 0.3220 - mae: 0.4435 - val_loss: 0.4748 - val_mse: 0.4748 - val_mae: 0.5202\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3335 - mse: 0.3335 - mae: 0.4482 - val_loss: 0.4549 - val_mse: 0.4549 - val_mae: 0.5230\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3122 - mse: 0.3122 - mae: 0.4374 - val_loss: 0.4536 - val_mse: 0.4536 - val_mae: 0.5182\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3377 - mse: 0.3377 - mae: 0.4493 - val_loss: 0.4334 - val_mse: 0.4334 - val_mae: 0.4994\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2982 - mse: 0.2982 - mae: 0.4281 - val_loss: 0.4417 - val_mse: 0.4417 - val_mae: 0.5034\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3149 - mse: 0.3149 - mae: 0.4421 - val_loss: 0.4218 - val_mse: 0.4218 - val_mae: 0.4920\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3406 - mse: 0.3406 - mae: 0.4499 - val_loss: 0.4536 - val_mse: 0.4536 - val_mae: 0.5095\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3404 - mse: 0.3404 - mae: 0.4594 - val_loss: 0.4267 - val_mse: 0.4267 - val_mae: 0.5029\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3284 - mse: 0.3284 - mae: 0.4433 - val_loss: 0.4372 - val_mse: 0.4372 - val_mae: 0.5049\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3371 - mse: 0.3371 - mae: 0.4464 - val_loss: 0.4396 - val_mse: 0.4396 - val_mae: 0.5104\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3140 - mse: 0.3140 - mae: 0.4298 - val_loss: 0.4393 - val_mse: 0.4393 - val_mae: 0.5027\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2944 - mse: 0.2944 - mae: 0.4154 - val_loss: 0.4359 - val_mse: 0.4359 - val_mae: 0.5025\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3298 - mse: 0.3298 - mae: 0.4472 - val_loss: 0.4659 - val_mse: 0.4659 - val_mae: 0.5151\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3086 - mse: 0.3086 - mae: 0.4247 - val_loss: 0.4341 - val_mse: 0.4341 - val_mae: 0.4962\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3140 - mse: 0.3140 - mae: 0.4342 - val_loss: 0.4389 - val_mse: 0.4389 - val_mae: 0.5046\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3162 - mse: 0.3162 - mae: 0.4400 - val_loss: 0.4188 - val_mse: 0.4188 - val_mae: 0.4939\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2868 - mse: 0.2868 - mae: 0.4064 - val_loss: 0.4336 - val_mse: 0.4336 - val_mae: 0.5008\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3009 - mse: 0.3009 - mae: 0.4277 - val_loss: 0.4329 - val_mse: 0.4329 - val_mae: 0.4990\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3046 - mse: 0.3046 - mae: 0.4316 - val_loss: 0.4734 - val_mse: 0.4734 - val_mae: 0.5292\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3202 - mse: 0.3202 - mae: 0.4460 - val_loss: 0.4604 - val_mse: 0.4604 - val_mae: 0.5206\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2975 - mse: 0.2975 - mae: 0.4184 - val_loss: 0.4527 - val_mse: 0.4527 - val_mae: 0.5080\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2746 - mse: 0.2746 - mae: 0.4074 - val_loss: 0.4427 - val_mse: 0.4427 - val_mae: 0.5026\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2899 - mse: 0.2899 - mae: 0.4211 - val_loss: 0.4556 - val_mse: 0.4556 - val_mae: 0.5127\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2988 - mse: 0.2988 - mae: 0.4228 - val_loss: 0.4768 - val_mse: 0.4768 - val_mae: 0.5278\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3133 - mse: 0.3133 - mae: 0.4318 - val_loss: 0.4600 - val_mse: 0.4600 - val_mae: 0.5164\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2868 - mse: 0.2868 - mae: 0.4179 - val_loss: 0.4695 - val_mse: 0.4695 - val_mae: 0.5255\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3005 - mse: 0.3005 - mae: 0.4325 - val_loss: 0.4560 - val_mse: 0.4560 - val_mae: 0.5172\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2795 - mse: 0.2795 - mae: 0.4116 - val_loss: 0.4607 - val_mse: 0.4607 - val_mae: 0.5240\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2969 - mse: 0.2969 - mae: 0.4218 - val_loss: 0.4244 - val_mse: 0.4244 - val_mae: 0.5019\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2987 - mse: 0.2987 - mae: 0.4255 - val_loss: 0.4407 - val_mse: 0.4407 - val_mae: 0.5072\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3137 - mse: 0.3137 - mae: 0.4417 - val_loss: 0.4533 - val_mse: 0.4533 - val_mae: 0.5126\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2896 - mse: 0.2896 - mae: 0.4187 - val_loss: 0.4417 - val_mse: 0.4417 - val_mae: 0.5104\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3085 - mse: 0.3085 - mae: 0.4370 - val_loss: 0.4682 - val_mse: 0.4682 - val_mae: 0.5268\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2880 - mse: 0.2880 - mae: 0.4145 - val_loss: 0.4387 - val_mse: 0.4387 - val_mae: 0.5108\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2823 - mse: 0.2823 - mae: 0.4066 - val_loss: 0.4665 - val_mse: 0.4665 - val_mae: 0.5211\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2878 - mse: 0.2878 - mae: 0.4176 - val_loss: 0.4458 - val_mse: 0.4458 - val_mae: 0.5135\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2752 - mse: 0.2752 - mae: 0.4047 - val_loss: 0.4361 - val_mse: 0.4361 - val_mae: 0.5055\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2913 - mse: 0.2913 - mae: 0.4224 - val_loss: 0.4326 - val_mse: 0.4326 - val_mae: 0.5056\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2660 - mse: 0.2660 - mae: 0.4046 - val_loss: 0.4556 - val_mse: 0.4556 - val_mae: 0.5204\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2786 - mse: 0.2786 - mae: 0.4183 - val_loss: 0.4278 - val_mse: 0.4278 - val_mae: 0.5002\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2869 - mse: 0.2869 - mae: 0.4219 - val_loss: 0.4624 - val_mse: 0.4624 - val_mae: 0.5215\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2762 - mse: 0.2762 - mae: 0.4085 - val_loss: 0.4269 - val_mse: 0.4269 - val_mae: 0.4995\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3026 - mse: 0.3026 - mae: 0.4207 - val_loss: 0.4359 - val_mse: 0.4359 - val_mae: 0.5037\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3075 - mse: 0.3075 - mae: 0.4328 - val_loss: 0.4432 - val_mse: 0.4432 - val_mae: 0.5137\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2878 - mse: 0.2878 - mae: 0.4183 - val_loss: 0.4533 - val_mse: 0.4533 - val_mae: 0.5179\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2809 - mse: 0.2809 - mae: 0.4067 - val_loss: 0.4336 - val_mse: 0.4336 - val_mae: 0.5047\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2957 - mse: 0.2957 - mae: 0.4255 - val_loss: 0.4439 - val_mse: 0.4439 - val_mae: 0.5106\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2832 - mse: 0.2832 - mae: 0.4132 - val_loss: 0.4616 - val_mse: 0.4616 - val_mae: 0.5211\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2826 - mse: 0.2826 - mae: 0.4066 - val_loss: 0.4752 - val_mse: 0.4752 - val_mae: 0.5280\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3027 - mse: 0.3027 - mae: 0.4305 - val_loss: 0.4325 - val_mse: 0.4325 - val_mae: 0.5076\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2943 - mse: 0.2943 - mae: 0.4264 - val_loss: 0.4421 - val_mse: 0.4421 - val_mae: 0.5073\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2778 - mse: 0.2778 - mae: 0.4111 - val_loss: 0.4352 - val_mse: 0.4352 - val_mae: 0.5078\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2799 - mse: 0.2799 - mae: 0.4110 - val_loss: 0.4533 - val_mse: 0.4533 - val_mae: 0.5255\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2602 - mse: 0.2602 - mae: 0.4028 - val_loss: 0.4546 - val_mse: 0.4546 - val_mae: 0.5251\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2662 - mse: 0.2662 - mae: 0.4067 - val_loss: 0.4847 - val_mse: 0.4847 - val_mae: 0.5450\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2743 - mse: 0.2743 - mae: 0.4070 - val_loss: 0.4525 - val_mse: 0.4525 - val_mae: 0.5168\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2681 - mse: 0.2681 - mae: 0.4024 - val_loss: 0.4284 - val_mse: 0.4284 - val_mae: 0.5051\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2707 - mse: 0.2707 - mae: 0.3984 - val_loss: 0.4591 - val_mse: 0.4591 - val_mae: 0.5216\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2720 - mse: 0.2720 - mae: 0.4081 - val_loss: 0.5381 - val_mse: 0.5381 - val_mae: 0.5727\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3015 - mse: 0.3015 - mae: 0.4295 - val_loss: 0.4375 - val_mse: 0.4375 - val_mae: 0.5064\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2761 - mse: 0.2761 - mae: 0.4089 - val_loss: 0.4336 - val_mse: 0.4336 - val_mae: 0.5084\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2701 - mse: 0.2701 - mae: 0.4028 - val_loss: 0.4503 - val_mse: 0.4503 - val_mae: 0.5182\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2964 - mse: 0.2964 - mae: 0.4178 - val_loss: 0.4282 - val_mse: 0.4282 - val_mae: 0.5010\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2747 - mse: 0.2747 - mae: 0.4098 - val_loss: 0.4574 - val_mse: 0.4574 - val_mae: 0.5253\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2755 - mse: 0.2755 - mae: 0.4041 - val_loss: 0.4619 - val_mse: 0.4619 - val_mae: 0.5257\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2621 - mse: 0.2621 - mae: 0.3997 - val_loss: 0.4308 - val_mse: 0.4308 - val_mae: 0.5046\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2595 - mse: 0.2595 - mae: 0.3984 - val_loss: 0.4255 - val_mse: 0.4255 - val_mae: 0.5044\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2856 - mse: 0.2856 - mae: 0.4123 - val_loss: 0.4473 - val_mse: 0.4473 - val_mae: 0.5112\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2714 - mse: 0.2714 - mae: 0.4109 - val_loss: 0.4453 - val_mse: 0.4453 - val_mae: 0.5117\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2551 - mse: 0.2551 - mae: 0.3990 - val_loss: 0.4830 - val_mse: 0.4830 - val_mae: 0.5355\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2923 - mse: 0.2923 - mae: 0.4203 - val_loss: 0.4252 - val_mse: 0.4252 - val_mae: 0.5012\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2838 - mse: 0.2838 - mae: 0.4099 - val_loss: 0.4459 - val_mse: 0.4459 - val_mae: 0.5170\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2567 - mse: 0.2567 - mae: 0.4002 - val_loss: 0.4983 - val_mse: 0.4983 - val_mae: 0.5509\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2712 - mse: 0.2712 - mae: 0.4012 - val_loss: 0.4754 - val_mse: 0.4754 - val_mae: 0.5386\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2643 - mse: 0.2643 - mae: 0.3975 - val_loss: 0.4437 - val_mse: 0.4437 - val_mae: 0.5157\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2637 - mse: 0.2637 - mae: 0.3980 - val_loss: 0.4267 - val_mse: 0.4267 - val_mae: 0.5063\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2510 - mse: 0.2510 - mae: 0.3882 - val_loss: 0.4361 - val_mse: 0.4361 - val_mae: 0.5073\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2646 - mse: 0.2646 - mae: 0.4015 - val_loss: 0.4235 - val_mse: 0.4235 - val_mae: 0.5081\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2573 - mse: 0.2573 - mae: 0.3890 - val_loss: 0.4626 - val_mse: 0.4626 - val_mae: 0.5271\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2828 - mse: 0.2828 - mae: 0.4115 - val_loss: 0.4234 - val_mse: 0.4234 - val_mae: 0.5029\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2589 - mse: 0.2589 - mae: 0.3914 - val_loss: 0.4208 - val_mse: 0.4208 - val_mae: 0.5030\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2739 - mse: 0.2739 - mae: 0.3990 - val_loss: 0.4334 - val_mse: 0.4334 - val_mae: 0.5114\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2704 - mse: 0.2704 - mae: 0.4064 - val_loss: 0.4390 - val_mse: 0.4390 - val_mae: 0.5058\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2728 - mse: 0.2728 - mae: 0.4145 - val_loss: 0.4310 - val_mse: 0.4310 - val_mae: 0.4984\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2570 - mse: 0.2570 - mae: 0.3857 - val_loss: 0.4438 - val_mse: 0.4438 - val_mae: 0.5072\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2714 - mse: 0.2714 - mae: 0.4074 - val_loss: 0.4439 - val_mse: 0.4439 - val_mae: 0.5096\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2725 - mse: 0.2725 - mae: 0.3991 - val_loss: 0.4457 - val_mse: 0.4457 - val_mae: 0.5051\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2613 - mse: 0.2613 - mae: 0.3961 - val_loss: 0.4395 - val_mse: 0.4395 - val_mae: 0.5043\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2675 - mse: 0.2675 - mae: 0.3996 - val_loss: 0.4429 - val_mse: 0.4429 - val_mae: 0.5142\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2672 - mse: 0.2672 - mae: 0.3997 - val_loss: 0.5083 - val_mse: 0.5083 - val_mae: 0.5505\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2606 - mse: 0.2606 - mae: 0.4006 - val_loss: 0.4563 - val_mse: 0.4563 - val_mae: 0.5245\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2533 - mse: 0.2533 - mae: 0.3920 - val_loss: 0.4452 - val_mse: 0.4452 - val_mae: 0.5176\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2485 - mse: 0.2485 - mae: 0.3925 - val_loss: 0.4464 - val_mse: 0.4464 - val_mae: 0.5155\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2398 - mse: 0.2398 - mae: 0.3819 - val_loss: 0.4530 - val_mse: 0.4530 - val_mae: 0.5226\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2383 - mse: 0.2383 - mae: 0.3836 - val_loss: 0.4376 - val_mse: 0.4376 - val_mae: 0.5134\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2494 - mse: 0.2494 - mae: 0.3836 - val_loss: 0.4567 - val_mse: 0.4567 - val_mae: 0.5165\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2665 - mse: 0.2665 - mae: 0.4071 - val_loss: 0.4517 - val_mse: 0.4517 - val_mae: 0.5166\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2508 - mse: 0.2508 - mae: 0.3869 - val_loss: 0.4750 - val_mse: 0.4750 - val_mae: 0.5359\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2492 - mse: 0.2492 - mae: 0.3941 - val_loss: 0.4630 - val_mse: 0.4630 - val_mae: 0.5285\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2544 - mse: 0.2544 - mae: 0.3956 - val_loss: 0.4371 - val_mse: 0.4371 - val_mae: 0.4997\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2610 - mse: 0.2610 - mae: 0.3981 - val_loss: 0.4443 - val_mse: 0.4443 - val_mae: 0.5086\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2644 - mse: 0.2644 - mae: 0.3963 - val_loss: 0.4285 - val_mse: 0.4285 - val_mae: 0.5049\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2599 - mse: 0.2599 - mae: 0.3915 - val_loss: 0.4314 - val_mse: 0.4314 - val_mae: 0.5048\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2515 - mse: 0.2515 - mae: 0.3866 - val_loss: 0.4594 - val_mse: 0.4594 - val_mae: 0.5278\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2841 - mse: 0.2841 - mae: 0.4185 - val_loss: 0.4756 - val_mse: 0.4756 - val_mae: 0.5320\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2437 - mse: 0.2437 - mae: 0.3807 - val_loss: 0.4383 - val_mse: 0.4383 - val_mae: 0.5186\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2597 - mse: 0.2597 - mae: 0.3963 - val_loss: 0.4498 - val_mse: 0.4498 - val_mae: 0.5176\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2718 - mse: 0.2718 - mae: 0.4005 - val_loss: 0.4398 - val_mse: 0.4398 - val_mae: 0.5100\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2609 - mse: 0.2609 - mae: 0.3944 - val_loss: 0.4462 - val_mse: 0.4462 - val_mae: 0.5103\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2750 - mse: 0.2750 - mae: 0.4049 - val_loss: 0.4636 - val_mse: 0.4636 - val_mae: 0.5317\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2651 - mse: 0.2651 - mae: 0.4037 - val_loss: 0.4781 - val_mse: 0.4781 - val_mae: 0.5393\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2409 - mse: 0.2409 - mae: 0.3890 - val_loss: 0.4280 - val_mse: 0.4280 - val_mae: 0.5070\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2526 - mse: 0.2526 - mae: 0.3911 - val_loss: 0.4913 - val_mse: 0.4913 - val_mae: 0.5370\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2438 - mse: 0.2438 - mae: 0.3847 - val_loss: 0.4370 - val_mse: 0.4370 - val_mae: 0.5131\n",
            "Epoch 252/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2700 - mse: 0.2700 - mae: 0.4050 - val_loss: 0.4513 - val_mse: 0.4513 - val_mae: 0.5296\n",
            "Epoch 253/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2658 - mse: 0.2658 - mae: 0.4080 - val_loss: 0.5009 - val_mse: 0.5009 - val_mae: 0.5455\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.5396 - mse: 0.5396 - mae: 0.5245\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 14ms/step - loss: 28.8842 - mse: 28.8842 - mae: 5.1395 - val_loss: 18.4389 - val_mse: 18.4389 - val_mae: 4.0229\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 13.2410 - mse: 13.2410 - mae: 3.2777 - val_loss: 8.1446 - val_mse: 8.1446 - val_mae: 2.4640\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 6.8584 - mse: 6.8584 - mae: 2.2221 - val_loss: 4.7831 - val_mse: 4.7831 - val_mae: 1.7959\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 4.3359 - mse: 4.3359 - mae: 1.7060 - val_loss: 3.6727 - val_mse: 3.6727 - val_mae: 1.5639\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 3.4475 - mse: 3.4475 - mae: 1.5168 - val_loss: 3.2421 - val_mse: 3.2421 - val_mae: 1.4399\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.9751 - mse: 2.9751 - mae: 1.3766 - val_loss: 3.0177 - val_mse: 3.0177 - val_mae: 1.3657\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.7501 - mse: 2.7501 - mae: 1.3168 - val_loss: 2.7857 - val_mse: 2.7857 - val_mae: 1.3176\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.4439 - mse: 2.4439 - mae: 1.2343 - val_loss: 2.6882 - val_mse: 2.6882 - val_mae: 1.2774\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.2765 - mse: 2.2765 - mae: 1.1874 - val_loss: 2.5031 - val_mse: 2.5031 - val_mae: 1.2318\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.0687 - mse: 2.0687 - mae: 1.1480 - val_loss: 2.3867 - val_mse: 2.3867 - val_mae: 1.2037\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.9772 - mse: 1.9772 - mae: 1.1261 - val_loss: 2.2786 - val_mse: 2.2786 - val_mae: 1.1797\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.0106 - mse: 2.0106 - mae: 1.1223 - val_loss: 2.1705 - val_mse: 2.1705 - val_mae: 1.1588\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.9604 - mse: 1.9604 - mae: 1.1168 - val_loss: 2.0907 - val_mse: 2.0907 - val_mae: 1.1369\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.7597 - mse: 1.7597 - mae: 1.0446 - val_loss: 1.9980 - val_mse: 1.9980 - val_mae: 1.1076\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.6684 - mse: 1.6684 - mae: 1.0248 - val_loss: 1.9405 - val_mse: 1.9405 - val_mae: 1.0939\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.6554 - mse: 1.6554 - mae: 1.0238 - val_loss: 1.8815 - val_mse: 1.8815 - val_mae: 1.0865\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.6067 - mse: 1.6067 - mae: 0.9990 - val_loss: 1.7951 - val_mse: 1.7951 - val_mae: 1.0688\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.4721 - mse: 1.4721 - mae: 0.9557 - val_loss: 1.7022 - val_mse: 1.7022 - val_mae: 1.0313\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.4823 - mse: 1.4823 - mae: 0.9502 - val_loss: 1.6337 - val_mse: 1.6337 - val_mae: 1.0150\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.3262 - mse: 1.3262 - mae: 0.8982 - val_loss: 1.5808 - val_mse: 1.5808 - val_mae: 1.0001\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.3317 - mse: 1.3317 - mae: 0.9082 - val_loss: 1.5661 - val_mse: 1.5661 - val_mae: 0.9872\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2413 - mse: 1.2413 - mae: 0.8753 - val_loss: 1.5016 - val_mse: 1.5016 - val_mae: 0.9661\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.2411 - mse: 1.2411 - mae: 0.8724 - val_loss: 1.4339 - val_mse: 1.4339 - val_mae: 0.9532\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1383 - mse: 1.1383 - mae: 0.8424 - val_loss: 1.3837 - val_mse: 1.3837 - val_mae: 0.9339\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1265 - mse: 1.1265 - mae: 0.8348 - val_loss: 1.3414 - val_mse: 1.3414 - val_mae: 0.9147\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0881 - mse: 1.0881 - mae: 0.8117 - val_loss: 1.2930 - val_mse: 1.2930 - val_mae: 0.8964\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0350 - mse: 1.0350 - mae: 0.8062 - val_loss: 1.2352 - val_mse: 1.2352 - val_mae: 0.8795\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.0408 - mse: 1.0408 - mae: 0.7893 - val_loss: 1.1833 - val_mse: 1.1833 - val_mae: 0.8584\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0475 - mse: 1.0475 - mae: 0.7935 - val_loss: 1.1319 - val_mse: 1.1319 - val_mae: 0.8396\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9265 - mse: 0.9265 - mae: 0.7578 - val_loss: 1.1014 - val_mse: 1.1014 - val_mae: 0.8272\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9100 - mse: 0.9100 - mae: 0.7428 - val_loss: 1.0556 - val_mse: 1.0556 - val_mae: 0.8157\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9193 - mse: 0.9193 - mae: 0.7412 - val_loss: 1.0293 - val_mse: 1.0293 - val_mae: 0.8012\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8498 - mse: 0.8498 - mae: 0.7151 - val_loss: 0.9967 - val_mse: 0.9967 - val_mae: 0.7856\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8506 - mse: 0.8506 - mae: 0.7227 - val_loss: 0.9647 - val_mse: 0.9647 - val_mae: 0.7741\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7805 - mse: 0.7805 - mae: 0.6889 - val_loss: 0.9335 - val_mse: 0.9335 - val_mae: 0.7583\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8210 - mse: 0.8210 - mae: 0.7091 - val_loss: 0.9097 - val_mse: 0.9097 - val_mae: 0.7505\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7486 - mse: 0.7486 - mae: 0.6730 - val_loss: 0.8717 - val_mse: 0.8717 - val_mae: 0.7344\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7568 - mse: 0.7568 - mae: 0.6789 - val_loss: 0.8637 - val_mse: 0.8637 - val_mae: 0.7303\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7140 - mse: 0.7140 - mae: 0.6580 - val_loss: 0.8554 - val_mse: 0.8554 - val_mae: 0.7239\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7397 - mse: 0.7397 - mae: 0.6674 - val_loss: 0.8195 - val_mse: 0.8195 - val_mae: 0.7053\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7024 - mse: 0.7024 - mae: 0.6440 - val_loss: 0.7734 - val_mse: 0.7734 - val_mae: 0.6908\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6610 - mse: 0.6610 - mae: 0.6346 - val_loss: 0.7493 - val_mse: 0.7493 - val_mae: 0.6870\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6842 - mse: 0.6842 - mae: 0.6368 - val_loss: 0.7349 - val_mse: 0.7349 - val_mae: 0.6724\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6392 - mse: 0.6392 - mae: 0.6291 - val_loss: 0.7205 - val_mse: 0.7205 - val_mae: 0.6665\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6196 - mse: 0.6196 - mae: 0.5977 - val_loss: 0.7061 - val_mse: 0.7061 - val_mae: 0.6596\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5949 - mse: 0.5949 - mae: 0.6009 - val_loss: 0.6848 - val_mse: 0.6848 - val_mae: 0.6529\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6365 - mse: 0.6365 - mae: 0.6281 - val_loss: 0.6793 - val_mse: 0.6793 - val_mae: 0.6430\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5906 - mse: 0.5906 - mae: 0.6076 - val_loss: 0.6593 - val_mse: 0.6593 - val_mae: 0.6412\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5711 - mse: 0.5711 - mae: 0.5919 - val_loss: 0.6618 - val_mse: 0.6618 - val_mae: 0.6369\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5489 - mse: 0.5489 - mae: 0.5727 - val_loss: 0.6519 - val_mse: 0.6519 - val_mae: 0.6292\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5369 - mse: 0.5369 - mae: 0.5722 - val_loss: 0.6199 - val_mse: 0.6199 - val_mae: 0.6199\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5277 - mse: 0.5277 - mae: 0.5719 - val_loss: 0.6106 - val_mse: 0.6106 - val_mae: 0.6156\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5599 - mse: 0.5599 - mae: 0.5867 - val_loss: 0.5995 - val_mse: 0.5995 - val_mae: 0.6088\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5241 - mse: 0.5241 - mae: 0.5578 - val_loss: 0.5863 - val_mse: 0.5863 - val_mae: 0.6044\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4760 - mse: 0.4760 - mae: 0.5410 - val_loss: 0.6095 - val_mse: 0.6095 - val_mae: 0.6082\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5233 - mse: 0.5233 - mae: 0.5686 - val_loss: 0.5714 - val_mse: 0.5714 - val_mae: 0.5980\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4780 - mse: 0.4780 - mae: 0.5354 - val_loss: 0.5593 - val_mse: 0.5593 - val_mae: 0.5927\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4746 - mse: 0.4746 - mae: 0.5404 - val_loss: 0.5502 - val_mse: 0.5502 - val_mae: 0.5896\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4806 - mse: 0.4806 - mae: 0.5343 - val_loss: 0.5383 - val_mse: 0.5383 - val_mae: 0.5838\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4749 - mse: 0.4749 - mae: 0.5338 - val_loss: 0.5454 - val_mse: 0.5454 - val_mae: 0.5902\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4746 - mse: 0.4746 - mae: 0.5339 - val_loss: 0.5452 - val_mse: 0.5452 - val_mae: 0.5895\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4923 - mse: 0.4923 - mae: 0.5509 - val_loss: 0.5296 - val_mse: 0.5296 - val_mae: 0.5809\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4898 - mse: 0.4898 - mae: 0.5418 - val_loss: 0.5252 - val_mse: 0.5252 - val_mae: 0.5767\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4502 - mse: 0.4502 - mae: 0.5205 - val_loss: 0.5210 - val_mse: 0.5210 - val_mae: 0.5759\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4555 - mse: 0.4555 - mae: 0.5267 - val_loss: 0.5240 - val_mse: 0.5240 - val_mae: 0.5741\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4387 - mse: 0.4387 - mae: 0.5051 - val_loss: 0.5028 - val_mse: 0.5028 - val_mae: 0.5666\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4397 - mse: 0.4397 - mae: 0.5104 - val_loss: 0.4981 - val_mse: 0.4981 - val_mae: 0.5615\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4369 - mse: 0.4369 - mae: 0.5098 - val_loss: 0.5111 - val_mse: 0.5111 - val_mae: 0.5698\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4435 - mse: 0.4435 - mae: 0.5178 - val_loss: 0.5126 - val_mse: 0.5126 - val_mae: 0.5739\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4304 - mse: 0.4304 - mae: 0.5051 - val_loss: 0.4985 - val_mse: 0.4985 - val_mae: 0.5679\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4565 - mse: 0.4565 - mae: 0.5314 - val_loss: 0.4953 - val_mse: 0.4953 - val_mae: 0.5635\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4396 - mse: 0.4396 - mae: 0.5126 - val_loss: 0.4961 - val_mse: 0.4961 - val_mae: 0.5635\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4320 - mse: 0.4320 - mae: 0.5164 - val_loss: 0.4795 - val_mse: 0.4795 - val_mae: 0.5535\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4324 - mse: 0.4324 - mae: 0.5163 - val_loss: 0.4969 - val_mse: 0.4969 - val_mae: 0.5612\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4310 - mse: 0.4310 - mae: 0.5157 - val_loss: 0.4765 - val_mse: 0.4765 - val_mae: 0.5545\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4014 - mse: 0.4014 - mae: 0.4920 - val_loss: 0.4811 - val_mse: 0.4811 - val_mae: 0.5508\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4061 - mse: 0.4061 - mae: 0.4960 - val_loss: 0.4828 - val_mse: 0.4828 - val_mae: 0.5519\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3847 - mse: 0.3847 - mae: 0.4849 - val_loss: 0.4722 - val_mse: 0.4722 - val_mae: 0.5464\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4179 - mse: 0.4179 - mae: 0.4967 - val_loss: 0.4715 - val_mse: 0.4715 - val_mae: 0.5515\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4098 - mse: 0.4098 - mae: 0.4898 - val_loss: 0.5112 - val_mse: 0.5112 - val_mae: 0.5680\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3918 - mse: 0.3918 - mae: 0.4831 - val_loss: 0.4791 - val_mse: 0.4791 - val_mae: 0.5558\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4182 - mse: 0.4182 - mae: 0.4938 - val_loss: 0.5208 - val_mse: 0.5208 - val_mae: 0.5715\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4101 - mse: 0.4101 - mae: 0.4942 - val_loss: 0.4994 - val_mse: 0.4994 - val_mae: 0.5608\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3966 - mse: 0.3966 - mae: 0.4951 - val_loss: 0.4717 - val_mse: 0.4717 - val_mae: 0.5537\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3960 - mse: 0.3960 - mae: 0.4838 - val_loss: 0.4793 - val_mse: 0.4793 - val_mae: 0.5569\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3798 - mse: 0.3798 - mae: 0.4773 - val_loss: 0.4754 - val_mse: 0.4754 - val_mae: 0.5516\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3921 - mse: 0.3921 - mae: 0.4842 - val_loss: 0.4737 - val_mse: 0.4737 - val_mae: 0.5503\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3884 - mse: 0.3884 - mae: 0.4834 - val_loss: 0.4658 - val_mse: 0.4658 - val_mae: 0.5442\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3908 - mse: 0.3908 - mae: 0.4864 - val_loss: 0.4656 - val_mse: 0.4656 - val_mae: 0.5440\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3623 - mse: 0.3623 - mae: 0.4693 - val_loss: 0.4586 - val_mse: 0.4586 - val_mae: 0.5400\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3764 - mse: 0.3764 - mae: 0.4770 - val_loss: 0.4583 - val_mse: 0.4583 - val_mae: 0.5389\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3960 - mse: 0.3960 - mae: 0.4844 - val_loss: 0.4572 - val_mse: 0.4572 - val_mae: 0.5411\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3654 - mse: 0.3654 - mae: 0.4759 - val_loss: 0.4526 - val_mse: 0.4526 - val_mae: 0.5341\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3667 - mse: 0.3667 - mae: 0.4715 - val_loss: 0.4716 - val_mse: 0.4716 - val_mae: 0.5420\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3944 - mse: 0.3944 - mae: 0.4884 - val_loss: 0.4534 - val_mse: 0.4534 - val_mae: 0.5349\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3588 - mse: 0.3588 - mae: 0.4622 - val_loss: 0.4492 - val_mse: 0.4492 - val_mae: 0.5328\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3663 - mse: 0.3663 - mae: 0.4643 - val_loss: 0.4505 - val_mse: 0.4505 - val_mae: 0.5369\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3739 - mse: 0.3739 - mae: 0.4721 - val_loss: 0.4510 - val_mse: 0.4510 - val_mae: 0.5366\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3498 - mse: 0.3498 - mae: 0.4560 - val_loss: 0.4534 - val_mse: 0.4534 - val_mae: 0.5385\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3891 - mse: 0.3891 - mae: 0.4820 - val_loss: 0.4459 - val_mse: 0.4459 - val_mae: 0.5323\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3794 - mse: 0.3794 - mae: 0.4701 - val_loss: 0.4433 - val_mse: 0.4433 - val_mae: 0.5298\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4024 - mse: 0.4024 - mae: 0.4983 - val_loss: 0.4478 - val_mse: 0.4478 - val_mae: 0.5370\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3515 - mse: 0.3515 - mae: 0.4610 - val_loss: 0.4480 - val_mse: 0.4480 - val_mae: 0.5353\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3493 - mse: 0.3493 - mae: 0.4613 - val_loss: 0.4391 - val_mse: 0.4391 - val_mae: 0.5306\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3562 - mse: 0.3562 - mae: 0.4653 - val_loss: 0.4309 - val_mse: 0.4309 - val_mae: 0.5272\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3533 - mse: 0.3533 - mae: 0.4704 - val_loss: 0.4313 - val_mse: 0.4313 - val_mae: 0.5233\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3231 - mse: 0.3231 - mae: 0.4427 - val_loss: 0.4455 - val_mse: 0.4455 - val_mae: 0.5314\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3516 - mse: 0.3516 - mae: 0.4552 - val_loss: 0.4475 - val_mse: 0.4475 - val_mae: 0.5332\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3391 - mse: 0.3391 - mae: 0.4463 - val_loss: 0.4380 - val_mse: 0.4380 - val_mae: 0.5295\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3562 - mse: 0.3562 - mae: 0.4660 - val_loss: 0.4580 - val_mse: 0.4580 - val_mae: 0.5372\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3606 - mse: 0.3606 - mae: 0.4724 - val_loss: 0.4395 - val_mse: 0.4395 - val_mae: 0.5318\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3532 - mse: 0.3532 - mae: 0.4617 - val_loss: 0.4347 - val_mse: 0.4347 - val_mae: 0.5266\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3686 - mse: 0.3686 - mae: 0.4774 - val_loss: 0.4360 - val_mse: 0.4360 - val_mae: 0.5267\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3529 - mse: 0.3529 - mae: 0.4632 - val_loss: 0.4307 - val_mse: 0.4307 - val_mae: 0.5245\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3499 - mse: 0.3499 - mae: 0.4554 - val_loss: 0.4305 - val_mse: 0.4305 - val_mae: 0.5198\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3376 - mse: 0.3376 - mae: 0.4478 - val_loss: 0.4259 - val_mse: 0.4259 - val_mae: 0.5227\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3199 - mse: 0.3199 - mae: 0.4434 - val_loss: 0.4226 - val_mse: 0.4226 - val_mae: 0.5246\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3420 - mse: 0.3420 - mae: 0.4597 - val_loss: 0.4258 - val_mse: 0.4258 - val_mae: 0.5296\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3735 - mse: 0.3735 - mae: 0.4730 - val_loss: 0.4393 - val_mse: 0.4393 - val_mae: 0.5312\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3372 - mse: 0.3372 - mae: 0.4525 - val_loss: 0.4279 - val_mse: 0.4279 - val_mae: 0.5256\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3243 - mse: 0.3243 - mae: 0.4460 - val_loss: 0.4257 - val_mse: 0.4257 - val_mae: 0.5201\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3256 - mse: 0.3256 - mae: 0.4423 - val_loss: 0.4260 - val_mse: 0.4260 - val_mae: 0.5212\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3504 - mse: 0.3504 - mae: 0.4564 - val_loss: 0.4385 - val_mse: 0.4385 - val_mae: 0.5211\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3334 - mse: 0.3334 - mae: 0.4454 - val_loss: 0.4214 - val_mse: 0.4214 - val_mae: 0.5147\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3415 - mse: 0.3415 - mae: 0.4484 - val_loss: 0.4472 - val_mse: 0.4472 - val_mae: 0.5255\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3441 - mse: 0.3441 - mae: 0.4568 - val_loss: 0.4301 - val_mse: 0.4301 - val_mae: 0.5171\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3392 - mse: 0.3392 - mae: 0.4484 - val_loss: 0.4152 - val_mse: 0.4152 - val_mae: 0.5128\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3251 - mse: 0.3251 - mae: 0.4317 - val_loss: 0.4353 - val_mse: 0.4353 - val_mae: 0.5172\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3481 - mse: 0.3481 - mae: 0.4523 - val_loss: 0.4385 - val_mse: 0.4385 - val_mae: 0.5216\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3319 - mse: 0.3319 - mae: 0.4471 - val_loss: 0.4713 - val_mse: 0.4713 - val_mae: 0.5350\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3452 - mse: 0.3452 - mae: 0.4710 - val_loss: 0.4314 - val_mse: 0.4314 - val_mae: 0.5189\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3175 - mse: 0.3175 - mae: 0.4337 - val_loss: 0.4289 - val_mse: 0.4289 - val_mae: 0.5164\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3213 - mse: 0.3213 - mae: 0.4436 - val_loss: 0.4289 - val_mse: 0.4289 - val_mae: 0.5147\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3306 - mse: 0.3306 - mae: 0.4475 - val_loss: 0.4193 - val_mse: 0.4193 - val_mae: 0.5126\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3403 - mse: 0.3403 - mae: 0.4482 - val_loss: 0.4186 - val_mse: 0.4186 - val_mae: 0.5142\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3257 - mse: 0.3257 - mae: 0.4495 - val_loss: 0.4388 - val_mse: 0.4388 - val_mae: 0.5235\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3160 - mse: 0.3160 - mae: 0.4358 - val_loss: 0.4208 - val_mse: 0.4208 - val_mae: 0.5150\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3054 - mse: 0.3054 - mae: 0.4312 - val_loss: 0.4206 - val_mse: 0.4206 - val_mae: 0.5135\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3340 - mse: 0.3340 - mae: 0.4496 - val_loss: 0.4189 - val_mse: 0.4189 - val_mae: 0.5145\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3245 - mse: 0.3245 - mae: 0.4425 - val_loss: 0.4161 - val_mse: 0.4161 - val_mae: 0.5148\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3268 - mse: 0.3268 - mae: 0.4416 - val_loss: 0.4047 - val_mse: 0.4047 - val_mae: 0.5074\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3307 - mse: 0.3307 - mae: 0.4462 - val_loss: 0.4153 - val_mse: 0.4153 - val_mae: 0.5142\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3290 - mse: 0.3290 - mae: 0.4467 - val_loss: 0.4048 - val_mse: 0.4048 - val_mae: 0.5052\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3171 - mse: 0.3171 - mae: 0.4371 - val_loss: 0.4080 - val_mse: 0.4080 - val_mae: 0.5075\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3153 - mse: 0.3153 - mae: 0.4388 - val_loss: 0.4126 - val_mse: 0.4126 - val_mae: 0.5082\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3189 - mse: 0.3189 - mae: 0.4390 - val_loss: 0.4444 - val_mse: 0.4444 - val_mae: 0.5159\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.3323 - mse: 0.3323 - mae: 0.4522 - val_loss: 0.4319 - val_mse: 0.4319 - val_mae: 0.5130\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3176 - mse: 0.3176 - mae: 0.4330 - val_loss: 0.3992 - val_mse: 0.3992 - val_mae: 0.4988\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2964 - mse: 0.2964 - mae: 0.4212 - val_loss: 0.3994 - val_mse: 0.3994 - val_mae: 0.5035\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3170 - mse: 0.3170 - mae: 0.4371 - val_loss: 0.4040 - val_mse: 0.4040 - val_mae: 0.4991\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3322 - mse: 0.3322 - mae: 0.4450 - val_loss: 0.3965 - val_mse: 0.3965 - val_mae: 0.4992\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3374 - mse: 0.3374 - mae: 0.4500 - val_loss: 0.4022 - val_mse: 0.4022 - val_mae: 0.4950\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3351 - mse: 0.3351 - mae: 0.4503 - val_loss: 0.3978 - val_mse: 0.3978 - val_mae: 0.4927\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3041 - mse: 0.3041 - mae: 0.4309 - val_loss: 0.3936 - val_mse: 0.3936 - val_mae: 0.4988\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.3032 - mse: 0.3032 - mae: 0.4209 - val_loss: 0.3974 - val_mse: 0.3974 - val_mae: 0.5002\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2946 - mse: 0.2946 - mae: 0.4144 - val_loss: 0.4141 - val_mse: 0.4141 - val_mae: 0.5048\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3107 - mse: 0.3107 - mae: 0.4405 - val_loss: 0.3945 - val_mse: 0.3945 - val_mae: 0.4975\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3084 - mse: 0.3084 - mae: 0.4320 - val_loss: 0.4026 - val_mse: 0.4026 - val_mae: 0.5007\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3108 - mse: 0.3108 - mae: 0.4313 - val_loss: 0.4076 - val_mse: 0.4076 - val_mae: 0.5057\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2986 - mse: 0.2986 - mae: 0.4246 - val_loss: 0.4106 - val_mse: 0.4106 - val_mae: 0.5084\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3125 - mse: 0.3125 - mae: 0.4385 - val_loss: 0.4335 - val_mse: 0.4335 - val_mae: 0.5149\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3256 - mse: 0.3256 - mae: 0.4385 - val_loss: 0.3967 - val_mse: 0.3967 - val_mae: 0.4949\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2935 - mse: 0.2935 - mae: 0.4134 - val_loss: 0.3964 - val_mse: 0.3964 - val_mae: 0.4920\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2816 - mse: 0.2816 - mae: 0.4086 - val_loss: 0.4243 - val_mse: 0.4243 - val_mae: 0.5017\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2978 - mse: 0.2978 - mae: 0.4189 - val_loss: 0.4104 - val_mse: 0.4104 - val_mae: 0.4982\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2948 - mse: 0.2948 - mae: 0.4248 - val_loss: 0.3961 - val_mse: 0.3961 - val_mae: 0.4965\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2947 - mse: 0.2947 - mae: 0.4235 - val_loss: 0.4073 - val_mse: 0.4073 - val_mae: 0.4975\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3118 - mse: 0.3118 - mae: 0.4326 - val_loss: 0.4122 - val_mse: 0.4122 - val_mae: 0.5004\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2983 - mse: 0.2983 - mae: 0.4280 - val_loss: 0.4029 - val_mse: 0.4029 - val_mae: 0.5034\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3073 - mse: 0.3073 - mae: 0.4313 - val_loss: 0.4410 - val_mse: 0.4410 - val_mae: 0.5196\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2985 - mse: 0.2985 - mae: 0.4270 - val_loss: 0.4056 - val_mse: 0.4056 - val_mae: 0.5006\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3113 - mse: 0.3113 - mae: 0.4251 - val_loss: 0.4007 - val_mse: 0.4007 - val_mae: 0.4958\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2829 - mse: 0.2829 - mae: 0.4078 - val_loss: 0.3959 - val_mse: 0.3959 - val_mae: 0.4905\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3429 - mse: 0.3429 - mae: 0.4589 - val_loss: 0.4098 - val_mse: 0.4098 - val_mae: 0.5024\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3145 - mse: 0.3145 - mae: 0.4315 - val_loss: 0.3981 - val_mse: 0.3981 - val_mae: 0.4988\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3090 - mse: 0.3090 - mae: 0.4236 - val_loss: 0.4038 - val_mse: 0.4038 - val_mae: 0.4922\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2953 - mse: 0.2953 - mae: 0.4161 - val_loss: 0.3850 - val_mse: 0.3850 - val_mae: 0.4850\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2957 - mse: 0.2957 - mae: 0.4144 - val_loss: 0.4346 - val_mse: 0.4346 - val_mae: 0.5055\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3086 - mse: 0.3086 - mae: 0.4273 - val_loss: 0.4377 - val_mse: 0.4377 - val_mae: 0.5055\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3050 - mse: 0.3050 - mae: 0.4340 - val_loss: 0.3892 - val_mse: 0.3892 - val_mae: 0.4925\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2904 - mse: 0.2904 - mae: 0.4170 - val_loss: 0.3825 - val_mse: 0.3825 - val_mae: 0.4877\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2661 - mse: 0.2661 - mae: 0.4013 - val_loss: 0.4161 - val_mse: 0.4161 - val_mae: 0.5025\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2901 - mse: 0.2901 - mae: 0.4161 - val_loss: 0.3970 - val_mse: 0.3970 - val_mae: 0.4934\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2760 - mse: 0.2760 - mae: 0.4065 - val_loss: 0.4036 - val_mse: 0.4036 - val_mae: 0.4958\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2885 - mse: 0.2885 - mae: 0.4199 - val_loss: 0.3902 - val_mse: 0.3902 - val_mae: 0.4826\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2832 - mse: 0.2832 - mae: 0.4089 - val_loss: 0.3855 - val_mse: 0.3855 - val_mae: 0.4821\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3063 - mse: 0.3063 - mae: 0.4265 - val_loss: 0.3949 - val_mse: 0.3949 - val_mae: 0.4845\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3059 - mse: 0.3059 - mae: 0.4278 - val_loss: 0.3903 - val_mse: 0.3903 - val_mae: 0.4806\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2974 - mse: 0.2974 - mae: 0.4232 - val_loss: 0.4124 - val_mse: 0.4124 - val_mae: 0.4974\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2905 - mse: 0.2905 - mae: 0.4183 - val_loss: 0.3998 - val_mse: 0.3998 - val_mae: 0.4906\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2729 - mse: 0.2729 - mae: 0.3978 - val_loss: 0.3925 - val_mse: 0.3925 - val_mae: 0.4877\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2900 - mse: 0.2900 - mae: 0.4161 - val_loss: 0.4282 - val_mse: 0.4282 - val_mae: 0.5052\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3355 - mse: 0.3355 - mae: 0.4535 - val_loss: 0.4102 - val_mse: 0.4102 - val_mae: 0.4974\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2791 - mse: 0.2791 - mae: 0.4092 - val_loss: 0.3851 - val_mse: 0.3851 - val_mae: 0.4814\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3022 - mse: 0.3022 - mae: 0.4205 - val_loss: 0.4079 - val_mse: 0.4079 - val_mae: 0.4927\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2703 - mse: 0.2703 - mae: 0.4067 - val_loss: 0.4159 - val_mse: 0.4159 - val_mae: 0.4986\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3061 - mse: 0.3061 - mae: 0.4351 - val_loss: 0.3901 - val_mse: 0.3901 - val_mae: 0.4869\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3203 - mse: 0.3203 - mae: 0.4377 - val_loss: 0.4052 - val_mse: 0.4052 - val_mae: 0.4899\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3075 - mse: 0.3075 - mae: 0.4356 - val_loss: 0.3843 - val_mse: 0.3843 - val_mae: 0.4787\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2889 - mse: 0.2889 - mae: 0.4254 - val_loss: 0.3804 - val_mse: 0.3804 - val_mae: 0.4748\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2808 - mse: 0.2808 - mae: 0.4100 - val_loss: 0.3873 - val_mse: 0.3873 - val_mae: 0.4765\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2954 - mse: 0.2954 - mae: 0.4156 - val_loss: 0.4933 - val_mse: 0.4933 - val_mae: 0.5385\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3363 - mse: 0.3363 - mae: 0.4549 - val_loss: 0.4175 - val_mse: 0.4175 - val_mae: 0.4901\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2815 - mse: 0.2815 - mae: 0.4094 - val_loss: 0.3829 - val_mse: 0.3829 - val_mae: 0.4796\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2938 - mse: 0.2938 - mae: 0.4143 - val_loss: 0.3993 - val_mse: 0.3993 - val_mae: 0.4922\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2692 - mse: 0.2692 - mae: 0.3994 - val_loss: 0.3897 - val_mse: 0.3897 - val_mae: 0.4797\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2901 - mse: 0.2901 - mae: 0.4110 - val_loss: 0.3836 - val_mse: 0.3836 - val_mae: 0.4798\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2736 - mse: 0.2736 - mae: 0.4036 - val_loss: 0.3813 - val_mse: 0.3813 - val_mae: 0.4701\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2585 - mse: 0.2585 - mae: 0.3960 - val_loss: 0.3828 - val_mse: 0.3828 - val_mae: 0.4761\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2638 - mse: 0.2638 - mae: 0.4000 - val_loss: 0.3982 - val_mse: 0.3982 - val_mae: 0.4895\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2873 - mse: 0.2873 - mae: 0.4203 - val_loss: 0.3923 - val_mse: 0.3923 - val_mae: 0.4828\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2663 - mse: 0.2663 - mae: 0.3996 - val_loss: 0.3871 - val_mse: 0.3871 - val_mae: 0.4832\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2663 - mse: 0.2663 - mae: 0.3970 - val_loss: 0.3819 - val_mse: 0.3819 - val_mae: 0.4776\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2682 - mse: 0.2682 - mae: 0.4028 - val_loss: 0.3878 - val_mse: 0.3878 - val_mae: 0.4742\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3007 - mse: 0.3007 - mae: 0.4200 - val_loss: 0.3790 - val_mse: 0.3790 - val_mae: 0.4753\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2881 - mse: 0.2881 - mae: 0.4116 - val_loss: 0.3895 - val_mse: 0.3895 - val_mae: 0.4815\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2658 - mse: 0.2658 - mae: 0.4084 - val_loss: 0.3775 - val_mse: 0.3775 - val_mae: 0.4695\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2985 - mse: 0.2985 - mae: 0.4227 - val_loss: 0.4555 - val_mse: 0.4555 - val_mae: 0.5151\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2701 - mse: 0.2701 - mae: 0.4005 - val_loss: 0.3895 - val_mse: 0.3895 - val_mae: 0.4758\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2636 - mse: 0.2636 - mae: 0.3926 - val_loss: 0.3876 - val_mse: 0.3876 - val_mae: 0.4761\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2789 - mse: 0.2789 - mae: 0.4087 - val_loss: 0.3873 - val_mse: 0.3873 - val_mae: 0.4797\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2700 - mse: 0.2700 - mae: 0.3994 - val_loss: 0.3948 - val_mse: 0.3948 - val_mae: 0.4798\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2920 - mse: 0.2920 - mae: 0.4167 - val_loss: 0.3849 - val_mse: 0.3849 - val_mae: 0.4841\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2858 - mse: 0.2858 - mae: 0.4108 - val_loss: 0.3862 - val_mse: 0.3862 - val_mae: 0.4770\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2970 - mse: 0.2970 - mae: 0.4242 - val_loss: 0.3819 - val_mse: 0.3819 - val_mae: 0.4803\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2636 - mse: 0.2636 - mae: 0.3921 - val_loss: 0.3832 - val_mse: 0.3832 - val_mae: 0.4689\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2854 - mse: 0.2854 - mae: 0.4159 - val_loss: 0.3710 - val_mse: 0.3710 - val_mae: 0.4694\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2703 - mse: 0.2703 - mae: 0.4021 - val_loss: 0.3737 - val_mse: 0.3737 - val_mae: 0.4696\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2513 - mse: 0.2513 - mae: 0.3886 - val_loss: 0.3695 - val_mse: 0.3695 - val_mae: 0.4673\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2576 - mse: 0.2576 - mae: 0.3849 - val_loss: 0.3759 - val_mse: 0.3759 - val_mae: 0.4651\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2571 - mse: 0.2571 - mae: 0.3929 - val_loss: 0.3859 - val_mse: 0.3859 - val_mae: 0.4738\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2650 - mse: 0.2650 - mae: 0.4013 - val_loss: 0.3866 - val_mse: 0.3866 - val_mae: 0.4792\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2669 - mse: 0.2669 - mae: 0.4040 - val_loss: 0.3803 - val_mse: 0.3803 - val_mae: 0.4684\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2564 - mse: 0.2564 - mae: 0.3863 - val_loss: 0.3866 - val_mse: 0.3866 - val_mae: 0.4704\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2767 - mse: 0.2767 - mae: 0.4178 - val_loss: 0.3902 - val_mse: 0.3902 - val_mae: 0.4841\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2713 - mse: 0.2713 - mae: 0.3964 - val_loss: 0.3867 - val_mse: 0.3867 - val_mae: 0.4801\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2728 - mse: 0.2728 - mae: 0.3960 - val_loss: 0.3776 - val_mse: 0.3776 - val_mae: 0.4732\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2596 - mse: 0.2596 - mae: 0.3915 - val_loss: 0.3854 - val_mse: 0.3854 - val_mae: 0.4785\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2567 - mse: 0.2567 - mae: 0.3921 - val_loss: 0.3805 - val_mse: 0.3805 - val_mae: 0.4773\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2728 - mse: 0.2728 - mae: 0.4090 - val_loss: 0.3833 - val_mse: 0.3833 - val_mae: 0.4756\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2640 - mse: 0.2640 - mae: 0.3986 - val_loss: 0.3810 - val_mse: 0.3810 - val_mae: 0.4731\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2832 - mse: 0.2832 - mae: 0.4081 - val_loss: 0.3817 - val_mse: 0.3817 - val_mae: 0.4728\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2857 - mse: 0.2857 - mae: 0.4175 - val_loss: 0.3795 - val_mse: 0.3795 - val_mae: 0.4699\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2574 - mse: 0.2574 - mae: 0.3940 - val_loss: 0.3807 - val_mse: 0.3807 - val_mae: 0.4742\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2929 - mse: 0.2929 - mae: 0.4208 - val_loss: 0.3805 - val_mse: 0.3805 - val_mae: 0.4772\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2747 - mse: 0.2747 - mae: 0.3977 - val_loss: 0.3693 - val_mse: 0.3693 - val_mae: 0.4605\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2663 - mse: 0.2663 - mae: 0.3947 - val_loss: 0.3780 - val_mse: 0.3780 - val_mae: 0.4738\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2695 - mse: 0.2695 - mae: 0.4039 - val_loss: 0.3665 - val_mse: 0.3665 - val_mae: 0.4629\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2655 - mse: 0.2655 - mae: 0.3943 - val_loss: 0.3799 - val_mse: 0.3799 - val_mae: 0.4711\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2600 - mse: 0.2600 - mae: 0.3953 - val_loss: 0.3864 - val_mse: 0.3864 - val_mae: 0.4723\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2544 - mse: 0.2544 - mae: 0.3945 - val_loss: 0.3924 - val_mse: 0.3924 - val_mae: 0.4777\n",
            "Epoch 252/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2775 - mse: 0.2775 - mae: 0.4098 - val_loss: 0.3808 - val_mse: 0.3808 - val_mae: 0.4717\n",
            "Epoch 253/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2562 - mse: 0.2562 - mae: 0.3828 - val_loss: 0.4043 - val_mse: 0.4043 - val_mae: 0.4781\n",
            "Epoch 254/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2727 - mse: 0.2727 - mae: 0.4044 - val_loss: 0.4050 - val_mse: 0.4050 - val_mae: 0.4878\n",
            "Epoch 255/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2879 - mse: 0.2879 - mae: 0.4163 - val_loss: 0.3958 - val_mse: 0.3958 - val_mae: 0.4811\n",
            "Epoch 256/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2866 - mse: 0.2866 - mae: 0.4158 - val_loss: 0.3705 - val_mse: 0.3705 - val_mae: 0.4669\n",
            "Epoch 257/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2705 - mse: 0.2705 - mae: 0.3988 - val_loss: 0.3995 - val_mse: 0.3995 - val_mae: 0.4863\n",
            "Epoch 258/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2781 - mse: 0.2781 - mae: 0.4045 - val_loss: 0.3861 - val_mse: 0.3861 - val_mae: 0.4795\n",
            "Epoch 259/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2776 - mse: 0.2776 - mae: 0.3992 - val_loss: 0.3791 - val_mse: 0.3791 - val_mae: 0.4686\n",
            "Epoch 260/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2548 - mse: 0.2548 - mae: 0.3914 - val_loss: 0.3876 - val_mse: 0.3876 - val_mae: 0.4830\n",
            "Epoch 261/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2575 - mse: 0.2575 - mae: 0.3891 - val_loss: 0.3923 - val_mse: 0.3923 - val_mae: 0.4770\n",
            "Epoch 262/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2605 - mse: 0.2605 - mae: 0.3974 - val_loss: 0.3699 - val_mse: 0.3699 - val_mae: 0.4683\n",
            "Epoch 263/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2755 - mse: 0.2755 - mae: 0.4020 - val_loss: 0.3682 - val_mse: 0.3682 - val_mae: 0.4669\n",
            "Epoch 264/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2645 - mse: 0.2645 - mae: 0.3935 - val_loss: 0.3945 - val_mse: 0.3945 - val_mae: 0.4755\n",
            "Epoch 265/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2467 - mse: 0.2467 - mae: 0.3832 - val_loss: 0.3933 - val_mse: 0.3933 - val_mae: 0.4819\n",
            "Epoch 266/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2490 - mse: 0.2490 - mae: 0.3886 - val_loss: 0.3756 - val_mse: 0.3756 - val_mae: 0.4655\n",
            "Epoch 267/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2706 - mse: 0.2706 - mae: 0.4028 - val_loss: 0.3630 - val_mse: 0.3630 - val_mae: 0.4622\n",
            "Epoch 268/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2997 - mse: 0.2997 - mae: 0.4239 - val_loss: 0.4299 - val_mse: 0.4299 - val_mae: 0.5139\n",
            "Epoch 269/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2783 - mse: 0.2783 - mae: 0.4136 - val_loss: 0.3776 - val_mse: 0.3776 - val_mae: 0.4671\n",
            "Epoch 270/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2414 - mse: 0.2414 - mae: 0.3794 - val_loss: 0.3749 - val_mse: 0.3749 - val_mae: 0.4613\n",
            "Epoch 271/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2590 - mse: 0.2590 - mae: 0.3915 - val_loss: 0.3734 - val_mse: 0.3734 - val_mae: 0.4697\n",
            "Epoch 272/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2596 - mse: 0.2596 - mae: 0.3948 - val_loss: 0.3751 - val_mse: 0.3751 - val_mae: 0.4705\n",
            "Epoch 273/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2683 - mse: 0.2683 - mae: 0.3934 - val_loss: 0.3802 - val_mse: 0.3802 - val_mae: 0.4707\n",
            "Epoch 274/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2716 - mse: 0.2716 - mae: 0.3945 - val_loss: 0.3817 - val_mse: 0.3817 - val_mae: 0.4638\n",
            "Epoch 275/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2408 - mse: 0.2408 - mae: 0.3845 - val_loss: 0.3851 - val_mse: 0.3851 - val_mae: 0.4752\n",
            "Epoch 276/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2407 - mse: 0.2407 - mae: 0.3813 - val_loss: 0.3861 - val_mse: 0.3861 - val_mae: 0.4772\n",
            "Epoch 277/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2587 - mse: 0.2587 - mae: 0.3903 - val_loss: 0.3798 - val_mse: 0.3798 - val_mae: 0.4660\n",
            "Epoch 278/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2393 - mse: 0.2393 - mae: 0.3743 - val_loss: 0.3930 - val_mse: 0.3930 - val_mae: 0.4888\n",
            "Epoch 279/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2623 - mse: 0.2623 - mae: 0.3903 - val_loss: 0.3911 - val_mse: 0.3911 - val_mae: 0.4856\n",
            "Epoch 280/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2350 - mse: 0.2350 - mae: 0.3768 - val_loss: 0.3758 - val_mse: 0.3758 - val_mae: 0.4654\n",
            "Epoch 281/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2616 - mse: 0.2616 - mae: 0.3948 - val_loss: 0.4049 - val_mse: 0.4049 - val_mae: 0.4926\n",
            "Epoch 282/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2665 - mse: 0.2665 - mae: 0.3905 - val_loss: 0.3746 - val_mse: 0.3746 - val_mae: 0.4651\n",
            "Epoch 283/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2385 - mse: 0.2385 - mae: 0.3820 - val_loss: 0.3761 - val_mse: 0.3761 - val_mae: 0.4689\n",
            "Epoch 284/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2395 - mse: 0.2395 - mae: 0.3786 - val_loss: 0.3822 - val_mse: 0.3822 - val_mae: 0.4787\n",
            "Epoch 285/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2711 - mse: 0.2711 - mae: 0.4065 - val_loss: 0.3911 - val_mse: 0.3911 - val_mae: 0.4840\n",
            "Epoch 286/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2583 - mse: 0.2583 - mae: 0.3945 - val_loss: 0.3828 - val_mse: 0.3828 - val_mae: 0.4802\n",
            "Epoch 287/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2480 - mse: 0.2480 - mae: 0.3831 - val_loss: 0.3759 - val_mse: 0.3759 - val_mae: 0.4709\n",
            "Epoch 288/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2531 - mse: 0.2531 - mae: 0.3915 - val_loss: 0.3847 - val_mse: 0.3847 - val_mae: 0.4759\n",
            "Epoch 289/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2534 - mse: 0.2534 - mae: 0.3869 - val_loss: 0.3961 - val_mse: 0.3961 - val_mae: 0.4823\n",
            "Epoch 290/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2415 - mse: 0.2415 - mae: 0.3755 - val_loss: 0.3821 - val_mse: 0.3821 - val_mae: 0.4693\n",
            "Epoch 291/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2509 - mse: 0.2509 - mae: 0.3827 - val_loss: 0.3814 - val_mse: 0.3814 - val_mae: 0.4643\n",
            "Epoch 292/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2525 - mse: 0.2525 - mae: 0.3907 - val_loss: 0.3798 - val_mse: 0.3798 - val_mae: 0.4738\n",
            "Epoch 293/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2476 - mse: 0.2476 - mae: 0.3789 - val_loss: 0.3899 - val_mse: 0.3899 - val_mae: 0.4749\n",
            "Epoch 294/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2523 - mse: 0.2523 - mae: 0.3901 - val_loss: 0.3935 - val_mse: 0.3935 - val_mae: 0.4794\n",
            "Epoch 295/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2614 - mse: 0.2614 - mae: 0.3988 - val_loss: 0.3987 - val_mse: 0.3987 - val_mae: 0.4944\n",
            "Epoch 296/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2428 - mse: 0.2428 - mae: 0.3862 - val_loss: 0.4085 - val_mse: 0.4085 - val_mae: 0.4846\n",
            "Epoch 297/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2453 - mse: 0.2453 - mae: 0.3764 - val_loss: 0.3820 - val_mse: 0.3820 - val_mae: 0.4761\n",
            "Epoch 298/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2558 - mse: 0.2558 - mae: 0.3927 - val_loss: 0.3713 - val_mse: 0.3713 - val_mae: 0.4633\n",
            "Epoch 299/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2525 - mse: 0.2525 - mae: 0.3854 - val_loss: 0.3829 - val_mse: 0.3829 - val_mae: 0.4775\n",
            "Epoch 300/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2446 - mse: 0.2446 - mae: 0.3817 - val_loss: 0.3842 - val_mse: 0.3842 - val_mae: 0.4712\n",
            "Epoch 301/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2540 - mse: 0.2540 - mae: 0.3894 - val_loss: 0.3917 - val_mse: 0.3917 - val_mae: 0.4802\n",
            "Epoch 302/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2612 - mse: 0.2612 - mae: 0.4001 - val_loss: 0.3926 - val_mse: 0.3926 - val_mae: 0.4951\n",
            "Epoch 303/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2713 - mse: 0.2713 - mae: 0.4085 - val_loss: 0.3714 - val_mse: 0.3714 - val_mae: 0.4645\n",
            "Epoch 304/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2366 - mse: 0.2366 - mae: 0.3716 - val_loss: 0.3906 - val_mse: 0.3906 - val_mae: 0.4796\n",
            "Epoch 305/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2369 - mse: 0.2369 - mae: 0.3761 - val_loss: 0.3739 - val_mse: 0.3739 - val_mae: 0.4672\n",
            "Epoch 306/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2471 - mse: 0.2471 - mae: 0.3816 - val_loss: 0.3691 - val_mse: 0.3691 - val_mae: 0.4699\n",
            "Epoch 307/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2370 - mse: 0.2370 - mae: 0.3783 - val_loss: 0.3745 - val_mse: 0.3745 - val_mae: 0.4727\n",
            "Epoch 308/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2309 - mse: 0.2309 - mae: 0.3710 - val_loss: 0.3868 - val_mse: 0.3868 - val_mae: 0.4738\n",
            "Epoch 309/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2631 - mse: 0.2631 - mae: 0.3945 - val_loss: 0.3946 - val_mse: 0.3946 - val_mae: 0.4744\n",
            "Epoch 310/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2418 - mse: 0.2418 - mae: 0.3850 - val_loss: 0.3895 - val_mse: 0.3895 - val_mae: 0.4756\n",
            "Epoch 311/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2466 - mse: 0.2466 - mae: 0.3805 - val_loss: 0.3881 - val_mse: 0.3881 - val_mae: 0.4779\n",
            "Epoch 312/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2589 - mse: 0.2589 - mae: 0.3958 - val_loss: 0.4101 - val_mse: 0.4101 - val_mae: 0.4895\n",
            "Epoch 313/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2450 - mse: 0.2450 - mae: 0.3724 - val_loss: 0.4002 - val_mse: 0.4002 - val_mae: 0.4888\n",
            "Epoch 314/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2445 - mse: 0.2445 - mae: 0.3782 - val_loss: 0.3846 - val_mse: 0.3846 - val_mae: 0.4763\n",
            "Epoch 315/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2419 - mse: 0.2419 - mae: 0.3776 - val_loss: 0.3943 - val_mse: 0.3943 - val_mae: 0.4838\n",
            "Epoch 316/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2382 - mse: 0.2382 - mae: 0.3852 - val_loss: 0.3935 - val_mse: 0.3935 - val_mae: 0.4788\n",
            "Epoch 317/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2401 - mse: 0.2401 - mae: 0.3806 - val_loss: 0.3842 - val_mse: 0.3842 - val_mae: 0.4731\n",
            "Epoch 318/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2501 - mse: 0.2501 - mae: 0.3886 - val_loss: 0.3887 - val_mse: 0.3887 - val_mae: 0.4798\n",
            "Epoch 319/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2413 - mse: 0.2413 - mae: 0.3820 - val_loss: 0.3772 - val_mse: 0.3772 - val_mae: 0.4724\n",
            "Epoch 320/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2291 - mse: 0.2291 - mae: 0.3677 - val_loss: 0.3723 - val_mse: 0.3723 - val_mae: 0.4726\n",
            "Epoch 321/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2318 - mse: 0.2318 - mae: 0.3729 - val_loss: 0.3811 - val_mse: 0.3811 - val_mae: 0.4754\n",
            "Epoch 322/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2310 - mse: 0.2310 - mae: 0.3783 - val_loss: 0.3780 - val_mse: 0.3780 - val_mae: 0.4740\n",
            "Epoch 323/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2552 - mse: 0.2552 - mae: 0.3876 - val_loss: 0.3867 - val_mse: 0.3867 - val_mae: 0.4759\n",
            "Epoch 324/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2481 - mse: 0.2481 - mae: 0.3800 - val_loss: 0.3942 - val_mse: 0.3942 - val_mae: 0.4820\n",
            "Epoch 325/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2369 - mse: 0.2369 - mae: 0.3727 - val_loss: 0.3765 - val_mse: 0.3765 - val_mae: 0.4683\n",
            "Epoch 326/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2577 - mse: 0.2577 - mae: 0.3857 - val_loss: 0.4057 - val_mse: 0.4057 - val_mae: 0.4864\n",
            "Epoch 327/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2462 - mse: 0.2462 - mae: 0.3870 - val_loss: 0.3839 - val_mse: 0.3839 - val_mae: 0.4737\n",
            "Epoch 328/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2379 - mse: 0.2379 - mae: 0.3700 - val_loss: 0.3938 - val_mse: 0.3938 - val_mae: 0.4836\n",
            "Epoch 329/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2501 - mse: 0.2501 - mae: 0.3811 - val_loss: 0.3828 - val_mse: 0.3828 - val_mae: 0.4776\n",
            "Epoch 330/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2325 - mse: 0.2325 - mae: 0.3757 - val_loss: 0.3856 - val_mse: 0.3856 - val_mae: 0.4742\n",
            "Epoch 331/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2371 - mse: 0.2371 - mae: 0.3710 - val_loss: 0.4210 - val_mse: 0.4210 - val_mae: 0.5015\n",
            "Epoch 332/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2248 - mse: 0.2248 - mae: 0.3635 - val_loss: 0.3985 - val_mse: 0.3985 - val_mae: 0.4870\n",
            "Epoch 333/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2277 - mse: 0.2277 - mae: 0.3652 - val_loss: 0.3839 - val_mse: 0.3839 - val_mae: 0.4779\n",
            "Epoch 334/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2357 - mse: 0.2357 - mae: 0.3706 - val_loss: 0.3831 - val_mse: 0.3831 - val_mae: 0.4793\n",
            "Epoch 335/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2405 - mse: 0.2405 - mae: 0.3817 - val_loss: 0.3961 - val_mse: 0.3961 - val_mae: 0.4916\n",
            "Epoch 336/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2508 - mse: 0.2508 - mae: 0.3885 - val_loss: 0.4053 - val_mse: 0.4053 - val_mae: 0.4900\n",
            "Epoch 337/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2413 - mse: 0.2413 - mae: 0.3780 - val_loss: 0.3842 - val_mse: 0.3842 - val_mae: 0.4745\n",
            "Epoch 338/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2336 - mse: 0.2336 - mae: 0.3725 - val_loss: 0.3836 - val_mse: 0.3836 - val_mae: 0.4736\n",
            "Epoch 339/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2361 - mse: 0.2361 - mae: 0.3701 - val_loss: 0.3804 - val_mse: 0.3804 - val_mae: 0.4773\n",
            "Epoch 340/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2438 - mse: 0.2438 - mae: 0.3842 - val_loss: 0.3827 - val_mse: 0.3827 - val_mae: 0.4712\n",
            "Epoch 341/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2478 - mse: 0.2478 - mae: 0.3904 - val_loss: 0.3878 - val_mse: 0.3878 - val_mae: 0.4719\n",
            "Epoch 342/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2336 - mse: 0.2336 - mae: 0.3766 - val_loss: 0.3741 - val_mse: 0.3741 - val_mae: 0.4727\n",
            "Epoch 343/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2318 - mse: 0.2318 - mae: 0.3733 - val_loss: 0.3904 - val_mse: 0.3904 - val_mae: 0.4791\n",
            "Epoch 344/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2251 - mse: 0.2251 - mae: 0.3633 - val_loss: 0.4047 - val_mse: 0.4047 - val_mae: 0.4901\n",
            "Epoch 345/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2479 - mse: 0.2479 - mae: 0.3823 - val_loss: 0.3892 - val_mse: 0.3892 - val_mae: 0.4762\n",
            "Epoch 346/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2513 - mse: 0.2513 - mae: 0.3821 - val_loss: 0.4053 - val_mse: 0.4053 - val_mae: 0.4919\n",
            "Epoch 347/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2561 - mse: 0.2561 - mae: 0.3968 - val_loss: 0.3969 - val_mse: 0.3969 - val_mae: 0.4797\n",
            "Epoch 348/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2298 - mse: 0.2298 - mae: 0.3710 - val_loss: 0.3983 - val_mse: 0.3983 - val_mae: 0.4807\n",
            "Epoch 349/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2392 - mse: 0.2392 - mae: 0.3804 - val_loss: 0.4078 - val_mse: 0.4078 - val_mae: 0.4927\n",
            "Epoch 350/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2385 - mse: 0.2385 - mae: 0.3723 - val_loss: 0.3926 - val_mse: 0.3926 - val_mae: 0.4793\n",
            "Epoch 351/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2079 - mse: 0.2079 - mae: 0.3516 - val_loss: 0.3962 - val_mse: 0.3962 - val_mae: 0.4807\n",
            "Epoch 352/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2258 - mse: 0.2258 - mae: 0.3715 - val_loss: 0.3912 - val_mse: 0.3912 - val_mae: 0.4796\n",
            "Epoch 353/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2184 - mse: 0.2184 - mae: 0.3633 - val_loss: 0.3932 - val_mse: 0.3932 - val_mae: 0.4777\n",
            "Epoch 354/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2281 - mse: 0.2281 - mae: 0.3653 - val_loss: 0.3889 - val_mse: 0.3889 - val_mae: 0.4804\n",
            "Epoch 355/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2269 - mse: 0.2269 - mae: 0.3771 - val_loss: 0.3832 - val_mse: 0.3832 - val_mae: 0.4780\n",
            "Epoch 356/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.2301 - mse: 0.2301 - mae: 0.3693 - val_loss: 0.4274 - val_mse: 0.4274 - val_mae: 0.5136\n",
            "Epoch 357/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2962 - mse: 0.2962 - mae: 0.4253 - val_loss: 0.3771 - val_mse: 0.3771 - val_mae: 0.4821\n",
            "Epoch 358/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2475 - mse: 0.2475 - mae: 0.3939 - val_loss: 0.3903 - val_mse: 0.3903 - val_mae: 0.4801\n",
            "Epoch 359/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2419 - mse: 0.2419 - mae: 0.3791 - val_loss: 0.3893 - val_mse: 0.3893 - val_mae: 0.4790\n",
            "Epoch 360/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2398 - mse: 0.2398 - mae: 0.3728 - val_loss: 0.3964 - val_mse: 0.3964 - val_mae: 0.4809\n",
            "Epoch 361/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2310 - mse: 0.2310 - mae: 0.3715 - val_loss: 0.4204 - val_mse: 0.4204 - val_mae: 0.5038\n",
            "Epoch 362/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2503 - mse: 0.2503 - mae: 0.3878 - val_loss: 0.3816 - val_mse: 0.3816 - val_mae: 0.4720\n",
            "Epoch 363/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2219 - mse: 0.2219 - mae: 0.3601 - val_loss: 0.3957 - val_mse: 0.3957 - val_mae: 0.4866\n",
            "Epoch 364/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2223 - mse: 0.2223 - mae: 0.3675 - val_loss: 0.4049 - val_mse: 0.4049 - val_mae: 0.4913\n",
            "Epoch 365/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2192 - mse: 0.2192 - mae: 0.3637 - val_loss: 0.3795 - val_mse: 0.3795 - val_mae: 0.4774\n",
            "Epoch 366/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2166 - mse: 0.2166 - mae: 0.3552 - val_loss: 0.3822 - val_mse: 0.3822 - val_mae: 0.4774\n",
            "Epoch 367/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2043 - mse: 0.2043 - mae: 0.3473 - val_loss: 0.3855 - val_mse: 0.3855 - val_mae: 0.4832\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.5022 - mse: 0.5022 - mae: 0.5299\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 14ms/step - loss: 18.7127 - mse: 18.7127 - mae: 4.0398 - val_loss: 9.5763 - val_mse: 9.5763 - val_mae: 2.7835\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 6.0980 - mse: 6.0980 - mae: 2.0804 - val_loss: 4.5677 - val_mse: 4.5677 - val_mae: 1.7504\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 3.8877 - mse: 3.8877 - mae: 1.5712 - val_loss: 3.8579 - val_mse: 3.8579 - val_mae: 1.5601\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 3.0227 - mse: 3.0227 - mae: 1.3992 - val_loss: 3.5275 - val_mse: 3.5275 - val_mae: 1.5006\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.6622 - mse: 2.6622 - mae: 1.3092 - val_loss: 3.2479 - val_mse: 3.2479 - val_mae: 1.4093\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.4509 - mse: 2.4509 - mae: 1.2543 - val_loss: 3.0568 - val_mse: 3.0568 - val_mae: 1.3383\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.1744 - mse: 2.1744 - mae: 1.1816 - val_loss: 2.8497 - val_mse: 2.8497 - val_mae: 1.3003\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.9683 - mse: 1.9683 - mae: 1.1347 - val_loss: 2.7174 - val_mse: 2.7174 - val_mae: 1.2572\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.8113 - mse: 1.8113 - mae: 1.0866 - val_loss: 2.5603 - val_mse: 2.5603 - val_mae: 1.2254\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.6880 - mse: 1.6880 - mae: 1.0352 - val_loss: 2.3980 - val_mse: 2.3980 - val_mae: 1.1795\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.6948 - mse: 1.6948 - mae: 1.0567 - val_loss: 2.3454 - val_mse: 2.3454 - val_mae: 1.1710\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.6346 - mse: 1.6346 - mae: 1.0072 - val_loss: 2.2864 - val_mse: 2.2864 - val_mae: 1.1380\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.6232 - mse: 1.6232 - mae: 1.0148 - val_loss: 2.1182 - val_mse: 2.1182 - val_mae: 1.1078\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.4103 - mse: 1.4103 - mae: 0.9513 - val_loss: 2.0081 - val_mse: 2.0081 - val_mae: 1.0777\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.3877 - mse: 1.3877 - mae: 0.9438 - val_loss: 1.9346 - val_mse: 1.9346 - val_mae: 1.0547\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.3444 - mse: 1.3444 - mae: 0.9288 - val_loss: 1.8364 - val_mse: 1.8364 - val_mae: 1.0325\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.2864 - mse: 1.2864 - mae: 0.9108 - val_loss: 1.7697 - val_mse: 1.7697 - val_mae: 1.0124\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2560 - mse: 1.2560 - mae: 0.9047 - val_loss: 1.7361 - val_mse: 1.7361 - val_mae: 0.9921\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2425 - mse: 1.2425 - mae: 0.8769 - val_loss: 1.6655 - val_mse: 1.6655 - val_mae: 0.9787\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0845 - mse: 1.0845 - mae: 0.8220 - val_loss: 1.5750 - val_mse: 1.5750 - val_mae: 0.9549\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1355 - mse: 1.1355 - mae: 0.8396 - val_loss: 1.5258 - val_mse: 1.5258 - val_mae: 0.9450\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0833 - mse: 1.0833 - mae: 0.8287 - val_loss: 1.4895 - val_mse: 1.4895 - val_mae: 0.9233\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0427 - mse: 1.0427 - mae: 0.8190 - val_loss: 1.4290 - val_mse: 1.4290 - val_mae: 0.9067\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0257 - mse: 1.0257 - mae: 0.8119 - val_loss: 1.3899 - val_mse: 1.3899 - val_mae: 0.8979\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9553 - mse: 0.9553 - mae: 0.7864 - val_loss: 1.3459 - val_mse: 1.3459 - val_mae: 0.8796\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9894 - mse: 0.9894 - mae: 0.7962 - val_loss: 1.2927 - val_mse: 1.2927 - val_mae: 0.8593\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.9464 - mse: 0.9464 - mae: 0.7639 - val_loss: 1.2693 - val_mse: 1.2693 - val_mae: 0.8514\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.8981 - mse: 0.8981 - mae: 0.7418 - val_loss: 1.2675 - val_mse: 1.2675 - val_mae: 0.8457\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8906 - mse: 0.8906 - mae: 0.7439 - val_loss: 1.1807 - val_mse: 1.1807 - val_mae: 0.8161\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.8743 - mse: 0.8743 - mae: 0.7496 - val_loss: 1.1427 - val_mse: 1.1427 - val_mae: 0.8077\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8303 - mse: 0.8303 - mae: 0.7310 - val_loss: 1.1139 - val_mse: 1.1139 - val_mae: 0.7894\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8281 - mse: 0.8281 - mae: 0.7245 - val_loss: 1.0908 - val_mse: 1.0908 - val_mae: 0.7882\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7804 - mse: 0.7804 - mae: 0.6943 - val_loss: 0.9985 - val_mse: 0.9985 - val_mae: 0.7671\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7744 - mse: 0.7744 - mae: 0.6905 - val_loss: 1.0105 - val_mse: 1.0105 - val_mae: 0.7566\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7425 - mse: 0.7425 - mae: 0.6866 - val_loss: 0.9748 - val_mse: 0.9748 - val_mae: 0.7530\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7383 - mse: 0.7383 - mae: 0.6708 - val_loss: 0.9784 - val_mse: 0.9784 - val_mae: 0.7433\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7488 - mse: 0.7488 - mae: 0.6778 - val_loss: 0.9995 - val_mse: 0.9995 - val_mae: 0.7290\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7602 - mse: 0.7602 - mae: 0.6910 - val_loss: 0.8909 - val_mse: 0.8909 - val_mae: 0.7151\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6997 - mse: 0.6997 - mae: 0.6603 - val_loss: 0.8614 - val_mse: 0.8614 - val_mae: 0.6966\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6760 - mse: 0.6760 - mae: 0.6405 - val_loss: 0.8643 - val_mse: 0.8643 - val_mae: 0.6927\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6537 - mse: 0.6537 - mae: 0.6309 - val_loss: 0.8709 - val_mse: 0.8709 - val_mae: 0.6909\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6406 - mse: 0.6406 - mae: 0.6243 - val_loss: 0.8081 - val_mse: 0.8081 - val_mae: 0.6874\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6347 - mse: 0.6347 - mae: 0.6277 - val_loss: 0.8147 - val_mse: 0.8147 - val_mae: 0.6696\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6146 - mse: 0.6146 - mae: 0.6211 - val_loss: 0.7811 - val_mse: 0.7811 - val_mae: 0.6597\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6537 - mse: 0.6537 - mae: 0.6275 - val_loss: 0.7769 - val_mse: 0.7769 - val_mae: 0.6522\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5937 - mse: 0.5937 - mae: 0.6025 - val_loss: 0.7525 - val_mse: 0.7525 - val_mae: 0.6461\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5818 - mse: 0.5818 - mae: 0.5904 - val_loss: 0.7486 - val_mse: 0.7486 - val_mae: 0.6423\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5866 - mse: 0.5866 - mae: 0.5977 - val_loss: 0.6858 - val_mse: 0.6858 - val_mae: 0.6348\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5278 - mse: 0.5278 - mae: 0.5731 - val_loss: 0.7000 - val_mse: 0.7000 - val_mae: 0.6293\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5917 - mse: 0.5917 - mae: 0.5994 - val_loss: 0.6801 - val_mse: 0.6801 - val_mae: 0.6286\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5584 - mse: 0.5584 - mae: 0.5831 - val_loss: 0.6699 - val_mse: 0.6699 - val_mae: 0.6201\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5470 - mse: 0.5470 - mae: 0.5803 - val_loss: 0.6692 - val_mse: 0.6692 - val_mae: 0.6101\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5337 - mse: 0.5337 - mae: 0.5675 - val_loss: 0.6401 - val_mse: 0.6401 - val_mae: 0.6292\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5590 - mse: 0.5590 - mae: 0.5832 - val_loss: 0.6419 - val_mse: 0.6419 - val_mae: 0.6102\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5302 - mse: 0.5302 - mae: 0.5673 - val_loss: 0.6223 - val_mse: 0.6223 - val_mae: 0.6012\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4938 - mse: 0.4938 - mae: 0.5501 - val_loss: 0.6350 - val_mse: 0.6350 - val_mae: 0.5967\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4658 - mse: 0.4658 - mae: 0.5275 - val_loss: 0.6154 - val_mse: 0.6154 - val_mae: 0.5991\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5180 - mse: 0.5180 - mae: 0.5582 - val_loss: 0.6056 - val_mse: 0.6056 - val_mae: 0.5936\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4979 - mse: 0.4979 - mae: 0.5516 - val_loss: 0.5687 - val_mse: 0.5687 - val_mae: 0.5923\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4734 - mse: 0.4734 - mae: 0.5396 - val_loss: 0.5723 - val_mse: 0.5723 - val_mae: 0.5850\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4913 - mse: 0.4913 - mae: 0.5386 - val_loss: 0.5832 - val_mse: 0.5832 - val_mae: 0.5796\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4821 - mse: 0.4821 - mae: 0.5476 - val_loss: 0.5785 - val_mse: 0.5785 - val_mae: 0.5748\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4868 - mse: 0.4868 - mae: 0.5480 - val_loss: 0.5593 - val_mse: 0.5593 - val_mae: 0.5813\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4790 - mse: 0.4790 - mae: 0.5407 - val_loss: 0.5626 - val_mse: 0.5626 - val_mae: 0.5900\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4517 - mse: 0.4517 - mae: 0.5314 - val_loss: 0.5567 - val_mse: 0.5567 - val_mae: 0.5634\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4904 - mse: 0.4904 - mae: 0.5420 - val_loss: 0.5383 - val_mse: 0.5383 - val_mae: 0.5632\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4402 - mse: 0.4402 - mae: 0.5075 - val_loss: 0.5401 - val_mse: 0.5401 - val_mae: 0.5581\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4804 - mse: 0.4804 - mae: 0.5389 - val_loss: 0.5148 - val_mse: 0.5148 - val_mae: 0.5562\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4723 - mse: 0.4723 - mae: 0.5397 - val_loss: 0.5469 - val_mse: 0.5469 - val_mae: 0.5471\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4272 - mse: 0.4272 - mae: 0.5132 - val_loss: 0.5101 - val_mse: 0.5101 - val_mae: 0.5473\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4116 - mse: 0.4116 - mae: 0.5003 - val_loss: 0.5582 - val_mse: 0.5582 - val_mae: 0.5518\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4484 - mse: 0.4484 - mae: 0.5284 - val_loss: 0.5147 - val_mse: 0.5147 - val_mae: 0.5376\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4288 - mse: 0.4288 - mae: 0.5034 - val_loss: 0.4916 - val_mse: 0.4916 - val_mae: 0.5363\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4233 - mse: 0.4233 - mae: 0.5013 - val_loss: 0.5123 - val_mse: 0.5123 - val_mae: 0.5364\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4270 - mse: 0.4270 - mae: 0.5041 - val_loss: 0.5020 - val_mse: 0.5020 - val_mae: 0.5382\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4219 - mse: 0.4219 - mae: 0.5076 - val_loss: 0.4966 - val_mse: 0.4966 - val_mae: 0.5275\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3934 - mse: 0.3934 - mae: 0.4869 - val_loss: 0.4922 - val_mse: 0.4922 - val_mae: 0.5264\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4031 - mse: 0.4031 - mae: 0.4917 - val_loss: 0.4846 - val_mse: 0.4846 - val_mae: 0.5215\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4222 - mse: 0.4222 - mae: 0.4944 - val_loss: 0.4620 - val_mse: 0.4620 - val_mae: 0.5279\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4113 - mse: 0.4113 - mae: 0.4998 - val_loss: 0.4680 - val_mse: 0.4680 - val_mae: 0.5245\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3923 - mse: 0.3923 - mae: 0.4844 - val_loss: 0.4595 - val_mse: 0.4595 - val_mae: 0.5147\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4014 - mse: 0.4014 - mae: 0.4919 - val_loss: 0.4511 - val_mse: 0.4511 - val_mae: 0.5168\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3927 - mse: 0.3927 - mae: 0.4854 - val_loss: 0.4908 - val_mse: 0.4908 - val_mae: 0.5196\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4137 - mse: 0.4137 - mae: 0.5071 - val_loss: 0.4704 - val_mse: 0.4704 - val_mae: 0.5155\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3947 - mse: 0.3947 - mae: 0.4821 - val_loss: 0.4364 - val_mse: 0.4364 - val_mae: 0.5138\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3822 - mse: 0.3822 - mae: 0.4809 - val_loss: 0.4686 - val_mse: 0.4686 - val_mae: 0.5130\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3948 - mse: 0.3948 - mae: 0.4918 - val_loss: 0.4482 - val_mse: 0.4482 - val_mae: 0.5024\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3875 - mse: 0.3875 - mae: 0.4725 - val_loss: 0.4510 - val_mse: 0.4510 - val_mae: 0.5030\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3733 - mse: 0.3733 - mae: 0.4738 - val_loss: 0.4380 - val_mse: 0.4380 - val_mae: 0.4937\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3867 - mse: 0.3867 - mae: 0.4829 - val_loss: 0.4357 - val_mse: 0.4357 - val_mae: 0.4961\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3860 - mse: 0.3860 - mae: 0.4777 - val_loss: 0.4300 - val_mse: 0.4300 - val_mae: 0.5020\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3901 - mse: 0.3901 - mae: 0.4834 - val_loss: 0.4330 - val_mse: 0.4330 - val_mae: 0.4959\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3737 - mse: 0.3737 - mae: 0.4747 - val_loss: 0.4864 - val_mse: 0.4864 - val_mae: 0.5190\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3846 - mse: 0.3846 - mae: 0.4818 - val_loss: 0.4673 - val_mse: 0.4673 - val_mae: 0.5115\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3734 - mse: 0.3734 - mae: 0.4703 - val_loss: 0.4344 - val_mse: 0.4344 - val_mae: 0.5014\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3605 - mse: 0.3605 - mae: 0.4643 - val_loss: 0.4584 - val_mse: 0.4584 - val_mae: 0.4974\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3885 - mse: 0.3885 - mae: 0.4853 - val_loss: 0.4282 - val_mse: 0.4282 - val_mae: 0.5005\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3606 - mse: 0.3606 - mae: 0.4715 - val_loss: 0.4286 - val_mse: 0.4286 - val_mae: 0.4856\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3532 - mse: 0.3532 - mae: 0.4636 - val_loss: 0.4184 - val_mse: 0.4184 - val_mae: 0.4876\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3576 - mse: 0.3576 - mae: 0.4619 - val_loss: 0.4112 - val_mse: 0.4112 - val_mae: 0.4951\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3888 - mse: 0.3888 - mae: 0.4803 - val_loss: 0.4074 - val_mse: 0.4074 - val_mae: 0.4815\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3819 - mse: 0.3819 - mae: 0.4817 - val_loss: 0.4125 - val_mse: 0.4125 - val_mae: 0.4922\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3687 - mse: 0.3687 - mae: 0.4700 - val_loss: 0.4699 - val_mse: 0.4699 - val_mae: 0.5141\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3705 - mse: 0.3705 - mae: 0.4748 - val_loss: 0.4762 - val_mse: 0.4762 - val_mae: 0.5200\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3868 - mse: 0.3868 - mae: 0.4758 - val_loss: 0.4190 - val_mse: 0.4190 - val_mae: 0.4747\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3437 - mse: 0.3437 - mae: 0.4526 - val_loss: 0.4415 - val_mse: 0.4415 - val_mae: 0.4967\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3633 - mse: 0.3633 - mae: 0.4613 - val_loss: 0.4170 - val_mse: 0.4170 - val_mae: 0.4843\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3514 - mse: 0.3514 - mae: 0.4586 - val_loss: 0.4510 - val_mse: 0.4510 - val_mae: 0.4966\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3581 - mse: 0.3581 - mae: 0.4624 - val_loss: 0.4081 - val_mse: 0.4081 - val_mae: 0.4832\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3736 - mse: 0.3736 - mae: 0.4745 - val_loss: 0.4075 - val_mse: 0.4075 - val_mae: 0.4831\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3964 - mse: 0.3964 - mae: 0.4713 - val_loss: 0.4137 - val_mse: 0.4137 - val_mae: 0.4918\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3236 - mse: 0.3236 - mae: 0.4473 - val_loss: 0.4178 - val_mse: 0.4178 - val_mae: 0.4884\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3226 - mse: 0.3226 - mae: 0.4402 - val_loss: 0.4110 - val_mse: 0.4110 - val_mae: 0.4812\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3342 - mse: 0.3342 - mae: 0.4464 - val_loss: 0.4076 - val_mse: 0.4076 - val_mae: 0.4894\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3382 - mse: 0.3382 - mae: 0.4478 - val_loss: 0.4138 - val_mse: 0.4138 - val_mae: 0.4810\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3151 - mse: 0.3151 - mae: 0.4328 - val_loss: 0.4123 - val_mse: 0.4123 - val_mae: 0.4934\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3555 - mse: 0.3555 - mae: 0.4642 - val_loss: 0.4064 - val_mse: 0.4064 - val_mae: 0.4800\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3563 - mse: 0.3563 - mae: 0.4527 - val_loss: 0.3977 - val_mse: 0.3977 - val_mae: 0.4692\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3350 - mse: 0.3350 - mae: 0.4483 - val_loss: 0.4002 - val_mse: 0.4002 - val_mae: 0.4831\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3571 - mse: 0.3571 - mae: 0.4511 - val_loss: 0.4157 - val_mse: 0.4157 - val_mae: 0.4760\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3457 - mse: 0.3457 - mae: 0.4566 - val_loss: 0.3970 - val_mse: 0.3970 - val_mae: 0.4699\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3416 - mse: 0.3416 - mae: 0.4483 - val_loss: 0.4386 - val_mse: 0.4386 - val_mae: 0.4930\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3383 - mse: 0.3383 - mae: 0.4496 - val_loss: 0.3902 - val_mse: 0.3902 - val_mae: 0.4723\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3471 - mse: 0.3471 - mae: 0.4595 - val_loss: 0.3917 - val_mse: 0.3917 - val_mae: 0.4710\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3155 - mse: 0.3155 - mae: 0.4306 - val_loss: 0.3911 - val_mse: 0.3911 - val_mae: 0.4749\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3128 - mse: 0.3128 - mae: 0.4319 - val_loss: 0.4037 - val_mse: 0.4037 - val_mae: 0.4656\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3452 - mse: 0.3452 - mae: 0.4487 - val_loss: 0.4185 - val_mse: 0.4185 - val_mae: 0.4834\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3203 - mse: 0.3203 - mae: 0.4295 - val_loss: 0.3932 - val_mse: 0.3932 - val_mae: 0.4700\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3208 - mse: 0.3208 - mae: 0.4384 - val_loss: 0.3858 - val_mse: 0.3858 - val_mae: 0.4704\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3323 - mse: 0.3323 - mae: 0.4435 - val_loss: 0.3952 - val_mse: 0.3952 - val_mae: 0.4724\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3332 - mse: 0.3332 - mae: 0.4391 - val_loss: 0.4624 - val_mse: 0.4624 - val_mae: 0.5093\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3344 - mse: 0.3344 - mae: 0.4518 - val_loss: 0.3914 - val_mse: 0.3914 - val_mae: 0.4591\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3296 - mse: 0.3296 - mae: 0.4476 - val_loss: 0.4406 - val_mse: 0.4406 - val_mae: 0.4907\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3298 - mse: 0.3298 - mae: 0.4405 - val_loss: 0.4493 - val_mse: 0.4493 - val_mae: 0.4961\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3094 - mse: 0.3094 - mae: 0.4323 - val_loss: 0.4464 - val_mse: 0.4464 - val_mae: 0.4958\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3205 - mse: 0.3205 - mae: 0.4332 - val_loss: 0.4391 - val_mse: 0.4391 - val_mae: 0.4927\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3133 - mse: 0.3133 - mae: 0.4332 - val_loss: 0.4198 - val_mse: 0.4198 - val_mae: 0.4875\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3204 - mse: 0.3204 - mae: 0.4342 - val_loss: 0.3916 - val_mse: 0.3916 - val_mae: 0.4727\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3061 - mse: 0.3061 - mae: 0.4199 - val_loss: 0.4131 - val_mse: 0.4131 - val_mae: 0.4818\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3005 - mse: 0.3005 - mae: 0.4237 - val_loss: 0.3956 - val_mse: 0.3956 - val_mae: 0.4764\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3345 - mse: 0.3345 - mae: 0.4442 - val_loss: 0.4012 - val_mse: 0.4012 - val_mae: 0.4856\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3230 - mse: 0.3230 - mae: 0.4369 - val_loss: 0.4038 - val_mse: 0.4038 - val_mae: 0.4800\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3148 - mse: 0.3148 - mae: 0.4318 - val_loss: 0.3970 - val_mse: 0.3970 - val_mae: 0.4706\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3047 - mse: 0.3047 - mae: 0.4288 - val_loss: 0.4079 - val_mse: 0.4079 - val_mae: 0.4787\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3108 - mse: 0.3108 - mae: 0.4316 - val_loss: 0.4094 - val_mse: 0.4094 - val_mae: 0.4727\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3322 - mse: 0.3322 - mae: 0.4466 - val_loss: 0.4137 - val_mse: 0.4137 - val_mae: 0.4806\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3380 - mse: 0.3380 - mae: 0.4546 - val_loss: 0.5739 - val_mse: 0.5739 - val_mae: 0.5721\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3486 - mse: 0.3486 - mae: 0.4572 - val_loss: 0.4293 - val_mse: 0.4293 - val_mae: 0.4895\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3138 - mse: 0.3138 - mae: 0.4316 - val_loss: 0.3898 - val_mse: 0.3898 - val_mae: 0.4684\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3167 - mse: 0.3167 - mae: 0.4324 - val_loss: 0.3892 - val_mse: 0.3892 - val_mae: 0.4738\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3056 - mse: 0.3056 - mae: 0.4309 - val_loss: 0.3932 - val_mse: 0.3932 - val_mae: 0.4704\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3093 - mse: 0.3093 - mae: 0.4251 - val_loss: 0.3850 - val_mse: 0.3850 - val_mae: 0.4723\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3077 - mse: 0.3077 - mae: 0.4206 - val_loss: 0.3888 - val_mse: 0.3888 - val_mae: 0.4700\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2940 - mse: 0.2940 - mae: 0.4185 - val_loss: 0.3943 - val_mse: 0.3943 - val_mae: 0.4695\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3128 - mse: 0.3128 - mae: 0.4304 - val_loss: 0.3912 - val_mse: 0.3912 - val_mae: 0.4661\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3106 - mse: 0.3106 - mae: 0.4214 - val_loss: 0.4129 - val_mse: 0.4129 - val_mae: 0.4865\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3032 - mse: 0.3032 - mae: 0.4290 - val_loss: 0.3788 - val_mse: 0.3788 - val_mae: 0.4629\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2976 - mse: 0.2976 - mae: 0.4180 - val_loss: 0.4144 - val_mse: 0.4144 - val_mae: 0.4867\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3125 - mse: 0.3125 - mae: 0.4378 - val_loss: 0.3826 - val_mse: 0.3826 - val_mae: 0.4616\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2936 - mse: 0.2936 - mae: 0.4159 - val_loss: 0.3960 - val_mse: 0.3960 - val_mae: 0.4846\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3107 - mse: 0.3107 - mae: 0.4307 - val_loss: 0.3875 - val_mse: 0.3875 - val_mae: 0.4606\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3256 - mse: 0.3256 - mae: 0.4337 - val_loss: 0.3948 - val_mse: 0.3948 - val_mae: 0.4740\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3030 - mse: 0.3030 - mae: 0.4316 - val_loss: 0.3867 - val_mse: 0.3867 - val_mae: 0.4707\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3045 - mse: 0.3045 - mae: 0.4276 - val_loss: 0.4005 - val_mse: 0.4005 - val_mae: 0.4778\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3115 - mse: 0.3115 - mae: 0.4369 - val_loss: 0.4789 - val_mse: 0.4789 - val_mae: 0.5215\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3230 - mse: 0.3230 - mae: 0.4497 - val_loss: 0.3697 - val_mse: 0.3697 - val_mae: 0.4602\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3011 - mse: 0.3011 - mae: 0.4260 - val_loss: 0.3890 - val_mse: 0.3890 - val_mae: 0.4745\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2946 - mse: 0.2946 - mae: 0.4123 - val_loss: 0.3762 - val_mse: 0.3762 - val_mae: 0.4652\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3044 - mse: 0.3044 - mae: 0.4257 - val_loss: 0.3713 - val_mse: 0.3713 - val_mae: 0.4636\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2953 - mse: 0.2953 - mae: 0.4198 - val_loss: 0.3722 - val_mse: 0.3722 - val_mae: 0.4566\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2799 - mse: 0.2799 - mae: 0.4031 - val_loss: 0.4011 - val_mse: 0.4011 - val_mae: 0.4787\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2883 - mse: 0.2883 - mae: 0.4124 - val_loss: 0.4061 - val_mse: 0.4061 - val_mae: 0.4773\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3018 - mse: 0.3018 - mae: 0.4220 - val_loss: 0.3774 - val_mse: 0.3774 - val_mae: 0.4693\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2898 - mse: 0.2898 - mae: 0.4188 - val_loss: 0.3887 - val_mse: 0.3887 - val_mae: 0.4667\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2757 - mse: 0.2757 - mae: 0.4066 - val_loss: 0.3733 - val_mse: 0.3733 - val_mae: 0.4646\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3098 - mse: 0.3098 - mae: 0.4257 - val_loss: 0.4012 - val_mse: 0.4012 - val_mae: 0.4791\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2966 - mse: 0.2966 - mae: 0.4231 - val_loss: 0.4039 - val_mse: 0.4039 - val_mae: 0.4807\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2900 - mse: 0.2900 - mae: 0.4195 - val_loss: 0.4361 - val_mse: 0.4361 - val_mae: 0.4993\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3260 - mse: 0.3260 - mae: 0.4504 - val_loss: 0.3679 - val_mse: 0.3679 - val_mae: 0.4619\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3112 - mse: 0.3112 - mae: 0.4278 - val_loss: 0.3976 - val_mse: 0.3976 - val_mae: 0.4749\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2761 - mse: 0.2761 - mae: 0.4003 - val_loss: 0.3990 - val_mse: 0.3990 - val_mae: 0.4763\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3020 - mse: 0.3020 - mae: 0.4200 - val_loss: 0.3979 - val_mse: 0.3979 - val_mae: 0.4750\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2800 - mse: 0.2800 - mae: 0.4060 - val_loss: 0.3693 - val_mse: 0.3693 - val_mae: 0.4570\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3059 - mse: 0.3059 - mae: 0.4297 - val_loss: 0.4458 - val_mse: 0.4458 - val_mae: 0.5077\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3393 - mse: 0.3393 - mae: 0.4464 - val_loss: 0.3978 - val_mse: 0.3978 - val_mae: 0.4756\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3104 - mse: 0.3104 - mae: 0.4272 - val_loss: 0.4041 - val_mse: 0.4041 - val_mae: 0.4846\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3046 - mse: 0.3046 - mae: 0.4317 - val_loss: 0.3936 - val_mse: 0.3936 - val_mae: 0.4781\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2827 - mse: 0.2827 - mae: 0.4114 - val_loss: 0.4434 - val_mse: 0.4434 - val_mae: 0.5100\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3137 - mse: 0.3137 - mae: 0.4324 - val_loss: 0.4101 - val_mse: 0.4101 - val_mae: 0.4860\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2878 - mse: 0.2878 - mae: 0.4182 - val_loss: 0.3914 - val_mse: 0.3914 - val_mae: 0.4756\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2751 - mse: 0.2751 - mae: 0.4069 - val_loss: 0.3699 - val_mse: 0.3699 - val_mae: 0.4656\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2775 - mse: 0.2775 - mae: 0.4058 - val_loss: 0.3818 - val_mse: 0.3818 - val_mae: 0.4669\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2692 - mse: 0.2692 - mae: 0.3924 - val_loss: 0.3957 - val_mse: 0.3957 - val_mae: 0.4780\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3102 - mse: 0.3102 - mae: 0.4341 - val_loss: 0.4016 - val_mse: 0.4016 - val_mae: 0.4778\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3095 - mse: 0.3095 - mae: 0.4269 - val_loss: 0.3734 - val_mse: 0.3734 - val_mae: 0.4682\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2995 - mse: 0.2995 - mae: 0.4287 - val_loss: 0.3860 - val_mse: 0.3860 - val_mae: 0.4656\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3018 - mse: 0.3018 - mae: 0.4149 - val_loss: 0.3777 - val_mse: 0.3777 - val_mae: 0.4651\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2890 - mse: 0.2890 - mae: 0.4106 - val_loss: 0.3614 - val_mse: 0.3614 - val_mae: 0.4612\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2832 - mse: 0.2832 - mae: 0.4155 - val_loss: 0.3666 - val_mse: 0.3666 - val_mae: 0.4598\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2861 - mse: 0.2861 - mae: 0.4123 - val_loss: 0.4560 - val_mse: 0.4560 - val_mae: 0.5153\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2813 - mse: 0.2813 - mae: 0.4137 - val_loss: 0.3750 - val_mse: 0.3750 - val_mae: 0.4611\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2770 - mse: 0.2770 - mae: 0.4032 - val_loss: 0.3701 - val_mse: 0.3701 - val_mae: 0.4623\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3001 - mse: 0.3001 - mae: 0.4303 - val_loss: 0.3677 - val_mse: 0.3677 - val_mae: 0.4568\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2759 - mse: 0.2759 - mae: 0.4052 - val_loss: 0.3540 - val_mse: 0.3540 - val_mae: 0.4495\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2804 - mse: 0.2804 - mae: 0.4069 - val_loss: 0.3721 - val_mse: 0.3721 - val_mae: 0.4740\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2874 - mse: 0.2874 - mae: 0.4180 - val_loss: 0.3651 - val_mse: 0.3651 - val_mae: 0.4598\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2855 - mse: 0.2855 - mae: 0.4070 - val_loss: 0.3656 - val_mse: 0.3656 - val_mae: 0.4559\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2601 - mse: 0.2601 - mae: 0.3940 - val_loss: 0.3692 - val_mse: 0.3692 - val_mae: 0.4638\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2885 - mse: 0.2885 - mae: 0.4111 - val_loss: 0.3796 - val_mse: 0.3796 - val_mae: 0.4724\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2758 - mse: 0.2758 - mae: 0.4079 - val_loss: 0.3717 - val_mse: 0.3717 - val_mae: 0.4525\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2619 - mse: 0.2619 - mae: 0.3884 - val_loss: 0.3754 - val_mse: 0.3754 - val_mae: 0.4735\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2648 - mse: 0.2648 - mae: 0.3870 - val_loss: 0.3754 - val_mse: 0.3754 - val_mae: 0.4656\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2496 - mse: 0.2496 - mae: 0.3829 - val_loss: 0.3688 - val_mse: 0.3688 - val_mae: 0.4633\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2851 - mse: 0.2851 - mae: 0.4120 - val_loss: 0.3840 - val_mse: 0.3840 - val_mae: 0.4674\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2701 - mse: 0.2701 - mae: 0.4038 - val_loss: 0.3743 - val_mse: 0.3743 - val_mae: 0.4615\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2905 - mse: 0.2905 - mae: 0.4134 - val_loss: 0.3787 - val_mse: 0.3787 - val_mae: 0.4702\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2800 - mse: 0.2800 - mae: 0.4082 - val_loss: 0.3761 - val_mse: 0.3761 - val_mae: 0.4610\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2884 - mse: 0.2884 - mae: 0.4116 - val_loss: 0.3800 - val_mse: 0.3800 - val_mae: 0.4708\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2744 - mse: 0.2744 - mae: 0.4083 - val_loss: 0.3911 - val_mse: 0.3911 - val_mae: 0.4683\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2796 - mse: 0.2796 - mae: 0.4074 - val_loss: 0.3968 - val_mse: 0.3968 - val_mae: 0.4775\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2897 - mse: 0.2897 - mae: 0.4161 - val_loss: 0.4163 - val_mse: 0.4163 - val_mae: 0.4902\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2913 - mse: 0.2913 - mae: 0.4171 - val_loss: 0.4106 - val_mse: 0.4106 - val_mae: 0.4862\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2728 - mse: 0.2728 - mae: 0.4049 - val_loss: 0.3636 - val_mse: 0.3636 - val_mae: 0.4491\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2809 - mse: 0.2809 - mae: 0.4081 - val_loss: 0.3690 - val_mse: 0.3690 - val_mae: 0.4701\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2760 - mse: 0.2760 - mae: 0.4056 - val_loss: 0.3608 - val_mse: 0.3608 - val_mae: 0.4561\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2646 - mse: 0.2646 - mae: 0.3975 - val_loss: 0.3908 - val_mse: 0.3908 - val_mae: 0.4769\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2821 - mse: 0.2821 - mae: 0.4090 - val_loss: 0.3689 - val_mse: 0.3689 - val_mae: 0.4693\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2923 - mse: 0.2923 - mae: 0.4132 - val_loss: 0.3524 - val_mse: 0.3524 - val_mae: 0.4426\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2883 - mse: 0.2883 - mae: 0.4178 - val_loss: 0.3848 - val_mse: 0.3848 - val_mae: 0.4731\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2852 - mse: 0.2852 - mae: 0.4063 - val_loss: 0.3933 - val_mse: 0.3933 - val_mae: 0.4798\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2828 - mse: 0.2828 - mae: 0.4060 - val_loss: 0.3747 - val_mse: 0.3747 - val_mae: 0.4592\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2781 - mse: 0.2781 - mae: 0.4128 - val_loss: 0.3947 - val_mse: 0.3947 - val_mae: 0.4730\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2823 - mse: 0.2823 - mae: 0.4086 - val_loss: 0.3569 - val_mse: 0.3569 - val_mae: 0.4518\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2857 - mse: 0.2857 - mae: 0.4093 - val_loss: 0.3588 - val_mse: 0.3588 - val_mae: 0.4496\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2632 - mse: 0.2632 - mae: 0.3953 - val_loss: 0.3901 - val_mse: 0.3901 - val_mae: 0.4771\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2601 - mse: 0.2601 - mae: 0.3894 - val_loss: 0.3505 - val_mse: 0.3505 - val_mae: 0.4462\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2744 - mse: 0.2744 - mae: 0.4089 - val_loss: 0.3556 - val_mse: 0.3556 - val_mae: 0.4562\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2767 - mse: 0.2767 - mae: 0.4140 - val_loss: 0.3546 - val_mse: 0.3546 - val_mae: 0.4550\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2536 - mse: 0.2536 - mae: 0.3870 - val_loss: 0.3569 - val_mse: 0.3569 - val_mae: 0.4534\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2804 - mse: 0.2804 - mae: 0.4136 - val_loss: 0.3706 - val_mse: 0.3706 - val_mae: 0.4611\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2831 - mse: 0.2831 - mae: 0.4063 - val_loss: 0.3807 - val_mse: 0.3807 - val_mae: 0.4644\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2674 - mse: 0.2674 - mae: 0.4001 - val_loss: 0.3763 - val_mse: 0.3763 - val_mae: 0.4611\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2811 - mse: 0.2811 - mae: 0.4100 - val_loss: 0.3961 - val_mse: 0.3961 - val_mae: 0.4754\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2561 - mse: 0.2561 - mae: 0.3915 - val_loss: 0.3755 - val_mse: 0.3755 - val_mae: 0.4599\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2544 - mse: 0.2544 - mae: 0.3867 - val_loss: 0.3616 - val_mse: 0.3616 - val_mae: 0.4443\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2796 - mse: 0.2796 - mae: 0.4131 - val_loss: 0.3567 - val_mse: 0.3567 - val_mae: 0.4495\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2651 - mse: 0.2651 - mae: 0.4041 - val_loss: 0.3586 - val_mse: 0.3586 - val_mae: 0.4454\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2596 - mse: 0.2596 - mae: 0.3937 - val_loss: 0.3601 - val_mse: 0.3601 - val_mae: 0.4595\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2547 - mse: 0.2547 - mae: 0.3911 - val_loss: 0.3808 - val_mse: 0.3808 - val_mae: 0.4670\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2567 - mse: 0.2567 - mae: 0.3917 - val_loss: 0.3767 - val_mse: 0.3767 - val_mae: 0.4719\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2697 - mse: 0.2697 - mae: 0.3971 - val_loss: 0.3558 - val_mse: 0.3558 - val_mae: 0.4580\n",
            "Epoch 252/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2570 - mse: 0.2570 - mae: 0.3900 - val_loss: 0.3551 - val_mse: 0.3551 - val_mae: 0.4703\n",
            "Epoch 253/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2685 - mse: 0.2685 - mae: 0.4116 - val_loss: 0.3615 - val_mse: 0.3615 - val_mae: 0.4509\n",
            "Epoch 254/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2639 - mse: 0.2639 - mae: 0.3997 - val_loss: 0.3571 - val_mse: 0.3571 - val_mae: 0.4567\n",
            "Epoch 255/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2620 - mse: 0.2620 - mae: 0.3999 - val_loss: 0.3505 - val_mse: 0.3505 - val_mae: 0.4402\n",
            "Epoch 256/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2630 - mse: 0.2630 - mae: 0.3976 - val_loss: 0.3583 - val_mse: 0.3583 - val_mae: 0.4578\n",
            "Epoch 257/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2590 - mse: 0.2590 - mae: 0.3925 - val_loss: 0.3802 - val_mse: 0.3802 - val_mae: 0.4663\n",
            "Epoch 258/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2663 - mse: 0.2663 - mae: 0.4011 - val_loss: 0.3624 - val_mse: 0.3624 - val_mae: 0.4532\n",
            "Epoch 259/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2673 - mse: 0.2673 - mae: 0.4001 - val_loss: 0.3453 - val_mse: 0.3453 - val_mae: 0.4526\n",
            "Epoch 260/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2575 - mse: 0.2575 - mae: 0.3951 - val_loss: 0.3423 - val_mse: 0.3423 - val_mae: 0.4392\n",
            "Epoch 261/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2405 - mse: 0.2405 - mae: 0.3802 - val_loss: 0.3478 - val_mse: 0.3478 - val_mae: 0.4447\n",
            "Epoch 262/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2665 - mse: 0.2665 - mae: 0.4033 - val_loss: 0.3420 - val_mse: 0.3420 - val_mae: 0.4433\n",
            "Epoch 263/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2829 - mse: 0.2829 - mae: 0.4088 - val_loss: 0.3564 - val_mse: 0.3564 - val_mae: 0.4542\n",
            "Epoch 264/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2592 - mse: 0.2592 - mae: 0.3969 - val_loss: 0.3605 - val_mse: 0.3605 - val_mae: 0.4586\n",
            "Epoch 265/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2555 - mse: 0.2555 - mae: 0.3908 - val_loss: 0.3624 - val_mse: 0.3624 - val_mae: 0.4601\n",
            "Epoch 266/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2554 - mse: 0.2554 - mae: 0.3834 - val_loss: 0.4089 - val_mse: 0.4089 - val_mae: 0.4863\n",
            "Epoch 267/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2670 - mse: 0.2670 - mae: 0.4043 - val_loss: 0.4311 - val_mse: 0.4311 - val_mae: 0.5008\n",
            "Epoch 268/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2769 - mse: 0.2769 - mae: 0.4096 - val_loss: 0.3464 - val_mse: 0.3464 - val_mae: 0.4455\n",
            "Epoch 269/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2713 - mse: 0.2713 - mae: 0.4088 - val_loss: 0.3674 - val_mse: 0.3674 - val_mae: 0.4719\n",
            "Epoch 270/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2566 - mse: 0.2566 - mae: 0.3892 - val_loss: 0.3396 - val_mse: 0.3396 - val_mae: 0.4436\n",
            "Epoch 271/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2820 - mse: 0.2820 - mae: 0.4108 - val_loss: 0.4049 - val_mse: 0.4049 - val_mae: 0.4782\n",
            "Epoch 272/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2877 - mse: 0.2877 - mae: 0.4115 - val_loss: 0.3485 - val_mse: 0.3485 - val_mae: 0.4468\n",
            "Epoch 273/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2587 - mse: 0.2587 - mae: 0.3890 - val_loss: 0.3681 - val_mse: 0.3681 - val_mae: 0.4626\n",
            "Epoch 274/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2878 - mse: 0.2878 - mae: 0.4194 - val_loss: 0.4059 - val_mse: 0.4059 - val_mae: 0.4886\n",
            "Epoch 275/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2682 - mse: 0.2682 - mae: 0.3940 - val_loss: 0.3664 - val_mse: 0.3664 - val_mae: 0.4557\n",
            "Epoch 276/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2565 - mse: 0.2565 - mae: 0.3987 - val_loss: 0.3774 - val_mse: 0.3774 - val_mae: 0.4701\n",
            "Epoch 277/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2743 - mse: 0.2743 - mae: 0.4089 - val_loss: 0.3485 - val_mse: 0.3485 - val_mae: 0.4481\n",
            "Epoch 278/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2472 - mse: 0.2472 - mae: 0.3815 - val_loss: 0.3397 - val_mse: 0.3397 - val_mae: 0.4403\n",
            "Epoch 279/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2547 - mse: 0.2547 - mae: 0.3864 - val_loss: 0.3364 - val_mse: 0.3364 - val_mae: 0.4423\n",
            "Epoch 280/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2525 - mse: 0.2525 - mae: 0.3966 - val_loss: 0.3676 - val_mse: 0.3676 - val_mae: 0.4633\n",
            "Epoch 281/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2399 - mse: 0.2399 - mae: 0.3730 - val_loss: 0.3444 - val_mse: 0.3444 - val_mae: 0.4473\n",
            "Epoch 282/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2434 - mse: 0.2434 - mae: 0.3836 - val_loss: 0.3438 - val_mse: 0.3438 - val_mae: 0.4394\n",
            "Epoch 283/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2467 - mse: 0.2467 - mae: 0.3848 - val_loss: 0.3566 - val_mse: 0.3566 - val_mae: 0.4530\n",
            "Epoch 284/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2545 - mse: 0.2545 - mae: 0.3851 - val_loss: 0.3452 - val_mse: 0.3452 - val_mae: 0.4464\n",
            "Epoch 285/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2373 - mse: 0.2373 - mae: 0.3689 - val_loss: 0.3527 - val_mse: 0.3527 - val_mae: 0.4505\n",
            "Epoch 286/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2578 - mse: 0.2578 - mae: 0.3974 - val_loss: 0.3775 - val_mse: 0.3775 - val_mae: 0.4679\n",
            "Epoch 287/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2509 - mse: 0.2509 - mae: 0.3914 - val_loss: 0.3591 - val_mse: 0.3591 - val_mae: 0.4487\n",
            "Epoch 288/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2611 - mse: 0.2611 - mae: 0.3905 - val_loss: 0.3489 - val_mse: 0.3489 - val_mae: 0.4575\n",
            "Epoch 289/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2516 - mse: 0.2516 - mae: 0.3888 - val_loss: 0.4036 - val_mse: 0.4036 - val_mae: 0.4810\n",
            "Epoch 290/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2482 - mse: 0.2482 - mae: 0.3838 - val_loss: 0.3605 - val_mse: 0.3605 - val_mae: 0.4577\n",
            "Epoch 291/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2514 - mse: 0.2514 - mae: 0.3873 - val_loss: 0.3507 - val_mse: 0.3507 - val_mae: 0.4431\n",
            "Epoch 292/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2526 - mse: 0.2526 - mae: 0.3875 - val_loss: 0.3361 - val_mse: 0.3361 - val_mae: 0.4343\n",
            "Epoch 293/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2426 - mse: 0.2426 - mae: 0.3810 - val_loss: 0.4025 - val_mse: 0.4025 - val_mae: 0.4886\n",
            "Epoch 294/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2497 - mse: 0.2497 - mae: 0.3925 - val_loss: 0.3482 - val_mse: 0.3482 - val_mae: 0.4480\n",
            "Epoch 295/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2532 - mse: 0.2532 - mae: 0.3884 - val_loss: 0.3478 - val_mse: 0.3478 - val_mae: 0.4485\n",
            "Epoch 296/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2573 - mse: 0.2573 - mae: 0.3935 - val_loss: 0.3511 - val_mse: 0.3511 - val_mae: 0.4515\n",
            "Epoch 297/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2449 - mse: 0.2449 - mae: 0.3790 - val_loss: 0.3399 - val_mse: 0.3399 - val_mae: 0.4438\n",
            "Epoch 298/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2506 - mse: 0.2506 - mae: 0.3922 - val_loss: 0.3411 - val_mse: 0.3411 - val_mae: 0.4485\n",
            "Epoch 299/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2358 - mse: 0.2358 - mae: 0.3759 - val_loss: 0.3543 - val_mse: 0.3543 - val_mae: 0.4479\n",
            "Epoch 300/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2512 - mse: 0.2512 - mae: 0.3846 - val_loss: 0.3498 - val_mse: 0.3498 - val_mae: 0.4481\n",
            "Epoch 301/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2420 - mse: 0.2420 - mae: 0.3813 - val_loss: 0.3422 - val_mse: 0.3422 - val_mae: 0.4501\n",
            "Epoch 302/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2769 - mse: 0.2769 - mae: 0.4035 - val_loss: 0.3408 - val_mse: 0.3408 - val_mae: 0.4474\n",
            "Epoch 303/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2490 - mse: 0.2490 - mae: 0.3857 - val_loss: 0.3773 - val_mse: 0.3773 - val_mae: 0.4666\n",
            "Epoch 304/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2519 - mse: 0.2519 - mae: 0.3908 - val_loss: 0.4068 - val_mse: 0.4068 - val_mae: 0.4859\n",
            "Epoch 305/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2482 - mse: 0.2482 - mae: 0.3871 - val_loss: 0.3781 - val_mse: 0.3781 - val_mae: 0.4656\n",
            "Epoch 306/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2486 - mse: 0.2486 - mae: 0.3918 - val_loss: 0.3374 - val_mse: 0.3374 - val_mae: 0.4458\n",
            "Epoch 307/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2353 - mse: 0.2353 - mae: 0.3709 - val_loss: 0.3594 - val_mse: 0.3594 - val_mae: 0.4600\n",
            "Epoch 308/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2441 - mse: 0.2441 - mae: 0.3853 - val_loss: 0.3601 - val_mse: 0.3601 - val_mae: 0.4568\n",
            "Epoch 309/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2422 - mse: 0.2422 - mae: 0.3860 - val_loss: 0.3561 - val_mse: 0.3561 - val_mae: 0.4602\n",
            "Epoch 310/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2544 - mse: 0.2544 - mae: 0.3910 - val_loss: 0.3443 - val_mse: 0.3443 - val_mae: 0.4510\n",
            "Epoch 311/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2479 - mse: 0.2479 - mae: 0.3830 - val_loss: 0.3454 - val_mse: 0.3454 - val_mae: 0.4502\n",
            "Epoch 312/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2541 - mse: 0.2541 - mae: 0.3898 - val_loss: 0.3662 - val_mse: 0.3662 - val_mae: 0.4616\n",
            "Epoch 313/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2416 - mse: 0.2416 - mae: 0.3726 - val_loss: 0.3432 - val_mse: 0.3432 - val_mae: 0.4473\n",
            "Epoch 314/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2460 - mse: 0.2460 - mae: 0.3871 - val_loss: 0.3522 - val_mse: 0.3522 - val_mae: 0.4561\n",
            "Epoch 315/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2374 - mse: 0.2374 - mae: 0.3771 - val_loss: 0.3494 - val_mse: 0.3494 - val_mae: 0.4566\n",
            "Epoch 316/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2393 - mse: 0.2393 - mae: 0.3792 - val_loss: 0.3751 - val_mse: 0.3751 - val_mae: 0.4610\n",
            "Epoch 317/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2367 - mse: 0.2367 - mae: 0.3748 - val_loss: 0.3562 - val_mse: 0.3562 - val_mae: 0.4578\n",
            "Epoch 318/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2363 - mse: 0.2363 - mae: 0.3793 - val_loss: 0.3249 - val_mse: 0.3249 - val_mae: 0.4354\n",
            "Epoch 319/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2404 - mse: 0.2404 - mae: 0.3791 - val_loss: 0.3462 - val_mse: 0.3462 - val_mae: 0.4520\n",
            "Epoch 320/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2282 - mse: 0.2282 - mae: 0.3699 - val_loss: 0.3502 - val_mse: 0.3502 - val_mae: 0.4528\n",
            "Epoch 321/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2376 - mse: 0.2376 - mae: 0.3749 - val_loss: 0.3514 - val_mse: 0.3514 - val_mae: 0.4661\n",
            "Epoch 322/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2568 - mse: 0.2568 - mae: 0.4007 - val_loss: 0.3410 - val_mse: 0.3410 - val_mae: 0.4416\n",
            "Epoch 323/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2478 - mse: 0.2478 - mae: 0.3803 - val_loss: 0.3392 - val_mse: 0.3392 - val_mae: 0.4495\n",
            "Epoch 324/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2576 - mse: 0.2576 - mae: 0.3953 - val_loss: 0.3458 - val_mse: 0.3458 - val_mae: 0.4520\n",
            "Epoch 325/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2474 - mse: 0.2474 - mae: 0.3826 - val_loss: 0.3523 - val_mse: 0.3523 - val_mae: 0.4485\n",
            "Epoch 326/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2352 - mse: 0.2352 - mae: 0.3758 - val_loss: 0.3431 - val_mse: 0.3431 - val_mae: 0.4524\n",
            "Epoch 327/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2225 - mse: 0.2225 - mae: 0.3670 - val_loss: 0.3441 - val_mse: 0.3441 - val_mae: 0.4502\n",
            "Epoch 328/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2461 - mse: 0.2461 - mae: 0.3863 - val_loss: 0.3520 - val_mse: 0.3520 - val_mae: 0.4507\n",
            "Epoch 329/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2393 - mse: 0.2393 - mae: 0.3833 - val_loss: 0.3494 - val_mse: 0.3494 - val_mae: 0.4428\n",
            "Epoch 330/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2207 - mse: 0.2207 - mae: 0.3604 - val_loss: 0.3553 - val_mse: 0.3553 - val_mae: 0.4475\n",
            "Epoch 331/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2495 - mse: 0.2495 - mae: 0.3905 - val_loss: 0.3337 - val_mse: 0.3337 - val_mae: 0.4453\n",
            "Epoch 332/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2480 - mse: 0.2480 - mae: 0.3838 - val_loss: 0.3336 - val_mse: 0.3336 - val_mae: 0.4387\n",
            "Epoch 333/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2507 - mse: 0.2507 - mae: 0.3889 - val_loss: 0.3429 - val_mse: 0.3429 - val_mae: 0.4595\n",
            "Epoch 334/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2679 - mse: 0.2679 - mae: 0.4107 - val_loss: 0.3319 - val_mse: 0.3319 - val_mae: 0.4381\n",
            "Epoch 335/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2631 - mse: 0.2631 - mae: 0.3945 - val_loss: 0.3798 - val_mse: 0.3798 - val_mae: 0.4706\n",
            "Epoch 336/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2482 - mse: 0.2482 - mae: 0.3872 - val_loss: 0.4347 - val_mse: 0.4347 - val_mae: 0.5059\n",
            "Epoch 337/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2459 - mse: 0.2459 - mae: 0.3887 - val_loss: 0.3841 - val_mse: 0.3841 - val_mae: 0.4720\n",
            "Epoch 338/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2355 - mse: 0.2355 - mae: 0.3729 - val_loss: 0.3517 - val_mse: 0.3517 - val_mae: 0.4455\n",
            "Epoch 339/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2433 - mse: 0.2433 - mae: 0.3840 - val_loss: 0.3397 - val_mse: 0.3397 - val_mae: 0.4376\n",
            "Epoch 340/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2212 - mse: 0.2212 - mae: 0.3653 - val_loss: 0.3388 - val_mse: 0.3388 - val_mae: 0.4450\n",
            "Epoch 341/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2638 - mse: 0.2638 - mae: 0.4012 - val_loss: 0.3328 - val_mse: 0.3328 - val_mae: 0.4347\n",
            "Epoch 342/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2533 - mse: 0.2533 - mae: 0.3937 - val_loss: 0.3554 - val_mse: 0.3554 - val_mae: 0.4536\n",
            "Epoch 343/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2309 - mse: 0.2309 - mae: 0.3739 - val_loss: 0.3628 - val_mse: 0.3628 - val_mae: 0.4566\n",
            "Epoch 344/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2390 - mse: 0.2390 - mae: 0.3769 - val_loss: 0.3408 - val_mse: 0.3408 - val_mae: 0.4356\n",
            "Epoch 345/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2308 - mse: 0.2308 - mae: 0.3702 - val_loss: 0.3490 - val_mse: 0.3490 - val_mae: 0.4500\n",
            "Epoch 346/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2662 - mse: 0.2662 - mae: 0.3894 - val_loss: 0.3727 - val_mse: 0.3727 - val_mae: 0.4618\n",
            "Epoch 347/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2538 - mse: 0.2538 - mae: 0.3859 - val_loss: 0.3867 - val_mse: 0.3867 - val_mae: 0.4640\n",
            "Epoch 348/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2444 - mse: 0.2444 - mae: 0.3830 - val_loss: 0.3553 - val_mse: 0.3553 - val_mae: 0.4398\n",
            "Epoch 349/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2377 - mse: 0.2377 - mae: 0.3840 - val_loss: 0.3482 - val_mse: 0.3482 - val_mae: 0.4421\n",
            "Epoch 350/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2384 - mse: 0.2384 - mae: 0.3824 - val_loss: 0.3482 - val_mse: 0.3482 - val_mae: 0.4519\n",
            "Epoch 351/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2478 - mse: 0.2478 - mae: 0.3877 - val_loss: 0.3339 - val_mse: 0.3339 - val_mae: 0.4283\n",
            "Epoch 352/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2330 - mse: 0.2330 - mae: 0.3776 - val_loss: 0.3452 - val_mse: 0.3452 - val_mae: 0.4444\n",
            "Epoch 353/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2323 - mse: 0.2323 - mae: 0.3726 - val_loss: 0.3718 - val_mse: 0.3718 - val_mae: 0.4635\n",
            "Epoch 354/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2454 - mse: 0.2454 - mae: 0.3854 - val_loss: 0.3341 - val_mse: 0.3341 - val_mae: 0.4363\n",
            "Epoch 355/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2443 - mse: 0.2443 - mae: 0.3781 - val_loss: 0.3674 - val_mse: 0.3674 - val_mae: 0.4597\n",
            "Epoch 356/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2449 - mse: 0.2449 - mae: 0.3847 - val_loss: 0.3780 - val_mse: 0.3780 - val_mae: 0.4636\n",
            "Epoch 357/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2377 - mse: 0.2377 - mae: 0.3814 - val_loss: 0.3337 - val_mse: 0.3337 - val_mae: 0.4359\n",
            "Epoch 358/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2319 - mse: 0.2319 - mae: 0.3692 - val_loss: 0.3343 - val_mse: 0.3343 - val_mae: 0.4365\n",
            "Epoch 359/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2268 - mse: 0.2268 - mae: 0.3667 - val_loss: 0.3369 - val_mse: 0.3369 - val_mae: 0.4361\n",
            "Epoch 360/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2313 - mse: 0.2313 - mae: 0.3648 - val_loss: 0.3587 - val_mse: 0.3587 - val_mae: 0.4648\n",
            "Epoch 361/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2753 - mse: 0.2753 - mae: 0.4040 - val_loss: 0.3882 - val_mse: 0.3882 - val_mae: 0.4749\n",
            "Epoch 362/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2360 - mse: 0.2360 - mae: 0.3780 - val_loss: 0.3344 - val_mse: 0.3344 - val_mae: 0.4340\n",
            "Epoch 363/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2228 - mse: 0.2228 - mae: 0.3618 - val_loss: 0.3222 - val_mse: 0.3222 - val_mae: 0.4268\n",
            "Epoch 364/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2271 - mse: 0.2271 - mae: 0.3725 - val_loss: 0.3537 - val_mse: 0.3537 - val_mae: 0.4485\n",
            "Epoch 365/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2511 - mse: 0.2511 - mae: 0.3928 - val_loss: 0.3480 - val_mse: 0.3480 - val_mae: 0.4424\n",
            "Epoch 366/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2269 - mse: 0.2269 - mae: 0.3680 - val_loss: 0.3511 - val_mse: 0.3511 - val_mae: 0.4546\n",
            "Epoch 367/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2235 - mse: 0.2235 - mae: 0.3645 - val_loss: 0.3354 - val_mse: 0.3354 - val_mae: 0.4357\n",
            "Epoch 368/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2373 - mse: 0.2373 - mae: 0.3775 - val_loss: 0.3411 - val_mse: 0.3411 - val_mae: 0.4497\n",
            "Epoch 369/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2411 - mse: 0.2411 - mae: 0.3842 - val_loss: 0.3429 - val_mse: 0.3429 - val_mae: 0.4331\n",
            "Epoch 370/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2266 - mse: 0.2266 - mae: 0.3739 - val_loss: 0.3630 - val_mse: 0.3630 - val_mae: 0.4583\n",
            "Epoch 371/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2435 - mse: 0.2435 - mae: 0.3885 - val_loss: 0.4374 - val_mse: 0.4374 - val_mae: 0.5087\n",
            "Epoch 372/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2602 - mse: 0.2602 - mae: 0.4002 - val_loss: 0.3788 - val_mse: 0.3788 - val_mae: 0.4639\n",
            "Epoch 373/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2399 - mse: 0.2399 - mae: 0.3781 - val_loss: 0.3503 - val_mse: 0.3503 - val_mae: 0.4423\n",
            "Epoch 374/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2237 - mse: 0.2237 - mae: 0.3675 - val_loss: 0.3585 - val_mse: 0.3585 - val_mae: 0.4476\n",
            "Epoch 375/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2452 - mse: 0.2452 - mae: 0.3852 - val_loss: 0.4885 - val_mse: 0.4885 - val_mae: 0.5404\n",
            "Epoch 376/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2430 - mse: 0.2430 - mae: 0.3811 - val_loss: 0.3596 - val_mse: 0.3596 - val_mae: 0.4476\n",
            "Epoch 377/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2311 - mse: 0.2311 - mae: 0.3744 - val_loss: 0.3525 - val_mse: 0.3525 - val_mae: 0.4633\n",
            "Epoch 378/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2389 - mse: 0.2389 - mae: 0.3758 - val_loss: 0.3498 - val_mse: 0.3498 - val_mae: 0.4432\n",
            "Epoch 379/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2176 - mse: 0.2176 - mae: 0.3619 - val_loss: 0.3738 - val_mse: 0.3738 - val_mae: 0.4568\n",
            "Epoch 380/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2224 - mse: 0.2224 - mae: 0.3638 - val_loss: 0.3523 - val_mse: 0.3523 - val_mae: 0.4442\n",
            "Epoch 381/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2205 - mse: 0.2205 - mae: 0.3680 - val_loss: 0.3547 - val_mse: 0.3547 - val_mae: 0.4398\n",
            "Epoch 382/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2544 - mse: 0.2544 - mae: 0.3849 - val_loss: 0.3516 - val_mse: 0.3516 - val_mae: 0.4403\n",
            "Epoch 383/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2474 - mse: 0.2474 - mae: 0.3834 - val_loss: 0.3448 - val_mse: 0.3448 - val_mae: 0.4388\n",
            "Epoch 384/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2442 - mse: 0.2442 - mae: 0.3748 - val_loss: 0.3473 - val_mse: 0.3473 - val_mae: 0.4398\n",
            "Epoch 385/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2120 - mse: 0.2120 - mae: 0.3602 - val_loss: 0.3764 - val_mse: 0.3764 - val_mae: 0.4575\n",
            "Epoch 386/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2018 - mse: 0.2018 - mae: 0.3476 - val_loss: 0.3598 - val_mse: 0.3598 - val_mae: 0.4432\n",
            "Epoch 387/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2311 - mse: 0.2311 - mae: 0.3746 - val_loss: 0.3567 - val_mse: 0.3567 - val_mae: 0.4451\n",
            "Epoch 388/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2310 - mse: 0.2310 - mae: 0.3738 - val_loss: 0.3404 - val_mse: 0.3404 - val_mae: 0.4408\n",
            "Epoch 389/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2314 - mse: 0.2314 - mae: 0.3707 - val_loss: 0.3770 - val_mse: 0.3770 - val_mae: 0.4643\n",
            "Epoch 390/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2575 - mse: 0.2575 - mae: 0.3910 - val_loss: 0.3726 - val_mse: 0.3726 - val_mae: 0.4520\n",
            "Epoch 391/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2244 - mse: 0.2244 - mae: 0.3621 - val_loss: 0.4050 - val_mse: 0.4050 - val_mae: 0.4796\n",
            "Epoch 392/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2442 - mse: 0.2442 - mae: 0.3795 - val_loss: 0.3683 - val_mse: 0.3683 - val_mae: 0.4529\n",
            "Epoch 393/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2305 - mse: 0.2305 - mae: 0.3679 - val_loss: 0.3546 - val_mse: 0.3546 - val_mae: 0.4503\n",
            "Epoch 394/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2618 - mse: 0.2618 - mae: 0.3953 - val_loss: 0.3680 - val_mse: 0.3680 - val_mae: 0.4717\n",
            "Epoch 395/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2580 - mse: 0.2580 - mae: 0.3971 - val_loss: 0.3489 - val_mse: 0.3489 - val_mae: 0.4517\n",
            "Epoch 396/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2229 - mse: 0.2229 - mae: 0.3727 - val_loss: 0.3623 - val_mse: 0.3623 - val_mae: 0.4540\n",
            "Epoch 397/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2320 - mse: 0.2320 - mae: 0.3682 - val_loss: 0.4416 - val_mse: 0.4416 - val_mae: 0.4972\n",
            "Epoch 398/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2437 - mse: 0.2437 - mae: 0.3821 - val_loss: 0.3609 - val_mse: 0.3609 - val_mae: 0.4554\n",
            "Epoch 399/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2647 - mse: 0.2647 - mae: 0.4054 - val_loss: 0.3477 - val_mse: 0.3477 - val_mae: 0.4523\n",
            "Epoch 400/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2376 - mse: 0.2376 - mae: 0.3798 - val_loss: 0.3552 - val_mse: 0.3552 - val_mae: 0.4525\n",
            "Epoch 401/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2437 - mse: 0.2437 - mae: 0.3862 - val_loss: 0.4224 - val_mse: 0.4224 - val_mae: 0.4879\n",
            "Epoch 402/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2469 - mse: 0.2469 - mae: 0.3898 - val_loss: 0.3626 - val_mse: 0.3626 - val_mae: 0.4532\n",
            "Epoch 403/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2518 - mse: 0.2518 - mae: 0.3903 - val_loss: 0.3458 - val_mse: 0.3458 - val_mae: 0.4540\n",
            "Epoch 404/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2627 - mse: 0.2627 - mae: 0.3877 - val_loss: 0.3584 - val_mse: 0.3584 - val_mae: 0.4452\n",
            "Epoch 405/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2124 - mse: 0.2124 - mae: 0.3589 - val_loss: 0.3508 - val_mse: 0.3508 - val_mae: 0.4438\n",
            "Epoch 406/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2291 - mse: 0.2291 - mae: 0.3683 - val_loss: 0.3744 - val_mse: 0.3744 - val_mae: 0.4548\n",
            "Epoch 407/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2103 - mse: 0.2103 - mae: 0.3549 - val_loss: 0.3454 - val_mse: 0.3454 - val_mae: 0.4452\n",
            "Epoch 408/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2444 - mse: 0.2444 - mae: 0.3760 - val_loss: 0.3781 - val_mse: 0.3781 - val_mae: 0.4594\n",
            "Epoch 409/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2232 - mse: 0.2232 - mae: 0.3646 - val_loss: 0.3961 - val_mse: 0.3961 - val_mae: 0.4742\n",
            "Epoch 410/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2311 - mse: 0.2311 - mae: 0.3668 - val_loss: 0.3682 - val_mse: 0.3682 - val_mae: 0.4554\n",
            "Epoch 411/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2486 - mse: 0.2486 - mae: 0.3876 - val_loss: 0.5498 - val_mse: 0.5498 - val_mae: 0.5708\n",
            "Epoch 412/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2582 - mse: 0.2582 - mae: 0.3942 - val_loss: 0.3495 - val_mse: 0.3495 - val_mae: 0.4445\n",
            "Epoch 413/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2491 - mse: 0.2491 - mae: 0.3784 - val_loss: 0.3469 - val_mse: 0.3469 - val_mae: 0.4367\n",
            "Epoch 414/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2490 - mse: 0.2490 - mae: 0.3856 - val_loss: 0.3952 - val_mse: 0.3952 - val_mae: 0.4658\n",
            "Epoch 415/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2523 - mse: 0.2523 - mae: 0.3958 - val_loss: 0.4145 - val_mse: 0.4145 - val_mae: 0.4875\n",
            "Epoch 416/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2256 - mse: 0.2256 - mae: 0.3737 - val_loss: 0.3654 - val_mse: 0.3654 - val_mae: 0.4540\n",
            "Epoch 417/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2396 - mse: 0.2396 - mae: 0.3736 - val_loss: 0.3582 - val_mse: 0.3582 - val_mae: 0.4566\n",
            "Epoch 418/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2245 - mse: 0.2245 - mae: 0.3674 - val_loss: 0.3495 - val_mse: 0.3495 - val_mae: 0.4414\n",
            "Epoch 419/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2263 - mse: 0.2263 - mae: 0.3688 - val_loss: 0.3630 - val_mse: 0.3630 - val_mae: 0.4445\n",
            "Epoch 420/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2184 - mse: 0.2184 - mae: 0.3588 - val_loss: 0.3897 - val_mse: 0.3897 - val_mae: 0.4715\n",
            "Epoch 421/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2456 - mse: 0.2456 - mae: 0.3814 - val_loss: 0.3568 - val_mse: 0.3568 - val_mae: 0.4502\n",
            "Epoch 422/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2195 - mse: 0.2195 - mae: 0.3647 - val_loss: 0.3532 - val_mse: 0.3532 - val_mae: 0.4522\n",
            "Epoch 423/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2154 - mse: 0.2154 - mae: 0.3588 - val_loss: 0.3735 - val_mse: 0.3735 - val_mae: 0.4625\n",
            "Epoch 424/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2366 - mse: 0.2366 - mae: 0.3826 - val_loss: 0.4066 - val_mse: 0.4066 - val_mae: 0.4798\n",
            "Epoch 425/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2278 - mse: 0.2278 - mae: 0.3730 - val_loss: 0.3549 - val_mse: 0.3549 - val_mae: 0.4456\n",
            "Epoch 426/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2454 - mse: 0.2454 - mae: 0.3816 - val_loss: 0.3548 - val_mse: 0.3548 - val_mae: 0.4519\n",
            "Epoch 427/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2118 - mse: 0.2118 - mae: 0.3560 - val_loss: 0.3979 - val_mse: 0.3979 - val_mae: 0.4687\n",
            "Epoch 428/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2404 - mse: 0.2404 - mae: 0.3767 - val_loss: 0.3833 - val_mse: 0.3833 - val_mae: 0.4694\n",
            "Epoch 429/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2184 - mse: 0.2184 - mae: 0.3656 - val_loss: 0.3894 - val_mse: 0.3894 - val_mae: 0.4652\n",
            "Epoch 430/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2356 - mse: 0.2356 - mae: 0.3757 - val_loss: 0.3576 - val_mse: 0.3576 - val_mae: 0.4441\n",
            "Epoch 431/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2357 - mse: 0.2357 - mae: 0.3735 - val_loss: 0.3614 - val_mse: 0.3614 - val_mae: 0.4491\n",
            "Epoch 432/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2362 - mse: 0.2362 - mae: 0.3765 - val_loss: 0.3529 - val_mse: 0.3529 - val_mae: 0.4404\n",
            "Epoch 433/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2184 - mse: 0.2184 - mae: 0.3650 - val_loss: 0.3815 - val_mse: 0.3815 - val_mae: 0.4598\n",
            "Epoch 434/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2210 - mse: 0.2210 - mae: 0.3600 - val_loss: 0.3737 - val_mse: 0.3737 - val_mae: 0.4618\n",
            "Epoch 435/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2164 - mse: 0.2164 - mae: 0.3586 - val_loss: 0.3769 - val_mse: 0.3769 - val_mae: 0.4631\n",
            "Epoch 436/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2096 - mse: 0.2096 - mae: 0.3576 - val_loss: 0.3689 - val_mse: 0.3689 - val_mae: 0.4617\n",
            "Epoch 437/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2245 - mse: 0.2245 - mae: 0.3697 - val_loss: 0.3813 - val_mse: 0.3813 - val_mae: 0.4635\n",
            "Epoch 438/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2386 - mse: 0.2386 - mae: 0.3759 - val_loss: 0.3596 - val_mse: 0.3596 - val_mae: 0.4515\n",
            "Epoch 439/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2273 - mse: 0.2273 - mae: 0.3669 - val_loss: 0.3562 - val_mse: 0.3562 - val_mae: 0.4492\n",
            "Epoch 440/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2165 - mse: 0.2165 - mae: 0.3585 - val_loss: 0.3514 - val_mse: 0.3514 - val_mae: 0.4465\n",
            "Epoch 441/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2169 - mse: 0.2169 - mae: 0.3578 - val_loss: 0.3812 - val_mse: 0.3812 - val_mae: 0.4595\n",
            "Epoch 442/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2289 - mse: 0.2289 - mae: 0.3691 - val_loss: 0.3575 - val_mse: 0.3575 - val_mae: 0.4487\n",
            "Epoch 443/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2299 - mse: 0.2299 - mae: 0.3682 - val_loss: 0.3721 - val_mse: 0.3721 - val_mae: 0.4597\n",
            "Epoch 444/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2099 - mse: 0.2099 - mae: 0.3522 - val_loss: 0.3771 - val_mse: 0.3771 - val_mae: 0.4561\n",
            "Epoch 445/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2045 - mse: 0.2045 - mae: 0.3468 - val_loss: 0.3583 - val_mse: 0.3583 - val_mae: 0.4481\n",
            "Epoch 446/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2271 - mse: 0.2271 - mae: 0.3732 - val_loss: 0.3648 - val_mse: 0.3648 - val_mae: 0.4726\n",
            "Epoch 447/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2286 - mse: 0.2286 - mae: 0.3702 - val_loss: 0.3533 - val_mse: 0.3533 - val_mae: 0.4420\n",
            "Epoch 448/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2047 - mse: 0.2047 - mae: 0.3454 - val_loss: 0.4392 - val_mse: 0.4392 - val_mae: 0.5058\n",
            "Epoch 449/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2423 - mse: 0.2423 - mae: 0.3786 - val_loss: 0.3842 - val_mse: 0.3842 - val_mae: 0.4676\n",
            "Epoch 450/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2162 - mse: 0.2162 - mae: 0.3599 - val_loss: 0.3681 - val_mse: 0.3681 - val_mae: 0.4640\n",
            "Epoch 451/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2185 - mse: 0.2185 - mae: 0.3644 - val_loss: 0.3786 - val_mse: 0.3786 - val_mae: 0.4617\n",
            "Epoch 452/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2064 - mse: 0.2064 - mae: 0.3524 - val_loss: 0.3741 - val_mse: 0.3741 - val_mae: 0.4662\n",
            "Epoch 453/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2302 - mse: 0.2302 - mae: 0.3690 - val_loss: 0.3796 - val_mse: 0.3796 - val_mae: 0.4616\n",
            "Epoch 454/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2088 - mse: 0.2088 - mae: 0.3533 - val_loss: 0.3541 - val_mse: 0.3541 - val_mae: 0.4461\n",
            "Epoch 455/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2248 - mse: 0.2248 - mae: 0.3658 - val_loss: 0.3709 - val_mse: 0.3709 - val_mae: 0.4603\n",
            "Epoch 456/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2167 - mse: 0.2167 - mae: 0.3536 - val_loss: 0.4111 - val_mse: 0.4111 - val_mae: 0.4876\n",
            "Epoch 457/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2155 - mse: 0.2155 - mae: 0.3601 - val_loss: 0.3776 - val_mse: 0.3776 - val_mae: 0.4670\n",
            "Epoch 458/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2497 - mse: 0.2497 - mae: 0.3864 - val_loss: 0.3589 - val_mse: 0.3589 - val_mae: 0.4439\n",
            "Epoch 459/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2320 - mse: 0.2320 - mae: 0.3688 - val_loss: 0.3416 - val_mse: 0.3416 - val_mae: 0.4341\n",
            "Epoch 460/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2258 - mse: 0.2258 - mae: 0.3664 - val_loss: 0.3489 - val_mse: 0.3489 - val_mae: 0.4452\n",
            "Epoch 461/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2222 - mse: 0.2222 - mae: 0.3645 - val_loss: 0.3584 - val_mse: 0.3584 - val_mae: 0.4507\n",
            "Epoch 462/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2272 - mse: 0.2272 - mae: 0.3666 - val_loss: 0.3375 - val_mse: 0.3375 - val_mae: 0.4326\n",
            "Epoch 463/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2429 - mse: 0.2429 - mae: 0.3856 - val_loss: 0.3777 - val_mse: 0.3777 - val_mae: 0.4566\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.5506 - mse: 0.5506 - mae: 0.5467\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 15ms/step - loss: 30.6462 - mse: 30.6462 - mae: 5.3903 - val_loss: 19.0748 - val_mse: 19.0748 - val_mae: 4.2482\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 13.3108 - mse: 13.3108 - mae: 3.3927 - val_loss: 8.1734 - val_mse: 8.1734 - val_mae: 2.5610\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 6.4617 - mse: 6.4617 - mae: 2.1756 - val_loss: 5.7781 - val_mse: 5.7781 - val_mae: 1.9752\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 4.9239 - mse: 4.9239 - mae: 1.8465 - val_loss: 4.8175 - val_mse: 4.8175 - val_mae: 1.7882\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 4.1047 - mse: 4.1047 - mae: 1.6847 - val_loss: 4.1000 - val_mse: 4.1000 - val_mae: 1.6331\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 3.7494 - mse: 3.7494 - mae: 1.5841 - val_loss: 3.6187 - val_mse: 3.6187 - val_mae: 1.5224\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 3.2686 - mse: 3.2686 - mae: 1.4851 - val_loss: 3.2425 - val_mse: 3.2425 - val_mae: 1.4323\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 2.8791 - mse: 2.8791 - mae: 1.3837 - val_loss: 2.8943 - val_mse: 2.8943 - val_mae: 1.3487\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.6244 - mse: 2.6244 - mae: 1.3032 - val_loss: 2.6392 - val_mse: 2.6392 - val_mae: 1.2972\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.5576 - mse: 2.5576 - mae: 1.2786 - val_loss: 2.4126 - val_mse: 2.4126 - val_mae: 1.2396\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.2282 - mse: 2.2282 - mae: 1.2032 - val_loss: 2.2642 - val_mse: 2.2642 - val_mae: 1.1987\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.1984 - mse: 2.1984 - mae: 1.1892 - val_loss: 2.1478 - val_mse: 2.1478 - val_mae: 1.1603\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.0174 - mse: 2.0174 - mae: 1.1270 - val_loss: 1.9807 - val_mse: 1.9807 - val_mae: 1.1140\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.9540 - mse: 1.9540 - mae: 1.1135 - val_loss: 1.8587 - val_mse: 1.8587 - val_mae: 1.0789\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.8731 - mse: 1.8731 - mae: 1.1046 - val_loss: 1.7453 - val_mse: 1.7453 - val_mae: 1.0404\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.6700 - mse: 1.6700 - mae: 1.0355 - val_loss: 1.6910 - val_mse: 1.6910 - val_mae: 1.0241\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.5890 - mse: 1.5890 - mae: 0.9997 - val_loss: 1.5983 - val_mse: 1.5983 - val_mae: 0.9847\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.6278 - mse: 1.6278 - mae: 1.0130 - val_loss: 1.5447 - val_mse: 1.5447 - val_mae: 0.9766\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.5884 - mse: 1.5884 - mae: 0.9967 - val_loss: 1.4615 - val_mse: 1.4615 - val_mae: 0.9425\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.5198 - mse: 1.5198 - mae: 0.9891 - val_loss: 1.4208 - val_mse: 1.4208 - val_mae: 0.9285\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.3875 - mse: 1.3875 - mae: 0.9389 - val_loss: 1.3421 - val_mse: 1.3421 - val_mae: 0.9084\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.4049 - mse: 1.4049 - mae: 0.9458 - val_loss: 1.2683 - val_mse: 1.2683 - val_mae: 0.8890\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2977 - mse: 1.2977 - mae: 0.9199 - val_loss: 1.2308 - val_mse: 1.2308 - val_mae: 0.8729\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2157 - mse: 1.2157 - mae: 0.8866 - val_loss: 1.1837 - val_mse: 1.1837 - val_mae: 0.8479\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2354 - mse: 1.2354 - mae: 0.8834 - val_loss: 1.1567 - val_mse: 1.1567 - val_mae: 0.8348\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1099 - mse: 1.1099 - mae: 0.8475 - val_loss: 1.1305 - val_mse: 1.1305 - val_mae: 0.8203\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0450 - mse: 1.0450 - mae: 0.8230 - val_loss: 1.1018 - val_mse: 1.1018 - val_mae: 0.8070\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0822 - mse: 1.0822 - mae: 0.8263 - val_loss: 1.0354 - val_mse: 1.0354 - val_mae: 0.7833\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0868 - mse: 1.0868 - mae: 0.8356 - val_loss: 1.0479 - val_mse: 1.0479 - val_mae: 0.7835\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0062 - mse: 1.0062 - mae: 0.7988 - val_loss: 0.9870 - val_mse: 0.9870 - val_mae: 0.7608\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9642 - mse: 0.9642 - mae: 0.7770 - val_loss: 0.9411 - val_mse: 0.9411 - val_mae: 0.7398\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9666 - mse: 0.9666 - mae: 0.7743 - val_loss: 0.9791 - val_mse: 0.9791 - val_mae: 0.7532\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.9090 - mse: 0.9090 - mae: 0.7659 - val_loss: 0.9359 - val_mse: 0.9359 - val_mae: 0.7276\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8707 - mse: 0.8707 - mae: 0.7456 - val_loss: 0.9096 - val_mse: 0.9096 - val_mae: 0.7206\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.8960 - mse: 0.8960 - mae: 0.7492 - val_loss: 0.9004 - val_mse: 0.9004 - val_mae: 0.7229\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7845 - mse: 0.7845 - mae: 0.7054 - val_loss: 0.8532 - val_mse: 0.8532 - val_mae: 0.7019\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7940 - mse: 0.7940 - mae: 0.7025 - val_loss: 0.8491 - val_mse: 0.8491 - val_mae: 0.6938\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8086 - mse: 0.8086 - mae: 0.7067 - val_loss: 0.8146 - val_mse: 0.8146 - val_mae: 0.6843\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7852 - mse: 0.7852 - mae: 0.7131 - val_loss: 0.8388 - val_mse: 0.8388 - val_mae: 0.6842\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7650 - mse: 0.7650 - mae: 0.6900 - val_loss: 0.7893 - val_mse: 0.7893 - val_mae: 0.6746\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7441 - mse: 0.7441 - mae: 0.6735 - val_loss: 0.7822 - val_mse: 0.7822 - val_mae: 0.6646\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7279 - mse: 0.7279 - mae: 0.6764 - val_loss: 0.7644 - val_mse: 0.7644 - val_mae: 0.6604\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7470 - mse: 0.7470 - mae: 0.6801 - val_loss: 0.7411 - val_mse: 0.7411 - val_mae: 0.6543\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7087 - mse: 0.7087 - mae: 0.6645 - val_loss: 0.7370 - val_mse: 0.7370 - val_mae: 0.6419\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6447 - mse: 0.6447 - mae: 0.6375 - val_loss: 0.7184 - val_mse: 0.7184 - val_mae: 0.6329\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6767 - mse: 0.6767 - mae: 0.6545 - val_loss: 0.7159 - val_mse: 0.7159 - val_mae: 0.6348\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6261 - mse: 0.6261 - mae: 0.6166 - val_loss: 0.6738 - val_mse: 0.6738 - val_mae: 0.6247\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6338 - mse: 0.6338 - mae: 0.6281 - val_loss: 0.7385 - val_mse: 0.7385 - val_mae: 0.6377\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6276 - mse: 0.6276 - mae: 0.6331 - val_loss: 0.6841 - val_mse: 0.6841 - val_mae: 0.6138\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5663 - mse: 0.5663 - mae: 0.5888 - val_loss: 0.6928 - val_mse: 0.6928 - val_mae: 0.6186\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5984 - mse: 0.5984 - mae: 0.6054 - val_loss: 0.6631 - val_mse: 0.6631 - val_mae: 0.6161\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6075 - mse: 0.6075 - mae: 0.6156 - val_loss: 0.6633 - val_mse: 0.6633 - val_mae: 0.6134\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5951 - mse: 0.5951 - mae: 0.6125 - val_loss: 0.6342 - val_mse: 0.6342 - val_mae: 0.6021\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5494 - mse: 0.5494 - mae: 0.5864 - val_loss: 0.6454 - val_mse: 0.6454 - val_mae: 0.5997\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5860 - mse: 0.5860 - mae: 0.6015 - val_loss: 0.6261 - val_mse: 0.6261 - val_mae: 0.5929\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5355 - mse: 0.5355 - mae: 0.5807 - val_loss: 0.6310 - val_mse: 0.6310 - val_mae: 0.5943\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5298 - mse: 0.5298 - mae: 0.5774 - val_loss: 0.6135 - val_mse: 0.6135 - val_mae: 0.5884\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5249 - mse: 0.5249 - mae: 0.5804 - val_loss: 0.6102 - val_mse: 0.6102 - val_mae: 0.5828\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4882 - mse: 0.4882 - mae: 0.5612 - val_loss: 0.6434 - val_mse: 0.6434 - val_mae: 0.5911\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5134 - mse: 0.5134 - mae: 0.5610 - val_loss: 0.5830 - val_mse: 0.5830 - val_mae: 0.5723\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4942 - mse: 0.4942 - mae: 0.5577 - val_loss: 0.5910 - val_mse: 0.5910 - val_mae: 0.5735\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5285 - mse: 0.5285 - mae: 0.5668 - val_loss: 0.5800 - val_mse: 0.5800 - val_mae: 0.5719\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4909 - mse: 0.4909 - mae: 0.5565 - val_loss: 0.5854 - val_mse: 0.5854 - val_mae: 0.5776\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4868 - mse: 0.4868 - mae: 0.5504 - val_loss: 0.5756 - val_mse: 0.5756 - val_mae: 0.5657\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4653 - mse: 0.4653 - mae: 0.5446 - val_loss: 0.5913 - val_mse: 0.5913 - val_mae: 0.5772\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5056 - mse: 0.5056 - mae: 0.5628 - val_loss: 0.6074 - val_mse: 0.6074 - val_mae: 0.5774\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4670 - mse: 0.4670 - mae: 0.5367 - val_loss: 0.5623 - val_mse: 0.5623 - val_mae: 0.5548\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4844 - mse: 0.4844 - mae: 0.5511 - val_loss: 0.5372 - val_mse: 0.5372 - val_mae: 0.5509\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4710 - mse: 0.4710 - mae: 0.5391 - val_loss: 0.5650 - val_mse: 0.5650 - val_mae: 0.5649\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4769 - mse: 0.4769 - mae: 0.5439 - val_loss: 0.5678 - val_mse: 0.5678 - val_mae: 0.5640\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.4524 - mse: 0.4524 - mae: 0.5346 - val_loss: 0.5290 - val_mse: 0.5290 - val_mae: 0.5487\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4326 - mse: 0.4326 - mae: 0.5183 - val_loss: 0.5357 - val_mse: 0.5357 - val_mae: 0.5523\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4446 - mse: 0.4446 - mae: 0.5279 - val_loss: 0.5515 - val_mse: 0.5515 - val_mae: 0.5547\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4573 - mse: 0.4573 - mae: 0.5295 - val_loss: 0.5380 - val_mse: 0.5380 - val_mae: 0.5447\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4451 - mse: 0.4451 - mae: 0.5265 - val_loss: 0.5147 - val_mse: 0.5147 - val_mae: 0.5468\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4688 - mse: 0.4688 - mae: 0.5427 - val_loss: 0.5466 - val_mse: 0.5466 - val_mae: 0.5516\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4512 - mse: 0.4512 - mae: 0.5242 - val_loss: 0.5475 - val_mse: 0.5475 - val_mae: 0.5509\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4524 - mse: 0.4524 - mae: 0.5360 - val_loss: 0.5258 - val_mse: 0.5258 - val_mae: 0.5389\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4270 - mse: 0.4270 - mae: 0.5189 - val_loss: 0.5197 - val_mse: 0.5197 - val_mae: 0.5384\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4314 - mse: 0.4314 - mae: 0.5198 - val_loss: 0.5052 - val_mse: 0.5052 - val_mae: 0.5373\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4131 - mse: 0.4131 - mae: 0.5087 - val_loss: 0.5132 - val_mse: 0.5132 - val_mae: 0.5377\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4000 - mse: 0.4000 - mae: 0.4989 - val_loss: 0.5361 - val_mse: 0.5361 - val_mae: 0.5483\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4493 - mse: 0.4493 - mae: 0.5308 - val_loss: 0.4909 - val_mse: 0.4909 - val_mae: 0.5311\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4584 - mse: 0.4584 - mae: 0.5318 - val_loss: 0.5889 - val_mse: 0.5889 - val_mae: 0.5764\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4259 - mse: 0.4259 - mae: 0.5129 - val_loss: 0.5419 - val_mse: 0.5419 - val_mae: 0.5500\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4093 - mse: 0.4093 - mae: 0.5016 - val_loss: 0.5086 - val_mse: 0.5086 - val_mae: 0.5333\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4182 - mse: 0.4182 - mae: 0.5121 - val_loss: 0.4955 - val_mse: 0.4955 - val_mae: 0.5415\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4196 - mse: 0.4196 - mae: 0.5187 - val_loss: 0.4924 - val_mse: 0.4924 - val_mae: 0.5334\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4316 - mse: 0.4316 - mae: 0.5162 - val_loss: 0.5158 - val_mse: 0.5158 - val_mae: 0.5469\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4038 - mse: 0.4038 - mae: 0.5070 - val_loss: 0.5203 - val_mse: 0.5203 - val_mae: 0.5376\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3893 - mse: 0.3893 - mae: 0.4904 - val_loss: 0.5134 - val_mse: 0.5134 - val_mae: 0.5425\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3968 - mse: 0.3968 - mae: 0.5004 - val_loss: 0.4760 - val_mse: 0.4760 - val_mae: 0.5202\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3899 - mse: 0.3899 - mae: 0.4990 - val_loss: 0.4986 - val_mse: 0.4986 - val_mae: 0.5302\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4018 - mse: 0.4018 - mae: 0.4948 - val_loss: 0.4987 - val_mse: 0.4987 - val_mae: 0.5414\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3712 - mse: 0.3712 - mae: 0.4786 - val_loss: 0.4944 - val_mse: 0.4944 - val_mae: 0.5363\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3820 - mse: 0.3820 - mae: 0.4986 - val_loss: 0.5010 - val_mse: 0.5010 - val_mae: 0.5320\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3867 - mse: 0.3867 - mae: 0.4803 - val_loss: 0.4979 - val_mse: 0.4979 - val_mae: 0.5316\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3909 - mse: 0.3909 - mae: 0.4888 - val_loss: 0.5026 - val_mse: 0.5026 - val_mae: 0.5394\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4024 - mse: 0.4024 - mae: 0.4987 - val_loss: 0.4737 - val_mse: 0.4737 - val_mae: 0.5277\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3848 - mse: 0.3848 - mae: 0.4870 - val_loss: 0.5078 - val_mse: 0.5078 - val_mae: 0.5408\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3858 - mse: 0.3858 - mae: 0.4871 - val_loss: 0.5055 - val_mse: 0.5055 - val_mae: 0.5398\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3838 - mse: 0.3838 - mae: 0.4874 - val_loss: 0.4857 - val_mse: 0.4857 - val_mae: 0.5268\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3746 - mse: 0.3746 - mae: 0.4872 - val_loss: 0.5225 - val_mse: 0.5225 - val_mae: 0.5476\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3905 - mse: 0.3905 - mae: 0.4954 - val_loss: 0.4696 - val_mse: 0.4696 - val_mae: 0.5243\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3664 - mse: 0.3664 - mae: 0.4724 - val_loss: 0.5025 - val_mse: 0.5025 - val_mae: 0.5328\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3685 - mse: 0.3685 - mae: 0.4781 - val_loss: 0.5053 - val_mse: 0.5053 - val_mae: 0.5362\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3970 - mse: 0.3970 - mae: 0.4943 - val_loss: 0.4866 - val_mse: 0.4866 - val_mae: 0.5285\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3938 - mse: 0.3938 - mae: 0.4934 - val_loss: 0.4709 - val_mse: 0.4709 - val_mae: 0.5270\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3685 - mse: 0.3685 - mae: 0.4810 - val_loss: 0.4670 - val_mse: 0.4670 - val_mae: 0.5217\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3663 - mse: 0.3663 - mae: 0.4704 - val_loss: 0.4586 - val_mse: 0.4586 - val_mae: 0.5239\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3462 - mse: 0.3462 - mae: 0.4713 - val_loss: 0.4679 - val_mse: 0.4679 - val_mae: 0.5326\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3533 - mse: 0.3533 - mae: 0.4739 - val_loss: 0.4848 - val_mse: 0.4848 - val_mae: 0.5358\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3692 - mse: 0.3692 - mae: 0.4847 - val_loss: 0.4824 - val_mse: 0.4824 - val_mae: 0.5341\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3564 - mse: 0.3564 - mae: 0.4721 - val_loss: 0.4710 - val_mse: 0.4710 - val_mae: 0.5276\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3743 - mse: 0.3743 - mae: 0.4775 - val_loss: 0.4721 - val_mse: 0.4721 - val_mae: 0.5279\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3780 - mse: 0.3780 - mae: 0.4844 - val_loss: 0.4703 - val_mse: 0.4703 - val_mae: 0.5297\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3623 - mse: 0.3623 - mae: 0.4795 - val_loss: 0.5084 - val_mse: 0.5084 - val_mae: 0.5473\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3586 - mse: 0.3586 - mae: 0.4685 - val_loss: 0.4804 - val_mse: 0.4804 - val_mae: 0.5349\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3487 - mse: 0.3487 - mae: 0.4687 - val_loss: 0.4969 - val_mse: 0.4969 - val_mae: 0.5387\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3563 - mse: 0.3563 - mae: 0.4638 - val_loss: 0.4676 - val_mse: 0.4676 - val_mae: 0.5254\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3587 - mse: 0.3587 - mae: 0.4699 - val_loss: 0.4801 - val_mse: 0.4801 - val_mae: 0.5381\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3701 - mse: 0.3701 - mae: 0.4719 - val_loss: 0.4794 - val_mse: 0.4794 - val_mae: 0.5250\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3753 - mse: 0.3753 - mae: 0.4777 - val_loss: 0.4695 - val_mse: 0.4695 - val_mae: 0.5256\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3542 - mse: 0.3542 - mae: 0.4683 - val_loss: 0.4642 - val_mse: 0.4642 - val_mae: 0.5232\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3552 - mse: 0.3552 - mae: 0.4735 - val_loss: 0.4583 - val_mse: 0.4583 - val_mae: 0.5231\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3339 - mse: 0.3339 - mae: 0.4578 - val_loss: 0.4735 - val_mse: 0.4735 - val_mae: 0.5256\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3537 - mse: 0.3537 - mae: 0.4677 - val_loss: 0.4729 - val_mse: 0.4729 - val_mae: 0.5295\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3299 - mse: 0.3299 - mae: 0.4507 - val_loss: 0.4969 - val_mse: 0.4969 - val_mae: 0.5371\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3594 - mse: 0.3594 - mae: 0.4654 - val_loss: 0.4662 - val_mse: 0.4662 - val_mae: 0.5230\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3260 - mse: 0.3260 - mae: 0.4400 - val_loss: 0.5114 - val_mse: 0.5114 - val_mae: 0.5461\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3450 - mse: 0.3450 - mae: 0.4629 - val_loss: 0.5171 - val_mse: 0.5171 - val_mae: 0.5446\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3399 - mse: 0.3399 - mae: 0.4499 - val_loss: 0.4870 - val_mse: 0.4870 - val_mae: 0.5351\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3580 - mse: 0.3580 - mae: 0.4720 - val_loss: 0.4572 - val_mse: 0.4572 - val_mae: 0.5204\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3482 - mse: 0.3482 - mae: 0.4588 - val_loss: 0.4628 - val_mse: 0.4628 - val_mae: 0.5230\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3318 - mse: 0.3318 - mae: 0.4482 - val_loss: 0.4974 - val_mse: 0.4974 - val_mae: 0.5458\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3481 - mse: 0.3481 - mae: 0.4666 - val_loss: 0.5251 - val_mse: 0.5251 - val_mae: 0.5571\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3559 - mse: 0.3559 - mae: 0.4722 - val_loss: 0.4509 - val_mse: 0.4509 - val_mae: 0.5157\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3433 - mse: 0.3433 - mae: 0.4481 - val_loss: 0.4601 - val_mse: 0.4601 - val_mae: 0.5238\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3492 - mse: 0.3492 - mae: 0.4554 - val_loss: 0.4665 - val_mse: 0.4665 - val_mae: 0.5243\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3468 - mse: 0.3468 - mae: 0.4558 - val_loss: 0.4509 - val_mse: 0.4509 - val_mae: 0.5183\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3364 - mse: 0.3364 - mae: 0.4579 - val_loss: 0.4779 - val_mse: 0.4779 - val_mae: 0.5362\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3346 - mse: 0.3346 - mae: 0.4487 - val_loss: 0.4589 - val_mse: 0.4589 - val_mae: 0.5109\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3149 - mse: 0.3149 - mae: 0.4421 - val_loss: 0.4558 - val_mse: 0.4558 - val_mae: 0.5217\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3342 - mse: 0.3342 - mae: 0.4509 - val_loss: 0.4547 - val_mse: 0.4547 - val_mae: 0.5159\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3291 - mse: 0.3291 - mae: 0.4491 - val_loss: 0.4589 - val_mse: 0.4589 - val_mae: 0.5220\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3419 - mse: 0.3419 - mae: 0.4561 - val_loss: 0.4605 - val_mse: 0.4605 - val_mae: 0.5178\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.3268 - mse: 0.3268 - mae: 0.4481 - val_loss: 0.4828 - val_mse: 0.4828 - val_mae: 0.5337\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3141 - mse: 0.3141 - mae: 0.4339 - val_loss: 0.4408 - val_mse: 0.4408 - val_mae: 0.5117\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3212 - mse: 0.3212 - mae: 0.4360 - val_loss: 0.4804 - val_mse: 0.4804 - val_mae: 0.5320\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3423 - mse: 0.3423 - mae: 0.4508 - val_loss: 0.4659 - val_mse: 0.4659 - val_mae: 0.5305\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3120 - mse: 0.3120 - mae: 0.4398 - val_loss: 0.5059 - val_mse: 0.5059 - val_mae: 0.5471\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3350 - mse: 0.3350 - mae: 0.4502 - val_loss: 0.4653 - val_mse: 0.4653 - val_mae: 0.5276\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3104 - mse: 0.3104 - mae: 0.4404 - val_loss: 0.4609 - val_mse: 0.4609 - val_mae: 0.5205\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3218 - mse: 0.3218 - mae: 0.4364 - val_loss: 0.5213 - val_mse: 0.5213 - val_mae: 0.5543\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3269 - mse: 0.3269 - mae: 0.4480 - val_loss: 0.4603 - val_mse: 0.4603 - val_mae: 0.5217\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.3230 - mse: 0.3230 - mae: 0.4377 - val_loss: 0.4606 - val_mse: 0.4606 - val_mae: 0.5281\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3302 - mse: 0.3302 - mae: 0.4503 - val_loss: 0.4346 - val_mse: 0.4346 - val_mae: 0.5097\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3326 - mse: 0.3326 - mae: 0.4486 - val_loss: 0.4400 - val_mse: 0.4400 - val_mae: 0.5129\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3053 - mse: 0.3053 - mae: 0.4336 - val_loss: 0.4854 - val_mse: 0.4854 - val_mae: 0.5335\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3233 - mse: 0.3233 - mae: 0.4389 - val_loss: 0.4794 - val_mse: 0.4794 - val_mae: 0.5266\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3070 - mse: 0.3070 - mae: 0.4405 - val_loss: 0.5037 - val_mse: 0.5037 - val_mae: 0.5541\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3422 - mse: 0.3422 - mae: 0.4567 - val_loss: 0.5106 - val_mse: 0.5106 - val_mae: 0.5515\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3432 - mse: 0.3432 - mae: 0.4583 - val_loss: 0.4507 - val_mse: 0.4507 - val_mae: 0.5256\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3101 - mse: 0.3101 - mae: 0.4309 - val_loss: 0.4555 - val_mse: 0.4555 - val_mae: 0.5236\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3114 - mse: 0.3114 - mae: 0.4371 - val_loss: 0.4465 - val_mse: 0.4465 - val_mae: 0.5140\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2986 - mse: 0.2986 - mae: 0.4300 - val_loss: 0.4342 - val_mse: 0.4342 - val_mae: 0.5097\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3035 - mse: 0.3035 - mae: 0.4303 - val_loss: 0.4515 - val_mse: 0.4515 - val_mae: 0.5183\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3133 - mse: 0.3133 - mae: 0.4366 - val_loss: 0.4567 - val_mse: 0.4567 - val_mae: 0.5312\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3140 - mse: 0.3140 - mae: 0.4372 - val_loss: 0.4433 - val_mse: 0.4433 - val_mae: 0.5211\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3071 - mse: 0.3071 - mae: 0.4196 - val_loss: 0.4298 - val_mse: 0.4298 - val_mae: 0.5048\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3098 - mse: 0.3098 - mae: 0.4295 - val_loss: 0.4434 - val_mse: 0.4434 - val_mae: 0.5160\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3140 - mse: 0.3140 - mae: 0.4331 - val_loss: 0.4351 - val_mse: 0.4351 - val_mae: 0.5064\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3038 - mse: 0.3038 - mae: 0.4326 - val_loss: 0.4252 - val_mse: 0.4252 - val_mae: 0.5000\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3154 - mse: 0.3154 - mae: 0.4343 - val_loss: 0.4712 - val_mse: 0.4712 - val_mae: 0.5326\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3194 - mse: 0.3194 - mae: 0.4358 - val_loss: 0.4452 - val_mse: 0.4452 - val_mae: 0.5148\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2914 - mse: 0.2914 - mae: 0.4241 - val_loss: 0.4241 - val_mse: 0.4241 - val_mae: 0.5044\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3068 - mse: 0.3068 - mae: 0.4285 - val_loss: 0.4579 - val_mse: 0.4579 - val_mae: 0.5168\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3177 - mse: 0.3177 - mae: 0.4360 - val_loss: 0.4756 - val_mse: 0.4756 - val_mae: 0.5314\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2985 - mse: 0.2985 - mae: 0.4215 - val_loss: 0.4738 - val_mse: 0.4738 - val_mae: 0.5339\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2916 - mse: 0.2916 - mae: 0.4196 - val_loss: 0.4412 - val_mse: 0.4412 - val_mae: 0.5102\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3064 - mse: 0.3064 - mae: 0.4293 - val_loss: 0.4265 - val_mse: 0.4265 - val_mae: 0.5040\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2889 - mse: 0.2889 - mae: 0.4169 - val_loss: 0.4330 - val_mse: 0.4330 - val_mae: 0.5029\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2848 - mse: 0.2848 - mae: 0.4139 - val_loss: 0.4459 - val_mse: 0.4459 - val_mae: 0.5147\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3371 - mse: 0.3371 - mae: 0.4590 - val_loss: 0.4314 - val_mse: 0.4314 - val_mae: 0.5016\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2859 - mse: 0.2859 - mae: 0.4058 - val_loss: 0.4209 - val_mse: 0.4209 - val_mae: 0.4993\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2995 - mse: 0.2995 - mae: 0.4272 - val_loss: 0.4245 - val_mse: 0.4245 - val_mae: 0.4944\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3113 - mse: 0.3113 - mae: 0.4329 - val_loss: 0.4307 - val_mse: 0.4307 - val_mae: 0.5077\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3233 - mse: 0.3233 - mae: 0.4392 - val_loss: 0.4340 - val_mse: 0.4340 - val_mae: 0.5124\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3244 - mse: 0.3244 - mae: 0.4459 - val_loss: 0.4420 - val_mse: 0.4420 - val_mae: 0.5151\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3101 - mse: 0.3101 - mae: 0.4235 - val_loss: 0.4895 - val_mse: 0.4895 - val_mae: 0.5446\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3097 - mse: 0.3097 - mae: 0.4314 - val_loss: 0.4391 - val_mse: 0.4391 - val_mae: 0.5129\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3140 - mse: 0.3140 - mae: 0.4406 - val_loss: 0.4329 - val_mse: 0.4329 - val_mae: 0.5102\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3100 - mse: 0.3100 - mae: 0.4361 - val_loss: 0.4341 - val_mse: 0.4341 - val_mae: 0.5111\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2917 - mse: 0.2917 - mae: 0.4195 - val_loss: 0.4243 - val_mse: 0.4243 - val_mae: 0.5039\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2865 - mse: 0.2865 - mae: 0.4103 - val_loss: 0.4413 - val_mse: 0.4413 - val_mae: 0.5145\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3103 - mse: 0.3103 - mae: 0.4256 - val_loss: 0.4143 - val_mse: 0.4143 - val_mae: 0.4964\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2934 - mse: 0.2934 - mae: 0.4152 - val_loss: 0.4210 - val_mse: 0.4210 - val_mae: 0.5031\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2907 - mse: 0.2907 - mae: 0.4197 - val_loss: 0.4361 - val_mse: 0.4361 - val_mae: 0.5161\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2858 - mse: 0.2858 - mae: 0.4152 - val_loss: 0.4412 - val_mse: 0.4412 - val_mae: 0.5157\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3066 - mse: 0.3066 - mae: 0.4249 - val_loss: 0.4412 - val_mse: 0.4412 - val_mae: 0.5147\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2826 - mse: 0.2826 - mae: 0.4093 - val_loss: 0.4754 - val_mse: 0.4754 - val_mae: 0.5338\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2933 - mse: 0.2933 - mae: 0.4237 - val_loss: 0.4468 - val_mse: 0.4468 - val_mae: 0.5133\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2911 - mse: 0.2911 - mae: 0.4173 - val_loss: 0.4330 - val_mse: 0.4330 - val_mae: 0.5104\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2881 - mse: 0.2881 - mae: 0.4157 - val_loss: 0.4978 - val_mse: 0.4978 - val_mae: 0.5472\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3081 - mse: 0.3081 - mae: 0.4290 - val_loss: 0.4447 - val_mse: 0.4447 - val_mae: 0.5202\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2931 - mse: 0.2931 - mae: 0.4155 - val_loss: 0.4921 - val_mse: 0.4921 - val_mae: 0.5439\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3149 - mse: 0.3149 - mae: 0.4317 - val_loss: 0.4465 - val_mse: 0.4465 - val_mae: 0.5159\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3053 - mse: 0.3053 - mae: 0.4288 - val_loss: 0.4324 - val_mse: 0.4324 - val_mae: 0.5047\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2909 - mse: 0.2909 - mae: 0.4134 - val_loss: 0.4346 - val_mse: 0.4346 - val_mae: 0.5065\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2657 - mse: 0.2657 - mae: 0.4027 - val_loss: 0.4513 - val_mse: 0.4513 - val_mae: 0.5158\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2896 - mse: 0.2896 - mae: 0.4181 - val_loss: 0.4300 - val_mse: 0.4300 - val_mae: 0.5041\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2933 - mse: 0.2933 - mae: 0.4144 - val_loss: 0.4442 - val_mse: 0.4442 - val_mae: 0.5147\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3125 - mse: 0.3125 - mae: 0.4317 - val_loss: 0.4325 - val_mse: 0.4325 - val_mae: 0.5107\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2784 - mse: 0.2784 - mae: 0.4048 - val_loss: 0.4492 - val_mse: 0.4492 - val_mae: 0.5140\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2827 - mse: 0.2827 - mae: 0.4103 - val_loss: 0.4268 - val_mse: 0.4268 - val_mae: 0.5022\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2784 - mse: 0.2784 - mae: 0.4063 - val_loss: 0.4406 - val_mse: 0.4406 - val_mae: 0.5084\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2701 - mse: 0.2701 - mae: 0.4060 - val_loss: 0.4520 - val_mse: 0.4520 - val_mae: 0.5237\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2818 - mse: 0.2818 - mae: 0.4161 - val_loss: 0.4369 - val_mse: 0.4369 - val_mae: 0.5067\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2895 - mse: 0.2895 - mae: 0.4121 - val_loss: 0.4279 - val_mse: 0.4279 - val_mae: 0.5008\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2802 - mse: 0.2802 - mae: 0.4027 - val_loss: 0.4616 - val_mse: 0.4616 - val_mae: 0.5270\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2878 - mse: 0.2878 - mae: 0.4167 - val_loss: 0.4465 - val_mse: 0.4465 - val_mae: 0.5164\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2822 - mse: 0.2822 - mae: 0.4075 - val_loss: 0.4360 - val_mse: 0.4360 - val_mae: 0.5080\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2750 - mse: 0.2750 - mae: 0.4080 - val_loss: 0.4330 - val_mse: 0.4330 - val_mae: 0.5062\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3172 - mse: 0.3172 - mae: 0.4423 - val_loss: 0.4334 - val_mse: 0.4334 - val_mae: 0.5107\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2840 - mse: 0.2840 - mae: 0.4128 - val_loss: 0.4363 - val_mse: 0.4363 - val_mae: 0.5043\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2885 - mse: 0.2885 - mae: 0.4130 - val_loss: 0.4519 - val_mse: 0.4519 - val_mae: 0.5141\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2725 - mse: 0.2725 - mae: 0.4005 - val_loss: 0.4558 - val_mse: 0.4558 - val_mae: 0.5102\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2883 - mse: 0.2883 - mae: 0.4193 - val_loss: 0.4383 - val_mse: 0.4383 - val_mae: 0.5065\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2718 - mse: 0.2718 - mae: 0.4092 - val_loss: 0.4331 - val_mse: 0.4331 - val_mae: 0.5025\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2680 - mse: 0.2680 - mae: 0.3968 - val_loss: 0.4516 - val_mse: 0.4516 - val_mae: 0.5194\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2999 - mse: 0.2999 - mae: 0.4239 - val_loss: 0.4432 - val_mse: 0.4432 - val_mae: 0.5051\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2960 - mse: 0.2960 - mae: 0.4255 - val_loss: 0.4420 - val_mse: 0.4420 - val_mae: 0.5011\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2578 - mse: 0.2578 - mae: 0.3904 - val_loss: 0.4390 - val_mse: 0.4390 - val_mae: 0.5107\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2550 - mse: 0.2550 - mae: 0.3917 - val_loss: 0.4429 - val_mse: 0.4429 - val_mae: 0.5049\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2838 - mse: 0.2838 - mae: 0.4114 - val_loss: 0.4357 - val_mse: 0.4357 - val_mae: 0.4998\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2576 - mse: 0.2576 - mae: 0.3961 - val_loss: 0.4314 - val_mse: 0.4314 - val_mae: 0.4980\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2913 - mse: 0.2913 - mae: 0.4105 - val_loss: 0.4441 - val_mse: 0.4441 - val_mae: 0.5041\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2680 - mse: 0.2680 - mae: 0.4079 - val_loss: 0.4490 - val_mse: 0.4490 - val_mae: 0.5133\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2720 - mse: 0.2720 - mae: 0.3940 - val_loss: 0.4586 - val_mse: 0.4586 - val_mae: 0.5239\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2594 - mse: 0.2594 - mae: 0.3904 - val_loss: 0.4582 - val_mse: 0.4582 - val_mae: 0.5300\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2747 - mse: 0.2747 - mae: 0.4152 - val_loss: 0.4369 - val_mse: 0.4369 - val_mae: 0.5081\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2833 - mse: 0.2833 - mae: 0.4174 - val_loss: 0.4366 - val_mse: 0.4366 - val_mae: 0.5053\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2609 - mse: 0.2609 - mae: 0.3927 - val_loss: 0.4325 - val_mse: 0.4325 - val_mae: 0.5029\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2701 - mse: 0.2701 - mae: 0.3997 - val_loss: 0.4361 - val_mse: 0.4361 - val_mae: 0.4988\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2880 - mse: 0.2880 - mae: 0.4199 - val_loss: 0.4321 - val_mse: 0.4321 - val_mae: 0.5016\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2763 - mse: 0.2763 - mae: 0.4082 - val_loss: 0.4498 - val_mse: 0.4498 - val_mae: 0.5144\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2718 - mse: 0.2718 - mae: 0.3938 - val_loss: 0.4544 - val_mse: 0.4544 - val_mae: 0.5131\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2730 - mse: 0.2730 - mae: 0.4072 - val_loss: 0.4770 - val_mse: 0.4770 - val_mae: 0.5339\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2759 - mse: 0.2759 - mae: 0.4110 - val_loss: 0.4222 - val_mse: 0.4222 - val_mae: 0.5010\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2526 - mse: 0.2526 - mae: 0.3931 - val_loss: 0.4497 - val_mse: 0.4497 - val_mae: 0.5095\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2594 - mse: 0.2594 - mae: 0.3870 - val_loss: 0.4395 - val_mse: 0.4395 - val_mae: 0.5109\n",
            "Epoch 252/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2507 - mse: 0.2507 - mae: 0.3902 - val_loss: 0.4662 - val_mse: 0.4662 - val_mae: 0.5241\n",
            "Epoch 253/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2779 - mse: 0.2779 - mae: 0.4046 - val_loss: 0.4216 - val_mse: 0.4216 - val_mae: 0.4893\n",
            "Epoch 254/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2712 - mse: 0.2712 - mae: 0.4100 - val_loss: 0.4449 - val_mse: 0.4449 - val_mae: 0.5086\n",
            "Epoch 255/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2848 - mse: 0.2848 - mae: 0.4098 - val_loss: 0.4260 - val_mse: 0.4260 - val_mae: 0.4906\n",
            "Epoch 256/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2983 - mse: 0.2983 - mae: 0.4251 - val_loss: 0.4537 - val_mse: 0.4537 - val_mae: 0.5152\n",
            "Epoch 257/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2715 - mse: 0.2715 - mae: 0.4097 - val_loss: 0.4805 - val_mse: 0.4805 - val_mae: 0.5265\n",
            "Epoch 258/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2587 - mse: 0.2587 - mae: 0.3924 - val_loss: 0.4671 - val_mse: 0.4671 - val_mae: 0.5202\n",
            "Epoch 259/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2690 - mse: 0.2690 - mae: 0.4016 - val_loss: 0.4444 - val_mse: 0.4444 - val_mae: 0.5119\n",
            "Epoch 260/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2648 - mse: 0.2648 - mae: 0.3986 - val_loss: 0.4346 - val_mse: 0.4346 - val_mae: 0.5033\n",
            "Epoch 261/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2724 - mse: 0.2724 - mae: 0.4023 - val_loss: 0.4607 - val_mse: 0.4607 - val_mae: 0.5226\n",
            "Epoch 262/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2486 - mse: 0.2486 - mae: 0.3908 - val_loss: 0.4462 - val_mse: 0.4462 - val_mae: 0.5103\n",
            "Epoch 263/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2784 - mse: 0.2784 - mae: 0.4166 - val_loss: 0.4505 - val_mse: 0.4505 - val_mae: 0.5038\n",
            "Epoch 264/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2725 - mse: 0.2725 - mae: 0.4045 - val_loss: 0.4466 - val_mse: 0.4466 - val_mae: 0.5045\n",
            "Epoch 265/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2579 - mse: 0.2579 - mae: 0.3900 - val_loss: 0.4628 - val_mse: 0.4628 - val_mae: 0.5171\n",
            "Epoch 266/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2589 - mse: 0.2589 - mae: 0.3929 - val_loss: 0.4707 - val_mse: 0.4707 - val_mae: 0.5220\n",
            "Epoch 267/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2746 - mse: 0.2746 - mae: 0.4056 - val_loss: 0.4303 - val_mse: 0.4303 - val_mae: 0.5047\n",
            "Epoch 268/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2685 - mse: 0.2685 - mae: 0.4064 - val_loss: 0.4291 - val_mse: 0.4291 - val_mae: 0.4964\n",
            "Epoch 269/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2615 - mse: 0.2615 - mae: 0.3950 - val_loss: 0.4478 - val_mse: 0.4478 - val_mae: 0.5078\n",
            "Epoch 270/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2431 - mse: 0.2431 - mae: 0.3798 - val_loss: 0.4883 - val_mse: 0.4883 - val_mae: 0.5362\n",
            "Epoch 271/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2603 - mse: 0.2603 - mae: 0.3922 - val_loss: 0.4877 - val_mse: 0.4877 - val_mae: 0.5374\n",
            "Epoch 272/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2629 - mse: 0.2629 - mae: 0.3922 - val_loss: 0.4797 - val_mse: 0.4797 - val_mae: 0.5204\n",
            "Epoch 273/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2675 - mse: 0.2675 - mae: 0.3981 - val_loss: 0.4773 - val_mse: 0.4773 - val_mae: 0.5326\n",
            "Epoch 274/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2535 - mse: 0.2535 - mae: 0.3951 - val_loss: 0.4713 - val_mse: 0.4713 - val_mae: 0.5192\n",
            "Epoch 275/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2742 - mse: 0.2742 - mae: 0.4006 - val_loss: 0.4700 - val_mse: 0.4700 - val_mae: 0.5223\n",
            "Epoch 276/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2392 - mse: 0.2392 - mae: 0.3773 - val_loss: 0.4928 - val_mse: 0.4928 - val_mae: 0.5377\n",
            "Epoch 277/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2811 - mse: 0.2811 - mae: 0.4013 - val_loss: 0.4603 - val_mse: 0.4603 - val_mae: 0.5221\n",
            "Epoch 278/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2647 - mse: 0.2647 - mae: 0.3998 - val_loss: 0.4638 - val_mse: 0.4638 - val_mae: 0.5215\n",
            "Epoch 279/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2484 - mse: 0.2484 - mae: 0.3822 - val_loss: 0.4423 - val_mse: 0.4423 - val_mae: 0.5047\n",
            "Epoch 280/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2688 - mse: 0.2688 - mae: 0.3988 - val_loss: 0.4371 - val_mse: 0.4371 - val_mae: 0.5020\n",
            "Epoch 281/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2806 - mse: 0.2806 - mae: 0.4092 - val_loss: 0.4461 - val_mse: 0.4461 - val_mae: 0.5073\n",
            "Epoch 282/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2598 - mse: 0.2598 - mae: 0.3927 - val_loss: 0.4492 - val_mse: 0.4492 - val_mae: 0.5031\n",
            "Epoch 283/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2542 - mse: 0.2542 - mae: 0.3871 - val_loss: 0.4314 - val_mse: 0.4314 - val_mae: 0.5033\n",
            "Epoch 284/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2694 - mse: 0.2694 - mae: 0.4009 - val_loss: 0.4451 - val_mse: 0.4451 - val_mae: 0.5079\n",
            "Epoch 285/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2956 - mse: 0.2956 - mae: 0.4244 - val_loss: 0.4346 - val_mse: 0.4346 - val_mae: 0.5016\n",
            "Epoch 286/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2729 - mse: 0.2729 - mae: 0.4086 - val_loss: 0.5247 - val_mse: 0.5247 - val_mae: 0.5535\n",
            "Epoch 287/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2726 - mse: 0.2726 - mae: 0.4051 - val_loss: 0.4444 - val_mse: 0.4444 - val_mae: 0.5083\n",
            "Epoch 288/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2728 - mse: 0.2728 - mae: 0.4002 - val_loss: 0.4380 - val_mse: 0.4380 - val_mae: 0.5014\n",
            "Epoch 289/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2457 - mse: 0.2457 - mae: 0.3877 - val_loss: 0.4663 - val_mse: 0.4663 - val_mae: 0.5211\n",
            "Epoch 290/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2573 - mse: 0.2573 - mae: 0.3930 - val_loss: 0.4306 - val_mse: 0.4306 - val_mae: 0.5013\n",
            "Epoch 291/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2506 - mse: 0.2506 - mae: 0.3863 - val_loss: 0.4826 - val_mse: 0.4826 - val_mae: 0.5303\n",
            "Epoch 292/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2712 - mse: 0.2712 - mae: 0.4071 - val_loss: 0.4496 - val_mse: 0.4496 - val_mae: 0.5143\n",
            "Epoch 293/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2485 - mse: 0.2485 - mae: 0.3887 - val_loss: 0.4654 - val_mse: 0.4654 - val_mae: 0.5176\n",
            "Epoch 294/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2375 - mse: 0.2375 - mae: 0.3831 - val_loss: 0.4513 - val_mse: 0.4513 - val_mae: 0.5171\n",
            "Epoch 295/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2462 - mse: 0.2462 - mae: 0.3918 - val_loss: 0.4478 - val_mse: 0.4478 - val_mae: 0.5126\n",
            "Epoch 296/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2607 - mse: 0.2607 - mae: 0.3923 - val_loss: 0.4469 - val_mse: 0.4469 - val_mae: 0.5110\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.5006 - mse: 0.5006 - mae: 0.5014\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 16ms/step - loss: 9.2990 - mse: 9.2990 - mae: 2.6230 - val_loss: 4.2860 - val_mse: 4.2860 - val_mae: 1.6130\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 4.3951 - mse: 4.3951 - mae: 1.6183 - val_loss: 3.2266 - val_mse: 3.2266 - val_mae: 1.4615\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 3.1193 - mse: 3.1193 - mae: 1.4037 - val_loss: 2.4705 - val_mse: 2.4705 - val_mae: 1.2398\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 2.5419 - mse: 2.5419 - mae: 1.2667 - val_loss: 2.2626 - val_mse: 2.2626 - val_mae: 1.1852\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.1719 - mse: 2.1719 - mae: 1.1463 - val_loss: 2.1484 - val_mse: 2.1484 - val_mae: 1.1603\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.0179 - mse: 2.0179 - mae: 1.1094 - val_loss: 1.9479 - val_mse: 1.9479 - val_mae: 1.0979\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.9432 - mse: 1.9432 - mae: 1.0873 - val_loss: 1.8431 - val_mse: 1.8431 - val_mae: 1.0803\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.6795 - mse: 1.6795 - mae: 1.0312 - val_loss: 1.7734 - val_mse: 1.7734 - val_mae: 1.0589\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.5794 - mse: 1.5794 - mae: 0.9894 - val_loss: 1.6920 - val_mse: 1.6920 - val_mae: 1.0264\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.4768 - mse: 1.4768 - mae: 0.9580 - val_loss: 1.6112 - val_mse: 1.6112 - val_mae: 1.0015\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.4302 - mse: 1.4302 - mae: 0.9399 - val_loss: 1.5919 - val_mse: 1.5919 - val_mae: 1.0016\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.3628 - mse: 1.3628 - mae: 0.9178 - val_loss: 1.4784 - val_mse: 1.4784 - val_mae: 0.9490\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2797 - mse: 1.2797 - mae: 0.8948 - val_loss: 1.4661 - val_mse: 1.4661 - val_mae: 0.9355\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2102 - mse: 1.2102 - mae: 0.8497 - val_loss: 1.3629 - val_mse: 1.3629 - val_mae: 0.9019\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.2212 - mse: 1.2212 - mae: 0.8677 - val_loss: 1.3451 - val_mse: 1.3451 - val_mae: 0.9059\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0734 - mse: 1.0734 - mae: 0.8212 - val_loss: 1.2677 - val_mse: 1.2677 - val_mae: 0.8762\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0436 - mse: 1.0436 - mae: 0.7864 - val_loss: 1.2390 - val_mse: 1.2390 - val_mae: 0.8771\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9605 - mse: 0.9605 - mae: 0.7690 - val_loss: 1.1966 - val_mse: 1.1966 - val_mae: 0.8407\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9581 - mse: 0.9581 - mae: 0.7761 - val_loss: 1.1047 - val_mse: 1.1047 - val_mae: 0.8127\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9194 - mse: 0.9194 - mae: 0.7436 - val_loss: 1.0453 - val_mse: 1.0453 - val_mae: 0.7958\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8066 - mse: 0.8066 - mae: 0.7004 - val_loss: 1.0507 - val_mse: 1.0507 - val_mae: 0.7822\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8087 - mse: 0.8087 - mae: 0.7125 - val_loss: 0.9854 - val_mse: 0.9854 - val_mae: 0.7646\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7836 - mse: 0.7836 - mae: 0.6853 - val_loss: 0.9687 - val_mse: 0.9687 - val_mae: 0.7688\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7490 - mse: 0.7490 - mae: 0.6702 - val_loss: 0.9301 - val_mse: 0.9301 - val_mae: 0.7555\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6949 - mse: 0.6949 - mae: 0.6506 - val_loss: 0.8741 - val_mse: 0.8741 - val_mae: 0.7371\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6840 - mse: 0.6840 - mae: 0.6351 - val_loss: 0.8384 - val_mse: 0.8384 - val_mae: 0.7029\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7128 - mse: 0.7128 - mae: 0.6602 - val_loss: 0.8442 - val_mse: 0.8442 - val_mae: 0.7121\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6416 - mse: 0.6416 - mae: 0.6203 - val_loss: 0.7934 - val_mse: 0.7934 - val_mae: 0.7002\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5972 - mse: 0.5972 - mae: 0.5969 - val_loss: 0.7984 - val_mse: 0.7984 - val_mae: 0.6974\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5996 - mse: 0.5996 - mae: 0.6047 - val_loss: 0.7376 - val_mse: 0.7376 - val_mae: 0.6793\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5991 - mse: 0.5991 - mae: 0.6024 - val_loss: 0.7485 - val_mse: 0.7485 - val_mae: 0.6543\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5595 - mse: 0.5595 - mae: 0.5854 - val_loss: 0.7181 - val_mse: 0.7181 - val_mae: 0.6435\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5656 - mse: 0.5656 - mae: 0.5817 - val_loss: 0.7189 - val_mse: 0.7189 - val_mae: 0.6729\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5201 - mse: 0.5201 - mae: 0.5640 - val_loss: 0.6894 - val_mse: 0.6894 - val_mae: 0.6500\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.5434 - mse: 0.5434 - mae: 0.5706 - val_loss: 0.7452 - val_mse: 0.7452 - val_mae: 0.6608\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5336 - mse: 0.5336 - mae: 0.5698 - val_loss: 0.6919 - val_mse: 0.6919 - val_mae: 0.6577\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4982 - mse: 0.4982 - mae: 0.5517 - val_loss: 0.6487 - val_mse: 0.6487 - val_mae: 0.6211\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5592 - mse: 0.5592 - mae: 0.5862 - val_loss: 0.7456 - val_mse: 0.7456 - val_mae: 0.6510\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5513 - mse: 0.5513 - mae: 0.5798 - val_loss: 0.6605 - val_mse: 0.6605 - val_mae: 0.6156\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4751 - mse: 0.4751 - mae: 0.5453 - val_loss: 0.6403 - val_mse: 0.6403 - val_mae: 0.6262\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4427 - mse: 0.4427 - mae: 0.5242 - val_loss: 0.5977 - val_mse: 0.5977 - val_mae: 0.5934\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4709 - mse: 0.4709 - mae: 0.5226 - val_loss: 0.6183 - val_mse: 0.6183 - val_mae: 0.5945\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4485 - mse: 0.4485 - mae: 0.5191 - val_loss: 0.6105 - val_mse: 0.6105 - val_mae: 0.6057\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4302 - mse: 0.4302 - mae: 0.5083 - val_loss: 0.6185 - val_mse: 0.6185 - val_mae: 0.6020\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4688 - mse: 0.4688 - mae: 0.5415 - val_loss: 0.5792 - val_mse: 0.5792 - val_mae: 0.5808\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4531 - mse: 0.4531 - mae: 0.5212 - val_loss: 0.5635 - val_mse: 0.5635 - val_mae: 0.5749\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4054 - mse: 0.4054 - mae: 0.4975 - val_loss: 0.5692 - val_mse: 0.5692 - val_mae: 0.5765\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3731 - mse: 0.3731 - mae: 0.4738 - val_loss: 0.5877 - val_mse: 0.5877 - val_mae: 0.6130\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4344 - mse: 0.4344 - mae: 0.5103 - val_loss: 0.6185 - val_mse: 0.6185 - val_mae: 0.6172\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4441 - mse: 0.4441 - mae: 0.5171 - val_loss: 0.5549 - val_mse: 0.5549 - val_mae: 0.5907\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4152 - mse: 0.4152 - mae: 0.5025 - val_loss: 0.5333 - val_mse: 0.5333 - val_mae: 0.5649\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4068 - mse: 0.4068 - mae: 0.5020 - val_loss: 0.5668 - val_mse: 0.5668 - val_mae: 0.5713\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3959 - mse: 0.3959 - mae: 0.4923 - val_loss: 0.5525 - val_mse: 0.5525 - val_mae: 0.5957\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3756 - mse: 0.3756 - mae: 0.4708 - val_loss: 0.5199 - val_mse: 0.5199 - val_mae: 0.5517\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3574 - mse: 0.3574 - mae: 0.4638 - val_loss: 0.5476 - val_mse: 0.5476 - val_mae: 0.5557\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3823 - mse: 0.3823 - mae: 0.4819 - val_loss: 0.5319 - val_mse: 0.5319 - val_mae: 0.5827\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3681 - mse: 0.3681 - mae: 0.4627 - val_loss: 0.5038 - val_mse: 0.5038 - val_mae: 0.5419\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3628 - mse: 0.3628 - mae: 0.4646 - val_loss: 0.5086 - val_mse: 0.5086 - val_mae: 0.5599\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3680 - mse: 0.3680 - mae: 0.4653 - val_loss: 0.4968 - val_mse: 0.4968 - val_mae: 0.5399\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3708 - mse: 0.3708 - mae: 0.4777 - val_loss: 0.5499 - val_mse: 0.5499 - val_mae: 0.5580\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4100 - mse: 0.4100 - mae: 0.4957 - val_loss: 0.4863 - val_mse: 0.4863 - val_mae: 0.5329\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3335 - mse: 0.3335 - mae: 0.4455 - val_loss: 0.5120 - val_mse: 0.5120 - val_mae: 0.5713\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3530 - mse: 0.3530 - mae: 0.4535 - val_loss: 0.4866 - val_mse: 0.4866 - val_mae: 0.5334\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3543 - mse: 0.3543 - mae: 0.4703 - val_loss: 0.5178 - val_mse: 0.5178 - val_mae: 0.5448\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3335 - mse: 0.3335 - mae: 0.4418 - val_loss: 0.4804 - val_mse: 0.4804 - val_mae: 0.5229\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3069 - mse: 0.3069 - mae: 0.4295 - val_loss: 0.4716 - val_mse: 0.4716 - val_mae: 0.5304\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3297 - mse: 0.3297 - mae: 0.4379 - val_loss: 0.4803 - val_mse: 0.4803 - val_mae: 0.5335\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3385 - mse: 0.3385 - mae: 0.4483 - val_loss: 0.4726 - val_mse: 0.4726 - val_mae: 0.5441\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3554 - mse: 0.3554 - mae: 0.4689 - val_loss: 0.4717 - val_mse: 0.4717 - val_mae: 0.5180\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3280 - mse: 0.3280 - mae: 0.4483 - val_loss: 0.4678 - val_mse: 0.4678 - val_mae: 0.5273\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3029 - mse: 0.3029 - mae: 0.4241 - val_loss: 0.5050 - val_mse: 0.5050 - val_mae: 0.5540\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3224 - mse: 0.3224 - mae: 0.4360 - val_loss: 0.5241 - val_mse: 0.5241 - val_mae: 0.5481\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3183 - mse: 0.3183 - mae: 0.4363 - val_loss: 0.4951 - val_mse: 0.4951 - val_mae: 0.5327\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.2991 - mse: 0.2991 - mae: 0.4272 - val_loss: 0.4544 - val_mse: 0.4544 - val_mae: 0.5182\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3035 - mse: 0.3035 - mae: 0.4198 - val_loss: 0.4727 - val_mse: 0.4727 - val_mae: 0.5248\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3240 - mse: 0.3240 - mae: 0.4373 - val_loss: 0.4681 - val_mse: 0.4681 - val_mae: 0.5174\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2993 - mse: 0.2993 - mae: 0.4199 - val_loss: 0.4536 - val_mse: 0.4536 - val_mae: 0.5089\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3164 - mse: 0.3164 - mae: 0.4344 - val_loss: 0.4814 - val_mse: 0.4814 - val_mae: 0.5364\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2865 - mse: 0.2865 - mae: 0.4078 - val_loss: 0.4739 - val_mse: 0.4739 - val_mae: 0.5226\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2984 - mse: 0.2984 - mae: 0.4219 - val_loss: 0.5056 - val_mse: 0.5056 - val_mae: 0.5421\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3055 - mse: 0.3055 - mae: 0.4272 - val_loss: 0.4766 - val_mse: 0.4766 - val_mae: 0.5247\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3011 - mse: 0.3011 - mae: 0.4300 - val_loss: 0.4732 - val_mse: 0.4732 - val_mae: 0.5260\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2958 - mse: 0.2958 - mae: 0.4143 - val_loss: 0.4694 - val_mse: 0.4694 - val_mae: 0.5255\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3009 - mse: 0.3009 - mae: 0.4299 - val_loss: 0.4806 - val_mse: 0.4806 - val_mae: 0.5236\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3053 - mse: 0.3053 - mae: 0.4254 - val_loss: 0.4681 - val_mse: 0.4681 - val_mae: 0.5280\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.3099 - mse: 0.3099 - mae: 0.4236 - val_loss: 0.4533 - val_mse: 0.4533 - val_mae: 0.5184\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2938 - mse: 0.2938 - mae: 0.4176 - val_loss: 0.4853 - val_mse: 0.4853 - val_mae: 0.5267\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2805 - mse: 0.2805 - mae: 0.4129 - val_loss: 0.4562 - val_mse: 0.4562 - val_mae: 0.5228\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.3149 - mse: 0.3149 - mae: 0.4343 - val_loss: 0.4699 - val_mse: 0.4699 - val_mae: 0.5195\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2694 - mse: 0.2694 - mae: 0.3970 - val_loss: 0.4562 - val_mse: 0.4562 - val_mae: 0.5253\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2769 - mse: 0.2769 - mae: 0.4128 - val_loss: 0.4909 - val_mse: 0.4909 - val_mae: 0.5455\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3021 - mse: 0.3021 - mae: 0.4149 - val_loss: 0.4616 - val_mse: 0.4616 - val_mae: 0.5277\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3031 - mse: 0.3031 - mae: 0.4300 - val_loss: 0.4886 - val_mse: 0.4886 - val_mae: 0.5463\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3036 - mse: 0.3036 - mae: 0.4243 - val_loss: 0.4442 - val_mse: 0.4442 - val_mae: 0.5122\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2631 - mse: 0.2631 - mae: 0.3894 - val_loss: 0.4471 - val_mse: 0.4471 - val_mae: 0.5207\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2937 - mse: 0.2937 - mae: 0.4158 - val_loss: 0.4756 - val_mse: 0.4756 - val_mae: 0.5343\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2732 - mse: 0.2732 - mae: 0.4015 - val_loss: 0.5364 - val_mse: 0.5364 - val_mae: 0.5500\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3002 - mse: 0.3002 - mae: 0.4314 - val_loss: 0.4570 - val_mse: 0.4570 - val_mae: 0.5262\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2686 - mse: 0.2686 - mae: 0.3958 - val_loss: 0.4515 - val_mse: 0.4515 - val_mae: 0.5261\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2694 - mse: 0.2694 - mae: 0.4062 - val_loss: 0.4528 - val_mse: 0.4528 - val_mae: 0.5135\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2571 - mse: 0.2571 - mae: 0.3871 - val_loss: 0.4705 - val_mse: 0.4705 - val_mae: 0.5194\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2872 - mse: 0.2872 - mae: 0.4197 - val_loss: 0.4560 - val_mse: 0.4560 - val_mae: 0.5218\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2953 - mse: 0.2953 - mae: 0.4098 - val_loss: 0.4395 - val_mse: 0.4395 - val_mae: 0.5084\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2569 - mse: 0.2569 - mae: 0.3945 - val_loss: 0.4564 - val_mse: 0.4564 - val_mae: 0.5257\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2623 - mse: 0.2623 - mae: 0.3998 - val_loss: 0.4666 - val_mse: 0.4666 - val_mae: 0.5279\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2709 - mse: 0.2709 - mae: 0.4028 - val_loss: 0.4341 - val_mse: 0.4341 - val_mae: 0.5074\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2577 - mse: 0.2577 - mae: 0.3868 - val_loss: 0.4525 - val_mse: 0.4525 - val_mae: 0.5181\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2696 - mse: 0.2696 - mae: 0.4031 - val_loss: 0.4451 - val_mse: 0.4451 - val_mae: 0.5099\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2429 - mse: 0.2429 - mae: 0.3804 - val_loss: 0.4696 - val_mse: 0.4696 - val_mae: 0.5187\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2521 - mse: 0.2521 - mae: 0.3896 - val_loss: 0.4447 - val_mse: 0.4447 - val_mae: 0.5062\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2570 - mse: 0.2570 - mae: 0.3921 - val_loss: 0.5084 - val_mse: 0.5084 - val_mae: 0.5418\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2946 - mse: 0.2946 - mae: 0.4235 - val_loss: 0.4652 - val_mse: 0.4652 - val_mae: 0.5332\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2720 - mse: 0.2720 - mae: 0.4108 - val_loss: 0.4951 - val_mse: 0.4951 - val_mae: 0.5439\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2813 - mse: 0.2813 - mae: 0.4133 - val_loss: 0.4551 - val_mse: 0.4551 - val_mae: 0.5155\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2508 - mse: 0.2508 - mae: 0.3874 - val_loss: 0.4514 - val_mse: 0.4514 - val_mae: 0.5100\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2498 - mse: 0.2498 - mae: 0.3845 - val_loss: 0.4398 - val_mse: 0.4398 - val_mae: 0.5137\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2510 - mse: 0.2510 - mae: 0.3865 - val_loss: 0.4574 - val_mse: 0.4574 - val_mae: 0.5268\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2800 - mse: 0.2800 - mae: 0.4052 - val_loss: 0.4350 - val_mse: 0.4350 - val_mae: 0.5075\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2330 - mse: 0.2330 - mae: 0.3713 - val_loss: 0.4443 - val_mse: 0.4443 - val_mae: 0.5169\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2658 - mse: 0.2658 - mae: 0.3946 - val_loss: 0.4529 - val_mse: 0.4529 - val_mae: 0.5141\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2533 - mse: 0.2533 - mae: 0.3923 - val_loss: 0.4503 - val_mse: 0.4503 - val_mae: 0.5174\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2678 - mse: 0.2678 - mae: 0.4018 - val_loss: 0.4783 - val_mse: 0.4783 - val_mae: 0.5302\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2629 - mse: 0.2629 - mae: 0.4009 - val_loss: 0.4400 - val_mse: 0.4400 - val_mae: 0.5131\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2744 - mse: 0.2744 - mae: 0.4044 - val_loss: 0.4335 - val_mse: 0.4335 - val_mae: 0.5081\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2521 - mse: 0.2521 - mae: 0.3813 - val_loss: 0.4459 - val_mse: 0.4459 - val_mae: 0.5179\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2773 - mse: 0.2773 - mae: 0.4113 - val_loss: 0.4561 - val_mse: 0.4561 - val_mae: 0.5235\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2806 - mse: 0.2806 - mae: 0.4153 - val_loss: 0.4623 - val_mse: 0.4623 - val_mae: 0.5172\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2482 - mse: 0.2482 - mae: 0.3845 - val_loss: 0.4626 - val_mse: 0.4626 - val_mae: 0.5189\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2511 - mse: 0.2511 - mae: 0.3831 - val_loss: 0.4817 - val_mse: 0.4817 - val_mae: 0.5327\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2834 - mse: 0.2834 - mae: 0.4114 - val_loss: 0.4902 - val_mse: 0.4902 - val_mae: 0.5441\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2903 - mse: 0.2903 - mae: 0.4173 - val_loss: 0.4384 - val_mse: 0.4384 - val_mae: 0.5121\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2331 - mse: 0.2331 - mae: 0.3697 - val_loss: 0.4764 - val_mse: 0.4764 - val_mae: 0.5313\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2413 - mse: 0.2413 - mae: 0.3833 - val_loss: 0.4309 - val_mse: 0.4309 - val_mae: 0.4978\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2470 - mse: 0.2470 - mae: 0.3872 - val_loss: 0.4721 - val_mse: 0.4721 - val_mae: 0.5203\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2439 - mse: 0.2439 - mae: 0.3871 - val_loss: 0.4282 - val_mse: 0.4282 - val_mae: 0.5007\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2598 - mse: 0.2598 - mae: 0.3909 - val_loss: 0.4447 - val_mse: 0.4447 - val_mae: 0.5069\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2483 - mse: 0.2483 - mae: 0.3863 - val_loss: 0.4068 - val_mse: 0.4068 - val_mae: 0.4924\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2767 - mse: 0.2767 - mae: 0.4012 - val_loss: 0.4385 - val_mse: 0.4385 - val_mae: 0.5177\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2600 - mse: 0.2600 - mae: 0.3859 - val_loss: 0.4727 - val_mse: 0.4727 - val_mae: 0.5320\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2413 - mse: 0.2413 - mae: 0.3791 - val_loss: 0.4824 - val_mse: 0.4824 - val_mae: 0.5346\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2368 - mse: 0.2368 - mae: 0.3803 - val_loss: 0.4637 - val_mse: 0.4637 - val_mae: 0.5237\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2474 - mse: 0.2474 - mae: 0.3757 - val_loss: 0.4535 - val_mse: 0.4535 - val_mae: 0.5175\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2354 - mse: 0.2354 - mae: 0.3726 - val_loss: 0.4503 - val_mse: 0.4503 - val_mae: 0.5142\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2417 - mse: 0.2417 - mae: 0.3812 - val_loss: 0.4365 - val_mse: 0.4365 - val_mae: 0.5123\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2470 - mse: 0.2470 - mae: 0.3824 - val_loss: 0.4517 - val_mse: 0.4517 - val_mae: 0.5163\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2316 - mse: 0.2316 - mae: 0.3701 - val_loss: 0.4379 - val_mse: 0.4379 - val_mae: 0.5156\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2371 - mse: 0.2371 - mae: 0.3697 - val_loss: 0.4289 - val_mse: 0.4289 - val_mae: 0.5133\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2691 - mse: 0.2691 - mae: 0.3994 - val_loss: 0.4823 - val_mse: 0.4823 - val_mae: 0.5357\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2367 - mse: 0.2367 - mae: 0.3751 - val_loss: 0.4280 - val_mse: 0.4280 - val_mae: 0.4975\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2503 - mse: 0.2503 - mae: 0.3853 - val_loss: 0.4503 - val_mse: 0.4503 - val_mae: 0.5159\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2617 - mse: 0.2617 - mae: 0.3878 - val_loss: 0.4206 - val_mse: 0.4206 - val_mae: 0.4974\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2369 - mse: 0.2369 - mae: 0.3751 - val_loss: 0.4369 - val_mse: 0.4369 - val_mae: 0.5028\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2214 - mse: 0.2214 - mae: 0.3614 - val_loss: 0.4207 - val_mse: 0.4207 - val_mae: 0.5063\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2561 - mse: 0.2561 - mae: 0.3857 - val_loss: 0.4218 - val_mse: 0.4218 - val_mae: 0.5007\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.2750 - mse: 0.2750 - mae: 0.3966 - val_loss: 0.4386 - val_mse: 0.4386 - val_mae: 0.5149\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2526 - mse: 0.2526 - mae: 0.3926 - val_loss: 0.4726 - val_mse: 0.4726 - val_mae: 0.5383\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2508 - mse: 0.2508 - mae: 0.3911 - val_loss: 0.4236 - val_mse: 0.4236 - val_mae: 0.5013\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2220 - mse: 0.2220 - mae: 0.3601 - val_loss: 0.4272 - val_mse: 0.4272 - val_mae: 0.5031\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2080 - mse: 0.2080 - mae: 0.3527 - val_loss: 0.4226 - val_mse: 0.4226 - val_mae: 0.4994\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.2357 - mse: 0.2357 - mae: 0.3794 - val_loss: 0.4509 - val_mse: 0.4509 - val_mae: 0.5144\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.2601 - mse: 0.2601 - mae: 0.3998 - val_loss: 0.4193 - val_mse: 0.4193 - val_mae: 0.5025\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2238 - mse: 0.2238 - mae: 0.3622 - val_loss: 0.4326 - val_mse: 0.4326 - val_mae: 0.5104\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2447 - mse: 0.2447 - mae: 0.3792 - val_loss: 0.4204 - val_mse: 0.4204 - val_mae: 0.5012\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2083 - mse: 0.2083 - mae: 0.3541 - val_loss: 0.4327 - val_mse: 0.4327 - val_mae: 0.4989\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2139 - mse: 0.2139 - mae: 0.3623 - val_loss: 0.4388 - val_mse: 0.4388 - val_mae: 0.5135\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2478 - mse: 0.2478 - mae: 0.3827 - val_loss: 0.4518 - val_mse: 0.4518 - val_mae: 0.5226\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.2128 - mse: 0.2128 - mae: 0.3494 - val_loss: 0.4510 - val_mse: 0.4510 - val_mae: 0.5205\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2464 - mse: 0.2464 - mae: 0.3737 - val_loss: 0.4317 - val_mse: 0.4317 - val_mae: 0.5204\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2607 - mse: 0.2607 - mae: 0.3935 - val_loss: 0.4156 - val_mse: 0.4156 - val_mae: 0.5062\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2410 - mse: 0.2410 - mae: 0.3805 - val_loss: 0.4645 - val_mse: 0.4645 - val_mae: 0.5274\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2213 - mse: 0.2213 - mae: 0.3619 - val_loss: 0.4184 - val_mse: 0.4184 - val_mae: 0.5017\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2246 - mse: 0.2246 - mae: 0.3720 - val_loss: 0.4307 - val_mse: 0.4307 - val_mae: 0.5079\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2288 - mse: 0.2288 - mae: 0.3649 - val_loss: 0.4291 - val_mse: 0.4291 - val_mae: 0.5088\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1993 - mse: 0.1993 - mae: 0.3411 - val_loss: 0.4223 - val_mse: 0.4223 - val_mae: 0.5050\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2391 - mse: 0.2391 - mae: 0.3792 - val_loss: 0.4258 - val_mse: 0.4258 - val_mae: 0.4996\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2194 - mse: 0.2194 - mae: 0.3607 - val_loss: 0.4284 - val_mse: 0.4284 - val_mae: 0.5026\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2349 - mse: 0.2349 - mae: 0.3757 - val_loss: 0.4487 - val_mse: 0.4487 - val_mae: 0.5170\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2374 - mse: 0.2374 - mae: 0.3685 - val_loss: 0.4625 - val_mse: 0.4625 - val_mae: 0.5304\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2335 - mse: 0.2335 - mae: 0.3777 - val_loss: 0.4423 - val_mse: 0.4423 - val_mae: 0.5147\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2230 - mse: 0.2230 - mae: 0.3603 - val_loss: 0.4193 - val_mse: 0.4193 - val_mae: 0.4969\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2133 - mse: 0.2133 - mae: 0.3518 - val_loss: 0.4150 - val_mse: 0.4150 - val_mae: 0.4941\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2043 - mse: 0.2043 - mae: 0.3514 - val_loss: 0.4416 - val_mse: 0.4416 - val_mae: 0.5153\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2159 - mse: 0.2159 - mae: 0.3594 - val_loss: 0.4241 - val_mse: 0.4241 - val_mae: 0.5081\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2074 - mse: 0.2074 - mae: 0.3493 - val_loss: 0.4385 - val_mse: 0.4385 - val_mae: 0.5167\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2107 - mse: 0.2107 - mae: 0.3496 - val_loss: 0.4900 - val_mse: 0.4900 - val_mae: 0.5456\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2413 - mse: 0.2413 - mae: 0.3794 - val_loss: 0.4360 - val_mse: 0.4360 - val_mae: 0.5135\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2391 - mse: 0.2391 - mae: 0.3794 - val_loss: 0.4286 - val_mse: 0.4286 - val_mae: 0.5063\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2055 - mse: 0.2055 - mae: 0.3477 - val_loss: 0.4286 - val_mse: 0.4286 - val_mae: 0.5076\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1942 - mse: 0.1942 - mae: 0.3383 - val_loss: 0.4483 - val_mse: 0.4483 - val_mae: 0.5193\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2238 - mse: 0.2238 - mae: 0.3553 - val_loss: 0.4341 - val_mse: 0.4341 - val_mae: 0.5051\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2340 - mse: 0.2340 - mae: 0.3752 - val_loss: 0.4342 - val_mse: 0.4342 - val_mae: 0.5058\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2379 - mse: 0.2379 - mae: 0.3774 - val_loss: 0.4277 - val_mse: 0.4277 - val_mae: 0.5064\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2565 - mse: 0.2565 - mae: 0.3947 - val_loss: 0.4403 - val_mse: 0.4403 - val_mae: 0.5136\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2460 - mse: 0.2460 - mae: 0.3926 - val_loss: 0.4164 - val_mse: 0.4164 - val_mae: 0.5012\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2216 - mse: 0.2216 - mae: 0.3733 - val_loss: 0.4299 - val_mse: 0.4299 - val_mae: 0.5066\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2341 - mse: 0.2341 - mae: 0.3784 - val_loss: 0.4151 - val_mse: 0.4151 - val_mae: 0.4927\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2341 - mse: 0.2341 - mae: 0.3750 - val_loss: 0.4263 - val_mse: 0.4263 - val_mae: 0.5033\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2462 - mse: 0.2462 - mae: 0.3790 - val_loss: 0.4242 - val_mse: 0.4242 - val_mae: 0.4996\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1984 - mse: 0.1984 - mae: 0.3403 - val_loss: 0.4385 - val_mse: 0.4385 - val_mae: 0.5148\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2053 - mse: 0.2053 - mae: 0.3451 - val_loss: 0.4471 - val_mse: 0.4471 - val_mae: 0.5186\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2080 - mse: 0.2080 - mae: 0.3507 - val_loss: 0.4204 - val_mse: 0.4204 - val_mae: 0.5009\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2283 - mse: 0.2283 - mae: 0.3562 - val_loss: 0.4392 - val_mse: 0.4392 - val_mae: 0.5066\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2083 - mse: 0.2083 - mae: 0.3542 - val_loss: 0.4446 - val_mse: 0.4446 - val_mae: 0.5074\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2076 - mse: 0.2076 - mae: 0.3459 - val_loss: 0.4268 - val_mse: 0.4268 - val_mae: 0.5023\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2504 - mse: 0.2504 - mae: 0.3961 - val_loss: 0.4278 - val_mse: 0.4278 - val_mae: 0.5001\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2133 - mse: 0.2133 - mae: 0.3531 - val_loss: 0.4440 - val_mse: 0.4440 - val_mae: 0.5122\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2006 - mse: 0.2006 - mae: 0.3483 - val_loss: 0.4347 - val_mse: 0.4347 - val_mae: 0.4990\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2109 - mse: 0.2109 - mae: 0.3567 - val_loss: 0.4496 - val_mse: 0.4496 - val_mae: 0.5101\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2580 - mse: 0.2580 - mae: 0.3855 - val_loss: 0.4372 - val_mse: 0.4372 - val_mae: 0.5083\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2106 - mse: 0.2106 - mae: 0.3529 - val_loss: 0.4245 - val_mse: 0.4245 - val_mae: 0.5045\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2064 - mse: 0.2064 - mae: 0.3510 - val_loss: 0.4383 - val_mse: 0.4383 - val_mae: 0.5167\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2022 - mse: 0.2022 - mae: 0.3489 - val_loss: 0.4204 - val_mse: 0.4204 - val_mae: 0.5035\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2017 - mse: 0.2017 - mae: 0.3431 - val_loss: 0.4287 - val_mse: 0.4287 - val_mae: 0.5037\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2049 - mse: 0.2049 - mae: 0.3490 - val_loss: 0.4396 - val_mse: 0.4396 - val_mae: 0.5173\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2174 - mse: 0.2174 - mae: 0.3599 - val_loss: 0.4054 - val_mse: 0.4054 - val_mae: 0.4937\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1863 - mse: 0.1863 - mae: 0.3343 - val_loss: 0.4284 - val_mse: 0.4284 - val_mae: 0.5120\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2059 - mse: 0.2059 - mae: 0.3513 - val_loss: 0.4208 - val_mse: 0.4208 - val_mae: 0.4989\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2094 - mse: 0.2094 - mae: 0.3506 - val_loss: 0.4342 - val_mse: 0.4342 - val_mae: 0.5117\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2145 - mse: 0.2145 - mae: 0.3646 - val_loss: 0.4362 - val_mse: 0.4362 - val_mae: 0.5123\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2164 - mse: 0.2164 - mae: 0.3628 - val_loss: 0.4652 - val_mse: 0.4652 - val_mae: 0.5253\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2124 - mse: 0.2124 - mae: 0.3493 - val_loss: 0.4383 - val_mse: 0.4383 - val_mae: 0.5111\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2186 - mse: 0.2186 - mae: 0.3625 - val_loss: 0.4366 - val_mse: 0.4366 - val_mae: 0.5125\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1896 - mse: 0.1896 - mae: 0.3333 - val_loss: 0.4363 - val_mse: 0.4363 - val_mae: 0.5033\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2085 - mse: 0.2085 - mae: 0.3583 - val_loss: 0.4347 - val_mse: 0.4347 - val_mae: 0.5118\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2113 - mse: 0.2113 - mae: 0.3541 - val_loss: 0.4745 - val_mse: 0.4745 - val_mae: 0.5248\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2100 - mse: 0.2100 - mae: 0.3553 - val_loss: 0.4228 - val_mse: 0.4228 - val_mae: 0.4987\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2047 - mse: 0.2047 - mae: 0.3539 - val_loss: 0.4066 - val_mse: 0.4066 - val_mae: 0.4866\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1922 - mse: 0.1922 - mae: 0.3317 - val_loss: 0.4414 - val_mse: 0.4414 - val_mae: 0.5097\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2019 - mse: 0.2019 - mae: 0.3455 - val_loss: 0.4163 - val_mse: 0.4163 - val_mae: 0.5025\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2087 - mse: 0.2087 - mae: 0.3566 - val_loss: 0.4103 - val_mse: 0.4103 - val_mae: 0.4911\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1986 - mse: 0.1986 - mae: 0.3464 - val_loss: 0.4146 - val_mse: 0.4146 - val_mae: 0.4975\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1919 - mse: 0.1919 - mae: 0.3363 - val_loss: 0.4233 - val_mse: 0.4233 - val_mae: 0.4951\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1900 - mse: 0.1900 - mae: 0.3356 - val_loss: 0.4214 - val_mse: 0.4214 - val_mae: 0.4965\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1949 - mse: 0.1949 - mae: 0.3397 - val_loss: 0.4100 - val_mse: 0.4100 - val_mae: 0.4947\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2113 - mse: 0.2113 - mae: 0.3582 - val_loss: 0.5218 - val_mse: 0.5218 - val_mae: 0.5666\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2159 - mse: 0.2159 - mae: 0.3615 - val_loss: 0.4075 - val_mse: 0.4075 - val_mae: 0.4924\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2053 - mse: 0.2053 - mae: 0.3501 - val_loss: 0.4313 - val_mse: 0.4313 - val_mae: 0.5063\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2039 - mse: 0.2039 - mae: 0.3463 - val_loss: 0.4113 - val_mse: 0.4113 - val_mae: 0.4979\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2069 - mse: 0.2069 - mae: 0.3499 - val_loss: 0.4136 - val_mse: 0.4136 - val_mae: 0.5001\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1903 - mse: 0.1903 - mae: 0.3371 - val_loss: 0.4685 - val_mse: 0.4685 - val_mae: 0.5374\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1802 - mse: 0.1802 - mae: 0.3282 - val_loss: 0.4198 - val_mse: 0.4198 - val_mae: 0.5071\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1935 - mse: 0.1935 - mae: 0.3388 - val_loss: 0.4204 - val_mse: 0.4204 - val_mae: 0.5058\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1974 - mse: 0.1974 - mae: 0.3435 - val_loss: 0.4381 - val_mse: 0.4381 - val_mae: 0.5085\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2051 - mse: 0.2051 - mae: 0.3474 - val_loss: 0.4128 - val_mse: 0.4128 - val_mae: 0.4921\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2002 - mse: 0.2002 - mae: 0.3515 - val_loss: 0.4119 - val_mse: 0.4119 - val_mae: 0.4966\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1788 - mse: 0.1788 - mae: 0.3202 - val_loss: 0.4338 - val_mse: 0.4338 - val_mae: 0.4972\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2143 - mse: 0.2143 - mae: 0.3531 - val_loss: 0.4298 - val_mse: 0.4298 - val_mae: 0.5103\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2003 - mse: 0.2003 - mae: 0.3428 - val_loss: 0.4237 - val_mse: 0.4237 - val_mae: 0.5062\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.1922 - mse: 0.1922 - mae: 0.3341 - val_loss: 0.4549 - val_mse: 0.4549 - val_mae: 0.5222\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2197 - mse: 0.2197 - mae: 0.3645 - val_loss: 0.4268 - val_mse: 0.4268 - val_mae: 0.5079\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2156 - mse: 0.2156 - mae: 0.3471 - val_loss: 0.4342 - val_mse: 0.4342 - val_mae: 0.5065\n",
            "Epoch 252/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2036 - mse: 0.2036 - mae: 0.3480 - val_loss: 0.4278 - val_mse: 0.4278 - val_mae: 0.5030\n",
            "Epoch 253/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2037 - mse: 0.2037 - mae: 0.3477 - val_loss: 0.4429 - val_mse: 0.4429 - val_mae: 0.5173\n",
            "Epoch 254/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1993 - mse: 0.1993 - mae: 0.3332 - val_loss: 0.4867 - val_mse: 0.4867 - val_mae: 0.5430\n",
            "Epoch 255/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2345 - mse: 0.2345 - mae: 0.3675 - val_loss: 0.4154 - val_mse: 0.4154 - val_mae: 0.4974\n",
            "Epoch 256/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2416 - mse: 0.2416 - mae: 0.3879 - val_loss: 0.4069 - val_mse: 0.4069 - val_mae: 0.4922\n",
            "Epoch 257/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1760 - mse: 0.1760 - mae: 0.3256 - val_loss: 0.4217 - val_mse: 0.4217 - val_mae: 0.4976\n",
            "Epoch 258/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1867 - mse: 0.1867 - mae: 0.3300 - val_loss: 0.4137 - val_mse: 0.4137 - val_mae: 0.4914\n",
            "Epoch 259/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2095 - mse: 0.2095 - mae: 0.3567 - val_loss: 0.4552 - val_mse: 0.4552 - val_mae: 0.5113\n",
            "Epoch 260/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1850 - mse: 0.1850 - mae: 0.3330 - val_loss: 0.4288 - val_mse: 0.4288 - val_mae: 0.4969\n",
            "Epoch 261/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1744 - mse: 0.1744 - mae: 0.3293 - val_loss: 0.4487 - val_mse: 0.4487 - val_mae: 0.5105\n",
            "Epoch 262/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1962 - mse: 0.1962 - mae: 0.3370 - val_loss: 0.4399 - val_mse: 0.4399 - val_mae: 0.5069\n",
            "Epoch 263/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1940 - mse: 0.1940 - mae: 0.3360 - val_loss: 0.4315 - val_mse: 0.4315 - val_mae: 0.5132\n",
            "Epoch 264/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1821 - mse: 0.1821 - mae: 0.3312 - val_loss: 0.4205 - val_mse: 0.4205 - val_mae: 0.4877\n",
            "Epoch 265/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1982 - mse: 0.1982 - mae: 0.3421 - val_loss: 0.4590 - val_mse: 0.4590 - val_mae: 0.5146\n",
            "Epoch 266/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1923 - mse: 0.1923 - mae: 0.3337 - val_loss: 0.4271 - val_mse: 0.4271 - val_mae: 0.4963\n",
            "Epoch 267/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1969 - mse: 0.1969 - mae: 0.3447 - val_loss: 0.4461 - val_mse: 0.4461 - val_mae: 0.5185\n",
            "Epoch 268/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2483 - mse: 0.2483 - mae: 0.3938 - val_loss: 0.4528 - val_mse: 0.4528 - val_mae: 0.5243\n",
            "Epoch 269/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2171 - mse: 0.2171 - mae: 0.3595 - val_loss: 0.4287 - val_mse: 0.4287 - val_mae: 0.5088\n",
            "Epoch 270/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1746 - mse: 0.1746 - mae: 0.3212 - val_loss: 0.4233 - val_mse: 0.4233 - val_mae: 0.4965\n",
            "Epoch 271/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2031 - mse: 0.2031 - mae: 0.3533 - val_loss: 0.4458 - val_mse: 0.4458 - val_mae: 0.5192\n",
            "Epoch 272/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1949 - mse: 0.1949 - mae: 0.3351 - val_loss: 0.4452 - val_mse: 0.4452 - val_mae: 0.5217\n",
            "Epoch 273/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1773 - mse: 0.1773 - mae: 0.3281 - val_loss: 0.4335 - val_mse: 0.4335 - val_mae: 0.5118\n",
            "Epoch 274/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1755 - mse: 0.1755 - mae: 0.3241 - val_loss: 0.4095 - val_mse: 0.4095 - val_mae: 0.4866\n",
            "Epoch 275/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2130 - mse: 0.2130 - mae: 0.3577 - val_loss: 0.4295 - val_mse: 0.4295 - val_mae: 0.5085\n",
            "Epoch 276/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1981 - mse: 0.1981 - mae: 0.3422 - val_loss: 0.4423 - val_mse: 0.4423 - val_mae: 0.5130\n",
            "Epoch 277/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1930 - mse: 0.1930 - mae: 0.3441 - val_loss: 0.4058 - val_mse: 0.4058 - val_mae: 0.4835\n",
            "Epoch 278/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1718 - mse: 0.1718 - mae: 0.3213 - val_loss: 0.4094 - val_mse: 0.4094 - val_mae: 0.4831\n",
            "Epoch 279/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1927 - mse: 0.1927 - mae: 0.3383 - val_loss: 0.3956 - val_mse: 0.3956 - val_mae: 0.4836\n",
            "Epoch 280/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1748 - mse: 0.1748 - mae: 0.3309 - val_loss: 0.4256 - val_mse: 0.4256 - val_mae: 0.4972\n",
            "Epoch 281/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2040 - mse: 0.2040 - mae: 0.3458 - val_loss: 0.4195 - val_mse: 0.4195 - val_mae: 0.4973\n",
            "Epoch 282/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2104 - mse: 0.2104 - mae: 0.3572 - val_loss: 0.4383 - val_mse: 0.4383 - val_mae: 0.5091\n",
            "Epoch 283/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1832 - mse: 0.1832 - mae: 0.3303 - val_loss: 0.4127 - val_mse: 0.4127 - val_mae: 0.4941\n",
            "Epoch 284/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2078 - mse: 0.2078 - mae: 0.3475 - val_loss: 0.4195 - val_mse: 0.4195 - val_mae: 0.4986\n",
            "Epoch 285/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1662 - mse: 0.1662 - mae: 0.3113 - val_loss: 0.4074 - val_mse: 0.4074 - val_mae: 0.4931\n",
            "Epoch 286/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1879 - mse: 0.1879 - mae: 0.3396 - val_loss: 0.4171 - val_mse: 0.4171 - val_mae: 0.4917\n",
            "Epoch 287/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1894 - mse: 0.1894 - mae: 0.3368 - val_loss: 0.4065 - val_mse: 0.4065 - val_mae: 0.4910\n",
            "Epoch 288/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1891 - mse: 0.1891 - mae: 0.3333 - val_loss: 0.4093 - val_mse: 0.4093 - val_mae: 0.4974\n",
            "Epoch 289/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2120 - mse: 0.2120 - mae: 0.3511 - val_loss: 0.3970 - val_mse: 0.3970 - val_mae: 0.4908\n",
            "Epoch 290/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1817 - mse: 0.1817 - mae: 0.3271 - val_loss: 0.4180 - val_mse: 0.4180 - val_mae: 0.4920\n",
            "Epoch 291/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1704 - mse: 0.1704 - mae: 0.3140 - val_loss: 0.4207 - val_mse: 0.4207 - val_mae: 0.5022\n",
            "Epoch 292/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1783 - mse: 0.1783 - mae: 0.3228 - val_loss: 0.4191 - val_mse: 0.4191 - val_mae: 0.5005\n",
            "Epoch 293/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1814 - mse: 0.1814 - mae: 0.3294 - val_loss: 0.4230 - val_mse: 0.4230 - val_mae: 0.5064\n",
            "Epoch 294/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1835 - mse: 0.1835 - mae: 0.3390 - val_loss: 0.4217 - val_mse: 0.4217 - val_mae: 0.5015\n",
            "Epoch 295/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1999 - mse: 0.1999 - mae: 0.3387 - val_loss: 0.4020 - val_mse: 0.4020 - val_mae: 0.4847\n",
            "Epoch 296/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2003 - mse: 0.2003 - mae: 0.3490 - val_loss: 0.4469 - val_mse: 0.4469 - val_mae: 0.5167\n",
            "Epoch 297/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1929 - mse: 0.1929 - mae: 0.3365 - val_loss: 0.4439 - val_mse: 0.4439 - val_mae: 0.5157\n",
            "Epoch 298/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2221 - mse: 0.2221 - mae: 0.3640 - val_loss: 0.4535 - val_mse: 0.4535 - val_mae: 0.5148\n",
            "Epoch 299/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1894 - mse: 0.1894 - mae: 0.3383 - val_loss: 0.4161 - val_mse: 0.4161 - val_mae: 0.4941\n",
            "Epoch 300/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1695 - mse: 0.1695 - mae: 0.3193 - val_loss: 0.4022 - val_mse: 0.4022 - val_mae: 0.4921\n",
            "Epoch 301/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1928 - mse: 0.1928 - mae: 0.3426 - val_loss: 0.4500 - val_mse: 0.4500 - val_mae: 0.5190\n",
            "Epoch 302/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2011 - mse: 0.2011 - mae: 0.3401 - val_loss: 0.4141 - val_mse: 0.4141 - val_mae: 0.4894\n",
            "Epoch 303/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1887 - mse: 0.1887 - mae: 0.3324 - val_loss: 0.4293 - val_mse: 0.4293 - val_mae: 0.4975\n",
            "Epoch 304/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1826 - mse: 0.1826 - mae: 0.3318 - val_loss: 0.4678 - val_mse: 0.4678 - val_mae: 0.5157\n",
            "Epoch 305/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1905 - mse: 0.1905 - mae: 0.3318 - val_loss: 0.4114 - val_mse: 0.4114 - val_mae: 0.4939\n",
            "Epoch 306/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1596 - mse: 0.1596 - mae: 0.3061 - val_loss: 0.4201 - val_mse: 0.4201 - val_mae: 0.4977\n",
            "Epoch 307/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1751 - mse: 0.1751 - mae: 0.3288 - val_loss: 0.4224 - val_mse: 0.4224 - val_mae: 0.4972\n",
            "Epoch 308/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1901 - mse: 0.1901 - mae: 0.3339 - val_loss: 0.4035 - val_mse: 0.4035 - val_mae: 0.4880\n",
            "Epoch 309/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1922 - mse: 0.1922 - mae: 0.3386 - val_loss: 0.3932 - val_mse: 0.3932 - val_mae: 0.4818\n",
            "Epoch 310/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1870 - mse: 0.1870 - mae: 0.3340 - val_loss: 0.4116 - val_mse: 0.4116 - val_mae: 0.4921\n",
            "Epoch 311/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1651 - mse: 0.1651 - mae: 0.3129 - val_loss: 0.3943 - val_mse: 0.3943 - val_mae: 0.4835\n",
            "Epoch 312/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1866 - mse: 0.1866 - mae: 0.3298 - val_loss: 0.4041 - val_mse: 0.4041 - val_mae: 0.4852\n",
            "Epoch 313/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1632 - mse: 0.1632 - mae: 0.3055 - val_loss: 0.4048 - val_mse: 0.4048 - val_mae: 0.4850\n",
            "Epoch 314/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1768 - mse: 0.1768 - mae: 0.3268 - val_loss: 0.4304 - val_mse: 0.4304 - val_mae: 0.4923\n",
            "Epoch 315/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1715 - mse: 0.1715 - mae: 0.3141 - val_loss: 0.4522 - val_mse: 0.4522 - val_mae: 0.5223\n",
            "Epoch 316/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1841 - mse: 0.1841 - mae: 0.3331 - val_loss: 0.4090 - val_mse: 0.4090 - val_mae: 0.4897\n",
            "Epoch 317/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1710 - mse: 0.1710 - mae: 0.3215 - val_loss: 0.4147 - val_mse: 0.4147 - val_mae: 0.4890\n",
            "Epoch 318/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.1787 - mse: 0.1787 - mae: 0.3257 - val_loss: 0.4209 - val_mse: 0.4209 - val_mae: 0.4970\n",
            "Epoch 319/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1993 - mse: 0.1993 - mae: 0.3426 - val_loss: 0.4228 - val_mse: 0.4228 - val_mae: 0.4964\n",
            "Epoch 320/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1939 - mse: 0.1939 - mae: 0.3366 - val_loss: 0.4855 - val_mse: 0.4855 - val_mae: 0.5405\n",
            "Epoch 321/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2212 - mse: 0.2212 - mae: 0.3638 - val_loss: 0.4189 - val_mse: 0.4189 - val_mae: 0.4970\n",
            "Epoch 322/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1683 - mse: 0.1683 - mae: 0.3189 - val_loss: 0.4470 - val_mse: 0.4470 - val_mae: 0.5215\n",
            "Epoch 323/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1855 - mse: 0.1855 - mae: 0.3258 - val_loss: 0.4161 - val_mse: 0.4161 - val_mae: 0.4921\n",
            "Epoch 324/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1717 - mse: 0.1717 - mae: 0.3192 - val_loss: 0.4378 - val_mse: 0.4378 - val_mae: 0.5076\n",
            "Epoch 325/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1619 - mse: 0.1619 - mae: 0.3096 - val_loss: 0.4134 - val_mse: 0.4134 - val_mae: 0.4928\n",
            "Epoch 326/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1630 - mse: 0.1630 - mae: 0.3086 - val_loss: 0.4396 - val_mse: 0.4396 - val_mae: 0.5042\n",
            "Epoch 327/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1828 - mse: 0.1828 - mae: 0.3399 - val_loss: 0.3898 - val_mse: 0.3898 - val_mae: 0.4842\n",
            "Epoch 328/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1706 - mse: 0.1706 - mae: 0.3231 - val_loss: 0.4481 - val_mse: 0.4481 - val_mae: 0.5168\n",
            "Epoch 329/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1711 - mse: 0.1711 - mae: 0.3223 - val_loss: 0.4108 - val_mse: 0.4108 - val_mae: 0.4859\n",
            "Epoch 330/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1740 - mse: 0.1740 - mae: 0.3276 - val_loss: 0.4563 - val_mse: 0.4563 - val_mae: 0.5196\n",
            "Epoch 331/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1925 - mse: 0.1925 - mae: 0.3418 - val_loss: 0.4469 - val_mse: 0.4469 - val_mae: 0.5312\n",
            "Epoch 332/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2027 - mse: 0.2027 - mae: 0.3492 - val_loss: 0.4666 - val_mse: 0.4666 - val_mae: 0.5249\n",
            "Epoch 333/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1831 - mse: 0.1831 - mae: 0.3378 - val_loss: 0.4218 - val_mse: 0.4218 - val_mae: 0.5032\n",
            "Epoch 334/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1529 - mse: 0.1529 - mae: 0.2997 - val_loss: 0.3999 - val_mse: 0.3999 - val_mae: 0.4854\n",
            "Epoch 335/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1875 - mse: 0.1875 - mae: 0.3301 - val_loss: 0.4058 - val_mse: 0.4058 - val_mae: 0.4898\n",
            "Epoch 336/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1676 - mse: 0.1676 - mae: 0.3174 - val_loss: 0.4259 - val_mse: 0.4259 - val_mae: 0.4919\n",
            "Epoch 337/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1786 - mse: 0.1786 - mae: 0.3286 - val_loss: 0.4432 - val_mse: 0.4432 - val_mae: 0.5094\n",
            "Epoch 338/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1887 - mse: 0.1887 - mae: 0.3348 - val_loss: 0.4241 - val_mse: 0.4241 - val_mae: 0.4950\n",
            "Epoch 339/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1930 - mse: 0.1930 - mae: 0.3296 - val_loss: 0.4321 - val_mse: 0.4321 - val_mae: 0.5072\n",
            "Epoch 340/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1667 - mse: 0.1667 - mae: 0.3163 - val_loss: 0.3885 - val_mse: 0.3885 - val_mae: 0.4787\n",
            "Epoch 341/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1793 - mse: 0.1793 - mae: 0.3251 - val_loss: 0.4853 - val_mse: 0.4853 - val_mae: 0.5331\n",
            "Epoch 342/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1744 - mse: 0.1744 - mae: 0.3218 - val_loss: 0.4038 - val_mse: 0.4038 - val_mae: 0.4826\n",
            "Epoch 343/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1689 - mse: 0.1689 - mae: 0.3202 - val_loss: 0.4209 - val_mse: 0.4209 - val_mae: 0.4925\n",
            "Epoch 344/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1794 - mse: 0.1794 - mae: 0.3245 - val_loss: 0.3846 - val_mse: 0.3846 - val_mae: 0.4804\n",
            "Epoch 345/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1770 - mse: 0.1770 - mae: 0.3276 - val_loss: 0.3910 - val_mse: 0.3910 - val_mae: 0.4818\n",
            "Epoch 346/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1507 - mse: 0.1507 - mae: 0.3005 - val_loss: 0.4067 - val_mse: 0.4067 - val_mae: 0.4922\n",
            "Epoch 347/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1811 - mse: 0.1811 - mae: 0.3288 - val_loss: 0.4289 - val_mse: 0.4289 - val_mae: 0.4948\n",
            "Epoch 348/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1620 - mse: 0.1620 - mae: 0.3172 - val_loss: 0.4121 - val_mse: 0.4121 - val_mae: 0.4888\n",
            "Epoch 349/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1788 - mse: 0.1788 - mae: 0.3177 - val_loss: 0.4360 - val_mse: 0.4360 - val_mae: 0.4979\n",
            "Epoch 350/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2195 - mse: 0.2195 - mae: 0.3662 - val_loss: 0.4175 - val_mse: 0.4175 - val_mae: 0.4926\n",
            "Epoch 351/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1748 - mse: 0.1748 - mae: 0.3204 - val_loss: 0.4301 - val_mse: 0.4301 - val_mae: 0.4923\n",
            "Epoch 352/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1632 - mse: 0.1632 - mae: 0.3141 - val_loss: 0.4204 - val_mse: 0.4204 - val_mae: 0.4850\n",
            "Epoch 353/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1623 - mse: 0.1623 - mae: 0.3136 - val_loss: 0.4279 - val_mse: 0.4279 - val_mae: 0.4944\n",
            "Epoch 354/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1724 - mse: 0.1724 - mae: 0.3140 - val_loss: 0.4396 - val_mse: 0.4396 - val_mae: 0.5134\n",
            "Epoch 355/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1624 - mse: 0.1624 - mae: 0.3145 - val_loss: 0.3946 - val_mse: 0.3946 - val_mae: 0.4846\n",
            "Epoch 356/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2027 - mse: 0.2027 - mae: 0.3534 - val_loss: 0.4354 - val_mse: 0.4354 - val_mae: 0.5057\n",
            "Epoch 357/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1811 - mse: 0.1811 - mae: 0.3288 - val_loss: 0.4322 - val_mse: 0.4322 - val_mae: 0.4942\n",
            "Epoch 358/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1886 - mse: 0.1886 - mae: 0.3390 - val_loss: 0.4074 - val_mse: 0.4074 - val_mae: 0.4851\n",
            "Epoch 359/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1770 - mse: 0.1770 - mae: 0.3267 - val_loss: 0.4049 - val_mse: 0.4049 - val_mae: 0.4873\n",
            "Epoch 360/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1809 - mse: 0.1809 - mae: 0.3303 - val_loss: 0.4465 - val_mse: 0.4465 - val_mae: 0.5065\n",
            "Epoch 361/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1613 - mse: 0.1613 - mae: 0.3109 - val_loss: 0.3944 - val_mse: 0.3944 - val_mae: 0.4852\n",
            "Epoch 362/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1623 - mse: 0.1623 - mae: 0.3126 - val_loss: 0.4168 - val_mse: 0.4168 - val_mae: 0.4867\n",
            "Epoch 363/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1565 - mse: 0.1565 - mae: 0.3014 - val_loss: 0.4217 - val_mse: 0.4217 - val_mae: 0.4927\n",
            "Epoch 364/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1849 - mse: 0.1849 - mae: 0.3220 - val_loss: 0.4261 - val_mse: 0.4261 - val_mae: 0.5016\n",
            "Epoch 365/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2322 - mse: 0.2322 - mae: 0.3732 - val_loss: 0.4042 - val_mse: 0.4042 - val_mae: 0.4844\n",
            "Epoch 366/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1600 - mse: 0.1600 - mae: 0.3081 - val_loss: 0.3826 - val_mse: 0.3826 - val_mae: 0.4692\n",
            "Epoch 367/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1837 - mse: 0.1837 - mae: 0.3308 - val_loss: 0.4613 - val_mse: 0.4613 - val_mae: 0.5245\n",
            "Epoch 368/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1764 - mse: 0.1764 - mae: 0.3276 - val_loss: 0.4397 - val_mse: 0.4397 - val_mae: 0.5067\n",
            "Epoch 369/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1661 - mse: 0.1661 - mae: 0.3123 - val_loss: 0.4284 - val_mse: 0.4284 - val_mae: 0.4957\n",
            "Epoch 370/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.1853 - mse: 0.1853 - mae: 0.3268 - val_loss: 0.4199 - val_mse: 0.4199 - val_mae: 0.4914\n",
            "Epoch 371/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1782 - mse: 0.1782 - mae: 0.3264 - val_loss: 0.4189 - val_mse: 0.4189 - val_mae: 0.4930\n",
            "Epoch 372/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1389 - mse: 0.1389 - mae: 0.2905 - val_loss: 0.4233 - val_mse: 0.4233 - val_mae: 0.4973\n",
            "Epoch 373/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1869 - mse: 0.1869 - mae: 0.3342 - val_loss: 0.4362 - val_mse: 0.4362 - val_mae: 0.5059\n",
            "Epoch 374/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1616 - mse: 0.1616 - mae: 0.3082 - val_loss: 0.4108 - val_mse: 0.4108 - val_mae: 0.4886\n",
            "Epoch 375/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1564 - mse: 0.1564 - mae: 0.3114 - val_loss: 0.4028 - val_mse: 0.4028 - val_mae: 0.4883\n",
            "Epoch 376/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1758 - mse: 0.1758 - mae: 0.3186 - val_loss: 0.4059 - val_mse: 0.4059 - val_mae: 0.4910\n",
            "Epoch 377/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1568 - mse: 0.1568 - mae: 0.3085 - val_loss: 0.4029 - val_mse: 0.4029 - val_mae: 0.4878\n",
            "Epoch 378/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1759 - mse: 0.1759 - mae: 0.3216 - val_loss: 0.4023 - val_mse: 0.4023 - val_mae: 0.4829\n",
            "Epoch 379/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1619 - mse: 0.1619 - mae: 0.3063 - val_loss: 0.4360 - val_mse: 0.4360 - val_mae: 0.5001\n",
            "Epoch 380/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1667 - mse: 0.1667 - mae: 0.3165 - val_loss: 0.4194 - val_mse: 0.4194 - val_mae: 0.4930\n",
            "Epoch 381/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1703 - mse: 0.1703 - mae: 0.3142 - val_loss: 0.4251 - val_mse: 0.4251 - val_mae: 0.4896\n",
            "Epoch 382/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1769 - mse: 0.1769 - mae: 0.3250 - val_loss: 0.4218 - val_mse: 0.4218 - val_mae: 0.4891\n",
            "Epoch 383/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1568 - mse: 0.1568 - mae: 0.2999 - val_loss: 0.4118 - val_mse: 0.4118 - val_mae: 0.4916\n",
            "Epoch 384/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1533 - mse: 0.1533 - mae: 0.3051 - val_loss: 0.3901 - val_mse: 0.3901 - val_mae: 0.4782\n",
            "Epoch 385/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1553 - mse: 0.1553 - mae: 0.2999 - val_loss: 0.4154 - val_mse: 0.4154 - val_mae: 0.5020\n",
            "Epoch 386/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1642 - mse: 0.1642 - mae: 0.3164 - val_loss: 0.4276 - val_mse: 0.4276 - val_mae: 0.5019\n",
            "Epoch 387/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1700 - mse: 0.1700 - mae: 0.3154 - val_loss: 0.4194 - val_mse: 0.4194 - val_mae: 0.5002\n",
            "Epoch 388/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1635 - mse: 0.1635 - mae: 0.3158 - val_loss: 0.4147 - val_mse: 0.4147 - val_mae: 0.4920\n",
            "Epoch 389/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1724 - mse: 0.1724 - mae: 0.3182 - val_loss: 0.4299 - val_mse: 0.4299 - val_mae: 0.5018\n",
            "Epoch 390/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1765 - mse: 0.1765 - mae: 0.3277 - val_loss: 0.4249 - val_mse: 0.4249 - val_mae: 0.5011\n",
            "Epoch 391/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1661 - mse: 0.1661 - mae: 0.3053 - val_loss: 0.4477 - val_mse: 0.4477 - val_mae: 0.5110\n",
            "Epoch 392/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1906 - mse: 0.1906 - mae: 0.3334 - val_loss: 0.4455 - val_mse: 0.4455 - val_mae: 0.5128\n",
            "Epoch 393/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1534 - mse: 0.1534 - mae: 0.2999 - val_loss: 0.4276 - val_mse: 0.4276 - val_mae: 0.4992\n",
            "Epoch 394/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1551 - mse: 0.1551 - mae: 0.3015 - val_loss: 0.4342 - val_mse: 0.4342 - val_mae: 0.5121\n",
            "Epoch 395/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1703 - mse: 0.1703 - mae: 0.3147 - val_loss: 0.4505 - val_mse: 0.4505 - val_mae: 0.5122\n",
            "Epoch 396/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1648 - mse: 0.1648 - mae: 0.3111 - val_loss: 0.4355 - val_mse: 0.4355 - val_mae: 0.5107\n",
            "Epoch 397/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1830 - mse: 0.1830 - mae: 0.3318 - val_loss: 0.4095 - val_mse: 0.4095 - val_mae: 0.4926\n",
            "Epoch 398/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1696 - mse: 0.1696 - mae: 0.3158 - val_loss: 0.4129 - val_mse: 0.4129 - val_mae: 0.4869\n",
            "Epoch 399/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1509 - mse: 0.1509 - mae: 0.2940 - val_loss: 0.3995 - val_mse: 0.3995 - val_mae: 0.4833\n",
            "Epoch 400/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1605 - mse: 0.1605 - mae: 0.3090 - val_loss: 0.4297 - val_mse: 0.4297 - val_mae: 0.5006\n",
            "Epoch 401/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1439 - mse: 0.1439 - mae: 0.2980 - val_loss: 0.4420 - val_mse: 0.4420 - val_mae: 0.5065\n",
            "Epoch 402/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1497 - mse: 0.1497 - mae: 0.3040 - val_loss: 0.4290 - val_mse: 0.4290 - val_mae: 0.4994\n",
            "Epoch 403/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1482 - mse: 0.1482 - mae: 0.2897 - val_loss: 0.4179 - val_mse: 0.4179 - val_mae: 0.4948\n",
            "Epoch 404/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1812 - mse: 0.1812 - mae: 0.3358 - val_loss: 0.4100 - val_mse: 0.4100 - val_mae: 0.4842\n",
            "Epoch 405/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1491 - mse: 0.1491 - mae: 0.2975 - val_loss: 0.4171 - val_mse: 0.4171 - val_mae: 0.4915\n",
            "Epoch 406/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1867 - mse: 0.1867 - mae: 0.3351 - val_loss: 0.4324 - val_mse: 0.4324 - val_mae: 0.4972\n",
            "Epoch 407/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1441 - mse: 0.1441 - mae: 0.2969 - val_loss: 0.4144 - val_mse: 0.4144 - val_mae: 0.4921\n",
            "Epoch 408/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1802 - mse: 0.1802 - mae: 0.3241 - val_loss: 0.4354 - val_mse: 0.4354 - val_mae: 0.5100\n",
            "Epoch 409/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1485 - mse: 0.1485 - mae: 0.2995 - val_loss: 0.4569 - val_mse: 0.4569 - val_mae: 0.5177\n",
            "Epoch 410/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1532 - mse: 0.1532 - mae: 0.2980 - val_loss: 0.4243 - val_mse: 0.4243 - val_mae: 0.4971\n",
            "Epoch 411/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1623 - mse: 0.1623 - mae: 0.3111 - val_loss: 0.4245 - val_mse: 0.4245 - val_mae: 0.4926\n",
            "Epoch 412/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1527 - mse: 0.1527 - mae: 0.3051 - val_loss: 0.4008 - val_mse: 0.4008 - val_mae: 0.4818\n",
            "Epoch 413/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1493 - mse: 0.1493 - mae: 0.2988 - val_loss: 0.4432 - val_mse: 0.4432 - val_mae: 0.5141\n",
            "Epoch 414/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1843 - mse: 0.1843 - mae: 0.3375 - val_loss: 0.4872 - val_mse: 0.4872 - val_mae: 0.5304\n",
            "Epoch 415/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1714 - mse: 0.1714 - mae: 0.3184 - val_loss: 0.4500 - val_mse: 0.4500 - val_mae: 0.5243\n",
            "Epoch 416/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1829 - mse: 0.1829 - mae: 0.3408 - val_loss: 0.4279 - val_mse: 0.4279 - val_mae: 0.5047\n",
            "Epoch 417/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1709 - mse: 0.1709 - mae: 0.3261 - val_loss: 0.4808 - val_mse: 0.4808 - val_mae: 0.5479\n",
            "Epoch 418/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2165 - mse: 0.2165 - mae: 0.3636 - val_loss: 0.5072 - val_mse: 0.5072 - val_mae: 0.5458\n",
            "Epoch 419/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1898 - mse: 0.1898 - mae: 0.3383 - val_loss: 0.4496 - val_mse: 0.4496 - val_mae: 0.5207\n",
            "Epoch 420/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1957 - mse: 0.1957 - mae: 0.3422 - val_loss: 0.4336 - val_mse: 0.4336 - val_mae: 0.5019\n",
            "Epoch 421/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1766 - mse: 0.1766 - mae: 0.3251 - val_loss: 0.4067 - val_mse: 0.4067 - val_mae: 0.4869\n",
            "Epoch 422/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1760 - mse: 0.1760 - mae: 0.3296 - val_loss: 0.4284 - val_mse: 0.4284 - val_mae: 0.5032\n",
            "Epoch 423/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1674 - mse: 0.1674 - mae: 0.3257 - val_loss: 0.3939 - val_mse: 0.3939 - val_mae: 0.4844\n",
            "Epoch 424/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1822 - mse: 0.1822 - mae: 0.3326 - val_loss: 0.4279 - val_mse: 0.4279 - val_mae: 0.4942\n",
            "Epoch 425/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1687 - mse: 0.1687 - mae: 0.3116 - val_loss: 0.4196 - val_mse: 0.4196 - val_mae: 0.4916\n",
            "Epoch 426/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1613 - mse: 0.1613 - mae: 0.3094 - val_loss: 0.4094 - val_mse: 0.4094 - val_mae: 0.4835\n",
            "Epoch 427/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1592 - mse: 0.1592 - mae: 0.3099 - val_loss: 0.4107 - val_mse: 0.4107 - val_mae: 0.4883\n",
            "Epoch 428/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1689 - mse: 0.1689 - mae: 0.3140 - val_loss: 0.4020 - val_mse: 0.4020 - val_mae: 0.4868\n",
            "Epoch 429/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1744 - mse: 0.1744 - mae: 0.3229 - val_loss: 0.4250 - val_mse: 0.4250 - val_mae: 0.4947\n",
            "Epoch 430/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1754 - mse: 0.1754 - mae: 0.3198 - val_loss: 0.4726 - val_mse: 0.4726 - val_mae: 0.5281\n",
            "Epoch 431/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1778 - mse: 0.1778 - mae: 0.3276 - val_loss: 0.4237 - val_mse: 0.4237 - val_mae: 0.5061\n",
            "Epoch 432/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1761 - mse: 0.1761 - mae: 0.3189 - val_loss: 0.4085 - val_mse: 0.4085 - val_mae: 0.4991\n",
            "Epoch 433/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1531 - mse: 0.1531 - mae: 0.3040 - val_loss: 0.4023 - val_mse: 0.4023 - val_mae: 0.4761\n",
            "Epoch 434/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1601 - mse: 0.1601 - mae: 0.3078 - val_loss: 0.4215 - val_mse: 0.4215 - val_mae: 0.4960\n",
            "Epoch 435/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1868 - mse: 0.1868 - mae: 0.3369 - val_loss: 0.4070 - val_mse: 0.4070 - val_mae: 0.4855\n",
            "Epoch 436/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1480 - mse: 0.1480 - mae: 0.2908 - val_loss: 0.4438 - val_mse: 0.4438 - val_mae: 0.5057\n",
            "Epoch 437/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1873 - mse: 0.1873 - mae: 0.3336 - val_loss: 0.4681 - val_mse: 0.4681 - val_mae: 0.5313\n",
            "Epoch 438/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1819 - mse: 0.1819 - mae: 0.3276 - val_loss: 0.4268 - val_mse: 0.4268 - val_mae: 0.4968\n",
            "Epoch 439/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1619 - mse: 0.1619 - mae: 0.3129 - val_loss: 0.4078 - val_mse: 0.4078 - val_mae: 0.4915\n",
            "Epoch 440/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1556 - mse: 0.1556 - mae: 0.3038 - val_loss: 0.4038 - val_mse: 0.4038 - val_mae: 0.4782\n",
            "Epoch 441/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1643 - mse: 0.1643 - mae: 0.3149 - val_loss: 0.4461 - val_mse: 0.4461 - val_mae: 0.5093\n",
            "Epoch 442/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1631 - mse: 0.1631 - mae: 0.3193 - val_loss: 0.4161 - val_mse: 0.4161 - val_mae: 0.4979\n",
            "Epoch 443/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1791 - mse: 0.1791 - mae: 0.3264 - val_loss: 0.3949 - val_mse: 0.3949 - val_mae: 0.4741\n",
            "Epoch 444/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1526 - mse: 0.1526 - mae: 0.2964 - val_loss: 0.4328 - val_mse: 0.4328 - val_mae: 0.5037\n",
            "Epoch 445/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1695 - mse: 0.1695 - mae: 0.3103 - val_loss: 0.3895 - val_mse: 0.3895 - val_mae: 0.4745\n",
            "Epoch 446/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1852 - mse: 0.1852 - mae: 0.3264 - val_loss: 0.4162 - val_mse: 0.4162 - val_mae: 0.4839\n",
            "Epoch 447/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1656 - mse: 0.1656 - mae: 0.3115 - val_loss: 0.4114 - val_mse: 0.4114 - val_mae: 0.4821\n",
            "Epoch 448/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1706 - mse: 0.1706 - mae: 0.3111 - val_loss: 0.3824 - val_mse: 0.3824 - val_mae: 0.4743\n",
            "Epoch 449/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1541 - mse: 0.1541 - mae: 0.3038 - val_loss: 0.4049 - val_mse: 0.4049 - val_mae: 0.4848\n",
            "Epoch 450/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1571 - mse: 0.1571 - mae: 0.3011 - val_loss: 0.3985 - val_mse: 0.3985 - val_mae: 0.4819\n",
            "Epoch 451/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1381 - mse: 0.1381 - mae: 0.2854 - val_loss: 0.4021 - val_mse: 0.4021 - val_mae: 0.4883\n",
            "Epoch 452/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2021 - mse: 0.2021 - mae: 0.3553 - val_loss: 0.4229 - val_mse: 0.4229 - val_mae: 0.5094\n",
            "Epoch 453/1000\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.2149 - mse: 0.2149 - mae: 0.3532 - val_loss: 0.4190 - val_mse: 0.4190 - val_mae: 0.4949\n",
            "Epoch 454/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.1901 - mse: 0.1901 - mae: 0.3346 - val_loss: 0.4611 - val_mse: 0.4611 - val_mae: 0.5268\n",
            "Epoch 455/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1920 - mse: 0.1920 - mae: 0.3411 - val_loss: 0.4301 - val_mse: 0.4301 - val_mae: 0.5022\n",
            "Epoch 456/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1674 - mse: 0.1674 - mae: 0.3169 - val_loss: 0.4008 - val_mse: 0.4008 - val_mae: 0.4879\n",
            "Epoch 457/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1623 - mse: 0.1623 - mae: 0.3068 - val_loss: 0.4017 - val_mse: 0.4017 - val_mae: 0.4887\n",
            "Epoch 458/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1499 - mse: 0.1499 - mae: 0.2953 - val_loss: 0.4070 - val_mse: 0.4070 - val_mae: 0.4853\n",
            "Epoch 459/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1682 - mse: 0.1682 - mae: 0.3202 - val_loss: 0.4270 - val_mse: 0.4270 - val_mae: 0.5060\n",
            "Epoch 460/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1699 - mse: 0.1699 - mae: 0.3193 - val_loss: 0.4004 - val_mse: 0.4004 - val_mae: 0.4811\n",
            "Epoch 461/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1391 - mse: 0.1391 - mae: 0.2876 - val_loss: 0.3966 - val_mse: 0.3966 - val_mae: 0.4825\n",
            "Epoch 462/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1444 - mse: 0.1444 - mae: 0.2958 - val_loss: 0.4391 - val_mse: 0.4391 - val_mae: 0.5037\n",
            "Epoch 463/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1726 - mse: 0.1726 - mae: 0.3165 - val_loss: 0.4032 - val_mse: 0.4032 - val_mae: 0.4852\n",
            "Epoch 464/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1513 - mse: 0.1513 - mae: 0.3007 - val_loss: 0.4145 - val_mse: 0.4145 - val_mae: 0.4831\n",
            "Epoch 465/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1541 - mse: 0.1541 - mae: 0.3046 - val_loss: 0.4231 - val_mse: 0.4231 - val_mae: 0.4915\n",
            "Epoch 466/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1582 - mse: 0.1582 - mae: 0.3044 - val_loss: 0.4119 - val_mse: 0.4119 - val_mae: 0.4837\n",
            "Epoch 467/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1528 - mse: 0.1528 - mae: 0.2909 - val_loss: 0.4301 - val_mse: 0.4301 - val_mae: 0.4979\n",
            "Epoch 468/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1583 - mse: 0.1583 - mae: 0.3039 - val_loss: 0.4005 - val_mse: 0.4005 - val_mae: 0.4854\n",
            "Epoch 469/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1940 - mse: 0.1940 - mae: 0.3433 - val_loss: 0.4368 - val_mse: 0.4368 - val_mae: 0.5040\n",
            "Epoch 470/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1557 - mse: 0.1557 - mae: 0.3087 - val_loss: 0.4213 - val_mse: 0.4213 - val_mae: 0.5010\n",
            "Epoch 471/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1485 - mse: 0.1485 - mae: 0.2999 - val_loss: 0.4593 - val_mse: 0.4593 - val_mae: 0.5148\n",
            "Epoch 472/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1648 - mse: 0.1648 - mae: 0.3166 - val_loss: 0.4091 - val_mse: 0.4091 - val_mae: 0.4851\n",
            "Epoch 473/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1510 - mse: 0.1510 - mae: 0.2951 - val_loss: 0.3945 - val_mse: 0.3945 - val_mae: 0.4805\n",
            "Epoch 474/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1568 - mse: 0.1568 - mae: 0.3041 - val_loss: 0.4015 - val_mse: 0.4015 - val_mae: 0.4791\n",
            "Epoch 475/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1770 - mse: 0.1770 - mae: 0.3143 - val_loss: 0.4537 - val_mse: 0.4537 - val_mae: 0.5162\n",
            "Epoch 476/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1842 - mse: 0.1842 - mae: 0.3308 - val_loss: 0.4295 - val_mse: 0.4295 - val_mae: 0.5019\n",
            "Epoch 477/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1624 - mse: 0.1624 - mae: 0.3044 - val_loss: 0.4217 - val_mse: 0.4217 - val_mae: 0.4982\n",
            "Epoch 478/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1410 - mse: 0.1410 - mae: 0.2869 - val_loss: 0.4349 - val_mse: 0.4349 - val_mae: 0.5051\n",
            "Epoch 479/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1586 - mse: 0.1586 - mae: 0.3045 - val_loss: 0.3866 - val_mse: 0.3866 - val_mae: 0.4682\n",
            "Epoch 480/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1524 - mse: 0.1524 - mae: 0.2998 - val_loss: 0.4129 - val_mse: 0.4129 - val_mae: 0.4796\n",
            "Epoch 481/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1488 - mse: 0.1488 - mae: 0.3072 - val_loss: 0.3937 - val_mse: 0.3937 - val_mae: 0.4675\n",
            "Epoch 482/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1530 - mse: 0.1530 - mae: 0.3045 - val_loss: 0.4033 - val_mse: 0.4033 - val_mae: 0.4779\n",
            "Epoch 483/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1741 - mse: 0.1741 - mae: 0.3214 - val_loss: 0.4082 - val_mse: 0.4082 - val_mae: 0.4838\n",
            "Epoch 484/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1585 - mse: 0.1585 - mae: 0.3082 - val_loss: 0.3980 - val_mse: 0.3980 - val_mae: 0.4808\n",
            "Epoch 485/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1630 - mse: 0.1630 - mae: 0.3139 - val_loss: 0.3802 - val_mse: 0.3802 - val_mae: 0.4664\n",
            "Epoch 486/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1414 - mse: 0.1414 - mae: 0.2935 - val_loss: 0.3991 - val_mse: 0.3991 - val_mae: 0.4777\n",
            "Epoch 487/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1887 - mse: 0.1887 - mae: 0.3304 - val_loss: 0.4010 - val_mse: 0.4010 - val_mae: 0.4794\n",
            "Epoch 488/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1539 - mse: 0.1539 - mae: 0.3027 - val_loss: 0.4170 - val_mse: 0.4170 - val_mae: 0.4965\n",
            "Epoch 489/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1600 - mse: 0.1600 - mae: 0.3120 - val_loss: 0.4004 - val_mse: 0.4004 - val_mae: 0.4795\n",
            "Epoch 490/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1511 - mse: 0.1511 - mae: 0.2979 - val_loss: 0.4131 - val_mse: 0.4131 - val_mae: 0.4812\n",
            "Epoch 491/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1581 - mse: 0.1581 - mae: 0.3132 - val_loss: 0.4384 - val_mse: 0.4384 - val_mae: 0.4990\n",
            "Epoch 492/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1555 - mse: 0.1555 - mae: 0.3070 - val_loss: 0.4158 - val_mse: 0.4158 - val_mae: 0.4917\n",
            "Epoch 493/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1591 - mse: 0.1591 - mae: 0.3103 - val_loss: 0.4236 - val_mse: 0.4236 - val_mae: 0.4961\n",
            "Epoch 494/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1488 - mse: 0.1488 - mae: 0.3003 - val_loss: 0.4438 - val_mse: 0.4438 - val_mae: 0.5050\n",
            "Epoch 495/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1721 - mse: 0.1721 - mae: 0.3186 - val_loss: 0.4161 - val_mse: 0.4161 - val_mae: 0.4886\n",
            "Epoch 496/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1672 - mse: 0.1672 - mae: 0.3117 - val_loss: 0.4103 - val_mse: 0.4103 - val_mae: 0.4826\n",
            "Epoch 497/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1547 - mse: 0.1547 - mae: 0.2999 - val_loss: 0.4036 - val_mse: 0.4036 - val_mae: 0.4742\n",
            "Epoch 498/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1437 - mse: 0.1437 - mae: 0.2903 - val_loss: 0.4304 - val_mse: 0.4304 - val_mae: 0.4976\n",
            "Epoch 499/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1673 - mse: 0.1673 - mae: 0.3184 - val_loss: 0.4015 - val_mse: 0.4015 - val_mae: 0.4787\n",
            "Epoch 500/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1394 - mse: 0.1394 - mae: 0.2882 - val_loss: 0.4039 - val_mse: 0.4039 - val_mae: 0.4797\n",
            "Epoch 501/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1474 - mse: 0.1474 - mae: 0.2873 - val_loss: 0.4340 - val_mse: 0.4340 - val_mae: 0.4977\n",
            "Epoch 502/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1741 - mse: 0.1741 - mae: 0.3227 - val_loss: 0.4727 - val_mse: 0.4727 - val_mae: 0.5217\n",
            "Epoch 503/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1472 - mse: 0.1472 - mae: 0.2962 - val_loss: 0.4233 - val_mse: 0.4233 - val_mae: 0.4927\n",
            "Epoch 504/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1702 - mse: 0.1702 - mae: 0.3181 - val_loss: 0.4030 - val_mse: 0.4030 - val_mae: 0.4834\n",
            "Epoch 505/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1519 - mse: 0.1519 - mae: 0.3008 - val_loss: 0.4374 - val_mse: 0.4374 - val_mae: 0.4985\n",
            "Epoch 506/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1459 - mse: 0.1459 - mae: 0.2943 - val_loss: 0.4070 - val_mse: 0.4070 - val_mae: 0.4806\n",
            "Epoch 507/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1682 - mse: 0.1682 - mae: 0.3093 - val_loss: 0.4152 - val_mse: 0.4152 - val_mae: 0.4904\n",
            "Epoch 508/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1327 - mse: 0.1327 - mae: 0.2790 - val_loss: 0.4488 - val_mse: 0.4488 - val_mae: 0.5072\n",
            "Epoch 509/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1369 - mse: 0.1369 - mae: 0.2891 - val_loss: 0.4228 - val_mse: 0.4228 - val_mae: 0.4979\n",
            "Epoch 510/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1430 - mse: 0.1430 - mae: 0.2866 - val_loss: 0.4212 - val_mse: 0.4212 - val_mae: 0.4986\n",
            "Epoch 511/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1773 - mse: 0.1773 - mae: 0.3288 - val_loss: 0.4047 - val_mse: 0.4047 - val_mae: 0.4785\n",
            "Epoch 512/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1645 - mse: 0.1645 - mae: 0.3149 - val_loss: 0.4537 - val_mse: 0.4537 - val_mae: 0.5100\n",
            "Epoch 513/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1459 - mse: 0.1459 - mae: 0.2949 - val_loss: 0.4103 - val_mse: 0.4103 - val_mae: 0.4846\n",
            "Epoch 514/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1554 - mse: 0.1554 - mae: 0.3030 - val_loss: 0.4202 - val_mse: 0.4202 - val_mae: 0.4889\n",
            "Epoch 515/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1708 - mse: 0.1708 - mae: 0.3181 - val_loss: 0.4225 - val_mse: 0.4225 - val_mae: 0.4863\n",
            "Epoch 516/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1620 - mse: 0.1620 - mae: 0.3077 - val_loss: 0.4204 - val_mse: 0.4204 - val_mae: 0.4893\n",
            "Epoch 517/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1452 - mse: 0.1452 - mae: 0.2854 - val_loss: 0.4336 - val_mse: 0.4336 - val_mae: 0.4970\n",
            "Epoch 518/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1946 - mse: 0.1946 - mae: 0.3335 - val_loss: 0.4599 - val_mse: 0.4599 - val_mae: 0.5286\n",
            "Epoch 519/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1806 - mse: 0.1806 - mae: 0.3334 - val_loss: 0.4726 - val_mse: 0.4726 - val_mae: 0.5186\n",
            "Epoch 520/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1631 - mse: 0.1631 - mae: 0.3053 - val_loss: 0.4047 - val_mse: 0.4047 - val_mae: 0.4864\n",
            "Epoch 521/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1423 - mse: 0.1423 - mae: 0.2901 - val_loss: 0.4033 - val_mse: 0.4033 - val_mae: 0.4830\n",
            "Epoch 522/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1444 - mse: 0.1444 - mae: 0.2909 - val_loss: 0.4596 - val_mse: 0.4596 - val_mae: 0.5179\n",
            "Epoch 523/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1774 - mse: 0.1774 - mae: 0.3228 - val_loss: 0.4237 - val_mse: 0.4237 - val_mae: 0.4966\n",
            "Epoch 524/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1760 - mse: 0.1760 - mae: 0.3100 - val_loss: 0.4186 - val_mse: 0.4186 - val_mae: 0.4863\n",
            "Epoch 525/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1777 - mse: 0.1777 - mae: 0.3225 - val_loss: 0.4104 - val_mse: 0.4104 - val_mae: 0.4922\n",
            "Epoch 526/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1593 - mse: 0.1593 - mae: 0.3112 - val_loss: 0.4090 - val_mse: 0.4090 - val_mae: 0.4870\n",
            "Epoch 527/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1545 - mse: 0.1545 - mae: 0.3023 - val_loss: 0.4136 - val_mse: 0.4136 - val_mae: 0.4874\n",
            "Epoch 528/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1629 - mse: 0.1629 - mae: 0.3054 - val_loss: 0.4353 - val_mse: 0.4353 - val_mae: 0.5151\n",
            "Epoch 529/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1677 - mse: 0.1677 - mae: 0.3236 - val_loss: 0.4309 - val_mse: 0.4309 - val_mae: 0.5021\n",
            "Epoch 530/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2065 - mse: 0.2065 - mae: 0.3520 - val_loss: 0.4108 - val_mse: 0.4108 - val_mae: 0.4901\n",
            "Epoch 531/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1509 - mse: 0.1509 - mae: 0.2987 - val_loss: 0.4159 - val_mse: 0.4159 - val_mae: 0.4963\n",
            "Epoch 532/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1301 - mse: 0.1301 - mae: 0.2777 - val_loss: 0.4189 - val_mse: 0.4189 - val_mae: 0.4890\n",
            "Epoch 533/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1504 - mse: 0.1504 - mae: 0.2933 - val_loss: 0.4171 - val_mse: 0.4171 - val_mae: 0.4960\n",
            "Epoch 534/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1536 - mse: 0.1536 - mae: 0.3085 - val_loss: 0.4524 - val_mse: 0.4524 - val_mae: 0.5143\n",
            "Epoch 535/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1277 - mse: 0.1277 - mae: 0.2807 - val_loss: 0.4088 - val_mse: 0.4088 - val_mae: 0.4910\n",
            "Epoch 536/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1497 - mse: 0.1497 - mae: 0.2932 - val_loss: 0.4439 - val_mse: 0.4439 - val_mae: 0.5078\n",
            "Epoch 537/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1548 - mse: 0.1548 - mae: 0.3086 - val_loss: 0.4370 - val_mse: 0.4370 - val_mae: 0.5083\n",
            "Epoch 538/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1350 - mse: 0.1350 - mae: 0.2848 - val_loss: 0.4136 - val_mse: 0.4136 - val_mae: 0.4840\n",
            "Epoch 539/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1599 - mse: 0.1599 - mae: 0.2967 - val_loss: 0.4160 - val_mse: 0.4160 - val_mae: 0.4938\n",
            "Epoch 540/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1572 - mse: 0.1572 - mae: 0.3052 - val_loss: 0.4213 - val_mse: 0.4213 - val_mae: 0.4896\n",
            "Epoch 541/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1476 - mse: 0.1476 - mae: 0.2993 - val_loss: 0.3980 - val_mse: 0.3980 - val_mae: 0.4790\n",
            "Epoch 542/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1310 - mse: 0.1310 - mae: 0.2811 - val_loss: 0.4148 - val_mse: 0.4148 - val_mae: 0.4855\n",
            "Epoch 543/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1575 - mse: 0.1575 - mae: 0.3079 - val_loss: 0.4197 - val_mse: 0.4197 - val_mae: 0.4900\n",
            "Epoch 544/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1818 - mse: 0.1818 - mae: 0.3327 - val_loss: 0.4148 - val_mse: 0.4148 - val_mae: 0.4889\n",
            "Epoch 545/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1467 - mse: 0.1467 - mae: 0.2948 - val_loss: 0.3930 - val_mse: 0.3930 - val_mae: 0.4758\n",
            "Epoch 546/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1626 - mse: 0.1626 - mae: 0.3150 - val_loss: 0.4320 - val_mse: 0.4320 - val_mae: 0.4937\n",
            "Epoch 547/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1565 - mse: 0.1565 - mae: 0.3062 - val_loss: 0.4218 - val_mse: 0.4218 - val_mae: 0.4933\n",
            "Epoch 548/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1400 - mse: 0.1400 - mae: 0.2880 - val_loss: 0.4238 - val_mse: 0.4238 - val_mae: 0.4864\n",
            "Epoch 549/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.1374 - mse: 0.1374 - mae: 0.2789 - val_loss: 0.4119 - val_mse: 0.4119 - val_mae: 0.4815\n",
            "Epoch 550/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1548 - mse: 0.1548 - mae: 0.2980 - val_loss: 0.4156 - val_mse: 0.4156 - val_mae: 0.4898\n",
            "Epoch 551/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1579 - mse: 0.1579 - mae: 0.3088 - val_loss: 0.4306 - val_mse: 0.4306 - val_mae: 0.4983\n",
            "Epoch 552/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1527 - mse: 0.1527 - mae: 0.3064 - val_loss: 0.4666 - val_mse: 0.4666 - val_mae: 0.5220\n",
            "Epoch 553/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1815 - mse: 0.1815 - mae: 0.3332 - val_loss: 0.4141 - val_mse: 0.4141 - val_mae: 0.4909\n",
            "Epoch 554/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1636 - mse: 0.1636 - mae: 0.3085 - val_loss: 0.4272 - val_mse: 0.4272 - val_mae: 0.4908\n",
            "Epoch 555/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1443 - mse: 0.1443 - mae: 0.2909 - val_loss: 0.4615 - val_mse: 0.4615 - val_mae: 0.5109\n",
            "Epoch 556/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1957 - mse: 0.1957 - mae: 0.3383 - val_loss: 0.4623 - val_mse: 0.4623 - val_mae: 0.5126\n",
            "Epoch 557/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1692 - mse: 0.1692 - mae: 0.3236 - val_loss: 0.4155 - val_mse: 0.4155 - val_mae: 0.4854\n",
            "Epoch 558/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1567 - mse: 0.1567 - mae: 0.3079 - val_loss: 0.4112 - val_mse: 0.4112 - val_mae: 0.4820\n",
            "Epoch 559/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1485 - mse: 0.1485 - mae: 0.2936 - val_loss: 0.4333 - val_mse: 0.4333 - val_mae: 0.5016\n",
            "Epoch 560/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1548 - mse: 0.1548 - mae: 0.3041 - val_loss: 0.4172 - val_mse: 0.4172 - val_mae: 0.4929\n",
            "Epoch 561/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1388 - mse: 0.1388 - mae: 0.2926 - val_loss: 0.4249 - val_mse: 0.4249 - val_mae: 0.4918\n",
            "Epoch 562/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1599 - mse: 0.1599 - mae: 0.3074 - val_loss: 0.4184 - val_mse: 0.4184 - val_mae: 0.4917\n",
            "Epoch 563/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1563 - mse: 0.1563 - mae: 0.3125 - val_loss: 0.4219 - val_mse: 0.4219 - val_mae: 0.4904\n",
            "Epoch 564/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1608 - mse: 0.1608 - mae: 0.3052 - val_loss: 0.4427 - val_mse: 0.4427 - val_mae: 0.5106\n",
            "Epoch 565/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1786 - mse: 0.1786 - mae: 0.3243 - val_loss: 0.4268 - val_mse: 0.4268 - val_mae: 0.4950\n",
            "Epoch 566/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1439 - mse: 0.1439 - mae: 0.2962 - val_loss: 0.4331 - val_mse: 0.4331 - val_mae: 0.5011\n",
            "Epoch 567/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1448 - mse: 0.1448 - mae: 0.2871 - val_loss: 0.4042 - val_mse: 0.4042 - val_mae: 0.4863\n",
            "Epoch 568/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1511 - mse: 0.1511 - mae: 0.3032 - val_loss: 0.4145 - val_mse: 0.4145 - val_mae: 0.4949\n",
            "Epoch 569/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1652 - mse: 0.1652 - mae: 0.3085 - val_loss: 0.5212 - val_mse: 0.5212 - val_mae: 0.5525\n",
            "Epoch 570/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1601 - mse: 0.1601 - mae: 0.3063 - val_loss: 0.4744 - val_mse: 0.4744 - val_mae: 0.5231\n",
            "Epoch 571/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1652 - mse: 0.1652 - mae: 0.3114 - val_loss: 0.4434 - val_mse: 0.4434 - val_mae: 0.5104\n",
            "Epoch 572/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1516 - mse: 0.1516 - mae: 0.2997 - val_loss: 0.4598 - val_mse: 0.4598 - val_mae: 0.5178\n",
            "Epoch 573/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1416 - mse: 0.1416 - mae: 0.2903 - val_loss: 0.4454 - val_mse: 0.4454 - val_mae: 0.5076\n",
            "Epoch 574/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1236 - mse: 0.1236 - mae: 0.2711 - val_loss: 0.4133 - val_mse: 0.4133 - val_mae: 0.4941\n",
            "Epoch 575/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1506 - mse: 0.1506 - mae: 0.2914 - val_loss: 0.4893 - val_mse: 0.4893 - val_mae: 0.5391\n",
            "Epoch 576/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1587 - mse: 0.1587 - mae: 0.3077 - val_loss: 0.4169 - val_mse: 0.4169 - val_mae: 0.4880\n",
            "Epoch 577/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1490 - mse: 0.1490 - mae: 0.2927 - val_loss: 0.4377 - val_mse: 0.4377 - val_mae: 0.4969\n",
            "Epoch 578/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1590 - mse: 0.1590 - mae: 0.3053 - val_loss: 0.4518 - val_mse: 0.4518 - val_mae: 0.5114\n",
            "Epoch 579/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1607 - mse: 0.1607 - mae: 0.3068 - val_loss: 0.4269 - val_mse: 0.4269 - val_mae: 0.4887\n",
            "Epoch 580/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1509 - mse: 0.1509 - mae: 0.2943 - val_loss: 0.3991 - val_mse: 0.3991 - val_mae: 0.4743\n",
            "Epoch 581/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1546 - mse: 0.1546 - mae: 0.3049 - val_loss: 0.4599 - val_mse: 0.4599 - val_mae: 0.5177\n",
            "Epoch 582/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1806 - mse: 0.1806 - mae: 0.3260 - val_loss: 0.4627 - val_mse: 0.4627 - val_mae: 0.5326\n",
            "Epoch 583/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1768 - mse: 0.1768 - mae: 0.3257 - val_loss: 0.4168 - val_mse: 0.4168 - val_mae: 0.4931\n",
            "Epoch 584/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1532 - mse: 0.1532 - mae: 0.3026 - val_loss: 0.4130 - val_mse: 0.4130 - val_mae: 0.4918\n",
            "Epoch 585/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1585 - mse: 0.1585 - mae: 0.3031 - val_loss: 0.4153 - val_mse: 0.4153 - val_mae: 0.4887\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.5487 - mse: 0.5487 - mae: 0.5487\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 16ms/step - loss: 22.5926 - mse: 22.5926 - mae: 4.2670 - val_loss: 8.0535 - val_mse: 8.0535 - val_mae: 2.3718\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5.3581 - mse: 5.3581 - mae: 1.7898 - val_loss: 4.1023 - val_mse: 4.1023 - val_mae: 1.4182\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 3.3716 - mse: 3.3716 - mae: 1.4467 - val_loss: 2.8983 - val_mse: 2.8983 - val_mae: 1.2878\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.5698 - mse: 2.5698 - mae: 1.2852 - val_loss: 2.5200 - val_mse: 2.5200 - val_mae: 1.1891\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 2.3063 - mse: 2.3063 - mae: 1.2544 - val_loss: 2.3191 - val_mse: 2.3191 - val_mae: 1.1521\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.0272 - mse: 2.0272 - mae: 1.1436 - val_loss: 2.1836 - val_mse: 2.1836 - val_mae: 1.1245\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.9673 - mse: 1.9673 - mae: 1.1283 - val_loss: 2.0457 - val_mse: 2.0457 - val_mae: 1.0952\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.8745 - mse: 1.8745 - mae: 1.0999 - val_loss: 1.9878 - val_mse: 1.9878 - val_mae: 1.0543\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.6524 - mse: 1.6524 - mae: 1.0349 - val_loss: 1.9331 - val_mse: 1.9331 - val_mae: 1.0515\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.6732 - mse: 1.6732 - mae: 1.0378 - val_loss: 1.8116 - val_mse: 1.8116 - val_mae: 1.0112\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.4970 - mse: 1.4970 - mae: 0.9879 - val_loss: 1.7012 - val_mse: 1.7012 - val_mae: 0.9776\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.4367 - mse: 1.4367 - mae: 0.9481 - val_loss: 1.6611 - val_mse: 1.6611 - val_mae: 0.9638\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 1.2644 - mse: 1.2644 - mae: 0.8922 - val_loss: 1.6770 - val_mse: 1.6770 - val_mae: 0.9698\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.2345 - mse: 1.2345 - mae: 0.8988 - val_loss: 1.5076 - val_mse: 1.5076 - val_mae: 0.9344\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2041 - mse: 1.2041 - mae: 0.8999 - val_loss: 1.5237 - val_mse: 1.5237 - val_mae: 0.9047\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.2192 - mse: 1.2192 - mae: 0.8774 - val_loss: 1.4342 - val_mse: 1.4342 - val_mae: 0.8855\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.1550 - mse: 1.1550 - mae: 0.8645 - val_loss: 1.3793 - val_mse: 1.3793 - val_mae: 0.8704\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.0510 - mse: 1.0510 - mae: 0.8149 - val_loss: 1.3560 - val_mse: 1.3560 - val_mae: 0.8611\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.0523 - mse: 1.0523 - mae: 0.8180 - val_loss: 1.3439 - val_mse: 1.3439 - val_mae: 0.8393\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9546 - mse: 0.9546 - mae: 0.7776 - val_loss: 1.2870 - val_mse: 1.2870 - val_mae: 0.8183\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9334 - mse: 0.9334 - mae: 0.7717 - val_loss: 1.1612 - val_mse: 1.1612 - val_mae: 0.8032\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9156 - mse: 0.9156 - mae: 0.7696 - val_loss: 1.1098 - val_mse: 1.1098 - val_mae: 0.8018\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.8424 - mse: 0.8424 - mae: 0.7274 - val_loss: 1.1014 - val_mse: 1.1014 - val_mae: 0.7809\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8569 - mse: 0.8569 - mae: 0.7349 - val_loss: 1.1015 - val_mse: 1.1015 - val_mae: 0.7786\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7281 - mse: 0.7281 - mae: 0.6810 - val_loss: 1.0548 - val_mse: 1.0548 - val_mae: 0.7598\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7832 - mse: 0.7832 - mae: 0.7058 - val_loss: 1.0008 - val_mse: 1.0008 - val_mae: 0.7635\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7220 - mse: 0.7220 - mae: 0.6746 - val_loss: 1.0167 - val_mse: 1.0167 - val_mae: 0.7494\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6969 - mse: 0.6969 - mae: 0.6578 - val_loss: 0.9371 - val_mse: 0.9371 - val_mae: 0.7264\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7266 - mse: 0.7266 - mae: 0.6594 - val_loss: 0.8938 - val_mse: 0.8938 - val_mae: 0.7270\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.6777 - mse: 0.6777 - mae: 0.6469 - val_loss: 0.9248 - val_mse: 0.9248 - val_mae: 0.7211\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6650 - mse: 0.6650 - mae: 0.6376 - val_loss: 0.8751 - val_mse: 0.8751 - val_mae: 0.6909\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6122 - mse: 0.6122 - mae: 0.6145 - val_loss: 0.8716 - val_mse: 0.8716 - val_mae: 0.7007\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6170 - mse: 0.6170 - mae: 0.6118 - val_loss: 0.8702 - val_mse: 0.8702 - val_mae: 0.6861\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5913 - mse: 0.5913 - mae: 0.6019 - val_loss: 0.9169 - val_mse: 0.9169 - val_mae: 0.6916\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6500 - mse: 0.6500 - mae: 0.6223 - val_loss: 0.8244 - val_mse: 0.8244 - val_mae: 0.6743\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5448 - mse: 0.5448 - mae: 0.5784 - val_loss: 0.7710 - val_mse: 0.7710 - val_mae: 0.6736\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.6333 - mse: 0.6333 - mae: 0.6200 - val_loss: 0.7765 - val_mse: 0.7765 - val_mae: 0.6834\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5960 - mse: 0.5960 - mae: 0.6014 - val_loss: 0.7091 - val_mse: 0.7091 - val_mae: 0.6402\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5549 - mse: 0.5549 - mae: 0.5830 - val_loss: 0.8121 - val_mse: 0.8121 - val_mae: 0.6707\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5177 - mse: 0.5177 - mae: 0.5690 - val_loss: 0.7640 - val_mse: 0.7640 - val_mae: 0.6385\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5038 - mse: 0.5038 - mae: 0.5510 - val_loss: 0.6925 - val_mse: 0.6925 - val_mae: 0.6335\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5446 - mse: 0.5446 - mae: 0.5783 - val_loss: 0.6695 - val_mse: 0.6695 - val_mae: 0.6274\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5054 - mse: 0.5054 - mae: 0.5528 - val_loss: 0.7243 - val_mse: 0.7243 - val_mae: 0.6270\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4371 - mse: 0.4371 - mae: 0.5267 - val_loss: 0.6592 - val_mse: 0.6592 - val_mae: 0.6156\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4569 - mse: 0.4569 - mae: 0.5288 - val_loss: 0.6311 - val_mse: 0.6311 - val_mae: 0.5974\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4867 - mse: 0.4867 - mae: 0.5488 - val_loss: 0.6951 - val_mse: 0.6951 - val_mae: 0.6146\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4477 - mse: 0.4477 - mae: 0.5248 - val_loss: 0.6272 - val_mse: 0.6272 - val_mae: 0.5980\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4500 - mse: 0.4500 - mae: 0.5317 - val_loss: 0.6105 - val_mse: 0.6105 - val_mae: 0.6074\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4468 - mse: 0.4468 - mae: 0.5151 - val_loss: 0.5978 - val_mse: 0.5978 - val_mae: 0.5880\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.4312 - mse: 0.4312 - mae: 0.5121 - val_loss: 0.5880 - val_mse: 0.5880 - val_mae: 0.5915\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4185 - mse: 0.4185 - mae: 0.5080 - val_loss: 0.5745 - val_mse: 0.5745 - val_mae: 0.5858\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4245 - mse: 0.4245 - mae: 0.5085 - val_loss: 0.5932 - val_mse: 0.5932 - val_mae: 0.5816\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4314 - mse: 0.4314 - mae: 0.5122 - val_loss: 0.6347 - val_mse: 0.6347 - val_mae: 0.6010\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4267 - mse: 0.4267 - mae: 0.5092 - val_loss: 0.5787 - val_mse: 0.5787 - val_mae: 0.5691\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3802 - mse: 0.3802 - mae: 0.4807 - val_loss: 0.5314 - val_mse: 0.5314 - val_mae: 0.5603\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3906 - mse: 0.3906 - mae: 0.4817 - val_loss: 0.5566 - val_mse: 0.5566 - val_mae: 0.5678\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3601 - mse: 0.3601 - mae: 0.4645 - val_loss: 0.5403 - val_mse: 0.5403 - val_mae: 0.5552\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3989 - mse: 0.3989 - mae: 0.4979 - val_loss: 0.5458 - val_mse: 0.5458 - val_mae: 0.5805\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3993 - mse: 0.3993 - mae: 0.4906 - val_loss: 0.5108 - val_mse: 0.5108 - val_mae: 0.5432\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3777 - mse: 0.3777 - mae: 0.4872 - val_loss: 0.5509 - val_mse: 0.5509 - val_mae: 0.5688\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3726 - mse: 0.3726 - mae: 0.4828 - val_loss: 0.5762 - val_mse: 0.5762 - val_mae: 0.5720\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3676 - mse: 0.3676 - mae: 0.4701 - val_loss: 0.5374 - val_mse: 0.5374 - val_mae: 0.5581\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3888 - mse: 0.3888 - mae: 0.4835 - val_loss: 0.5218 - val_mse: 0.5218 - val_mae: 0.5555\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3741 - mse: 0.3741 - mae: 0.4770 - val_loss: 0.5214 - val_mse: 0.5214 - val_mae: 0.5464\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3646 - mse: 0.3646 - mae: 0.4726 - val_loss: 0.5517 - val_mse: 0.5517 - val_mae: 0.5548\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3717 - mse: 0.3717 - mae: 0.4771 - val_loss: 0.4793 - val_mse: 0.4793 - val_mae: 0.5350\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3618 - mse: 0.3618 - mae: 0.4636 - val_loss: 0.5112 - val_mse: 0.5112 - val_mae: 0.5578\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3624 - mse: 0.3624 - mae: 0.4728 - val_loss: 0.4914 - val_mse: 0.4914 - val_mae: 0.5321\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3673 - mse: 0.3673 - mae: 0.4648 - val_loss: 0.5499 - val_mse: 0.5499 - val_mae: 0.5873\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3610 - mse: 0.3610 - mae: 0.4682 - val_loss: 0.4688 - val_mse: 0.4688 - val_mae: 0.5271\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3484 - mse: 0.3484 - mae: 0.4581 - val_loss: 0.4608 - val_mse: 0.4608 - val_mae: 0.5258\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3328 - mse: 0.3328 - mae: 0.4530 - val_loss: 0.4767 - val_mse: 0.4767 - val_mae: 0.5222\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3401 - mse: 0.3401 - mae: 0.4493 - val_loss: 0.4685 - val_mse: 0.4685 - val_mae: 0.5189\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3404 - mse: 0.3404 - mae: 0.4478 - val_loss: 0.4563 - val_mse: 0.4563 - val_mae: 0.5235\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3515 - mse: 0.3515 - mae: 0.4637 - val_loss: 0.4960 - val_mse: 0.4960 - val_mae: 0.5416\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3285 - mse: 0.3285 - mae: 0.4473 - val_loss: 0.4997 - val_mse: 0.4997 - val_mae: 0.5295\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3345 - mse: 0.3345 - mae: 0.4525 - val_loss: 0.4629 - val_mse: 0.4629 - val_mae: 0.5095\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3329 - mse: 0.3329 - mae: 0.4501 - val_loss: 0.5083 - val_mse: 0.5083 - val_mae: 0.5416\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3225 - mse: 0.3225 - mae: 0.4470 - val_loss: 0.4336 - val_mse: 0.4336 - val_mae: 0.5092\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3337 - mse: 0.3337 - mae: 0.4478 - val_loss: 0.6048 - val_mse: 0.6048 - val_mae: 0.5870\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3408 - mse: 0.3408 - mae: 0.4491 - val_loss: 0.4511 - val_mse: 0.4511 - val_mae: 0.5103\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3093 - mse: 0.3093 - mae: 0.4284 - val_loss: 0.4334 - val_mse: 0.4334 - val_mae: 0.5143\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3114 - mse: 0.3114 - mae: 0.4324 - val_loss: 0.4788 - val_mse: 0.4788 - val_mae: 0.5314\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3335 - mse: 0.3335 - mae: 0.4483 - val_loss: 0.4409 - val_mse: 0.4409 - val_mae: 0.5291\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3219 - mse: 0.3219 - mae: 0.4401 - val_loss: 0.4349 - val_mse: 0.4349 - val_mae: 0.5016\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3400 - mse: 0.3400 - mae: 0.4530 - val_loss: 0.5050 - val_mse: 0.5050 - val_mae: 0.5421\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3198 - mse: 0.3198 - mae: 0.4361 - val_loss: 0.4436 - val_mse: 0.4436 - val_mae: 0.5065\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2931 - mse: 0.2931 - mae: 0.4213 - val_loss: 0.4507 - val_mse: 0.4507 - val_mae: 0.5244\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2912 - mse: 0.2912 - mae: 0.4212 - val_loss: 0.4620 - val_mse: 0.4620 - val_mae: 0.5325\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3008 - mse: 0.3008 - mae: 0.4275 - val_loss: 0.5328 - val_mse: 0.5328 - val_mae: 0.5501\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3128 - mse: 0.3128 - mae: 0.4319 - val_loss: 0.4343 - val_mse: 0.4343 - val_mae: 0.4989\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2894 - mse: 0.2894 - mae: 0.4115 - val_loss: 0.4181 - val_mse: 0.4181 - val_mae: 0.4883\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3039 - mse: 0.3039 - mae: 0.4215 - val_loss: 0.4532 - val_mse: 0.4532 - val_mae: 0.5086\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3281 - mse: 0.3281 - mae: 0.4456 - val_loss: 0.4166 - val_mse: 0.4166 - val_mae: 0.4976\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3287 - mse: 0.3287 - mae: 0.4450 - val_loss: 0.4325 - val_mse: 0.4325 - val_mae: 0.5084\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2916 - mse: 0.2916 - mae: 0.4158 - val_loss: 0.4828 - val_mse: 0.4828 - val_mae: 0.5247\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2934 - mse: 0.2934 - mae: 0.4211 - val_loss: 0.4344 - val_mse: 0.4344 - val_mae: 0.5083\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2978 - mse: 0.2978 - mae: 0.4293 - val_loss: 0.4438 - val_mse: 0.4438 - val_mae: 0.5113\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2880 - mse: 0.2880 - mae: 0.4114 - val_loss: 0.4128 - val_mse: 0.4128 - val_mae: 0.4954\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2952 - mse: 0.2952 - mae: 0.4176 - val_loss: 0.4179 - val_mse: 0.4179 - val_mae: 0.4956\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2987 - mse: 0.2987 - mae: 0.4251 - val_loss: 0.4299 - val_mse: 0.4299 - val_mae: 0.5065\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2887 - mse: 0.2887 - mae: 0.4079 - val_loss: 0.4720 - val_mse: 0.4720 - val_mae: 0.5098\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2951 - mse: 0.2951 - mae: 0.4210 - val_loss: 0.4644 - val_mse: 0.4644 - val_mae: 0.5188\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2884 - mse: 0.2884 - mae: 0.4090 - val_loss: 0.4809 - val_mse: 0.4809 - val_mae: 0.5309\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2778 - mse: 0.2778 - mae: 0.4055 - val_loss: 0.4179 - val_mse: 0.4179 - val_mae: 0.4996\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2795 - mse: 0.2795 - mae: 0.4130 - val_loss: 0.4955 - val_mse: 0.4955 - val_mae: 0.5277\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2905 - mse: 0.2905 - mae: 0.4203 - val_loss: 0.4230 - val_mse: 0.4230 - val_mae: 0.4974\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2929 - mse: 0.2929 - mae: 0.4221 - val_loss: 0.4368 - val_mse: 0.4368 - val_mae: 0.5085\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2832 - mse: 0.2832 - mae: 0.4156 - val_loss: 0.4040 - val_mse: 0.4040 - val_mae: 0.4971\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2635 - mse: 0.2635 - mae: 0.4030 - val_loss: 0.4248 - val_mse: 0.4248 - val_mae: 0.4939\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2940 - mse: 0.2940 - mae: 0.4180 - val_loss: 0.4364 - val_mse: 0.4364 - val_mae: 0.4983\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2881 - mse: 0.2881 - mae: 0.4082 - val_loss: 0.3926 - val_mse: 0.3926 - val_mae: 0.4793\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2911 - mse: 0.2911 - mae: 0.4127 - val_loss: 0.4041 - val_mse: 0.4041 - val_mae: 0.4860\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3106 - mse: 0.3106 - mae: 0.4297 - val_loss: 0.4152 - val_mse: 0.4152 - val_mae: 0.4962\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2750 - mse: 0.2750 - mae: 0.4041 - val_loss: 0.4237 - val_mse: 0.4237 - val_mae: 0.4965\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2846 - mse: 0.2846 - mae: 0.4099 - val_loss: 0.4165 - val_mse: 0.4165 - val_mae: 0.5013\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2741 - mse: 0.2741 - mae: 0.4048 - val_loss: 0.4186 - val_mse: 0.4186 - val_mae: 0.4920\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2862 - mse: 0.2862 - mae: 0.4180 - val_loss: 0.4393 - val_mse: 0.4393 - val_mae: 0.4973\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2639 - mse: 0.2639 - mae: 0.3865 - val_loss: 0.4272 - val_mse: 0.4272 - val_mae: 0.5072\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2824 - mse: 0.2824 - mae: 0.4080 - val_loss: 0.4417 - val_mse: 0.4417 - val_mae: 0.5019\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2482 - mse: 0.2482 - mae: 0.3880 - val_loss: 0.4417 - val_mse: 0.4417 - val_mae: 0.5006\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2853 - mse: 0.2853 - mae: 0.4147 - val_loss: 0.4030 - val_mse: 0.4030 - val_mae: 0.4800\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2564 - mse: 0.2564 - mae: 0.3978 - val_loss: 0.4306 - val_mse: 0.4306 - val_mae: 0.4979\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2685 - mse: 0.2685 - mae: 0.4028 - val_loss: 0.4282 - val_mse: 0.4282 - val_mae: 0.5115\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2603 - mse: 0.2603 - mae: 0.3953 - val_loss: 0.4393 - val_mse: 0.4393 - val_mae: 0.4975\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2801 - mse: 0.2801 - mae: 0.4096 - val_loss: 0.4257 - val_mse: 0.4257 - val_mae: 0.4997\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2902 - mse: 0.2902 - mae: 0.4222 - val_loss: 0.4089 - val_mse: 0.4089 - val_mae: 0.4950\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2592 - mse: 0.2592 - mae: 0.3989 - val_loss: 0.4493 - val_mse: 0.4493 - val_mae: 0.5118\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2476 - mse: 0.2476 - mae: 0.3822 - val_loss: 0.4036 - val_mse: 0.4036 - val_mae: 0.4940\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2429 - mse: 0.2429 - mae: 0.3794 - val_loss: 0.4589 - val_mse: 0.4589 - val_mae: 0.5052\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2593 - mse: 0.2593 - mae: 0.4005 - val_loss: 0.4885 - val_mse: 0.4885 - val_mae: 0.5256\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2729 - mse: 0.2729 - mae: 0.4044 - val_loss: 0.4472 - val_mse: 0.4472 - val_mae: 0.5116\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2743 - mse: 0.2743 - mae: 0.4026 - val_loss: 0.4141 - val_mse: 0.4141 - val_mae: 0.4862\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2590 - mse: 0.2590 - mae: 0.3917 - val_loss: 0.4502 - val_mse: 0.4502 - val_mae: 0.5009\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2695 - mse: 0.2695 - mae: 0.4044 - val_loss: 0.4087 - val_mse: 0.4087 - val_mae: 0.4868\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2552 - mse: 0.2552 - mae: 0.3908 - val_loss: 0.4247 - val_mse: 0.4247 - val_mae: 0.4890\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2772 - mse: 0.2772 - mae: 0.4098 - val_loss: 0.3853 - val_mse: 0.3853 - val_mae: 0.4661\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2420 - mse: 0.2420 - mae: 0.3789 - val_loss: 0.4604 - val_mse: 0.4604 - val_mae: 0.5138\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2612 - mse: 0.2612 - mae: 0.3928 - val_loss: 0.4394 - val_mse: 0.4394 - val_mae: 0.5040\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2425 - mse: 0.2425 - mae: 0.3842 - val_loss: 0.4056 - val_mse: 0.4056 - val_mae: 0.4770\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2655 - mse: 0.2655 - mae: 0.4054 - val_loss: 0.3859 - val_mse: 0.3859 - val_mae: 0.4760\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2473 - mse: 0.2473 - mae: 0.3931 - val_loss: 0.3796 - val_mse: 0.3796 - val_mae: 0.4717\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2349 - mse: 0.2349 - mae: 0.3747 - val_loss: 0.3991 - val_mse: 0.3991 - val_mae: 0.4750\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2539 - mse: 0.2539 - mae: 0.3884 - val_loss: 0.3872 - val_mse: 0.3872 - val_mae: 0.4793\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2829 - mse: 0.2829 - mae: 0.4148 - val_loss: 0.4216 - val_mse: 0.4216 - val_mae: 0.4973\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2508 - mse: 0.2508 - mae: 0.3843 - val_loss: 0.3909 - val_mse: 0.3909 - val_mae: 0.4808\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2742 - mse: 0.2742 - mae: 0.4053 - val_loss: 0.4201 - val_mse: 0.4201 - val_mae: 0.4837\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2425 - mse: 0.2425 - mae: 0.3776 - val_loss: 0.3683 - val_mse: 0.3683 - val_mae: 0.4627\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2672 - mse: 0.2672 - mae: 0.3984 - val_loss: 0.4084 - val_mse: 0.4084 - val_mae: 0.4734\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2510 - mse: 0.2510 - mae: 0.3872 - val_loss: 0.3891 - val_mse: 0.3891 - val_mae: 0.4788\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2615 - mse: 0.2615 - mae: 0.3997 - val_loss: 0.4864 - val_mse: 0.4864 - val_mae: 0.5220\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2444 - mse: 0.2444 - mae: 0.3860 - val_loss: 0.3760 - val_mse: 0.3760 - val_mae: 0.4657\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2481 - mse: 0.2481 - mae: 0.3869 - val_loss: 0.4214 - val_mse: 0.4214 - val_mae: 0.4809\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2717 - mse: 0.2717 - mae: 0.4019 - val_loss: 0.3949 - val_mse: 0.3949 - val_mae: 0.4775\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2537 - mse: 0.2537 - mae: 0.3936 - val_loss: 0.4628 - val_mse: 0.4628 - val_mae: 0.4897\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2471 - mse: 0.2471 - mae: 0.3866 - val_loss: 0.3744 - val_mse: 0.3744 - val_mae: 0.4715\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2783 - mse: 0.2783 - mae: 0.4091 - val_loss: 0.4560 - val_mse: 0.4560 - val_mae: 0.5085\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2927 - mse: 0.2927 - mae: 0.4246 - val_loss: 0.3806 - val_mse: 0.3806 - val_mae: 0.4609\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2527 - mse: 0.2527 - mae: 0.3878 - val_loss: 0.4137 - val_mse: 0.4137 - val_mae: 0.4699\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2615 - mse: 0.2615 - mae: 0.3909 - val_loss: 0.4476 - val_mse: 0.4476 - val_mae: 0.4878\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2253 - mse: 0.2253 - mae: 0.3677 - val_loss: 0.3868 - val_mse: 0.3868 - val_mae: 0.4666\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2659 - mse: 0.2659 - mae: 0.3945 - val_loss: 0.4458 - val_mse: 0.4458 - val_mae: 0.5010\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2307 - mse: 0.2307 - mae: 0.3748 - val_loss: 0.4050 - val_mse: 0.4050 - val_mae: 0.4756\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2695 - mse: 0.2695 - mae: 0.4073 - val_loss: 0.4180 - val_mse: 0.4180 - val_mae: 0.4751\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2491 - mse: 0.2491 - mae: 0.3862 - val_loss: 0.4191 - val_mse: 0.4191 - val_mae: 0.4885\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2373 - mse: 0.2373 - mae: 0.3834 - val_loss: 0.3985 - val_mse: 0.3985 - val_mae: 0.4663\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2302 - mse: 0.2302 - mae: 0.3735 - val_loss: 0.3964 - val_mse: 0.3964 - val_mae: 0.4703\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2415 - mse: 0.2415 - mae: 0.3759 - val_loss: 0.4044 - val_mse: 0.4044 - val_mae: 0.4931\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2504 - mse: 0.2504 - mae: 0.3849 - val_loss: 0.4106 - val_mse: 0.4106 - val_mae: 0.4710\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2368 - mse: 0.2368 - mae: 0.3745 - val_loss: 0.3803 - val_mse: 0.3803 - val_mae: 0.4692\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2421 - mse: 0.2421 - mae: 0.3802 - val_loss: 0.4348 - val_mse: 0.4348 - val_mae: 0.4842\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2340 - mse: 0.2340 - mae: 0.3745 - val_loss: 0.4982 - val_mse: 0.4982 - val_mae: 0.5327\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2514 - mse: 0.2514 - mae: 0.3841 - val_loss: 0.4111 - val_mse: 0.4111 - val_mae: 0.4847\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2314 - mse: 0.2314 - mae: 0.3700 - val_loss: 0.3878 - val_mse: 0.3878 - val_mae: 0.4641\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2368 - mse: 0.2368 - mae: 0.3716 - val_loss: 0.4387 - val_mse: 0.4387 - val_mae: 0.4968\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2529 - mse: 0.2529 - mae: 0.3881 - val_loss: 0.3960 - val_mse: 0.3960 - val_mae: 0.4850\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2244 - mse: 0.2244 - mae: 0.3693 - val_loss: 0.4539 - val_mse: 0.4539 - val_mae: 0.5009\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2391 - mse: 0.2391 - mae: 0.3813 - val_loss: 0.4050 - val_mse: 0.4050 - val_mae: 0.4816\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2385 - mse: 0.2385 - mae: 0.3740 - val_loss: 0.4068 - val_mse: 0.4068 - val_mae: 0.4785\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2380 - mse: 0.2380 - mae: 0.3733 - val_loss: 0.4738 - val_mse: 0.4738 - val_mae: 0.5107\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2493 - mse: 0.2493 - mae: 0.3855 - val_loss: 0.3931 - val_mse: 0.3931 - val_mae: 0.4738\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2473 - mse: 0.2473 - mae: 0.3879 - val_loss: 0.4545 - val_mse: 0.4545 - val_mae: 0.5054\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2583 - mse: 0.2583 - mae: 0.3968 - val_loss: 0.4005 - val_mse: 0.4005 - val_mae: 0.4816\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2186 - mse: 0.2186 - mae: 0.3531 - val_loss: 0.3942 - val_mse: 0.3942 - val_mae: 0.4700\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2250 - mse: 0.2250 - mae: 0.3705 - val_loss: 0.4158 - val_mse: 0.4158 - val_mae: 0.4762\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2077 - mse: 0.2077 - mae: 0.3538 - val_loss: 0.4019 - val_mse: 0.4019 - val_mae: 0.4726\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2365 - mse: 0.2365 - mae: 0.3696 - val_loss: 0.4002 - val_mse: 0.4002 - val_mae: 0.4800\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2650 - mse: 0.2650 - mae: 0.3993 - val_loss: 0.4105 - val_mse: 0.4105 - val_mae: 0.4805\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2344 - mse: 0.2344 - mae: 0.3737 - val_loss: 0.4172 - val_mse: 0.4172 - val_mae: 0.4722\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2176 - mse: 0.2176 - mae: 0.3613 - val_loss: 0.3732 - val_mse: 0.3732 - val_mae: 0.4621\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2264 - mse: 0.2264 - mae: 0.3660 - val_loss: 0.3705 - val_mse: 0.3705 - val_mae: 0.4494\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2245 - mse: 0.2245 - mae: 0.3602 - val_loss: 0.3804 - val_mse: 0.3804 - val_mae: 0.4545\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2206 - mse: 0.2206 - mae: 0.3661 - val_loss: 0.3877 - val_mse: 0.3877 - val_mae: 0.4614\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2389 - mse: 0.2389 - mae: 0.3715 - val_loss: 0.3817 - val_mse: 0.3817 - val_mae: 0.4624\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2543 - mse: 0.2543 - mae: 0.3886 - val_loss: 0.4041 - val_mse: 0.4041 - val_mae: 0.4728\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2204 - mse: 0.2204 - mae: 0.3616 - val_loss: 0.3954 - val_mse: 0.3954 - val_mae: 0.4697\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2545 - mse: 0.2545 - mae: 0.3954 - val_loss: 0.3801 - val_mse: 0.3801 - val_mae: 0.4558\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2240 - mse: 0.2240 - mae: 0.3602 - val_loss: 0.3738 - val_mse: 0.3738 - val_mae: 0.4511\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2095 - mse: 0.2095 - mae: 0.3527 - val_loss: 0.3967 - val_mse: 0.3967 - val_mae: 0.4707\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2339 - mse: 0.2339 - mae: 0.3721 - val_loss: 0.3870 - val_mse: 0.3870 - val_mae: 0.4577\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2170 - mse: 0.2170 - mae: 0.3600 - val_loss: 0.4184 - val_mse: 0.4184 - val_mae: 0.4739\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2074 - mse: 0.2074 - mae: 0.3484 - val_loss: 0.3719 - val_mse: 0.3719 - val_mae: 0.4461\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2189 - mse: 0.2189 - mae: 0.3528 - val_loss: 0.3740 - val_mse: 0.3740 - val_mae: 0.4585\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2232 - mse: 0.2232 - mae: 0.3592 - val_loss: 0.4056 - val_mse: 0.4056 - val_mae: 0.4936\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2403 - mse: 0.2403 - mae: 0.3775 - val_loss: 0.3807 - val_mse: 0.3807 - val_mae: 0.4583\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2384 - mse: 0.2384 - mae: 0.3760 - val_loss: 0.4965 - val_mse: 0.4965 - val_mae: 0.5202\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2186 - mse: 0.2186 - mae: 0.3651 - val_loss: 0.3638 - val_mse: 0.3638 - val_mae: 0.4558\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2258 - mse: 0.2258 - mae: 0.3639 - val_loss: 0.4157 - val_mse: 0.4157 - val_mae: 0.4726\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2060 - mse: 0.2060 - mae: 0.3568 - val_loss: 0.4167 - val_mse: 0.4167 - val_mae: 0.4862\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2133 - mse: 0.2133 - mae: 0.3601 - val_loss: 0.4053 - val_mse: 0.4053 - val_mae: 0.4739\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2254 - mse: 0.2254 - mae: 0.3616 - val_loss: 0.4493 - val_mse: 0.4493 - val_mae: 0.4950\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2145 - mse: 0.2145 - mae: 0.3660 - val_loss: 0.4026 - val_mse: 0.4026 - val_mae: 0.4903\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2235 - mse: 0.2235 - mae: 0.3637 - val_loss: 0.4167 - val_mse: 0.4167 - val_mae: 0.4831\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2145 - mse: 0.2145 - mae: 0.3592 - val_loss: 0.4088 - val_mse: 0.4088 - val_mae: 0.4869\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2261 - mse: 0.2261 - mae: 0.3736 - val_loss: 0.3998 - val_mse: 0.3998 - val_mae: 0.4769\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2186 - mse: 0.2186 - mae: 0.3677 - val_loss: 0.3749 - val_mse: 0.3749 - val_mae: 0.4524\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2360 - mse: 0.2360 - mae: 0.3798 - val_loss: 0.4341 - val_mse: 0.4341 - val_mae: 0.4892\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2412 - mse: 0.2412 - mae: 0.3674 - val_loss: 0.5241 - val_mse: 0.5241 - val_mae: 0.5483\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2527 - mse: 0.2527 - mae: 0.3830 - val_loss: 0.4194 - val_mse: 0.4194 - val_mae: 0.4805\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2255 - mse: 0.2255 - mae: 0.3669 - val_loss: 0.4226 - val_mse: 0.4226 - val_mae: 0.4825\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2048 - mse: 0.2048 - mae: 0.3488 - val_loss: 0.4075 - val_mse: 0.4075 - val_mae: 0.4758\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2115 - mse: 0.2115 - mae: 0.3588 - val_loss: 0.3923 - val_mse: 0.3923 - val_mae: 0.4672\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2115 - mse: 0.2115 - mae: 0.3552 - val_loss: 0.3813 - val_mse: 0.3813 - val_mae: 0.4636\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1860 - mse: 0.1860 - mae: 0.3365 - val_loss: 0.4553 - val_mse: 0.4553 - val_mae: 0.4988\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2048 - mse: 0.2048 - mae: 0.3490 - val_loss: 0.4180 - val_mse: 0.4180 - val_mae: 0.4732\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2010 - mse: 0.2010 - mae: 0.3448 - val_loss: 0.3863 - val_mse: 0.3863 - val_mae: 0.4646\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2008 - mse: 0.2008 - mae: 0.3499 - val_loss: 0.3962 - val_mse: 0.3962 - val_mae: 0.4708\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2092 - mse: 0.2092 - mae: 0.3508 - val_loss: 0.4023 - val_mse: 0.4023 - val_mae: 0.4752\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1974 - mse: 0.1974 - mae: 0.3412 - val_loss: 0.3971 - val_mse: 0.3971 - val_mae: 0.4741\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2068 - mse: 0.2068 - mae: 0.3523 - val_loss: 0.3973 - val_mse: 0.3973 - val_mae: 0.4552\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2059 - mse: 0.2059 - mae: 0.3469 - val_loss: 0.4002 - val_mse: 0.4002 - val_mae: 0.4750\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2301 - mse: 0.2301 - mae: 0.3684 - val_loss: 0.4030 - val_mse: 0.4030 - val_mae: 0.4628\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.1871 - mse: 0.1871 - mae: 0.3414 - val_loss: 0.4241 - val_mse: 0.4241 - val_mae: 0.4876\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2048 - mse: 0.2048 - mae: 0.3535 - val_loss: 0.4144 - val_mse: 0.4144 - val_mae: 0.4673\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1956 - mse: 0.1956 - mae: 0.3399 - val_loss: 0.4066 - val_mse: 0.4066 - val_mae: 0.4779\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2138 - mse: 0.2138 - mae: 0.3609 - val_loss: 0.3855 - val_mse: 0.3855 - val_mae: 0.4657\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2132 - mse: 0.2132 - mae: 0.3566 - val_loss: 0.3937 - val_mse: 0.3937 - val_mae: 0.4674\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2397 - mse: 0.2397 - mae: 0.3859 - val_loss: 0.3948 - val_mse: 0.3948 - val_mae: 0.4687\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2726 - mse: 0.2726 - mae: 0.4061 - val_loss: 0.5329 - val_mse: 0.5329 - val_mae: 0.5446\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2279 - mse: 0.2279 - mae: 0.3727 - val_loss: 0.4025 - val_mse: 0.4025 - val_mae: 0.4782\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2084 - mse: 0.2084 - mae: 0.3510 - val_loss: 0.4170 - val_mse: 0.4170 - val_mae: 0.4723\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.1890 - mse: 0.1890 - mae: 0.3307 - val_loss: 0.3996 - val_mse: 0.3996 - val_mae: 0.4657\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.1831 - mse: 0.1831 - mae: 0.3262 - val_loss: 0.4172 - val_mse: 0.4172 - val_mae: 0.4784\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.1849 - mse: 0.1849 - mae: 0.3315 - val_loss: 0.4006 - val_mse: 0.4006 - val_mae: 0.4778\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2139 - mse: 0.2139 - mae: 0.3528 - val_loss: 0.4328 - val_mse: 0.4328 - val_mae: 0.4925\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2119 - mse: 0.2119 - mae: 0.3643 - val_loss: 0.3909 - val_mse: 0.3909 - val_mae: 0.4672\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1942 - mse: 0.1942 - mae: 0.3410 - val_loss: 0.4080 - val_mse: 0.4080 - val_mae: 0.4786\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1826 - mse: 0.1826 - mae: 0.3305 - val_loss: 0.3767 - val_mse: 0.3767 - val_mae: 0.4560\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1943 - mse: 0.1943 - mae: 0.3430 - val_loss: 0.3916 - val_mse: 0.3916 - val_mae: 0.4677\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1954 - mse: 0.1954 - mae: 0.3480 - val_loss: 0.3944 - val_mse: 0.3944 - val_mae: 0.4632\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2016 - mse: 0.2016 - mae: 0.3396 - val_loss: 0.3732 - val_mse: 0.3732 - val_mae: 0.4476\n",
            "Epoch 252/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1948 - mse: 0.1948 - mae: 0.3369 - val_loss: 0.4063 - val_mse: 0.4063 - val_mae: 0.4815\n",
            "Epoch 253/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1942 - mse: 0.1942 - mae: 0.3429 - val_loss: 0.3878 - val_mse: 0.3878 - val_mae: 0.4656\n",
            "Epoch 254/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1916 - mse: 0.1916 - mae: 0.3439 - val_loss: 0.4434 - val_mse: 0.4434 - val_mae: 0.4992\n",
            "Epoch 255/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2090 - mse: 0.2090 - mae: 0.3493 - val_loss: 0.3734 - val_mse: 0.3734 - val_mae: 0.4527\n",
            "Epoch 256/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1929 - mse: 0.1929 - mae: 0.3401 - val_loss: 0.4162 - val_mse: 0.4162 - val_mae: 0.4884\n",
            "Epoch 257/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1973 - mse: 0.1973 - mae: 0.3471 - val_loss: 0.3840 - val_mse: 0.3840 - val_mae: 0.4566\n",
            "Epoch 258/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1936 - mse: 0.1936 - mae: 0.3437 - val_loss: 0.6560 - val_mse: 0.6560 - val_mae: 0.6158\n",
            "Epoch 259/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2497 - mse: 0.2497 - mae: 0.3906 - val_loss: 0.3802 - val_mse: 0.3802 - val_mae: 0.4648\n",
            "Epoch 260/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2086 - mse: 0.2086 - mae: 0.3502 - val_loss: 0.4042 - val_mse: 0.4042 - val_mae: 0.4688\n",
            "Epoch 261/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2126 - mse: 0.2126 - mae: 0.3634 - val_loss: 0.4065 - val_mse: 0.4065 - val_mae: 0.4846\n",
            "Epoch 262/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2051 - mse: 0.2051 - mae: 0.3480 - val_loss: 0.3689 - val_mse: 0.3689 - val_mae: 0.4428\n",
            "Epoch 263/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1982 - mse: 0.1982 - mae: 0.3377 - val_loss: 0.3734 - val_mse: 0.3734 - val_mae: 0.4607\n",
            "Epoch 264/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2201 - mse: 0.2201 - mae: 0.3487 - val_loss: 0.4216 - val_mse: 0.4216 - val_mae: 0.4863\n",
            "Epoch 265/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2139 - mse: 0.2139 - mae: 0.3502 - val_loss: 0.4079 - val_mse: 0.4079 - val_mae: 0.4754\n",
            "Epoch 266/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1890 - mse: 0.1890 - mae: 0.3383 - val_loss: 0.3822 - val_mse: 0.3822 - val_mae: 0.4691\n",
            "Epoch 267/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1962 - mse: 0.1962 - mae: 0.3447 - val_loss: 0.4391 - val_mse: 0.4391 - val_mae: 0.4885\n",
            "Epoch 268/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1810 - mse: 0.1810 - mae: 0.3286 - val_loss: 0.3975 - val_mse: 0.3975 - val_mae: 0.4810\n",
            "Epoch 269/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1971 - mse: 0.1971 - mae: 0.3431 - val_loss: 0.3863 - val_mse: 0.3863 - val_mae: 0.4486\n",
            "Epoch 270/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1950 - mse: 0.1950 - mae: 0.3422 - val_loss: 0.4363 - val_mse: 0.4363 - val_mae: 0.4809\n",
            "Epoch 271/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1838 - mse: 0.1838 - mae: 0.3308 - val_loss: 0.4459 - val_mse: 0.4459 - val_mae: 0.4957\n",
            "Epoch 272/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1694 - mse: 0.1694 - mae: 0.3130 - val_loss: 0.3938 - val_mse: 0.3938 - val_mae: 0.4643\n",
            "Epoch 273/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1811 - mse: 0.1811 - mae: 0.3230 - val_loss: 0.4187 - val_mse: 0.4187 - val_mae: 0.4735\n",
            "Epoch 274/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1995 - mse: 0.1995 - mae: 0.3469 - val_loss: 0.3888 - val_mse: 0.3888 - val_mae: 0.4596\n",
            "Epoch 275/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1827 - mse: 0.1827 - mae: 0.3320 - val_loss: 0.3825 - val_mse: 0.3825 - val_mae: 0.4518\n",
            "Epoch 276/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1802 - mse: 0.1802 - mae: 0.3201 - val_loss: 0.4035 - val_mse: 0.4035 - val_mae: 0.4778\n",
            "Epoch 277/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2032 - mse: 0.2032 - mae: 0.3470 - val_loss: 0.4208 - val_mse: 0.4208 - val_mae: 0.4736\n",
            "Epoch 278/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2051 - mse: 0.2051 - mae: 0.3473 - val_loss: 0.3925 - val_mse: 0.3925 - val_mae: 0.4651\n",
            "Epoch 279/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1924 - mse: 0.1924 - mae: 0.3462 - val_loss: 0.4170 - val_mse: 0.4170 - val_mae: 0.4713\n",
            "Epoch 280/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1758 - mse: 0.1758 - mae: 0.3153 - val_loss: 0.3919 - val_mse: 0.3919 - val_mae: 0.4570\n",
            "Epoch 281/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1831 - mse: 0.1831 - mae: 0.3308 - val_loss: 0.4074 - val_mse: 0.4074 - val_mae: 0.4774\n",
            "Epoch 282/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1827 - mse: 0.1827 - mae: 0.3329 - val_loss: 0.4495 - val_mse: 0.4495 - val_mae: 0.5028\n",
            "Epoch 283/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2300 - mse: 0.2300 - mae: 0.3756 - val_loss: 0.4188 - val_mse: 0.4188 - val_mae: 0.4863\n",
            "Epoch 284/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1793 - mse: 0.1793 - mae: 0.3277 - val_loss: 0.4240 - val_mse: 0.4240 - val_mae: 0.4827\n",
            "Epoch 285/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1921 - mse: 0.1921 - mae: 0.3371 - val_loss: 0.4316 - val_mse: 0.4316 - val_mae: 0.4917\n",
            "Epoch 286/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1852 - mse: 0.1852 - mae: 0.3270 - val_loss: 0.3992 - val_mse: 0.3992 - val_mae: 0.4639\n",
            "Epoch 287/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1755 - mse: 0.1755 - mae: 0.3203 - val_loss: 0.4183 - val_mse: 0.4183 - val_mae: 0.4844\n",
            "Epoch 288/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1668 - mse: 0.1668 - mae: 0.3160 - val_loss: 0.3927 - val_mse: 0.3927 - val_mae: 0.4667\n",
            "Epoch 289/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1772 - mse: 0.1772 - mae: 0.3282 - val_loss: 0.3907 - val_mse: 0.3907 - val_mae: 0.4634\n",
            "Epoch 290/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2101 - mse: 0.2101 - mae: 0.3616 - val_loss: 0.4014 - val_mse: 0.4014 - val_mae: 0.4776\n",
            "Epoch 291/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1638 - mse: 0.1638 - mae: 0.3134 - val_loss: 0.4057 - val_mse: 0.4057 - val_mae: 0.4630\n",
            "Epoch 292/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1867 - mse: 0.1867 - mae: 0.3372 - val_loss: 0.4214 - val_mse: 0.4214 - val_mae: 0.4840\n",
            "Epoch 293/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1804 - mse: 0.1804 - mae: 0.3258 - val_loss: 0.4530 - val_mse: 0.4530 - val_mae: 0.4960\n",
            "Epoch 294/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1675 - mse: 0.1675 - mae: 0.3123 - val_loss: 0.4371 - val_mse: 0.4371 - val_mae: 0.4927\n",
            "Epoch 295/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1894 - mse: 0.1894 - mae: 0.3418 - val_loss: 0.4039 - val_mse: 0.4039 - val_mae: 0.4657\n",
            "Epoch 296/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1826 - mse: 0.1826 - mae: 0.3321 - val_loss: 0.4110 - val_mse: 0.4110 - val_mae: 0.4754\n",
            "Epoch 297/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1780 - mse: 0.1780 - mae: 0.3310 - val_loss: 0.4380 - val_mse: 0.4380 - val_mae: 0.4863\n",
            "Epoch 298/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1808 - mse: 0.1808 - mae: 0.3279 - val_loss: 0.4246 - val_mse: 0.4246 - val_mae: 0.4868\n",
            "Epoch 299/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1853 - mse: 0.1853 - mae: 0.3354 - val_loss: 0.4394 - val_mse: 0.4394 - val_mae: 0.4949\n",
            "Epoch 300/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1772 - mse: 0.1772 - mae: 0.3216 - val_loss: 0.4439 - val_mse: 0.4439 - val_mae: 0.4868\n",
            "Epoch 301/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.1671 - mse: 0.1671 - mae: 0.3143 - val_loss: 0.4372 - val_mse: 0.4372 - val_mae: 0.4952\n",
            "Epoch 302/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1833 - mse: 0.1833 - mae: 0.3382 - val_loss: 0.4958 - val_mse: 0.4958 - val_mae: 0.5269\n",
            "Epoch 303/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1933 - mse: 0.1933 - mae: 0.3355 - val_loss: 0.4362 - val_mse: 0.4362 - val_mae: 0.4971\n",
            "Epoch 304/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2119 - mse: 0.2119 - mae: 0.3630 - val_loss: 0.4232 - val_mse: 0.4232 - val_mae: 0.4780\n",
            "Epoch 305/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1696 - mse: 0.1696 - mae: 0.3196 - val_loss: 0.4197 - val_mse: 0.4197 - val_mae: 0.4709\n",
            "Epoch 306/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1803 - mse: 0.1803 - mae: 0.3242 - val_loss: 0.4166 - val_mse: 0.4166 - val_mae: 0.4704\n",
            "Epoch 307/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1784 - mse: 0.1784 - mae: 0.3266 - val_loss: 0.4594 - val_mse: 0.4594 - val_mae: 0.4988\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.6302 - mse: 0.6302 - mae: 0.5772\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 16ms/step - loss: 33.9104 - mse: 33.9104 - mae: 5.4623 - val_loss: 10.0378 - val_mse: 10.0378 - val_mae: 2.9665\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5.5923 - mse: 5.5923 - mae: 1.9960 - val_loss: 5.0884 - val_mse: 5.0884 - val_mae: 1.7612\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 3.9334 - mse: 3.9334 - mae: 1.5788 - val_loss: 3.4287 - val_mse: 3.4287 - val_mae: 1.5168\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.8118 - mse: 2.8118 - mae: 1.3504 - val_loss: 2.6167 - val_mse: 2.6167 - val_mae: 1.2636\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 2.3964 - mse: 2.3964 - mae: 1.2298 - val_loss: 2.2775 - val_mse: 2.2775 - val_mae: 1.1639\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.2151 - mse: 2.2151 - mae: 1.1683 - val_loss: 2.0145 - val_mse: 2.0145 - val_mae: 1.0982\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.9676 - mse: 1.9676 - mae: 1.1247 - val_loss: 1.9070 - val_mse: 1.9070 - val_mae: 1.0475\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.9070 - mse: 1.9070 - mae: 1.0933 - val_loss: 1.8490 - val_mse: 1.8490 - val_mae: 1.0266\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.7982 - mse: 1.7982 - mae: 1.0556 - val_loss: 1.7042 - val_mse: 1.7042 - val_mae: 0.9913\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.6198 - mse: 1.6198 - mae: 1.0102 - val_loss: 1.7026 - val_mse: 1.7026 - val_mae: 0.9694\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.6339 - mse: 1.6339 - mae: 1.0028 - val_loss: 1.5754 - val_mse: 1.5754 - val_mae: 0.9377\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.5378 - mse: 1.5378 - mae: 0.9744 - val_loss: 1.5478 - val_mse: 1.5478 - val_mae: 0.9279\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.5017 - mse: 1.5017 - mae: 0.9686 - val_loss: 1.4808 - val_mse: 1.4808 - val_mae: 0.8962\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.4168 - mse: 1.4168 - mae: 0.9434 - val_loss: 1.4838 - val_mse: 1.4838 - val_mae: 0.8917\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.3544 - mse: 1.3544 - mae: 0.9126 - val_loss: 1.4848 - val_mse: 1.4848 - val_mae: 0.8884\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.2052 - mse: 1.2052 - mae: 0.8609 - val_loss: 1.4319 - val_mse: 1.4319 - val_mae: 0.8642\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.1487 - mse: 1.1487 - mae: 0.8553 - val_loss: 1.2689 - val_mse: 1.2689 - val_mae: 0.8306\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1207 - mse: 1.1207 - mae: 0.8421 - val_loss: 1.2591 - val_mse: 1.2591 - val_mae: 0.8260\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.1344 - mse: 1.1344 - mae: 0.8415 - val_loss: 1.2585 - val_mse: 1.2585 - val_mae: 0.8181\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0569 - mse: 1.0569 - mae: 0.8162 - val_loss: 1.1765 - val_mse: 1.1765 - val_mae: 0.7864\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9692 - mse: 0.9692 - mae: 0.7764 - val_loss: 1.1210 - val_mse: 1.1210 - val_mae: 0.7816\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9966 - mse: 0.9966 - mae: 0.7964 - val_loss: 1.1270 - val_mse: 1.1270 - val_mae: 0.7730\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.9151 - mse: 0.9151 - mae: 0.7535 - val_loss: 1.0674 - val_mse: 1.0674 - val_mae: 0.7539\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9259 - mse: 0.9259 - mae: 0.7640 - val_loss: 0.9537 - val_mse: 0.9537 - val_mae: 0.7149\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8686 - mse: 0.8686 - mae: 0.7345 - val_loss: 1.0080 - val_mse: 1.0080 - val_mae: 0.7281\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7881 - mse: 0.7881 - mae: 0.6979 - val_loss: 0.9405 - val_mse: 0.9405 - val_mae: 0.7089\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7888 - mse: 0.7888 - mae: 0.7069 - val_loss: 0.9304 - val_mse: 0.9304 - val_mae: 0.7010\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7475 - mse: 0.7475 - mae: 0.6875 - val_loss: 0.8929 - val_mse: 0.8929 - val_mae: 0.6991\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7662 - mse: 0.7662 - mae: 0.6963 - val_loss: 0.8940 - val_mse: 0.8940 - val_mae: 0.6806\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7137 - mse: 0.7137 - mae: 0.6638 - val_loss: 0.7800 - val_mse: 0.7800 - val_mae: 0.6566\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7344 - mse: 0.7344 - mae: 0.6781 - val_loss: 0.8467 - val_mse: 0.8467 - val_mae: 0.6796\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.6866 - mse: 0.6866 - mae: 0.6612 - val_loss: 0.8289 - val_mse: 0.8289 - val_mae: 0.6661\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6619 - mse: 0.6619 - mae: 0.6436 - val_loss: 0.7578 - val_mse: 0.7578 - val_mae: 0.6611\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.6428 - mse: 0.6428 - mae: 0.6343 - val_loss: 0.7992 - val_mse: 0.7992 - val_mae: 0.6698\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6489 - mse: 0.6489 - mae: 0.6359 - val_loss: 0.7078 - val_mse: 0.7078 - val_mae: 0.6317\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6114 - mse: 0.6114 - mae: 0.6250 - val_loss: 0.7501 - val_mse: 0.7501 - val_mae: 0.6370\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5485 - mse: 0.5485 - mae: 0.5836 - val_loss: 0.7492 - val_mse: 0.7492 - val_mae: 0.6352\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.5601 - mse: 0.5601 - mae: 0.5949 - val_loss: 0.7793 - val_mse: 0.7793 - val_mae: 0.6486\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6153 - mse: 0.6153 - mae: 0.6045 - val_loss: 0.6885 - val_mse: 0.6885 - val_mae: 0.6103\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5908 - mse: 0.5908 - mae: 0.6044 - val_loss: 0.7389 - val_mse: 0.7389 - val_mae: 0.6271\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5432 - mse: 0.5432 - mae: 0.5751 - val_loss: 0.6288 - val_mse: 0.6288 - val_mae: 0.6061\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.6080 - mse: 0.6080 - mae: 0.6060 - val_loss: 0.6386 - val_mse: 0.6386 - val_mae: 0.5962\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5772 - mse: 0.5772 - mae: 0.5984 - val_loss: 0.6335 - val_mse: 0.6335 - val_mae: 0.5934\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.5313 - mse: 0.5313 - mae: 0.5713 - val_loss: 0.6302 - val_mse: 0.6302 - val_mae: 0.5922\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4845 - mse: 0.4845 - mae: 0.5514 - val_loss: 0.6834 - val_mse: 0.6834 - val_mae: 0.6046\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4784 - mse: 0.4784 - mae: 0.5444 - val_loss: 0.6845 - val_mse: 0.6845 - val_mae: 0.5986\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4885 - mse: 0.4885 - mae: 0.5488 - val_loss: 0.6695 - val_mse: 0.6695 - val_mae: 0.6006\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5261 - mse: 0.5261 - mae: 0.5653 - val_loss: 0.6200 - val_mse: 0.6200 - val_mae: 0.5777\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4915 - mse: 0.4915 - mae: 0.5510 - val_loss: 0.6070 - val_mse: 0.6070 - val_mae: 0.5731\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4941 - mse: 0.4941 - mae: 0.5505 - val_loss: 0.5860 - val_mse: 0.5860 - val_mae: 0.5637\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4495 - mse: 0.4495 - mae: 0.5262 - val_loss: 0.6274 - val_mse: 0.6274 - val_mae: 0.5764\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4700 - mse: 0.4700 - mae: 0.5246 - val_loss: 0.5947 - val_mse: 0.5947 - val_mae: 0.5567\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4665 - mse: 0.4665 - mae: 0.5353 - val_loss: 0.5653 - val_mse: 0.5653 - val_mae: 0.5556\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4227 - mse: 0.4227 - mae: 0.5149 - val_loss: 0.5252 - val_mse: 0.5252 - val_mae: 0.5382\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4573 - mse: 0.4573 - mae: 0.5385 - val_loss: 0.5821 - val_mse: 0.5821 - val_mae: 0.5593\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4274 - mse: 0.4274 - mae: 0.5136 - val_loss: 0.5560 - val_mse: 0.5560 - val_mae: 0.5502\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4221 - mse: 0.4221 - mae: 0.5129 - val_loss: 0.5222 - val_mse: 0.5222 - val_mae: 0.5388\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4346 - mse: 0.4346 - mae: 0.5296 - val_loss: 0.5860 - val_mse: 0.5860 - val_mae: 0.5590\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4129 - mse: 0.4129 - mae: 0.5083 - val_loss: 0.5415 - val_mse: 0.5415 - val_mae: 0.5350\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4249 - mse: 0.4249 - mae: 0.5038 - val_loss: 0.5047 - val_mse: 0.5047 - val_mae: 0.5265\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3893 - mse: 0.3893 - mae: 0.4925 - val_loss: 0.5134 - val_mse: 0.5134 - val_mae: 0.5338\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4116 - mse: 0.4116 - mae: 0.5127 - val_loss: 0.5545 - val_mse: 0.5545 - val_mae: 0.5405\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3805 - mse: 0.3805 - mae: 0.4821 - val_loss: 0.5854 - val_mse: 0.5854 - val_mae: 0.5553\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4350 - mse: 0.4350 - mae: 0.5204 - val_loss: 0.5213 - val_mse: 0.5213 - val_mae: 0.5412\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3866 - mse: 0.3866 - mae: 0.4872 - val_loss: 0.5665 - val_mse: 0.5665 - val_mae: 0.5478\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3658 - mse: 0.3658 - mae: 0.4695 - val_loss: 0.5123 - val_mse: 0.5123 - val_mae: 0.5323\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3580 - mse: 0.3580 - mae: 0.4719 - val_loss: 0.4998 - val_mse: 0.4998 - val_mae: 0.5274\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3479 - mse: 0.3479 - mae: 0.4626 - val_loss: 0.5550 - val_mse: 0.5550 - val_mae: 0.5452\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4076 - mse: 0.4076 - mae: 0.5017 - val_loss: 0.5137 - val_mse: 0.5137 - val_mae: 0.5302\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3630 - mse: 0.3630 - mae: 0.4755 - val_loss: 0.4729 - val_mse: 0.4729 - val_mae: 0.5179\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3637 - mse: 0.3637 - mae: 0.4754 - val_loss: 0.5095 - val_mse: 0.5095 - val_mae: 0.5310\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3506 - mse: 0.3506 - mae: 0.4626 - val_loss: 0.4843 - val_mse: 0.4843 - val_mae: 0.5190\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3869 - mse: 0.3869 - mae: 0.4904 - val_loss: 0.5075 - val_mse: 0.5075 - val_mae: 0.5244\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3780 - mse: 0.3780 - mae: 0.4860 - val_loss: 0.6037 - val_mse: 0.6037 - val_mae: 0.5701\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3790 - mse: 0.3790 - mae: 0.4821 - val_loss: 0.5401 - val_mse: 0.5401 - val_mae: 0.5378\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3356 - mse: 0.3356 - mae: 0.4516 - val_loss: 0.5066 - val_mse: 0.5066 - val_mae: 0.5187\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3514 - mse: 0.3514 - mae: 0.4657 - val_loss: 0.4979 - val_mse: 0.4979 - val_mae: 0.5230\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3549 - mse: 0.3549 - mae: 0.4645 - val_loss: 0.4713 - val_mse: 0.4713 - val_mae: 0.5118\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3266 - mse: 0.3266 - mae: 0.4413 - val_loss: 0.4621 - val_mse: 0.4621 - val_mae: 0.5087\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3252 - mse: 0.3252 - mae: 0.4419 - val_loss: 0.5338 - val_mse: 0.5338 - val_mae: 0.5402\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3508 - mse: 0.3508 - mae: 0.4718 - val_loss: 0.5693 - val_mse: 0.5693 - val_mae: 0.5570\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3608 - mse: 0.3608 - mae: 0.4712 - val_loss: 0.5007 - val_mse: 0.5007 - val_mae: 0.5239\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3567 - mse: 0.3567 - mae: 0.4674 - val_loss: 0.4799 - val_mse: 0.4799 - val_mae: 0.5129\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3146 - mse: 0.3146 - mae: 0.4399 - val_loss: 0.4956 - val_mse: 0.4956 - val_mae: 0.5195\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3307 - mse: 0.3307 - mae: 0.4530 - val_loss: 0.5497 - val_mse: 0.5497 - val_mae: 0.5570\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3439 - mse: 0.3439 - mae: 0.4604 - val_loss: 0.4646 - val_mse: 0.4646 - val_mae: 0.5084\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3066 - mse: 0.3066 - mae: 0.4376 - val_loss: 0.4714 - val_mse: 0.4714 - val_mae: 0.5108\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3106 - mse: 0.3106 - mae: 0.4394 - val_loss: 0.5240 - val_mse: 0.5240 - val_mae: 0.5341\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3301 - mse: 0.3301 - mae: 0.4482 - val_loss: 0.4514 - val_mse: 0.4514 - val_mae: 0.4991\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3523 - mse: 0.3523 - mae: 0.4676 - val_loss: 0.4387 - val_mse: 0.4387 - val_mae: 0.4961\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3102 - mse: 0.3102 - mae: 0.4344 - val_loss: 0.4490 - val_mse: 0.4490 - val_mae: 0.5097\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3157 - mse: 0.3157 - mae: 0.4347 - val_loss: 0.4898 - val_mse: 0.4898 - val_mae: 0.5174\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3331 - mse: 0.3331 - mae: 0.4527 - val_loss: 0.4402 - val_mse: 0.4402 - val_mae: 0.4939\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3009 - mse: 0.3009 - mae: 0.4290 - val_loss: 0.4678 - val_mse: 0.4678 - val_mae: 0.5103\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2987 - mse: 0.2987 - mae: 0.4245 - val_loss: 0.4754 - val_mse: 0.4754 - val_mae: 0.5205\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3189 - mse: 0.3189 - mae: 0.4290 - val_loss: 0.4867 - val_mse: 0.4867 - val_mae: 0.5034\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3190 - mse: 0.3190 - mae: 0.4465 - val_loss: 0.5223 - val_mse: 0.5223 - val_mae: 0.5310\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3087 - mse: 0.3087 - mae: 0.4264 - val_loss: 0.4759 - val_mse: 0.4759 - val_mae: 0.5092\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3117 - mse: 0.3117 - mae: 0.4365 - val_loss: 0.5026 - val_mse: 0.5026 - val_mae: 0.5318\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2970 - mse: 0.2970 - mae: 0.4270 - val_loss: 0.4597 - val_mse: 0.4597 - val_mae: 0.5062\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3201 - mse: 0.3201 - mae: 0.4371 - val_loss: 0.4645 - val_mse: 0.4645 - val_mae: 0.5094\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3390 - mse: 0.3390 - mae: 0.4570 - val_loss: 0.4488 - val_mse: 0.4488 - val_mae: 0.4988\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3140 - mse: 0.3140 - mae: 0.4363 - val_loss: 0.4808 - val_mse: 0.4808 - val_mae: 0.5304\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3065 - mse: 0.3065 - mae: 0.4344 - val_loss: 0.4173 - val_mse: 0.4173 - val_mae: 0.4868\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3285 - mse: 0.3285 - mae: 0.4462 - val_loss: 0.4467 - val_mse: 0.4467 - val_mae: 0.5053\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2848 - mse: 0.2848 - mae: 0.4190 - val_loss: 0.4474 - val_mse: 0.4474 - val_mae: 0.5025\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3285 - mse: 0.3285 - mae: 0.4481 - val_loss: 0.4409 - val_mse: 0.4409 - val_mae: 0.5086\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2825 - mse: 0.2825 - mae: 0.4184 - val_loss: 0.4742 - val_mse: 0.4742 - val_mae: 0.5044\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2859 - mse: 0.2859 - mae: 0.4156 - val_loss: 0.4450 - val_mse: 0.4450 - val_mae: 0.5020\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2909 - mse: 0.2909 - mae: 0.4197 - val_loss: 0.4923 - val_mse: 0.4923 - val_mae: 0.5475\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3310 - mse: 0.3310 - mae: 0.4506 - val_loss: 0.4609 - val_mse: 0.4609 - val_mae: 0.5105\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2918 - mse: 0.2918 - mae: 0.4151 - val_loss: 0.4469 - val_mse: 0.4469 - val_mae: 0.5045\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3066 - mse: 0.3066 - mae: 0.4319 - val_loss: 0.4752 - val_mse: 0.4752 - val_mae: 0.5312\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3059 - mse: 0.3059 - mae: 0.4351 - val_loss: 0.4448 - val_mse: 0.4448 - val_mae: 0.5049\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2855 - mse: 0.2855 - mae: 0.4080 - val_loss: 0.4367 - val_mse: 0.4367 - val_mae: 0.5133\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3008 - mse: 0.3008 - mae: 0.4319 - val_loss: 0.4111 - val_mse: 0.4111 - val_mae: 0.4912\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3099 - mse: 0.3099 - mae: 0.4291 - val_loss: 0.4035 - val_mse: 0.4035 - val_mae: 0.4884\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2918 - mse: 0.2918 - mae: 0.4178 - val_loss: 0.4434 - val_mse: 0.4434 - val_mae: 0.5021\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2696 - mse: 0.2696 - mae: 0.4029 - val_loss: 0.4611 - val_mse: 0.4611 - val_mae: 0.5079\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3057 - mse: 0.3057 - mae: 0.4307 - val_loss: 0.4294 - val_mse: 0.4294 - val_mae: 0.4881\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2969 - mse: 0.2969 - mae: 0.4297 - val_loss: 0.4061 - val_mse: 0.4061 - val_mae: 0.4901\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2987 - mse: 0.2987 - mae: 0.4247 - val_loss: 0.4168 - val_mse: 0.4168 - val_mae: 0.4957\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3070 - mse: 0.3070 - mae: 0.4368 - val_loss: 0.4823 - val_mse: 0.4823 - val_mae: 0.5337\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3108 - mse: 0.3108 - mae: 0.4397 - val_loss: 0.4758 - val_mse: 0.4758 - val_mae: 0.5286\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2975 - mse: 0.2975 - mae: 0.4300 - val_loss: 0.4257 - val_mse: 0.4257 - val_mae: 0.4931\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2810 - mse: 0.2810 - mae: 0.4138 - val_loss: 0.4328 - val_mse: 0.4328 - val_mae: 0.5080\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2748 - mse: 0.2748 - mae: 0.4118 - val_loss: 0.5381 - val_mse: 0.5381 - val_mae: 0.5706\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3019 - mse: 0.3019 - mae: 0.4254 - val_loss: 0.4510 - val_mse: 0.4510 - val_mae: 0.5091\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2893 - mse: 0.2893 - mae: 0.4208 - val_loss: 0.4594 - val_mse: 0.4594 - val_mae: 0.5179\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3050 - mse: 0.3050 - mae: 0.4304 - val_loss: 0.4231 - val_mse: 0.4231 - val_mae: 0.4858\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3055 - mse: 0.3055 - mae: 0.4351 - val_loss: 0.4149 - val_mse: 0.4149 - val_mae: 0.4871\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2726 - mse: 0.2726 - mae: 0.4093 - val_loss: 0.4777 - val_mse: 0.4777 - val_mae: 0.5290\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2781 - mse: 0.2781 - mae: 0.4073 - val_loss: 0.4206 - val_mse: 0.4206 - val_mae: 0.4796\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2946 - mse: 0.2946 - mae: 0.4173 - val_loss: 0.4652 - val_mse: 0.4652 - val_mae: 0.5173\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3011 - mse: 0.3011 - mae: 0.4226 - val_loss: 0.4532 - val_mse: 0.4532 - val_mae: 0.5076\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3127 - mse: 0.3127 - mae: 0.4421 - val_loss: 0.4287 - val_mse: 0.4287 - val_mae: 0.5014\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2897 - mse: 0.2897 - mae: 0.4228 - val_loss: 0.4424 - val_mse: 0.4424 - val_mae: 0.5001\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2768 - mse: 0.2768 - mae: 0.4026 - val_loss: 0.4133 - val_mse: 0.4133 - val_mae: 0.5036\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3040 - mse: 0.3040 - mae: 0.4241 - val_loss: 0.4388 - val_mse: 0.4388 - val_mae: 0.4979\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2739 - mse: 0.2739 - mae: 0.4068 - val_loss: 0.4610 - val_mse: 0.4610 - val_mae: 0.5268\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2565 - mse: 0.2565 - mae: 0.3975 - val_loss: 0.4067 - val_mse: 0.4067 - val_mae: 0.4938\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2726 - mse: 0.2726 - mae: 0.4000 - val_loss: 0.4108 - val_mse: 0.4108 - val_mae: 0.4943\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2685 - mse: 0.2685 - mae: 0.4121 - val_loss: 0.4053 - val_mse: 0.4053 - val_mae: 0.4756\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2539 - mse: 0.2539 - mae: 0.3943 - val_loss: 0.4132 - val_mse: 0.4132 - val_mae: 0.4842\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2631 - mse: 0.2631 - mae: 0.3969 - val_loss: 0.4156 - val_mse: 0.4156 - val_mae: 0.4856\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2404 - mse: 0.2404 - mae: 0.3868 - val_loss: 0.4235 - val_mse: 0.4235 - val_mae: 0.4934\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2585 - mse: 0.2585 - mae: 0.3933 - val_loss: 0.4596 - val_mse: 0.4596 - val_mae: 0.5228\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2858 - mse: 0.2858 - mae: 0.4200 - val_loss: 0.4612 - val_mse: 0.4612 - val_mae: 0.5013\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2811 - mse: 0.2811 - mae: 0.4106 - val_loss: 0.4334 - val_mse: 0.4334 - val_mae: 0.5000\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2588 - mse: 0.2588 - mae: 0.3927 - val_loss: 0.4593 - val_mse: 0.4593 - val_mae: 0.5261\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2824 - mse: 0.2824 - mae: 0.4214 - val_loss: 0.4187 - val_mse: 0.4187 - val_mae: 0.5123\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2793 - mse: 0.2793 - mae: 0.4105 - val_loss: 0.4339 - val_mse: 0.4339 - val_mae: 0.5180\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2877 - mse: 0.2877 - mae: 0.4205 - val_loss: 0.4035 - val_mse: 0.4035 - val_mae: 0.4816\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2551 - mse: 0.2551 - mae: 0.3968 - val_loss: 0.4115 - val_mse: 0.4115 - val_mae: 0.4964\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2579 - mse: 0.2579 - mae: 0.3984 - val_loss: 0.4077 - val_mse: 0.4077 - val_mae: 0.4927\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2575 - mse: 0.2575 - mae: 0.3986 - val_loss: 0.4228 - val_mse: 0.4228 - val_mae: 0.4956\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2457 - mse: 0.2457 - mae: 0.3934 - val_loss: 0.4031 - val_mse: 0.4031 - val_mae: 0.4916\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2841 - mse: 0.2841 - mae: 0.4181 - val_loss: 0.3982 - val_mse: 0.3982 - val_mae: 0.4897\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2953 - mse: 0.2953 - mae: 0.4149 - val_loss: 0.3827 - val_mse: 0.3827 - val_mae: 0.4694\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2793 - mse: 0.2793 - mae: 0.4231 - val_loss: 0.5021 - val_mse: 0.5021 - val_mae: 0.5535\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2570 - mse: 0.2570 - mae: 0.4013 - val_loss: 0.4066 - val_mse: 0.4066 - val_mae: 0.4837\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2766 - mse: 0.2766 - mae: 0.4093 - val_loss: 0.3965 - val_mse: 0.3965 - val_mae: 0.4868\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2552 - mse: 0.2552 - mae: 0.3894 - val_loss: 0.4010 - val_mse: 0.4010 - val_mae: 0.4784\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2460 - mse: 0.2460 - mae: 0.3798 - val_loss: 0.3974 - val_mse: 0.3974 - val_mae: 0.4697\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2535 - mse: 0.2535 - mae: 0.3925 - val_loss: 0.3886 - val_mse: 0.3886 - val_mae: 0.4704\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2557 - mse: 0.2557 - mae: 0.3931 - val_loss: 0.3939 - val_mse: 0.3939 - val_mae: 0.4891\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2655 - mse: 0.2655 - mae: 0.4062 - val_loss: 0.3912 - val_mse: 0.3912 - val_mae: 0.4825\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2630 - mse: 0.2630 - mae: 0.4042 - val_loss: 0.4353 - val_mse: 0.4353 - val_mae: 0.4871\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2533 - mse: 0.2533 - mae: 0.3989 - val_loss: 0.4386 - val_mse: 0.4386 - val_mae: 0.4910\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2388 - mse: 0.2388 - mae: 0.3805 - val_loss: 0.4220 - val_mse: 0.4220 - val_mae: 0.4966\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2246 - mse: 0.2246 - mae: 0.3649 - val_loss: 0.4187 - val_mse: 0.4187 - val_mae: 0.4951\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2513 - mse: 0.2513 - mae: 0.3743 - val_loss: 0.3947 - val_mse: 0.3947 - val_mae: 0.4782\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2435 - mse: 0.2435 - mae: 0.3834 - val_loss: 0.4040 - val_mse: 0.4040 - val_mae: 0.4811\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2572 - mse: 0.2572 - mae: 0.3894 - val_loss: 0.4208 - val_mse: 0.4208 - val_mae: 0.5082\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2426 - mse: 0.2426 - mae: 0.3836 - val_loss: 0.4036 - val_mse: 0.4036 - val_mae: 0.4927\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2652 - mse: 0.2652 - mae: 0.3988 - val_loss: 0.4270 - val_mse: 0.4270 - val_mae: 0.5088\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2417 - mse: 0.2417 - mae: 0.3808 - val_loss: 0.4533 - val_mse: 0.4533 - val_mae: 0.5206\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2610 - mse: 0.2610 - mae: 0.3934 - val_loss: 0.4282 - val_mse: 0.4282 - val_mae: 0.4794\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2523 - mse: 0.2523 - mae: 0.3913 - val_loss: 0.4053 - val_mse: 0.4053 - val_mae: 0.4791\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2570 - mse: 0.2570 - mae: 0.3927 - val_loss: 0.4041 - val_mse: 0.4041 - val_mae: 0.4718\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2532 - mse: 0.2532 - mae: 0.3840 - val_loss: 0.4146 - val_mse: 0.4146 - val_mae: 0.5008\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2817 - mse: 0.2817 - mae: 0.4062 - val_loss: 0.4292 - val_mse: 0.4292 - val_mae: 0.4952\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2506 - mse: 0.2506 - mae: 0.3902 - val_loss: 0.4317 - val_mse: 0.4317 - val_mae: 0.5146\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2582 - mse: 0.2582 - mae: 0.3921 - val_loss: 0.4024 - val_mse: 0.4024 - val_mae: 0.4754\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2291 - mse: 0.2291 - mae: 0.3682 - val_loss: 0.4211 - val_mse: 0.4211 - val_mae: 0.4869\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2347 - mse: 0.2347 - mae: 0.3792 - val_loss: 0.4628 - val_mse: 0.4628 - val_mae: 0.5327\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2552 - mse: 0.2552 - mae: 0.3969 - val_loss: 0.3854 - val_mse: 0.3854 - val_mae: 0.4782\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2397 - mse: 0.2397 - mae: 0.3806 - val_loss: 0.4063 - val_mse: 0.4063 - val_mae: 0.5023\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2684 - mse: 0.2684 - mae: 0.4075 - val_loss: 0.4015 - val_mse: 0.4015 - val_mae: 0.4766\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2496 - mse: 0.2496 - mae: 0.3852 - val_loss: 0.4240 - val_mse: 0.4240 - val_mae: 0.5101\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2444 - mse: 0.2444 - mae: 0.3870 - val_loss: 0.3710 - val_mse: 0.3710 - val_mae: 0.4696\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2560 - mse: 0.2560 - mae: 0.3916 - val_loss: 0.3961 - val_mse: 0.3961 - val_mae: 0.4957\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2248 - mse: 0.2248 - mae: 0.3736 - val_loss: 0.3853 - val_mse: 0.3853 - val_mae: 0.4782\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2386 - mse: 0.2386 - mae: 0.3833 - val_loss: 0.4025 - val_mse: 0.4025 - val_mae: 0.4778\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2267 - mse: 0.2267 - mae: 0.3662 - val_loss: 0.3906 - val_mse: 0.3906 - val_mae: 0.4913\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2745 - mse: 0.2745 - mae: 0.4104 - val_loss: 0.4364 - val_mse: 0.4364 - val_mae: 0.4804\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2461 - mse: 0.2461 - mae: 0.3855 - val_loss: 0.3863 - val_mse: 0.3863 - val_mae: 0.4842\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2363 - mse: 0.2363 - mae: 0.3781 - val_loss: 0.4122 - val_mse: 0.4122 - val_mae: 0.4838\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2428 - mse: 0.2428 - mae: 0.3905 - val_loss: 0.4199 - val_mse: 0.4199 - val_mae: 0.5081\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2575 - mse: 0.2575 - mae: 0.3913 - val_loss: 0.3872 - val_mse: 0.3872 - val_mae: 0.4743\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2396 - mse: 0.2396 - mae: 0.3856 - val_loss: 0.4378 - val_mse: 0.4378 - val_mae: 0.5100\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2323 - mse: 0.2323 - mae: 0.3789 - val_loss: 0.3841 - val_mse: 0.3841 - val_mae: 0.4698\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2382 - mse: 0.2382 - mae: 0.3756 - val_loss: 0.4502 - val_mse: 0.4502 - val_mae: 0.5247\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2266 - mse: 0.2266 - mae: 0.3720 - val_loss: 0.3892 - val_mse: 0.3892 - val_mae: 0.4748\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2466 - mse: 0.2466 - mae: 0.3876 - val_loss: 0.4096 - val_mse: 0.4096 - val_mae: 0.4798\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2447 - mse: 0.2447 - mae: 0.3827 - val_loss: 0.4181 - val_mse: 0.4181 - val_mae: 0.4956\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2322 - mse: 0.2322 - mae: 0.3723 - val_loss: 0.3742 - val_mse: 0.3742 - val_mae: 0.4800\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2169 - mse: 0.2169 - mae: 0.3584 - val_loss: 0.3845 - val_mse: 0.3845 - val_mae: 0.4908\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2201 - mse: 0.2201 - mae: 0.3646 - val_loss: 0.3767 - val_mse: 0.3767 - val_mae: 0.4722\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2298 - mse: 0.2298 - mae: 0.3749 - val_loss: 0.3736 - val_mse: 0.3736 - val_mae: 0.4781\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2201 - mse: 0.2201 - mae: 0.3645 - val_loss: 0.4277 - val_mse: 0.4277 - val_mae: 0.5145\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2343 - mse: 0.2343 - mae: 0.3715 - val_loss: 0.4045 - val_mse: 0.4045 - val_mae: 0.4841\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2254 - mse: 0.2254 - mae: 0.3706 - val_loss: 0.4217 - val_mse: 0.4217 - val_mae: 0.4954\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2314 - mse: 0.2314 - mae: 0.3759 - val_loss: 0.3774 - val_mse: 0.3774 - val_mae: 0.4661\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2175 - mse: 0.2175 - mae: 0.3556 - val_loss: 0.4123 - val_mse: 0.4123 - val_mae: 0.5019\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2375 - mse: 0.2375 - mae: 0.3754 - val_loss: 0.3943 - val_mse: 0.3943 - val_mae: 0.4785\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2689 - mse: 0.2689 - mae: 0.4046 - val_loss: 0.4193 - val_mse: 0.4193 - val_mae: 0.5135\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2339 - mse: 0.2339 - mae: 0.3691 - val_loss: 0.3984 - val_mse: 0.3984 - val_mae: 0.4768\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2238 - mse: 0.2238 - mae: 0.3711 - val_loss: 0.4183 - val_mse: 0.4183 - val_mae: 0.4893\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2341 - mse: 0.2341 - mae: 0.3763 - val_loss: 0.4081 - val_mse: 0.4081 - val_mae: 0.4991\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2634 - mse: 0.2634 - mae: 0.3972 - val_loss: 0.4008 - val_mse: 0.4008 - val_mae: 0.4824\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2355 - mse: 0.2355 - mae: 0.3799 - val_loss: 0.3885 - val_mse: 0.3885 - val_mae: 0.4844\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2630 - mse: 0.2630 - mae: 0.3998 - val_loss: 0.5918 - val_mse: 0.5918 - val_mae: 0.6183\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2962 - mse: 0.2962 - mae: 0.4230 - val_loss: 0.4017 - val_mse: 0.4017 - val_mae: 0.4874\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2536 - mse: 0.2536 - mae: 0.3929 - val_loss: 0.4624 - val_mse: 0.4624 - val_mae: 0.5398\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2371 - mse: 0.2371 - mae: 0.3790 - val_loss: 0.3946 - val_mse: 0.3946 - val_mae: 0.4815\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2287 - mse: 0.2287 - mae: 0.3676 - val_loss: 0.3804 - val_mse: 0.3804 - val_mae: 0.4665\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2171 - mse: 0.2171 - mae: 0.3582 - val_loss: 0.3958 - val_mse: 0.3958 - val_mae: 0.4934\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2243 - mse: 0.2243 - mae: 0.3727 - val_loss: 0.3721 - val_mse: 0.3721 - val_mae: 0.4723\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2218 - mse: 0.2218 - mae: 0.3655 - val_loss: 0.3770 - val_mse: 0.3770 - val_mae: 0.4685\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2521 - mse: 0.2521 - mae: 0.3902 - val_loss: 0.3671 - val_mse: 0.3671 - val_mae: 0.4716\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2394 - mse: 0.2394 - mae: 0.3816 - val_loss: 0.3722 - val_mse: 0.3722 - val_mae: 0.4720\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2181 - mse: 0.2181 - mae: 0.3624 - val_loss: 0.3747 - val_mse: 0.3747 - val_mae: 0.4791\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2153 - mse: 0.2153 - mae: 0.3561 - val_loss: 0.4043 - val_mse: 0.4043 - val_mae: 0.5026\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2368 - mse: 0.2368 - mae: 0.3790 - val_loss: 0.3778 - val_mse: 0.3778 - val_mae: 0.4685\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2289 - mse: 0.2289 - mae: 0.3738 - val_loss: 0.4160 - val_mse: 0.4160 - val_mae: 0.4806\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2389 - mse: 0.2389 - mae: 0.3899 - val_loss: 0.4704 - val_mse: 0.4704 - val_mae: 0.5420\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2292 - mse: 0.2292 - mae: 0.3681 - val_loss: 0.3841 - val_mse: 0.3841 - val_mae: 0.4683\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2100 - mse: 0.2100 - mae: 0.3600 - val_loss: 0.4289 - val_mse: 0.4289 - val_mae: 0.5103\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2161 - mse: 0.2161 - mae: 0.3596 - val_loss: 0.3891 - val_mse: 0.3891 - val_mae: 0.4820\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2186 - mse: 0.2186 - mae: 0.3670 - val_loss: 0.4026 - val_mse: 0.4026 - val_mae: 0.4982\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2132 - mse: 0.2132 - mae: 0.3654 - val_loss: 0.3806 - val_mse: 0.3806 - val_mae: 0.4832\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2016 - mse: 0.2016 - mae: 0.3568 - val_loss: 0.3939 - val_mse: 0.3939 - val_mae: 0.4916\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1985 - mse: 0.1985 - mae: 0.3412 - val_loss: 0.3957 - val_mse: 0.3957 - val_mae: 0.4955\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2444 - mse: 0.2444 - mae: 0.3885 - val_loss: 0.3966 - val_mse: 0.3966 - val_mae: 0.4894\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2300 - mse: 0.2300 - mae: 0.3684 - val_loss: 0.3731 - val_mse: 0.3731 - val_mae: 0.4699\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2167 - mse: 0.2167 - mae: 0.3566 - val_loss: 0.4596 - val_mse: 0.4596 - val_mae: 0.5436\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2205 - mse: 0.2205 - mae: 0.3738 - val_loss: 0.3924 - val_mse: 0.3924 - val_mae: 0.4868\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2147 - mse: 0.2147 - mae: 0.3558 - val_loss: 0.3769 - val_mse: 0.3769 - val_mae: 0.4710\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2116 - mse: 0.2116 - mae: 0.3547 - val_loss: 0.3817 - val_mse: 0.3817 - val_mae: 0.4836\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2142 - mse: 0.2142 - mae: 0.3594 - val_loss: 0.3843 - val_mse: 0.3843 - val_mae: 0.4742\n",
            "Epoch 252/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2209 - mse: 0.2209 - mae: 0.3684 - val_loss: 0.4070 - val_mse: 0.4070 - val_mae: 0.4932\n",
            "Epoch 253/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2228 - mse: 0.2228 - mae: 0.3682 - val_loss: 0.3796 - val_mse: 0.3796 - val_mae: 0.4785\n",
            "Epoch 254/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2027 - mse: 0.2027 - mae: 0.3504 - val_loss: 0.4018 - val_mse: 0.4018 - val_mae: 0.4864\n",
            "Epoch 255/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2236 - mse: 0.2236 - mae: 0.3586 - val_loss: 0.3941 - val_mse: 0.3941 - val_mae: 0.4936\n",
            "Epoch 256/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2248 - mse: 0.2248 - mae: 0.3639 - val_loss: 0.4122 - val_mse: 0.4122 - val_mae: 0.5112\n",
            "Epoch 257/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2443 - mse: 0.2443 - mae: 0.3915 - val_loss: 0.3993 - val_mse: 0.3993 - val_mae: 0.4916\n",
            "Epoch 258/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2335 - mse: 0.2335 - mae: 0.3767 - val_loss: 0.4131 - val_mse: 0.4131 - val_mae: 0.5000\n",
            "Epoch 259/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2340 - mse: 0.2340 - mae: 0.3778 - val_loss: 0.4255 - val_mse: 0.4255 - val_mae: 0.5169\n",
            "Epoch 260/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2126 - mse: 0.2126 - mae: 0.3615 - val_loss: 0.3787 - val_mse: 0.3787 - val_mae: 0.4889\n",
            "Epoch 261/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2225 - mse: 0.2225 - mae: 0.3667 - val_loss: 0.4048 - val_mse: 0.4048 - val_mae: 0.4855\n",
            "Epoch 262/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1877 - mse: 0.1877 - mae: 0.3381 - val_loss: 0.3908 - val_mse: 0.3908 - val_mae: 0.4754\n",
            "Epoch 263/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2031 - mse: 0.2031 - mae: 0.3545 - val_loss: 0.3920 - val_mse: 0.3920 - val_mae: 0.4901\n",
            "Epoch 264/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2012 - mse: 0.2012 - mae: 0.3445 - val_loss: 0.3838 - val_mse: 0.3838 - val_mae: 0.4776\n",
            "Epoch 265/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2179 - mse: 0.2179 - mae: 0.3646 - val_loss: 0.4455 - val_mse: 0.4455 - val_mae: 0.5205\n",
            "Epoch 266/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2248 - mse: 0.2248 - mae: 0.3655 - val_loss: 0.4292 - val_mse: 0.4292 - val_mae: 0.5158\n",
            "Epoch 267/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1916 - mse: 0.1916 - mae: 0.3440 - val_loss: 0.4034 - val_mse: 0.4034 - val_mae: 0.4842\n",
            "Epoch 268/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1973 - mse: 0.1973 - mae: 0.3386 - val_loss: 0.3912 - val_mse: 0.3912 - val_mae: 0.4876\n",
            "Epoch 269/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2021 - mse: 0.2021 - mae: 0.3530 - val_loss: 0.4147 - val_mse: 0.4147 - val_mae: 0.4867\n",
            "Epoch 270/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1989 - mse: 0.1989 - mae: 0.3452 - val_loss: 0.3891 - val_mse: 0.3891 - val_mae: 0.4898\n",
            "Epoch 271/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2101 - mse: 0.2101 - mae: 0.3558 - val_loss: 0.3716 - val_mse: 0.3716 - val_mae: 0.4774\n",
            "Epoch 272/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2128 - mse: 0.2128 - mae: 0.3629 - val_loss: 0.3800 - val_mse: 0.3800 - val_mae: 0.4809\n",
            "Epoch 273/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2344 - mse: 0.2344 - mae: 0.3793 - val_loss: 0.3750 - val_mse: 0.3750 - val_mae: 0.4727\n",
            "Epoch 274/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2272 - mse: 0.2272 - mae: 0.3734 - val_loss: 0.3801 - val_mse: 0.3801 - val_mae: 0.4762\n",
            "Epoch 275/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2179 - mse: 0.2179 - mae: 0.3604 - val_loss: 0.4137 - val_mse: 0.4137 - val_mae: 0.5026\n",
            "Epoch 276/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2035 - mse: 0.2035 - mae: 0.3522 - val_loss: 0.3927 - val_mse: 0.3927 - val_mae: 0.4931\n",
            "Epoch 277/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2203 - mse: 0.2203 - mae: 0.3760 - val_loss: 0.3764 - val_mse: 0.3764 - val_mae: 0.4658\n",
            "Epoch 278/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1978 - mse: 0.1978 - mae: 0.3497 - val_loss: 0.4393 - val_mse: 0.4393 - val_mae: 0.5029\n",
            "Epoch 279/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2220 - mse: 0.2220 - mae: 0.3613 - val_loss: 0.3833 - val_mse: 0.3833 - val_mae: 0.4731\n",
            "Epoch 280/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1996 - mse: 0.1996 - mae: 0.3490 - val_loss: 0.4109 - val_mse: 0.4109 - val_mae: 0.5022\n",
            "Epoch 281/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1897 - mse: 0.1897 - mae: 0.3431 - val_loss: 0.3844 - val_mse: 0.3844 - val_mae: 0.4715\n",
            "Epoch 282/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2031 - mse: 0.2031 - mae: 0.3566 - val_loss: 0.4276 - val_mse: 0.4276 - val_mae: 0.5078\n",
            "Epoch 283/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2147 - mse: 0.2147 - mae: 0.3604 - val_loss: 0.3692 - val_mse: 0.3692 - val_mae: 0.4636\n",
            "Epoch 284/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2164 - mse: 0.2164 - mae: 0.3634 - val_loss: 0.3789 - val_mse: 0.3789 - val_mae: 0.4766\n",
            "Epoch 285/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2157 - mse: 0.2157 - mae: 0.3556 - val_loss: 0.3973 - val_mse: 0.3973 - val_mae: 0.4744\n",
            "Epoch 286/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2289 - mse: 0.2289 - mae: 0.3588 - val_loss: 0.4000 - val_mse: 0.4000 - val_mae: 0.4950\n",
            "Epoch 287/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2047 - mse: 0.2047 - mae: 0.3469 - val_loss: 0.3889 - val_mse: 0.3889 - val_mae: 0.4833\n",
            "Epoch 288/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1921 - mse: 0.1921 - mae: 0.3389 - val_loss: 0.4071 - val_mse: 0.4071 - val_mae: 0.4710\n",
            "Epoch 289/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1730 - mse: 0.1730 - mae: 0.3288 - val_loss: 0.4232 - val_mse: 0.4232 - val_mae: 0.5055\n",
            "Epoch 290/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2041 - mse: 0.2041 - mae: 0.3532 - val_loss: 0.3914 - val_mse: 0.3914 - val_mae: 0.4827\n",
            "Epoch 291/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1998 - mse: 0.1998 - mae: 0.3518 - val_loss: 0.4063 - val_mse: 0.4063 - val_mae: 0.4957\n",
            "Epoch 292/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2009 - mse: 0.2009 - mae: 0.3539 - val_loss: 0.4260 - val_mse: 0.4260 - val_mae: 0.5111\n",
            "Epoch 293/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2367 - mse: 0.2367 - mae: 0.3807 - val_loss: 0.4157 - val_mse: 0.4157 - val_mae: 0.4954\n",
            "Epoch 294/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2106 - mse: 0.2106 - mae: 0.3567 - val_loss: 0.3695 - val_mse: 0.3695 - val_mae: 0.4655\n",
            "Epoch 295/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1963 - mse: 0.1963 - mae: 0.3395 - val_loss: 0.3683 - val_mse: 0.3683 - val_mae: 0.4613\n",
            "Epoch 296/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1894 - mse: 0.1894 - mae: 0.3365 - val_loss: 0.3877 - val_mse: 0.3877 - val_mae: 0.4721\n",
            "Epoch 297/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1944 - mse: 0.1944 - mae: 0.3323 - val_loss: 0.4046 - val_mse: 0.4046 - val_mae: 0.4930\n",
            "Epoch 298/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2153 - mse: 0.2153 - mae: 0.3493 - val_loss: 0.4052 - val_mse: 0.4052 - val_mae: 0.4901\n",
            "Epoch 299/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1898 - mse: 0.1898 - mae: 0.3418 - val_loss: 0.3769 - val_mse: 0.3769 - val_mae: 0.4721\n",
            "Epoch 300/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1941 - mse: 0.1941 - mae: 0.3398 - val_loss: 0.4088 - val_mse: 0.4088 - val_mae: 0.4973\n",
            "Epoch 301/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1991 - mse: 0.1991 - mae: 0.3463 - val_loss: 0.4099 - val_mse: 0.4099 - val_mae: 0.4782\n",
            "Epoch 302/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2161 - mse: 0.2161 - mae: 0.3612 - val_loss: 0.3835 - val_mse: 0.3835 - val_mae: 0.4738\n",
            "Epoch 303/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1804 - mse: 0.1804 - mae: 0.3240 - val_loss: 0.4473 - val_mse: 0.4473 - val_mae: 0.5195\n",
            "Epoch 304/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2034 - mse: 0.2034 - mae: 0.3513 - val_loss: 0.4096 - val_mse: 0.4096 - val_mae: 0.4951\n",
            "Epoch 305/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2327 - mse: 0.2327 - mae: 0.3777 - val_loss: 0.5063 - val_mse: 0.5063 - val_mae: 0.5565\n",
            "Epoch 306/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1867 - mse: 0.1867 - mae: 0.3355 - val_loss: 0.4443 - val_mse: 0.4443 - val_mae: 0.5090\n",
            "Epoch 307/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1974 - mse: 0.1974 - mae: 0.3429 - val_loss: 0.3990 - val_mse: 0.3990 - val_mae: 0.4900\n",
            "Epoch 308/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2011 - mse: 0.2011 - mae: 0.3428 - val_loss: 0.3840 - val_mse: 0.3840 - val_mae: 0.4688\n",
            "Epoch 309/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1956 - mse: 0.1956 - mae: 0.3439 - val_loss: 0.3925 - val_mse: 0.3925 - val_mae: 0.4698\n",
            "Epoch 310/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1753 - mse: 0.1753 - mae: 0.3271 - val_loss: 0.3753 - val_mse: 0.3753 - val_mae: 0.4730\n",
            "Epoch 311/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1923 - mse: 0.1923 - mae: 0.3401 - val_loss: 0.4212 - val_mse: 0.4212 - val_mae: 0.5091\n",
            "Epoch 312/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1932 - mse: 0.1932 - mae: 0.3372 - val_loss: 0.3951 - val_mse: 0.3951 - val_mae: 0.4888\n",
            "Epoch 313/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1941 - mse: 0.1941 - mae: 0.3390 - val_loss: 0.4265 - val_mse: 0.4265 - val_mae: 0.5053\n",
            "Epoch 314/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1899 - mse: 0.1899 - mae: 0.3321 - val_loss: 0.4026 - val_mse: 0.4026 - val_mae: 0.4842\n",
            "Epoch 315/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1879 - mse: 0.1879 - mae: 0.3376 - val_loss: 0.3733 - val_mse: 0.3733 - val_mae: 0.4643\n",
            "Epoch 316/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1820 - mse: 0.1820 - mae: 0.3354 - val_loss: 0.3875 - val_mse: 0.3875 - val_mae: 0.4760\n",
            "Epoch 317/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1715 - mse: 0.1715 - mae: 0.3251 - val_loss: 0.4130 - val_mse: 0.4130 - val_mae: 0.5040\n",
            "Epoch 318/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1872 - mse: 0.1872 - mae: 0.3290 - val_loss: 0.3840 - val_mse: 0.3840 - val_mae: 0.4845\n",
            "Epoch 319/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1847 - mse: 0.1847 - mae: 0.3259 - val_loss: 0.3852 - val_mse: 0.3852 - val_mae: 0.4660\n",
            "Epoch 320/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2108 - mse: 0.2108 - mae: 0.3602 - val_loss: 0.3821 - val_mse: 0.3821 - val_mae: 0.4803\n",
            "Epoch 321/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1899 - mse: 0.1899 - mae: 0.3362 - val_loss: 0.4101 - val_mse: 0.4101 - val_mae: 0.5021\n",
            "Epoch 322/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1908 - mse: 0.1908 - mae: 0.3381 - val_loss: 0.3977 - val_mse: 0.3977 - val_mae: 0.4712\n",
            "Epoch 323/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1800 - mse: 0.1800 - mae: 0.3266 - val_loss: 0.3796 - val_mse: 0.3796 - val_mae: 0.4660\n",
            "Epoch 324/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1838 - mse: 0.1838 - mae: 0.3301 - val_loss: 0.4160 - val_mse: 0.4160 - val_mae: 0.4866\n",
            "Epoch 325/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1761 - mse: 0.1761 - mae: 0.3251 - val_loss: 0.3772 - val_mse: 0.3772 - val_mae: 0.4719\n",
            "Epoch 326/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1884 - mse: 0.1884 - mae: 0.3356 - val_loss: 0.4103 - val_mse: 0.4103 - val_mae: 0.4962\n",
            "Epoch 327/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1792 - mse: 0.1792 - mae: 0.3247 - val_loss: 0.4024 - val_mse: 0.4024 - val_mae: 0.4746\n",
            "Epoch 328/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1982 - mse: 0.1982 - mae: 0.3425 - val_loss: 0.4283 - val_mse: 0.4283 - val_mae: 0.4868\n",
            "Epoch 329/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1994 - mse: 0.1994 - mae: 0.3544 - val_loss: 0.4086 - val_mse: 0.4086 - val_mae: 0.4839\n",
            "Epoch 330/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2055 - mse: 0.2055 - mae: 0.3536 - val_loss: 0.4182 - val_mse: 0.4182 - val_mae: 0.5083\n",
            "Epoch 331/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2015 - mse: 0.2015 - mae: 0.3401 - val_loss: 0.4799 - val_mse: 0.4799 - val_mae: 0.5414\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.5660 - mse: 0.5660 - mae: 0.5601\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 15ms/step - loss: 9.5107 - mse: 9.5107 - mae: 2.5705 - val_loss: 3.7057 - val_mse: 3.7057 - val_mae: 1.4685\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 3.2735 - mse: 3.2735 - mae: 1.4299 - val_loss: 2.9134 - val_mse: 2.9134 - val_mae: 1.3918\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 2.5819 - mse: 2.5819 - mae: 1.2861 - val_loss: 2.4495 - val_mse: 2.4495 - val_mae: 1.2480\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.2568 - mse: 2.2568 - mae: 1.1830 - val_loss: 2.3304 - val_mse: 2.3304 - val_mae: 1.2173\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.0492 - mse: 2.0492 - mae: 1.1506 - val_loss: 2.1969 - val_mse: 2.1969 - val_mae: 1.1814\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.8834 - mse: 1.8834 - mae: 1.0878 - val_loss: 2.0906 - val_mse: 2.0906 - val_mae: 1.1581\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.9923 - mse: 1.9923 - mae: 1.1248 - val_loss: 1.9467 - val_mse: 1.9467 - val_mae: 1.1202\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.7312 - mse: 1.7312 - mae: 1.0422 - val_loss: 1.9070 - val_mse: 1.9070 - val_mae: 1.1283\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.6507 - mse: 1.6507 - mae: 1.0252 - val_loss: 1.7683 - val_mse: 1.7683 - val_mae: 1.0687\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.5269 - mse: 1.5269 - mae: 0.9847 - val_loss: 1.7030 - val_mse: 1.7030 - val_mae: 1.0515\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.3466 - mse: 1.3466 - mae: 0.9100 - val_loss: 1.5851 - val_mse: 1.5851 - val_mae: 1.0290\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.3130 - mse: 1.3130 - mae: 0.9120 - val_loss: 1.5747 - val_mse: 1.5747 - val_mae: 1.0018\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.3176 - mse: 1.3176 - mae: 0.9032 - val_loss: 1.5091 - val_mse: 1.5091 - val_mae: 1.0029\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.1341 - mse: 1.1341 - mae: 0.8345 - val_loss: 1.4179 - val_mse: 1.4179 - val_mae: 0.9735\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1139 - mse: 1.1139 - mae: 0.8410 - val_loss: 1.3356 - val_mse: 1.3356 - val_mae: 0.9458\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.1441 - mse: 1.1441 - mae: 0.8473 - val_loss: 1.2758 - val_mse: 1.2758 - val_mae: 0.9112\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.0638 - mse: 1.0638 - mae: 0.8177 - val_loss: 1.2104 - val_mse: 1.2104 - val_mae: 0.8824\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9902 - mse: 0.9902 - mae: 0.7787 - val_loss: 1.2308 - val_mse: 1.2308 - val_mae: 0.8816\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9007 - mse: 0.9007 - mae: 0.7461 - val_loss: 1.1117 - val_mse: 1.1117 - val_mae: 0.8534\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9521 - mse: 0.9521 - mae: 0.7711 - val_loss: 1.0864 - val_mse: 1.0864 - val_mae: 0.8267\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8942 - mse: 0.8942 - mae: 0.7349 - val_loss: 1.0317 - val_mse: 1.0317 - val_mae: 0.8155\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7890 - mse: 0.7890 - mae: 0.6873 - val_loss: 0.9726 - val_mse: 0.9726 - val_mae: 0.7741\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7608 - mse: 0.7608 - mae: 0.6859 - val_loss: 0.9226 - val_mse: 0.9226 - val_mae: 0.7670\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7401 - mse: 0.7401 - mae: 0.6748 - val_loss: 0.9160 - val_mse: 0.9160 - val_mae: 0.7542\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7216 - mse: 0.7216 - mae: 0.6671 - val_loss: 0.8411 - val_mse: 0.8411 - val_mae: 0.7153\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.6741 - mse: 0.6741 - mae: 0.6259 - val_loss: 0.8316 - val_mse: 0.8316 - val_mae: 0.7219\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6531 - mse: 0.6531 - mae: 0.6264 - val_loss: 0.8084 - val_mse: 0.8084 - val_mae: 0.6994\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.6556 - mse: 0.6556 - mae: 0.6187 - val_loss: 0.7740 - val_mse: 0.7740 - val_mae: 0.6871\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.6335 - mse: 0.6335 - mae: 0.6251 - val_loss: 0.7894 - val_mse: 0.7894 - val_mae: 0.7008\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5962 - mse: 0.5962 - mae: 0.6109 - val_loss: 0.7308 - val_mse: 0.7308 - val_mae: 0.6764\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.5453 - mse: 0.5453 - mae: 0.5789 - val_loss: 0.7077 - val_mse: 0.7077 - val_mae: 0.6651\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5850 - mse: 0.5850 - mae: 0.6048 - val_loss: 0.6941 - val_mse: 0.6941 - val_mae: 0.6432\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5558 - mse: 0.5558 - mae: 0.5836 - val_loss: 0.6857 - val_mse: 0.6857 - val_mae: 0.6578\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5097 - mse: 0.5097 - mae: 0.5469 - val_loss: 0.6532 - val_mse: 0.6532 - val_mae: 0.6266\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.5491 - mse: 0.5491 - mae: 0.5714 - val_loss: 0.6349 - val_mse: 0.6349 - val_mae: 0.6112\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4704 - mse: 0.4704 - mae: 0.5334 - val_loss: 0.6170 - val_mse: 0.6170 - val_mae: 0.6172\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4727 - mse: 0.4727 - mae: 0.5374 - val_loss: 0.5994 - val_mse: 0.5994 - val_mae: 0.6028\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4874 - mse: 0.4874 - mae: 0.5325 - val_loss: 0.6022 - val_mse: 0.6022 - val_mae: 0.6149\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4566 - mse: 0.4566 - mae: 0.5264 - val_loss: 0.6560 - val_mse: 0.6560 - val_mae: 0.6263\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4540 - mse: 0.4540 - mae: 0.5221 - val_loss: 0.5867 - val_mse: 0.5867 - val_mae: 0.6052\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4551 - mse: 0.4551 - mae: 0.5250 - val_loss: 0.5764 - val_mse: 0.5764 - val_mae: 0.5958\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4327 - mse: 0.4327 - mae: 0.5072 - val_loss: 0.6434 - val_mse: 0.6434 - val_mae: 0.6136\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4688 - mse: 0.4688 - mae: 0.5351 - val_loss: 0.5709 - val_mse: 0.5709 - val_mae: 0.6083\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4406 - mse: 0.4406 - mae: 0.5211 - val_loss: 0.5483 - val_mse: 0.5483 - val_mae: 0.5875\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4366 - mse: 0.4366 - mae: 0.5135 - val_loss: 0.5674 - val_mse: 0.5674 - val_mae: 0.5905\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4014 - mse: 0.4014 - mae: 0.4934 - val_loss: 0.5547 - val_mse: 0.5547 - val_mae: 0.5873\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.4367 - mse: 0.4367 - mae: 0.5148 - val_loss: 0.5513 - val_mse: 0.5513 - val_mae: 0.5771\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4215 - mse: 0.4215 - mae: 0.5008 - val_loss: 0.5236 - val_mse: 0.5236 - val_mae: 0.5802\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4003 - mse: 0.4003 - mae: 0.5019 - val_loss: 0.5270 - val_mse: 0.5270 - val_mae: 0.5671\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3609 - mse: 0.3609 - mae: 0.4753 - val_loss: 0.5254 - val_mse: 0.5254 - val_mae: 0.5703\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3674 - mse: 0.3674 - mae: 0.4752 - val_loss: 0.5337 - val_mse: 0.5337 - val_mae: 0.5882\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3804 - mse: 0.3804 - mae: 0.4832 - val_loss: 0.5751 - val_mse: 0.5751 - val_mae: 0.5946\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3631 - mse: 0.3631 - mae: 0.4685 - val_loss: 0.5020 - val_mse: 0.5020 - val_mae: 0.5609\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3964 - mse: 0.3964 - mae: 0.4960 - val_loss: 0.6526 - val_mse: 0.6526 - val_mae: 0.6376\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3816 - mse: 0.3816 - mae: 0.4720 - val_loss: 0.4898 - val_mse: 0.4898 - val_mae: 0.5539\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3531 - mse: 0.3531 - mae: 0.4702 - val_loss: 0.4816 - val_mse: 0.4816 - val_mae: 0.5449\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3466 - mse: 0.3466 - mae: 0.4539 - val_loss: 0.4849 - val_mse: 0.4849 - val_mae: 0.5433\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3685 - mse: 0.3685 - mae: 0.4691 - val_loss: 0.5517 - val_mse: 0.5517 - val_mae: 0.5869\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3642 - mse: 0.3642 - mae: 0.4757 - val_loss: 0.5518 - val_mse: 0.5518 - val_mae: 0.5915\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3936 - mse: 0.3936 - mae: 0.4927 - val_loss: 0.5097 - val_mse: 0.5097 - val_mae: 0.5662\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3710 - mse: 0.3710 - mae: 0.4831 - val_loss: 0.4824 - val_mse: 0.4824 - val_mae: 0.5423\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3049 - mse: 0.3049 - mae: 0.4279 - val_loss: 0.4855 - val_mse: 0.4855 - val_mae: 0.5468\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3602 - mse: 0.3602 - mae: 0.4604 - val_loss: 0.5109 - val_mse: 0.5109 - val_mae: 0.5575\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3045 - mse: 0.3045 - mae: 0.4282 - val_loss: 0.5164 - val_mse: 0.5164 - val_mae: 0.5637\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3175 - mse: 0.3175 - mae: 0.4409 - val_loss: 0.4932 - val_mse: 0.4932 - val_mae: 0.5513\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3283 - mse: 0.3283 - mae: 0.4454 - val_loss: 0.4548 - val_mse: 0.4548 - val_mae: 0.5325\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2982 - mse: 0.2982 - mae: 0.4213 - val_loss: 0.4682 - val_mse: 0.4682 - val_mae: 0.5329\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3345 - mse: 0.3345 - mae: 0.4555 - val_loss: 0.4803 - val_mse: 0.4803 - val_mae: 0.5388\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3138 - mse: 0.3138 - mae: 0.4400 - val_loss: 0.4649 - val_mse: 0.4649 - val_mae: 0.5245\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3279 - mse: 0.3279 - mae: 0.4471 - val_loss: 0.4848 - val_mse: 0.4848 - val_mae: 0.5432\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3410 - mse: 0.3410 - mae: 0.4576 - val_loss: 0.4999 - val_mse: 0.4999 - val_mae: 0.5615\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3277 - mse: 0.3277 - mae: 0.4425 - val_loss: 0.4617 - val_mse: 0.4617 - val_mae: 0.5376\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2869 - mse: 0.2869 - mae: 0.4106 - val_loss: 0.4893 - val_mse: 0.4893 - val_mae: 0.5583\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3456 - mse: 0.3456 - mae: 0.4468 - val_loss: 0.4466 - val_mse: 0.4466 - val_mae: 0.5217\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2984 - mse: 0.2984 - mae: 0.4249 - val_loss: 0.4479 - val_mse: 0.4479 - val_mae: 0.5240\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2668 - mse: 0.2668 - mae: 0.4067 - val_loss: 0.4509 - val_mse: 0.4509 - val_mae: 0.5227\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2664 - mse: 0.2664 - mae: 0.4001 - val_loss: 0.4776 - val_mse: 0.4776 - val_mae: 0.5391\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2910 - mse: 0.2910 - mae: 0.4223 - val_loss: 0.4709 - val_mse: 0.4709 - val_mae: 0.5416\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2931 - mse: 0.2931 - mae: 0.4236 - val_loss: 0.4467 - val_mse: 0.4467 - val_mae: 0.5148\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3355 - mse: 0.3355 - mae: 0.4484 - val_loss: 0.4701 - val_mse: 0.4701 - val_mae: 0.5348\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2690 - mse: 0.2690 - mae: 0.4025 - val_loss: 0.4317 - val_mse: 0.4317 - val_mae: 0.5012\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2897 - mse: 0.2897 - mae: 0.4177 - val_loss: 0.4880 - val_mse: 0.4880 - val_mae: 0.5549\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2881 - mse: 0.2881 - mae: 0.4190 - val_loss: 0.4310 - val_mse: 0.4310 - val_mae: 0.5058\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2742 - mse: 0.2742 - mae: 0.4028 - val_loss: 0.4572 - val_mse: 0.4572 - val_mae: 0.5233\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2714 - mse: 0.2714 - mae: 0.4057 - val_loss: 0.4421 - val_mse: 0.4421 - val_mae: 0.5229\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2746 - mse: 0.2746 - mae: 0.4120 - val_loss: 0.4679 - val_mse: 0.4679 - val_mae: 0.5335\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2585 - mse: 0.2585 - mae: 0.3983 - val_loss: 0.4316 - val_mse: 0.4316 - val_mae: 0.5052\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2699 - mse: 0.2699 - mae: 0.3961 - val_loss: 0.4309 - val_mse: 0.4309 - val_mae: 0.5070\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2653 - mse: 0.2653 - mae: 0.3959 - val_loss: 0.4690 - val_mse: 0.4690 - val_mae: 0.5376\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2649 - mse: 0.2649 - mae: 0.4024 - val_loss: 0.4528 - val_mse: 0.4528 - val_mae: 0.5165\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2693 - mse: 0.2693 - mae: 0.4071 - val_loss: 0.5199 - val_mse: 0.5199 - val_mae: 0.5561\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2679 - mse: 0.2679 - mae: 0.4003 - val_loss: 0.4632 - val_mse: 0.4632 - val_mae: 0.5458\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2732 - mse: 0.2732 - mae: 0.4007 - val_loss: 0.4319 - val_mse: 0.4319 - val_mae: 0.5103\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2562 - mse: 0.2562 - mae: 0.3948 - val_loss: 0.4444 - val_mse: 0.4444 - val_mae: 0.5282\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2891 - mse: 0.2891 - mae: 0.4190 - val_loss: 0.4386 - val_mse: 0.4386 - val_mae: 0.5208\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2636 - mse: 0.2636 - mae: 0.3979 - val_loss: 0.4279 - val_mse: 0.4279 - val_mae: 0.5113\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2608 - mse: 0.2608 - mae: 0.3929 - val_loss: 0.4345 - val_mse: 0.4345 - val_mae: 0.5162\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2600 - mse: 0.2600 - mae: 0.3907 - val_loss: 0.4296 - val_mse: 0.4296 - val_mae: 0.5137\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2643 - mse: 0.2643 - mae: 0.3889 - val_loss: 0.4687 - val_mse: 0.4687 - val_mae: 0.5361\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2876 - mse: 0.2876 - mae: 0.4160 - val_loss: 0.4658 - val_mse: 0.4658 - val_mae: 0.5448\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2700 - mse: 0.2700 - mae: 0.4024 - val_loss: 0.4386 - val_mse: 0.4386 - val_mae: 0.5211\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2734 - mse: 0.2734 - mae: 0.4122 - val_loss: 0.4465 - val_mse: 0.4465 - val_mae: 0.5171\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2742 - mse: 0.2742 - mae: 0.4165 - val_loss: 0.4241 - val_mse: 0.4241 - val_mae: 0.5198\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2534 - mse: 0.2534 - mae: 0.3855 - val_loss: 0.4073 - val_mse: 0.4073 - val_mae: 0.4988\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2681 - mse: 0.2681 - mae: 0.4006 - val_loss: 0.4024 - val_mse: 0.4024 - val_mae: 0.4912\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2707 - mse: 0.2707 - mae: 0.4043 - val_loss: 0.4624 - val_mse: 0.4624 - val_mae: 0.5396\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2744 - mse: 0.2744 - mae: 0.4127 - val_loss: 0.4186 - val_mse: 0.4186 - val_mae: 0.5005\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2523 - mse: 0.2523 - mae: 0.3900 - val_loss: 0.4219 - val_mse: 0.4219 - val_mae: 0.5113\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2671 - mse: 0.2671 - mae: 0.3971 - val_loss: 0.4238 - val_mse: 0.4238 - val_mae: 0.5141\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2258 - mse: 0.2258 - mae: 0.3650 - val_loss: 0.4064 - val_mse: 0.4064 - val_mae: 0.5072\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2592 - mse: 0.2592 - mae: 0.4008 - val_loss: 0.4368 - val_mse: 0.4368 - val_mae: 0.5121\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2590 - mse: 0.2590 - mae: 0.3989 - val_loss: 0.4295 - val_mse: 0.4295 - val_mae: 0.5104\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2587 - mse: 0.2587 - mae: 0.3958 - val_loss: 0.4278 - val_mse: 0.4278 - val_mae: 0.5059\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2539 - mse: 0.2539 - mae: 0.3884 - val_loss: 0.4040 - val_mse: 0.4040 - val_mae: 0.4904\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2289 - mse: 0.2289 - mae: 0.3653 - val_loss: 0.4232 - val_mse: 0.4232 - val_mae: 0.5047\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2296 - mse: 0.2296 - mae: 0.3697 - val_loss: 0.4257 - val_mse: 0.4257 - val_mae: 0.5040\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2451 - mse: 0.2451 - mae: 0.3804 - val_loss: 0.4491 - val_mse: 0.4491 - val_mae: 0.5203\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2533 - mse: 0.2533 - mae: 0.3933 - val_loss: 0.4170 - val_mse: 0.4170 - val_mae: 0.4977\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2535 - mse: 0.2535 - mae: 0.3963 - val_loss: 0.4990 - val_mse: 0.4990 - val_mae: 0.5618\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2612 - mse: 0.2612 - mae: 0.3956 - val_loss: 0.3954 - val_mse: 0.3954 - val_mae: 0.4882\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2331 - mse: 0.2331 - mae: 0.3720 - val_loss: 0.4463 - val_mse: 0.4463 - val_mae: 0.5248\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2410 - mse: 0.2410 - mae: 0.3803 - val_loss: 0.4331 - val_mse: 0.4331 - val_mae: 0.5129\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2314 - mse: 0.2314 - mae: 0.3757 - val_loss: 0.5270 - val_mse: 0.5270 - val_mae: 0.5798\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2444 - mse: 0.2444 - mae: 0.3808 - val_loss: 0.4190 - val_mse: 0.4190 - val_mae: 0.5067\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2425 - mse: 0.2425 - mae: 0.3869 - val_loss: 0.3823 - val_mse: 0.3823 - val_mae: 0.4835\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2263 - mse: 0.2263 - mae: 0.3603 - val_loss: 0.3912 - val_mse: 0.3912 - val_mae: 0.4917\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2321 - mse: 0.2321 - mae: 0.3722 - val_loss: 0.4292 - val_mse: 0.4292 - val_mae: 0.5255\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2235 - mse: 0.2235 - mae: 0.3640 - val_loss: 0.4233 - val_mse: 0.4233 - val_mae: 0.4991\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2438 - mse: 0.2438 - mae: 0.3843 - val_loss: 0.4178 - val_mse: 0.4178 - val_mae: 0.4991\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2465 - mse: 0.2465 - mae: 0.3870 - val_loss: 0.4001 - val_mse: 0.4001 - val_mae: 0.4971\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2268 - mse: 0.2268 - mae: 0.3642 - val_loss: 0.4425 - val_mse: 0.4425 - val_mae: 0.5169\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2222 - mse: 0.2222 - mae: 0.3614 - val_loss: 0.4092 - val_mse: 0.4092 - val_mae: 0.4994\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2218 - mse: 0.2218 - mae: 0.3621 - val_loss: 0.3932 - val_mse: 0.3932 - val_mae: 0.4855\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2299 - mse: 0.2299 - mae: 0.3639 - val_loss: 0.4493 - val_mse: 0.4493 - val_mae: 0.5231\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2421 - mse: 0.2421 - mae: 0.3883 - val_loss: 0.4125 - val_mse: 0.4125 - val_mae: 0.5098\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2116 - mse: 0.2116 - mae: 0.3487 - val_loss: 0.4027 - val_mse: 0.4027 - val_mae: 0.4958\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2037 - mse: 0.2037 - mae: 0.3512 - val_loss: 0.4385 - val_mse: 0.4385 - val_mae: 0.5197\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2302 - mse: 0.2302 - mae: 0.3651 - val_loss: 0.4197 - val_mse: 0.4197 - val_mae: 0.5123\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2396 - mse: 0.2396 - mae: 0.3796 - val_loss: 0.3989 - val_mse: 0.3989 - val_mae: 0.4956\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2395 - mse: 0.2395 - mae: 0.3811 - val_loss: 0.4098 - val_mse: 0.4098 - val_mae: 0.5004\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2285 - mse: 0.2285 - mae: 0.3690 - val_loss: 0.4468 - val_mse: 0.4468 - val_mae: 0.5269\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2204 - mse: 0.2204 - mae: 0.3580 - val_loss: 0.4162 - val_mse: 0.4162 - val_mae: 0.5048\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2126 - mse: 0.2126 - mae: 0.3533 - val_loss: 0.4069 - val_mse: 0.4069 - val_mae: 0.4927\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2327 - mse: 0.2327 - mae: 0.3584 - val_loss: 0.3976 - val_mse: 0.3976 - val_mae: 0.4844\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2049 - mse: 0.2049 - mae: 0.3521 - val_loss: 0.4170 - val_mse: 0.4170 - val_mae: 0.5055\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2042 - mse: 0.2042 - mae: 0.3484 - val_loss: 0.4058 - val_mse: 0.4058 - val_mae: 0.4935\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1918 - mse: 0.1918 - mae: 0.3332 - val_loss: 0.4688 - val_mse: 0.4688 - val_mae: 0.5403\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2195 - mse: 0.2195 - mae: 0.3579 - val_loss: 0.3999 - val_mse: 0.3999 - val_mae: 0.4871\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1947 - mse: 0.1947 - mae: 0.3334 - val_loss: 0.4128 - val_mse: 0.4128 - val_mae: 0.4964\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2379 - mse: 0.2379 - mae: 0.3754 - val_loss: 0.4107 - val_mse: 0.4107 - val_mae: 0.4946\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.2370 - mse: 0.2370 - mae: 0.3781 - val_loss: 0.4746 - val_mse: 0.4746 - val_mae: 0.5498\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.2534 - mse: 0.2534 - mae: 0.3875 - val_loss: 0.4871 - val_mse: 0.4871 - val_mae: 0.5462\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.2350 - mse: 0.2350 - mae: 0.3791 - val_loss: 0.4163 - val_mse: 0.4163 - val_mae: 0.4985\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2364 - mse: 0.2364 - mae: 0.3793 - val_loss: 0.4277 - val_mse: 0.4277 - val_mae: 0.5085\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2214 - mse: 0.2214 - mae: 0.3617 - val_loss: 0.3948 - val_mse: 0.3948 - val_mae: 0.4859\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2505 - mse: 0.2505 - mae: 0.3910 - val_loss: 0.4955 - val_mse: 0.4955 - val_mae: 0.5624\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2346 - mse: 0.2346 - mae: 0.3763 - val_loss: 0.3927 - val_mse: 0.3927 - val_mae: 0.4880\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2270 - mse: 0.2270 - mae: 0.3698 - val_loss: 0.4630 - val_mse: 0.4630 - val_mae: 0.5281\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2219 - mse: 0.2219 - mae: 0.3577 - val_loss: 0.4365 - val_mse: 0.4365 - val_mae: 0.5096\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.2295 - mse: 0.2295 - mae: 0.3810 - val_loss: 0.4024 - val_mse: 0.4024 - val_mae: 0.4850\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2134 - mse: 0.2134 - mae: 0.3607 - val_loss: 0.4078 - val_mse: 0.4078 - val_mae: 0.4876\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2655 - mse: 0.2655 - mae: 0.4052 - val_loss: 0.4093 - val_mse: 0.4093 - val_mae: 0.4931\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2748 - mse: 0.2748 - mae: 0.4039 - val_loss: 0.4032 - val_mse: 0.4032 - val_mae: 0.4865\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2199 - mse: 0.2199 - mae: 0.3610 - val_loss: 0.4120 - val_mse: 0.4120 - val_mae: 0.4939\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.2165 - mse: 0.2165 - mae: 0.3518 - val_loss: 0.4637 - val_mse: 0.4637 - val_mae: 0.5259\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2710 - mse: 0.2710 - mae: 0.3957 - val_loss: 0.4105 - val_mse: 0.4105 - val_mae: 0.4973\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2090 - mse: 0.2090 - mae: 0.3518 - val_loss: 0.4147 - val_mse: 0.4147 - val_mae: 0.4962\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2095 - mse: 0.2095 - mae: 0.3486 - val_loss: 0.4900 - val_mse: 0.4900 - val_mae: 0.5546\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2223 - mse: 0.2223 - mae: 0.3556 - val_loss: 0.4175 - val_mse: 0.4175 - val_mae: 0.5103\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2332 - mse: 0.2332 - mae: 0.3727 - val_loss: 0.4309 - val_mse: 0.4309 - val_mae: 0.5095\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2179 - mse: 0.2179 - mae: 0.3651 - val_loss: 0.5015 - val_mse: 0.5015 - val_mae: 0.5500\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2311 - mse: 0.2311 - mae: 0.3806 - val_loss: 0.4110 - val_mse: 0.4110 - val_mae: 0.4989\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2063 - mse: 0.2063 - mae: 0.3479 - val_loss: 0.4329 - val_mse: 0.4329 - val_mae: 0.5156\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2410 - mse: 0.2410 - mae: 0.3771 - val_loss: 0.3926 - val_mse: 0.3926 - val_mae: 0.4879\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1977 - mse: 0.1977 - mae: 0.3419 - val_loss: 0.4089 - val_mse: 0.4089 - val_mae: 0.4988\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2337 - mse: 0.2337 - mae: 0.3721 - val_loss: 0.4277 - val_mse: 0.4277 - val_mae: 0.5072\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1956 - mse: 0.1956 - mae: 0.3401 - val_loss: 0.4266 - val_mse: 0.4266 - val_mae: 0.5041\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1992 - mse: 0.1992 - mae: 0.3426 - val_loss: 0.4117 - val_mse: 0.4117 - val_mae: 0.5038\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2100 - mse: 0.2100 - mae: 0.3464 - val_loss: 0.3907 - val_mse: 0.3907 - val_mae: 0.4789\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1915 - mse: 0.1915 - mae: 0.3389 - val_loss: 0.4042 - val_mse: 0.4042 - val_mae: 0.4985\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2304 - mse: 0.2304 - mae: 0.3622 - val_loss: 0.4645 - val_mse: 0.4645 - val_mae: 0.5289\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2260 - mse: 0.2260 - mae: 0.3643 - val_loss: 0.4158 - val_mse: 0.4158 - val_mae: 0.5024\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1822 - mse: 0.1822 - mae: 0.3231 - val_loss: 0.3789 - val_mse: 0.3789 - val_mae: 0.4732\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1929 - mse: 0.1929 - mae: 0.3369 - val_loss: 0.3879 - val_mse: 0.3879 - val_mae: 0.4852\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1830 - mse: 0.1830 - mae: 0.3255 - val_loss: 0.4085 - val_mse: 0.4085 - val_mae: 0.5045\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2084 - mse: 0.2084 - mae: 0.3508 - val_loss: 0.4155 - val_mse: 0.4155 - val_mae: 0.4920\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2302 - mse: 0.2302 - mae: 0.3745 - val_loss: 0.4366 - val_mse: 0.4366 - val_mae: 0.5137\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1957 - mse: 0.1957 - mae: 0.3454 - val_loss: 0.3998 - val_mse: 0.3998 - val_mae: 0.4927\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2007 - mse: 0.2007 - mae: 0.3447 - val_loss: 0.4110 - val_mse: 0.4110 - val_mae: 0.4997\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2164 - mse: 0.2164 - mae: 0.3599 - val_loss: 0.3966 - val_mse: 0.3966 - val_mae: 0.4800\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1787 - mse: 0.1787 - mae: 0.3258 - val_loss: 0.3880 - val_mse: 0.3880 - val_mae: 0.4705\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1994 - mse: 0.1994 - mae: 0.3412 - val_loss: 0.4290 - val_mse: 0.4290 - val_mae: 0.4994\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2084 - mse: 0.2084 - mae: 0.3530 - val_loss: 0.3909 - val_mse: 0.3909 - val_mae: 0.4810\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1589 - mse: 0.1589 - mae: 0.3079 - val_loss: 0.4039 - val_mse: 0.4039 - val_mae: 0.4939\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2058 - mse: 0.2058 - mae: 0.3482 - val_loss: 0.4596 - val_mse: 0.4596 - val_mae: 0.5379\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1963 - mse: 0.1963 - mae: 0.3408 - val_loss: 0.4306 - val_mse: 0.4306 - val_mae: 0.4986\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1988 - mse: 0.1988 - mae: 0.3484 - val_loss: 0.4021 - val_mse: 0.4021 - val_mae: 0.4827\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2357 - mse: 0.2357 - mae: 0.3756 - val_loss: 0.4368 - val_mse: 0.4368 - val_mae: 0.5162\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1794 - mse: 0.1794 - mae: 0.3242 - val_loss: 0.4046 - val_mse: 0.4046 - val_mae: 0.4924\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1900 - mse: 0.1900 - mae: 0.3367 - val_loss: 0.4500 - val_mse: 0.4500 - val_mae: 0.5243\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2313 - mse: 0.2313 - mae: 0.3668 - val_loss: 0.4541 - val_mse: 0.4541 - val_mae: 0.5271\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1907 - mse: 0.1907 - mae: 0.3388 - val_loss: 0.4073 - val_mse: 0.4073 - val_mae: 0.4874\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1848 - mse: 0.1848 - mae: 0.3300 - val_loss: 0.4114 - val_mse: 0.4114 - val_mae: 0.4899\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2033 - mse: 0.2033 - mae: 0.3528 - val_loss: 0.3945 - val_mse: 0.3945 - val_mae: 0.4798\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1791 - mse: 0.1791 - mae: 0.3297 - val_loss: 0.4407 - val_mse: 0.4407 - val_mae: 0.5085\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1870 - mse: 0.1870 - mae: 0.3302 - val_loss: 0.4247 - val_mse: 0.4247 - val_mae: 0.4967\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1832 - mse: 0.1832 - mae: 0.3269 - val_loss: 0.4544 - val_mse: 0.4544 - val_mae: 0.5210\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1816 - mse: 0.1816 - mae: 0.3236 - val_loss: 0.3998 - val_mse: 0.3998 - val_mae: 0.4856\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1731 - mse: 0.1731 - mae: 0.3185 - val_loss: 0.3985 - val_mse: 0.3985 - val_mae: 0.4829\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1978 - mse: 0.1978 - mae: 0.3420 - val_loss: 0.3981 - val_mse: 0.3981 - val_mae: 0.4800\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1868 - mse: 0.1868 - mae: 0.3292 - val_loss: 0.4182 - val_mse: 0.4182 - val_mae: 0.4936\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1850 - mse: 0.1850 - mae: 0.3278 - val_loss: 0.4010 - val_mse: 0.4010 - val_mae: 0.4796\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1630 - mse: 0.1630 - mae: 0.3093 - val_loss: 0.4008 - val_mse: 0.4008 - val_mae: 0.4801\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1879 - mse: 0.1879 - mae: 0.3383 - val_loss: 0.4174 - val_mse: 0.4174 - val_mae: 0.4915\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2043 - mse: 0.2043 - mae: 0.3469 - val_loss: 0.4009 - val_mse: 0.4009 - val_mae: 0.4837\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.1942 - mse: 0.1942 - mae: 0.3384 - val_loss: 0.4589 - val_mse: 0.4589 - val_mae: 0.5196\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2407 - mse: 0.2407 - mae: 0.3779 - val_loss: 0.4442 - val_mse: 0.4442 - val_mae: 0.5115\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2219 - mse: 0.2219 - mae: 0.3592 - val_loss: 0.4326 - val_mse: 0.4326 - val_mae: 0.4974\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1909 - mse: 0.1909 - mae: 0.3378 - val_loss: 0.5193 - val_mse: 0.5193 - val_mae: 0.5445\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1996 - mse: 0.1996 - mae: 0.3422 - val_loss: 0.4680 - val_mse: 0.4680 - val_mae: 0.5207\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1956 - mse: 0.1956 - mae: 0.3458 - val_loss: 0.3970 - val_mse: 0.3970 - val_mae: 0.4802\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1690 - mse: 0.1690 - mae: 0.3125 - val_loss: 0.4497 - val_mse: 0.4497 - val_mae: 0.5131\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1746 - mse: 0.1746 - mae: 0.3148 - val_loss: 0.4460 - val_mse: 0.4460 - val_mae: 0.5194\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1691 - mse: 0.1691 - mae: 0.3112 - val_loss: 0.3974 - val_mse: 0.3974 - val_mae: 0.4779\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1728 - mse: 0.1728 - mae: 0.3187 - val_loss: 0.4247 - val_mse: 0.4247 - val_mae: 0.4992\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1878 - mse: 0.1878 - mae: 0.3380 - val_loss: 0.4314 - val_mse: 0.4314 - val_mae: 0.5046\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2213 - mse: 0.2213 - mae: 0.3666 - val_loss: 0.4826 - val_mse: 0.4826 - val_mae: 0.5290\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1919 - mse: 0.1919 - mae: 0.3392 - val_loss: 0.4141 - val_mse: 0.4141 - val_mae: 0.4920\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1806 - mse: 0.1806 - mae: 0.3286 - val_loss: 0.3837 - val_mse: 0.3837 - val_mae: 0.4774\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1892 - mse: 0.1892 - mae: 0.3371 - val_loss: 0.4286 - val_mse: 0.4286 - val_mae: 0.4945\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1738 - mse: 0.1738 - mae: 0.3265 - val_loss: 0.3999 - val_mse: 0.3999 - val_mae: 0.4769\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1703 - mse: 0.1703 - mae: 0.3122 - val_loss: 0.4014 - val_mse: 0.4014 - val_mae: 0.4861\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2063 - mse: 0.2063 - mae: 0.3561 - val_loss: 0.4342 - val_mse: 0.4342 - val_mae: 0.5076\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1653 - mse: 0.1653 - mae: 0.3143 - val_loss: 0.4892 - val_mse: 0.4892 - val_mae: 0.5305\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1942 - mse: 0.1942 - mae: 0.3407 - val_loss: 0.3959 - val_mse: 0.3959 - val_mae: 0.4741\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1796 - mse: 0.1796 - mae: 0.3263 - val_loss: 0.3979 - val_mse: 0.3979 - val_mae: 0.4901\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1763 - mse: 0.1763 - mae: 0.3271 - val_loss: 0.4650 - val_mse: 0.4650 - val_mae: 0.5205\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1724 - mse: 0.1724 - mae: 0.3235 - val_loss: 0.3975 - val_mse: 0.3975 - val_mae: 0.4750\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1623 - mse: 0.1623 - mae: 0.3038 - val_loss: 0.4363 - val_mse: 0.4363 - val_mae: 0.5155\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1873 - mse: 0.1873 - mae: 0.3320 - val_loss: 0.3955 - val_mse: 0.3955 - val_mae: 0.4809\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1787 - mse: 0.1787 - mae: 0.3318 - val_loss: 0.4067 - val_mse: 0.4067 - val_mae: 0.4918\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1718 - mse: 0.1718 - mae: 0.3141 - val_loss: 0.4051 - val_mse: 0.4051 - val_mae: 0.4899\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1968 - mse: 0.1968 - mae: 0.3388 - val_loss: 0.3845 - val_mse: 0.3845 - val_mae: 0.4750\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1909 - mse: 0.1909 - mae: 0.3375 - val_loss: 0.4255 - val_mse: 0.4255 - val_mae: 0.4955\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1982 - mse: 0.1982 - mae: 0.3411 - val_loss: 0.3978 - val_mse: 0.3978 - val_mae: 0.4884\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1703 - mse: 0.1703 - mae: 0.3143 - val_loss: 0.4098 - val_mse: 0.4098 - val_mae: 0.4805\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1894 - mse: 0.1894 - mae: 0.3374 - val_loss: 0.4254 - val_mse: 0.4254 - val_mae: 0.4892\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.1875 - mse: 0.1875 - mae: 0.3348 - val_loss: 0.4160 - val_mse: 0.4160 - val_mae: 0.4822\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1831 - mse: 0.1831 - mae: 0.3237 - val_loss: 0.4359 - val_mse: 0.4359 - val_mae: 0.5010\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1616 - mse: 0.1616 - mae: 0.3051 - val_loss: 0.3926 - val_mse: 0.3926 - val_mae: 0.4707\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1652 - mse: 0.1652 - mae: 0.3087 - val_loss: 0.3681 - val_mse: 0.3681 - val_mae: 0.4631\n",
            "Epoch 252/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1588 - mse: 0.1588 - mae: 0.3036 - val_loss: 0.3992 - val_mse: 0.3992 - val_mae: 0.4824\n",
            "Epoch 253/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1637 - mse: 0.1637 - mae: 0.3132 - val_loss: 0.3903 - val_mse: 0.3903 - val_mae: 0.4738\n",
            "Epoch 254/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1882 - mse: 0.1882 - mae: 0.3334 - val_loss: 0.3910 - val_mse: 0.3910 - val_mae: 0.4741\n",
            "Epoch 255/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1656 - mse: 0.1656 - mae: 0.3083 - val_loss: 0.3967 - val_mse: 0.3967 - val_mae: 0.4801\n",
            "Epoch 256/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1822 - mse: 0.1822 - mae: 0.3270 - val_loss: 0.3829 - val_mse: 0.3829 - val_mae: 0.4743\n",
            "Epoch 257/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1895 - mse: 0.1895 - mae: 0.3253 - val_loss: 0.4019 - val_mse: 0.4019 - val_mae: 0.4719\n",
            "Epoch 258/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1799 - mse: 0.1799 - mae: 0.3239 - val_loss: 0.4626 - val_mse: 0.4626 - val_mae: 0.5267\n",
            "Epoch 259/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1818 - mse: 0.1818 - mae: 0.3321 - val_loss: 0.3635 - val_mse: 0.3635 - val_mae: 0.4628\n",
            "Epoch 260/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1716 - mse: 0.1716 - mae: 0.3160 - val_loss: 0.3815 - val_mse: 0.3815 - val_mae: 0.4755\n",
            "Epoch 261/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1855 - mse: 0.1855 - mae: 0.3329 - val_loss: 0.4585 - val_mse: 0.4585 - val_mae: 0.5257\n",
            "Epoch 262/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1757 - mse: 0.1757 - mae: 0.3230 - val_loss: 0.3882 - val_mse: 0.3882 - val_mae: 0.4698\n",
            "Epoch 263/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1701 - mse: 0.1701 - mae: 0.3132 - val_loss: 0.4220 - val_mse: 0.4220 - val_mae: 0.5031\n",
            "Epoch 264/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1596 - mse: 0.1596 - mae: 0.3109 - val_loss: 0.3974 - val_mse: 0.3974 - val_mae: 0.4808\n",
            "Epoch 265/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1632 - mse: 0.1632 - mae: 0.3027 - val_loss: 0.4192 - val_mse: 0.4192 - val_mae: 0.4960\n",
            "Epoch 266/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1572 - mse: 0.1572 - mae: 0.3001 - val_loss: 0.4045 - val_mse: 0.4045 - val_mae: 0.4865\n",
            "Epoch 267/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1628 - mse: 0.1628 - mae: 0.3172 - val_loss: 0.3826 - val_mse: 0.3826 - val_mae: 0.4727\n",
            "Epoch 268/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1746 - mse: 0.1746 - mae: 0.3240 - val_loss: 0.4098 - val_mse: 0.4098 - val_mae: 0.4965\n",
            "Epoch 269/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1666 - mse: 0.1666 - mae: 0.3154 - val_loss: 0.4035 - val_mse: 0.4035 - val_mae: 0.4884\n",
            "Epoch 270/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1553 - mse: 0.1553 - mae: 0.3059 - val_loss: 0.4533 - val_mse: 0.4533 - val_mae: 0.5124\n",
            "Epoch 271/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.1632 - mse: 0.1632 - mae: 0.3109 - val_loss: 0.4082 - val_mse: 0.4082 - val_mae: 0.4916\n",
            "Epoch 272/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1753 - mse: 0.1753 - mae: 0.3183 - val_loss: 0.4141 - val_mse: 0.4141 - val_mae: 0.4885\n",
            "Epoch 273/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1669 - mse: 0.1669 - mae: 0.3147 - val_loss: 0.4093 - val_mse: 0.4093 - val_mae: 0.4885\n",
            "Epoch 274/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1689 - mse: 0.1689 - mae: 0.3121 - val_loss: 0.4137 - val_mse: 0.4137 - val_mae: 0.4950\n",
            "Epoch 275/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1660 - mse: 0.1660 - mae: 0.3150 - val_loss: 0.4833 - val_mse: 0.4833 - val_mae: 0.5471\n",
            "Epoch 276/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2183 - mse: 0.2183 - mae: 0.3631 - val_loss: 0.3898 - val_mse: 0.3898 - val_mae: 0.4891\n",
            "Epoch 277/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1728 - mse: 0.1728 - mae: 0.3260 - val_loss: 0.4118 - val_mse: 0.4118 - val_mae: 0.4867\n",
            "Epoch 278/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1963 - mse: 0.1963 - mae: 0.3376 - val_loss: 0.4846 - val_mse: 0.4846 - val_mae: 0.5356\n",
            "Epoch 279/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2397 - mse: 0.2397 - mae: 0.3808 - val_loss: 0.4006 - val_mse: 0.4006 - val_mae: 0.4842\n",
            "Epoch 280/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1663 - mse: 0.1663 - mae: 0.3138 - val_loss: 0.4193 - val_mse: 0.4193 - val_mae: 0.4904\n",
            "Epoch 281/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1595 - mse: 0.1595 - mae: 0.3116 - val_loss: 0.4095 - val_mse: 0.4095 - val_mae: 0.4869\n",
            "Epoch 282/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1608 - mse: 0.1608 - mae: 0.3061 - val_loss: 0.4004 - val_mse: 0.4004 - val_mae: 0.4845\n",
            "Epoch 283/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1572 - mse: 0.1572 - mae: 0.3054 - val_loss: 0.4228 - val_mse: 0.4228 - val_mae: 0.4879\n",
            "Epoch 284/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1572 - mse: 0.1572 - mae: 0.3007 - val_loss: 0.5118 - val_mse: 0.5118 - val_mae: 0.5526\n",
            "Epoch 285/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2273 - mse: 0.2273 - mae: 0.3734 - val_loss: 0.4036 - val_mse: 0.4036 - val_mae: 0.4886\n",
            "Epoch 286/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1762 - mse: 0.1762 - mae: 0.3203 - val_loss: 0.4024 - val_mse: 0.4024 - val_mae: 0.4875\n",
            "Epoch 287/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1719 - mse: 0.1719 - mae: 0.3155 - val_loss: 0.5167 - val_mse: 0.5167 - val_mae: 0.5571\n",
            "Epoch 288/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1901 - mse: 0.1901 - mae: 0.3381 - val_loss: 0.4296 - val_mse: 0.4296 - val_mae: 0.5120\n",
            "Epoch 289/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1632 - mse: 0.1632 - mae: 0.3075 - val_loss: 0.4461 - val_mse: 0.4461 - val_mae: 0.5052\n",
            "Epoch 290/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1650 - mse: 0.1650 - mae: 0.3105 - val_loss: 0.3947 - val_mse: 0.3947 - val_mae: 0.4880\n",
            "Epoch 291/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1760 - mse: 0.1760 - mae: 0.3205 - val_loss: 0.3877 - val_mse: 0.3877 - val_mae: 0.4755\n",
            "Epoch 292/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2056 - mse: 0.2056 - mae: 0.3447 - val_loss: 0.3938 - val_mse: 0.3938 - val_mae: 0.4753\n",
            "Epoch 293/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1697 - mse: 0.1697 - mae: 0.3174 - val_loss: 0.3860 - val_mse: 0.3860 - val_mae: 0.4792\n",
            "Epoch 294/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1680 - mse: 0.1680 - mae: 0.3112 - val_loss: 0.4436 - val_mse: 0.4436 - val_mae: 0.5142\n",
            "Epoch 295/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1692 - mse: 0.1692 - mae: 0.3092 - val_loss: 0.4082 - val_mse: 0.4082 - val_mae: 0.4951\n",
            "Epoch 296/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1756 - mse: 0.1756 - mae: 0.3200 - val_loss: 0.3875 - val_mse: 0.3875 - val_mae: 0.4679\n",
            "Epoch 297/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1509 - mse: 0.1509 - mae: 0.2996 - val_loss: 0.3862 - val_mse: 0.3862 - val_mae: 0.4694\n",
            "Epoch 298/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1525 - mse: 0.1525 - mae: 0.2946 - val_loss: 0.3896 - val_mse: 0.3896 - val_mae: 0.4710\n",
            "Epoch 299/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1392 - mse: 0.1392 - mae: 0.2853 - val_loss: 0.3860 - val_mse: 0.3860 - val_mae: 0.4747\n",
            "Epoch 300/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1737 - mse: 0.1737 - mae: 0.3214 - val_loss: 0.4198 - val_mse: 0.4198 - val_mae: 0.5024\n",
            "Epoch 301/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1575 - mse: 0.1575 - mae: 0.3047 - val_loss: 0.3806 - val_mse: 0.3806 - val_mae: 0.4725\n",
            "Epoch 302/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1640 - mse: 0.1640 - mae: 0.3059 - val_loss: 0.3947 - val_mse: 0.3947 - val_mae: 0.4777\n",
            "Epoch 303/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1461 - mse: 0.1461 - mae: 0.2860 - val_loss: 0.4434 - val_mse: 0.4434 - val_mae: 0.5005\n",
            "Epoch 304/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1733 - mse: 0.1733 - mae: 0.3125 - val_loss: 0.4360 - val_mse: 0.4360 - val_mae: 0.5030\n",
            "Epoch 305/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1637 - mse: 0.1637 - mae: 0.3139 - val_loss: 0.3999 - val_mse: 0.3999 - val_mae: 0.4873\n",
            "Epoch 306/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1578 - mse: 0.1578 - mae: 0.3095 - val_loss: 0.4095 - val_mse: 0.4095 - val_mae: 0.4864\n",
            "Epoch 307/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1676 - mse: 0.1676 - mae: 0.3092 - val_loss: 0.4094 - val_mse: 0.4094 - val_mae: 0.4820\n",
            "Epoch 308/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1430 - mse: 0.1430 - mae: 0.2855 - val_loss: 0.4176 - val_mse: 0.4176 - val_mae: 0.4931\n",
            "Epoch 309/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1479 - mse: 0.1479 - mae: 0.2917 - val_loss: 0.4048 - val_mse: 0.4048 - val_mae: 0.4848\n",
            "Epoch 310/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1734 - mse: 0.1734 - mae: 0.3166 - val_loss: 0.4297 - val_mse: 0.4297 - val_mae: 0.4942\n",
            "Epoch 311/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1568 - mse: 0.1568 - mae: 0.2957 - val_loss: 0.4451 - val_mse: 0.4451 - val_mae: 0.5136\n",
            "Epoch 312/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1567 - mse: 0.1567 - mae: 0.3041 - val_loss: 0.5046 - val_mse: 0.5046 - val_mae: 0.5528\n",
            "Epoch 313/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2090 - mse: 0.2090 - mae: 0.3559 - val_loss: 0.4299 - val_mse: 0.4299 - val_mae: 0.5064\n",
            "Epoch 314/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1530 - mse: 0.1530 - mae: 0.3011 - val_loss: 0.4049 - val_mse: 0.4049 - val_mae: 0.4792\n",
            "Epoch 315/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1295 - mse: 0.1295 - mae: 0.2773 - val_loss: 0.4651 - val_mse: 0.4651 - val_mae: 0.5211\n",
            "Epoch 316/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1826 - mse: 0.1826 - mae: 0.3254 - val_loss: 0.4073 - val_mse: 0.4073 - val_mae: 0.4851\n",
            "Epoch 317/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1663 - mse: 0.1663 - mae: 0.3128 - val_loss: 0.4137 - val_mse: 0.4137 - val_mae: 0.4893\n",
            "Epoch 318/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1417 - mse: 0.1417 - mae: 0.2936 - val_loss: 0.4055 - val_mse: 0.4055 - val_mae: 0.4957\n",
            "Epoch 319/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1558 - mse: 0.1558 - mae: 0.3037 - val_loss: 0.4402 - val_mse: 0.4402 - val_mae: 0.5092\n",
            "Epoch 320/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1768 - mse: 0.1768 - mae: 0.3210 - val_loss: 0.4122 - val_mse: 0.4122 - val_mae: 0.4917\n",
            "Epoch 321/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.1720 - mse: 0.1720 - mae: 0.3225 - val_loss: 0.3991 - val_mse: 0.3991 - val_mae: 0.4699\n",
            "Epoch 322/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1812 - mse: 0.1812 - mae: 0.3209 - val_loss: 0.4145 - val_mse: 0.4145 - val_mae: 0.4821\n",
            "Epoch 323/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1509 - mse: 0.1509 - mae: 0.2939 - val_loss: 0.4258 - val_mse: 0.4258 - val_mae: 0.4911\n",
            "Epoch 324/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1369 - mse: 0.1369 - mae: 0.2824 - val_loss: 0.3955 - val_mse: 0.3955 - val_mae: 0.4779\n",
            "Epoch 325/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1503 - mse: 0.1503 - mae: 0.2926 - val_loss: 0.3954 - val_mse: 0.3954 - val_mae: 0.4771\n",
            "Epoch 326/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1511 - mse: 0.1511 - mae: 0.2978 - val_loss: 0.3993 - val_mse: 0.3993 - val_mae: 0.4695\n",
            "Epoch 327/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1403 - mse: 0.1403 - mae: 0.2896 - val_loss: 0.3948 - val_mse: 0.3948 - val_mae: 0.4780\n",
            "Epoch 328/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1552 - mse: 0.1552 - mae: 0.3058 - val_loss: 0.4141 - val_mse: 0.4141 - val_mae: 0.4939\n",
            "Epoch 329/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1667 - mse: 0.1667 - mae: 0.3197 - val_loss: 0.4525 - val_mse: 0.4525 - val_mae: 0.5116\n",
            "Epoch 330/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1658 - mse: 0.1658 - mae: 0.3165 - val_loss: 0.4122 - val_mse: 0.4122 - val_mae: 0.4951\n",
            "Epoch 331/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1800 - mse: 0.1800 - mae: 0.3172 - val_loss: 0.3848 - val_mse: 0.3848 - val_mae: 0.4773\n",
            "Epoch 332/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1603 - mse: 0.1603 - mae: 0.3126 - val_loss: 0.3868 - val_mse: 0.3868 - val_mae: 0.4740\n",
            "Epoch 333/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1444 - mse: 0.1444 - mae: 0.2873 - val_loss: 0.4023 - val_mse: 0.4023 - val_mae: 0.4803\n",
            "Epoch 334/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1738 - mse: 0.1738 - mae: 0.3132 - val_loss: 0.4045 - val_mse: 0.4045 - val_mae: 0.4807\n",
            "Epoch 335/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1531 - mse: 0.1531 - mae: 0.2978 - val_loss: 0.4031 - val_mse: 0.4031 - val_mae: 0.4937\n",
            "Epoch 336/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1523 - mse: 0.1523 - mae: 0.2967 - val_loss: 0.3946 - val_mse: 0.3946 - val_mae: 0.4855\n",
            "Epoch 337/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1883 - mse: 0.1883 - mae: 0.3277 - val_loss: 0.4064 - val_mse: 0.4064 - val_mae: 0.4851\n",
            "Epoch 338/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1720 - mse: 0.1720 - mae: 0.3133 - val_loss: 0.3953 - val_mse: 0.3953 - val_mae: 0.4772\n",
            "Epoch 339/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1557 - mse: 0.1557 - mae: 0.3066 - val_loss: 0.3947 - val_mse: 0.3947 - val_mae: 0.4827\n",
            "Epoch 340/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1452 - mse: 0.1452 - mae: 0.2896 - val_loss: 0.3986 - val_mse: 0.3986 - val_mae: 0.4827\n",
            "Epoch 341/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1603 - mse: 0.1603 - mae: 0.3108 - val_loss: 0.4094 - val_mse: 0.4094 - val_mae: 0.4910\n",
            "Epoch 342/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1815 - mse: 0.1815 - mae: 0.3213 - val_loss: 0.4118 - val_mse: 0.4118 - val_mae: 0.4907\n",
            "Epoch 343/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1678 - mse: 0.1678 - mae: 0.3153 - val_loss: 0.4174 - val_mse: 0.4174 - val_mae: 0.4950\n",
            "Epoch 344/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2105 - mse: 0.2105 - mae: 0.3654 - val_loss: 0.4280 - val_mse: 0.4280 - val_mae: 0.4959\n",
            "Epoch 345/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1712 - mse: 0.1712 - mae: 0.3171 - val_loss: 0.4370 - val_mse: 0.4370 - val_mae: 0.4975\n",
            "Epoch 346/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1749 - mse: 0.1749 - mae: 0.3340 - val_loss: 0.4120 - val_mse: 0.4120 - val_mae: 0.4843\n",
            "Epoch 347/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1489 - mse: 0.1489 - mae: 0.2981 - val_loss: 0.4141 - val_mse: 0.4141 - val_mae: 0.4906\n",
            "Epoch 348/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1803 - mse: 0.1803 - mae: 0.3329 - val_loss: 0.5062 - val_mse: 0.5062 - val_mae: 0.5596\n",
            "Epoch 349/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1805 - mse: 0.1805 - mae: 0.3264 - val_loss: 0.4230 - val_mse: 0.4230 - val_mae: 0.4998\n",
            "Epoch 350/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1458 - mse: 0.1458 - mae: 0.2878 - val_loss: 0.4450 - val_mse: 0.4450 - val_mae: 0.5083\n",
            "Epoch 351/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1594 - mse: 0.1594 - mae: 0.3073 - val_loss: 0.4192 - val_mse: 0.4192 - val_mae: 0.4890\n",
            "Epoch 352/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1453 - mse: 0.1453 - mae: 0.2880 - val_loss: 0.4261 - val_mse: 0.4261 - val_mae: 0.5036\n",
            "Epoch 353/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1815 - mse: 0.1815 - mae: 0.3250 - val_loss: 0.4026 - val_mse: 0.4026 - val_mae: 0.4847\n",
            "Epoch 354/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1565 - mse: 0.1565 - mae: 0.3007 - val_loss: 0.4265 - val_mse: 0.4265 - val_mae: 0.5056\n",
            "Epoch 355/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1634 - mse: 0.1634 - mae: 0.3132 - val_loss: 0.4104 - val_mse: 0.4104 - val_mae: 0.4944\n",
            "Epoch 356/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1493 - mse: 0.1493 - mae: 0.2988 - val_loss: 0.4259 - val_mse: 0.4259 - val_mae: 0.5019\n",
            "Epoch 357/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1443 - mse: 0.1443 - mae: 0.2883 - val_loss: 0.3982 - val_mse: 0.3982 - val_mae: 0.4856\n",
            "Epoch 358/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1383 - mse: 0.1383 - mae: 0.2835 - val_loss: 0.4206 - val_mse: 0.4206 - val_mae: 0.5010\n",
            "Epoch 359/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1803 - mse: 0.1803 - mae: 0.3261 - val_loss: 0.4324 - val_mse: 0.4324 - val_mae: 0.5085\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.4796 - mse: 0.4796 - mae: 0.5141\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 23ms/step - loss: 10.8643 - mse: 10.8643 - mae: 2.8135 - val_loss: 5.8645 - val_mse: 5.8645 - val_mae: 1.8340\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 3.6358 - mse: 3.6358 - mae: 1.5149 - val_loss: 3.8899 - val_mse: 3.8899 - val_mae: 1.5481\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 2.6990 - mse: 2.6990 - mae: 1.3374 - val_loss: 3.3306 - val_mse: 3.3306 - val_mae: 1.3961\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 2.1665 - mse: 2.1665 - mae: 1.1757 - val_loss: 2.9338 - val_mse: 2.9338 - val_mae: 1.3144\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 1.9307 - mse: 1.9307 - mae: 1.1120 - val_loss: 2.6672 - val_mse: 2.6672 - val_mae: 1.2513\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 1.8811 - mse: 1.8811 - mae: 1.1043 - val_loss: 2.5131 - val_mse: 2.5131 - val_mae: 1.2103\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.7914 - mse: 1.7914 - mae: 1.0642 - val_loss: 2.3635 - val_mse: 2.3635 - val_mae: 1.1743\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.6119 - mse: 1.6119 - mae: 1.0193 - val_loss: 2.2210 - val_mse: 2.2210 - val_mae: 1.1339\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.6073 - mse: 1.6073 - mae: 1.0067 - val_loss: 2.0537 - val_mse: 2.0537 - val_mae: 1.1043\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 1.4851 - mse: 1.4851 - mae: 0.9758 - val_loss: 1.8859 - val_mse: 1.8859 - val_mae: 1.0505\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 1.3906 - mse: 1.3906 - mae: 0.9537 - val_loss: 1.7480 - val_mse: 1.7480 - val_mae: 1.0160\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.3320 - mse: 1.3320 - mae: 0.9206 - val_loss: 1.7050 - val_mse: 1.7050 - val_mae: 0.9926\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.2071 - mse: 1.2071 - mae: 0.8670 - val_loss: 1.5790 - val_mse: 1.5790 - val_mae: 0.9502\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.1404 - mse: 1.1404 - mae: 0.8465 - val_loss: 1.4999 - val_mse: 1.4999 - val_mae: 0.9411\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.0724 - mse: 1.0724 - mae: 0.8141 - val_loss: 1.4202 - val_mse: 1.4202 - val_mae: 0.9054\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 1.1183 - mse: 1.1183 - mae: 0.8382 - val_loss: 1.5433 - val_mse: 1.5433 - val_mae: 0.9097\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.9457 - mse: 0.9457 - mae: 0.7728 - val_loss: 1.2993 - val_mse: 1.2993 - val_mae: 0.8800\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.8893 - mse: 0.8893 - mae: 0.7470 - val_loss: 1.2754 - val_mse: 1.2754 - val_mae: 0.8388\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.8961 - mse: 0.8961 - mae: 0.7381 - val_loss: 1.1853 - val_mse: 1.1853 - val_mae: 0.8478\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.8605 - mse: 0.8605 - mae: 0.7244 - val_loss: 1.1632 - val_mse: 1.1632 - val_mae: 0.8133\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.8128 - mse: 0.8128 - mae: 0.6990 - val_loss: 1.1209 - val_mse: 1.1209 - val_mae: 0.7938\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.8291 - mse: 0.8291 - mae: 0.7140 - val_loss: 1.0865 - val_mse: 1.0865 - val_mae: 0.8028\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7824 - mse: 0.7824 - mae: 0.6985 - val_loss: 0.9715 - val_mse: 0.9715 - val_mae: 0.7789\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7515 - mse: 0.7515 - mae: 0.6902 - val_loss: 1.0244 - val_mse: 1.0244 - val_mae: 0.7489\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7123 - mse: 0.7123 - mae: 0.6672 - val_loss: 0.9228 - val_mse: 0.9228 - val_mae: 0.7356\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6576 - mse: 0.6576 - mae: 0.6309 - val_loss: 0.9559 - val_mse: 0.9559 - val_mae: 0.7531\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.6439 - mse: 0.6439 - mae: 0.6383 - val_loss: 0.8326 - val_mse: 0.8326 - val_mae: 0.7177\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6236 - mse: 0.6236 - mae: 0.6227 - val_loss: 0.8641 - val_mse: 0.8641 - val_mae: 0.7133\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.6294 - mse: 0.6294 - mae: 0.6279 - val_loss: 0.8499 - val_mse: 0.8499 - val_mae: 0.6907\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6309 - mse: 0.6309 - mae: 0.6158 - val_loss: 0.7975 - val_mse: 0.7975 - val_mae: 0.6867\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.6202 - mse: 0.6202 - mae: 0.6152 - val_loss: 0.7837 - val_mse: 0.7837 - val_mae: 0.6628\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.5498 - mse: 0.5498 - mae: 0.5754 - val_loss: 0.6849 - val_mse: 0.6849 - val_mae: 0.6635\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4910 - mse: 0.4910 - mae: 0.5386 - val_loss: 0.8645 - val_mse: 0.8645 - val_mae: 0.6940\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.5745 - mse: 0.5745 - mae: 0.5890 - val_loss: 0.7746 - val_mse: 0.7746 - val_mae: 0.6559\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5284 - mse: 0.5284 - mae: 0.5642 - val_loss: 0.6711 - val_mse: 0.6711 - val_mae: 0.6401\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.5785 - mse: 0.5785 - mae: 0.5976 - val_loss: 0.6670 - val_mse: 0.6670 - val_mae: 0.6712\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5003 - mse: 0.5003 - mae: 0.5533 - val_loss: 0.6447 - val_mse: 0.6447 - val_mae: 0.6146\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5000 - mse: 0.5000 - mae: 0.5528 - val_loss: 0.6310 - val_mse: 0.6310 - val_mae: 0.5861\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4970 - mse: 0.4970 - mae: 0.5601 - val_loss: 0.6776 - val_mse: 0.6776 - val_mae: 0.6605\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4744 - mse: 0.4744 - mae: 0.5447 - val_loss: 0.6144 - val_mse: 0.6144 - val_mae: 0.6247\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4599 - mse: 0.4599 - mae: 0.5234 - val_loss: 0.6107 - val_mse: 0.6107 - val_mae: 0.6007\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4427 - mse: 0.4427 - mae: 0.5119 - val_loss: 0.6100 - val_mse: 0.6100 - val_mae: 0.5977\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4424 - mse: 0.4424 - mae: 0.5178 - val_loss: 0.6322 - val_mse: 0.6322 - val_mae: 0.5959\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4261 - mse: 0.4261 - mae: 0.5067 - val_loss: 0.5737 - val_mse: 0.5737 - val_mae: 0.5846\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4465 - mse: 0.4465 - mae: 0.5140 - val_loss: 0.5961 - val_mse: 0.5961 - val_mae: 0.5811\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4161 - mse: 0.4161 - mae: 0.5013 - val_loss: 0.5259 - val_mse: 0.5259 - val_mae: 0.5793\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3644 - mse: 0.3644 - mae: 0.4680 - val_loss: 0.5949 - val_mse: 0.5949 - val_mae: 0.5793\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4119 - mse: 0.4119 - mae: 0.4956 - val_loss: 0.5763 - val_mse: 0.5763 - val_mae: 0.5572\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4110 - mse: 0.4110 - mae: 0.4968 - val_loss: 0.5329 - val_mse: 0.5329 - val_mae: 0.5593\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4018 - mse: 0.4018 - mae: 0.4956 - val_loss: 0.5022 - val_mse: 0.5022 - val_mae: 0.5535\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3667 - mse: 0.3667 - mae: 0.4676 - val_loss: 0.5469 - val_mse: 0.5469 - val_mae: 0.5634\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3903 - mse: 0.3903 - mae: 0.4839 - val_loss: 0.6486 - val_mse: 0.6486 - val_mae: 0.5921\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3964 - mse: 0.3964 - mae: 0.4851 - val_loss: 0.4780 - val_mse: 0.4780 - val_mae: 0.5483\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4021 - mse: 0.4021 - mae: 0.4786 - val_loss: 0.4681 - val_mse: 0.4681 - val_mae: 0.5358\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3764 - mse: 0.3764 - mae: 0.4727 - val_loss: 0.6342 - val_mse: 0.6342 - val_mae: 0.6056\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3872 - mse: 0.3872 - mae: 0.4854 - val_loss: 0.4589 - val_mse: 0.4589 - val_mae: 0.5143\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3721 - mse: 0.3721 - mae: 0.4694 - val_loss: 0.5096 - val_mse: 0.5096 - val_mae: 0.5334\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3522 - mse: 0.3522 - mae: 0.4569 - val_loss: 0.4406 - val_mse: 0.4406 - val_mae: 0.5081\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3391 - mse: 0.3391 - mae: 0.4427 - val_loss: 0.5253 - val_mse: 0.5253 - val_mae: 0.5476\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3395 - mse: 0.3395 - mae: 0.4411 - val_loss: 0.5848 - val_mse: 0.5848 - val_mae: 0.5667\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3412 - mse: 0.3412 - mae: 0.4487 - val_loss: 0.4818 - val_mse: 0.4818 - val_mae: 0.5310\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3210 - mse: 0.3210 - mae: 0.4352 - val_loss: 0.4589 - val_mse: 0.4589 - val_mae: 0.5104\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3329 - mse: 0.3329 - mae: 0.4435 - val_loss: 0.4771 - val_mse: 0.4771 - val_mae: 0.5200\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3594 - mse: 0.3594 - mae: 0.4667 - val_loss: 0.4145 - val_mse: 0.4145 - val_mae: 0.4955\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3791 - mse: 0.3791 - mae: 0.4729 - val_loss: 0.4098 - val_mse: 0.4098 - val_mae: 0.4924\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3570 - mse: 0.3570 - mae: 0.4624 - val_loss: 0.4676 - val_mse: 0.4676 - val_mae: 0.5167\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3259 - mse: 0.3259 - mae: 0.4447 - val_loss: 0.5046 - val_mse: 0.5046 - val_mae: 0.5300\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3416 - mse: 0.3416 - mae: 0.4464 - val_loss: 0.4697 - val_mse: 0.4697 - val_mae: 0.5122\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3393 - mse: 0.3393 - mae: 0.4429 - val_loss: 0.4588 - val_mse: 0.4588 - val_mae: 0.4995\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2878 - mse: 0.2878 - mae: 0.4147 - val_loss: 0.4471 - val_mse: 0.4471 - val_mae: 0.4988\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3128 - mse: 0.3128 - mae: 0.4388 - val_loss: 0.4922 - val_mse: 0.4922 - val_mae: 0.5214\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3208 - mse: 0.3208 - mae: 0.4425 - val_loss: 0.4923 - val_mse: 0.4923 - val_mae: 0.5214\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3106 - mse: 0.3106 - mae: 0.4369 - val_loss: 0.4024 - val_mse: 0.4024 - val_mae: 0.4793\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3168 - mse: 0.3168 - mae: 0.4279 - val_loss: 0.4031 - val_mse: 0.4031 - val_mae: 0.4794\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3052 - mse: 0.3052 - mae: 0.4299 - val_loss: 0.4341 - val_mse: 0.4341 - val_mae: 0.4888\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2663 - mse: 0.2663 - mae: 0.3970 - val_loss: 0.4272 - val_mse: 0.4272 - val_mae: 0.4874\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2834 - mse: 0.2834 - mae: 0.4045 - val_loss: 0.4582 - val_mse: 0.4582 - val_mae: 0.5053\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2928 - mse: 0.2928 - mae: 0.4154 - val_loss: 0.5290 - val_mse: 0.5290 - val_mae: 0.5524\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3325 - mse: 0.3325 - mae: 0.4513 - val_loss: 0.4496 - val_mse: 0.4496 - val_mae: 0.4969\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3205 - mse: 0.3205 - mae: 0.4301 - val_loss: 0.4296 - val_mse: 0.4296 - val_mae: 0.4827\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2819 - mse: 0.2819 - mae: 0.4138 - val_loss: 0.4215 - val_mse: 0.4215 - val_mae: 0.4884\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2781 - mse: 0.2781 - mae: 0.3948 - val_loss: 0.4271 - val_mse: 0.4271 - val_mae: 0.5160\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2766 - mse: 0.2766 - mae: 0.4051 - val_loss: 0.4361 - val_mse: 0.4361 - val_mae: 0.4885\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3085 - mse: 0.3085 - mae: 0.4342 - val_loss: 0.4760 - val_mse: 0.4760 - val_mae: 0.5112\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3051 - mse: 0.3051 - mae: 0.4312 - val_loss: 0.4651 - val_mse: 0.4651 - val_mae: 0.5069\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2789 - mse: 0.2789 - mae: 0.4055 - val_loss: 0.4332 - val_mse: 0.4332 - val_mae: 0.4913\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2949 - mse: 0.2949 - mae: 0.4192 - val_loss: 0.4334 - val_mse: 0.4334 - val_mae: 0.5032\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2875 - mse: 0.2875 - mae: 0.4174 - val_loss: 0.3970 - val_mse: 0.3970 - val_mae: 0.4745\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2618 - mse: 0.2618 - mae: 0.3938 - val_loss: 0.4403 - val_mse: 0.4403 - val_mae: 0.5000\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2943 - mse: 0.2943 - mae: 0.4254 - val_loss: 0.4556 - val_mse: 0.4556 - val_mae: 0.5271\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3235 - mse: 0.3235 - mae: 0.4484 - val_loss: 0.5430 - val_mse: 0.5430 - val_mae: 0.5441\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2949 - mse: 0.2949 - mae: 0.4185 - val_loss: 0.4212 - val_mse: 0.4212 - val_mae: 0.4967\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3116 - mse: 0.3116 - mae: 0.4245 - val_loss: 0.4821 - val_mse: 0.4821 - val_mae: 0.5165\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2813 - mse: 0.2813 - mae: 0.4099 - val_loss: 0.4057 - val_mse: 0.4057 - val_mae: 0.5028\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2556 - mse: 0.2556 - mae: 0.3831 - val_loss: 0.4142 - val_mse: 0.4142 - val_mae: 0.4724\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2533 - mse: 0.2533 - mae: 0.3915 - val_loss: 0.4000 - val_mse: 0.4000 - val_mae: 0.4667\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2317 - mse: 0.2317 - mae: 0.3679 - val_loss: 0.4003 - val_mse: 0.4003 - val_mae: 0.4723\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2448 - mse: 0.2448 - mae: 0.3728 - val_loss: 0.4254 - val_mse: 0.4254 - val_mae: 0.4978\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2715 - mse: 0.2715 - mae: 0.4022 - val_loss: 0.4213 - val_mse: 0.4213 - val_mae: 0.4740\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2618 - mse: 0.2618 - mae: 0.3976 - val_loss: 0.4073 - val_mse: 0.4073 - val_mae: 0.4776\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2715 - mse: 0.2715 - mae: 0.4022 - val_loss: 0.4583 - val_mse: 0.4583 - val_mae: 0.4985\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2373 - mse: 0.2373 - mae: 0.3740 - val_loss: 0.3934 - val_mse: 0.3934 - val_mae: 0.4743\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2674 - mse: 0.2674 - mae: 0.3998 - val_loss: 0.4168 - val_mse: 0.4168 - val_mae: 0.4995\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2772 - mse: 0.2772 - mae: 0.4066 - val_loss: 0.4686 - val_mse: 0.4686 - val_mae: 0.5014\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2574 - mse: 0.2574 - mae: 0.3890 - val_loss: 0.4296 - val_mse: 0.4296 - val_mae: 0.4845\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2545 - mse: 0.2545 - mae: 0.3915 - val_loss: 0.4689 - val_mse: 0.4689 - val_mae: 0.4988\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2506 - mse: 0.2506 - mae: 0.3936 - val_loss: 0.4239 - val_mse: 0.4239 - val_mae: 0.4878\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2395 - mse: 0.2395 - mae: 0.3853 - val_loss: 0.4353 - val_mse: 0.4353 - val_mae: 0.4850\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2358 - mse: 0.2358 - mae: 0.3743 - val_loss: 0.4375 - val_mse: 0.4375 - val_mae: 0.4811\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2506 - mse: 0.2506 - mae: 0.3837 - val_loss: 0.4191 - val_mse: 0.4191 - val_mae: 0.4897\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2358 - mse: 0.2358 - mae: 0.3788 - val_loss: 0.4043 - val_mse: 0.4043 - val_mae: 0.4840\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2465 - mse: 0.2465 - mae: 0.3823 - val_loss: 0.4296 - val_mse: 0.4296 - val_mae: 0.4776\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2449 - mse: 0.2449 - mae: 0.3865 - val_loss: 0.4059 - val_mse: 0.4059 - val_mae: 0.4648\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2332 - mse: 0.2332 - mae: 0.3748 - val_loss: 0.4640 - val_mse: 0.4640 - val_mae: 0.4944\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2254 - mse: 0.2254 - mae: 0.3576 - val_loss: 0.3980 - val_mse: 0.3980 - val_mae: 0.4589\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2379 - mse: 0.2379 - mae: 0.3794 - val_loss: 0.4435 - val_mse: 0.4435 - val_mae: 0.4760\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2399 - mse: 0.2399 - mae: 0.3758 - val_loss: 0.3955 - val_mse: 0.3955 - val_mae: 0.4629\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2270 - mse: 0.2270 - mae: 0.3582 - val_loss: 0.3803 - val_mse: 0.3803 - val_mae: 0.4489\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2211 - mse: 0.2211 - mae: 0.3495 - val_loss: 0.4384 - val_mse: 0.4384 - val_mae: 0.4936\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2951 - mse: 0.2951 - mae: 0.4242 - val_loss: 0.4079 - val_mse: 0.4079 - val_mae: 0.4995\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2719 - mse: 0.2719 - mae: 0.4061 - val_loss: 0.4495 - val_mse: 0.4495 - val_mae: 0.4857\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2497 - mse: 0.2497 - mae: 0.3873 - val_loss: 0.4290 - val_mse: 0.4290 - val_mae: 0.4979\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2424 - mse: 0.2424 - mae: 0.3809 - val_loss: 0.4547 - val_mse: 0.4547 - val_mae: 0.4885\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2353 - mse: 0.2353 - mae: 0.3788 - val_loss: 0.4386 - val_mse: 0.4386 - val_mae: 0.5156\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2878 - mse: 0.2878 - mae: 0.4239 - val_loss: 0.5854 - val_mse: 0.5854 - val_mae: 0.5646\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2978 - mse: 0.2978 - mae: 0.4275 - val_loss: 0.4229 - val_mse: 0.4229 - val_mae: 0.5103\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2254 - mse: 0.2254 - mae: 0.3691 - val_loss: 0.4303 - val_mse: 0.4303 - val_mae: 0.4659\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2363 - mse: 0.2363 - mae: 0.3809 - val_loss: 0.4470 - val_mse: 0.4470 - val_mae: 0.4869\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2373 - mse: 0.2373 - mae: 0.3765 - val_loss: 0.4204 - val_mse: 0.4204 - val_mae: 0.4725\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2235 - mse: 0.2235 - mae: 0.3667 - val_loss: 0.3968 - val_mse: 0.3968 - val_mae: 0.4675\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2213 - mse: 0.2213 - mae: 0.3627 - val_loss: 0.4324 - val_mse: 0.4324 - val_mae: 0.4698\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2233 - mse: 0.2233 - mae: 0.3648 - val_loss: 0.4242 - val_mse: 0.4242 - val_mae: 0.4685\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2273 - mse: 0.2273 - mae: 0.3692 - val_loss: 0.4344 - val_mse: 0.4344 - val_mae: 0.4838\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2256 - mse: 0.2256 - mae: 0.3736 - val_loss: 0.4731 - val_mse: 0.4731 - val_mae: 0.4998\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2136 - mse: 0.2136 - mae: 0.3593 - val_loss: 0.4109 - val_mse: 0.4109 - val_mae: 0.4791\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2257 - mse: 0.2257 - mae: 0.3670 - val_loss: 0.3956 - val_mse: 0.3956 - val_mae: 0.4540\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2075 - mse: 0.2075 - mae: 0.3552 - val_loss: 0.4138 - val_mse: 0.4138 - val_mae: 0.4773\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2265 - mse: 0.2265 - mae: 0.3650 - val_loss: 0.4312 - val_mse: 0.4312 - val_mae: 0.4685\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2278 - mse: 0.2278 - mae: 0.3677 - val_loss: 0.3922 - val_mse: 0.3922 - val_mae: 0.4695\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2227 - mse: 0.2227 - mae: 0.3677 - val_loss: 0.3931 - val_mse: 0.3931 - val_mae: 0.4617\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2350 - mse: 0.2350 - mae: 0.3767 - val_loss: 0.4058 - val_mse: 0.4058 - val_mae: 0.4805\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2116 - mse: 0.2116 - mae: 0.3548 - val_loss: 0.5668 - val_mse: 0.5668 - val_mae: 0.5513\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2140 - mse: 0.2140 - mae: 0.3560 - val_loss: 0.4067 - val_mse: 0.4067 - val_mae: 0.4847\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2261 - mse: 0.2261 - mae: 0.3684 - val_loss: 0.3849 - val_mse: 0.3849 - val_mae: 0.4592\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1977 - mse: 0.1977 - mae: 0.3405 - val_loss: 0.4551 - val_mse: 0.4551 - val_mae: 0.4849\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2266 - mse: 0.2266 - mae: 0.3591 - val_loss: 0.4072 - val_mse: 0.4072 - val_mae: 0.4668\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2064 - mse: 0.2064 - mae: 0.3503 - val_loss: 0.4046 - val_mse: 0.4046 - val_mae: 0.4485\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2078 - mse: 0.2078 - mae: 0.3533 - val_loss: 0.4051 - val_mse: 0.4051 - val_mae: 0.4566\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2212 - mse: 0.2212 - mae: 0.3505 - val_loss: 0.4052 - val_mse: 0.4052 - val_mae: 0.4631\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1968 - mse: 0.1968 - mae: 0.3418 - val_loss: 0.4312 - val_mse: 0.4312 - val_mae: 0.4733\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2133 - mse: 0.2133 - mae: 0.3507 - val_loss: 0.4417 - val_mse: 0.4417 - val_mae: 0.5158\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2223 - mse: 0.2223 - mae: 0.3650 - val_loss: 0.3840 - val_mse: 0.3840 - val_mae: 0.4487\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1963 - mse: 0.1963 - mae: 0.3484 - val_loss: 0.3955 - val_mse: 0.3955 - val_mae: 0.4641\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2144 - mse: 0.2144 - mae: 0.3569 - val_loss: 0.4340 - val_mse: 0.4340 - val_mae: 0.4767\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2055 - mse: 0.2055 - mae: 0.3481 - val_loss: 0.3830 - val_mse: 0.3830 - val_mae: 0.4583\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1956 - mse: 0.1956 - mae: 0.3432 - val_loss: 0.3672 - val_mse: 0.3672 - val_mae: 0.4440\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2185 - mse: 0.2185 - mae: 0.3657 - val_loss: 0.4363 - val_mse: 0.4363 - val_mae: 0.4645\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1858 - mse: 0.1858 - mae: 0.3316 - val_loss: 0.3979 - val_mse: 0.3979 - val_mae: 0.4556\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1910 - mse: 0.1910 - mae: 0.3413 - val_loss: 0.3796 - val_mse: 0.3796 - val_mae: 0.4636\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2147 - mse: 0.2147 - mae: 0.3577 - val_loss: 0.4836 - val_mse: 0.4836 - val_mae: 0.5002\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2492 - mse: 0.2492 - mae: 0.3903 - val_loss: 0.4212 - val_mse: 0.4212 - val_mae: 0.5032\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2222 - mse: 0.2222 - mae: 0.3598 - val_loss: 0.4496 - val_mse: 0.4496 - val_mae: 0.5250\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2161 - mse: 0.2161 - mae: 0.3619 - val_loss: 0.4297 - val_mse: 0.4297 - val_mae: 0.4728\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1900 - mse: 0.1900 - mae: 0.3374 - val_loss: 0.4510 - val_mse: 0.4510 - val_mae: 0.4987\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2342 - mse: 0.2342 - mae: 0.3771 - val_loss: 0.4087 - val_mse: 0.4087 - val_mae: 0.4919\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2069 - mse: 0.2069 - mae: 0.3512 - val_loss: 0.3982 - val_mse: 0.3982 - val_mae: 0.4648\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2116 - mse: 0.2116 - mae: 0.3507 - val_loss: 0.4149 - val_mse: 0.4149 - val_mae: 0.4675\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2250 - mse: 0.2250 - mae: 0.3672 - val_loss: 0.4185 - val_mse: 0.4185 - val_mae: 0.4799\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2327 - mse: 0.2327 - mae: 0.3747 - val_loss: 0.4970 - val_mse: 0.4970 - val_mae: 0.5054\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1951 - mse: 0.1951 - mae: 0.3355 - val_loss: 0.4269 - val_mse: 0.4269 - val_mae: 0.4716\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1983 - mse: 0.1983 - mae: 0.3417 - val_loss: 0.4328 - val_mse: 0.4328 - val_mae: 0.4757\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.1960 - mse: 0.1960 - mae: 0.3382 - val_loss: 0.4194 - val_mse: 0.4194 - val_mae: 0.4562\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1959 - mse: 0.1959 - mae: 0.3473 - val_loss: 0.4370 - val_mse: 0.4370 - val_mae: 0.4881\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1783 - mse: 0.1783 - mae: 0.3237 - val_loss: 0.4265 - val_mse: 0.4265 - val_mae: 0.4708\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1889 - mse: 0.1889 - mae: 0.3340 - val_loss: 0.4109 - val_mse: 0.4109 - val_mae: 0.4585\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1879 - mse: 0.1879 - mae: 0.3297 - val_loss: 0.4093 - val_mse: 0.4093 - val_mae: 0.4784\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1974 - mse: 0.1974 - mae: 0.3433 - val_loss: 0.5711 - val_mse: 0.5711 - val_mae: 0.5660\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2196 - mse: 0.2196 - mae: 0.3698 - val_loss: 0.3950 - val_mse: 0.3950 - val_mae: 0.4613\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2039 - mse: 0.2039 - mae: 0.3519 - val_loss: 0.4230 - val_mse: 0.4230 - val_mae: 0.4664\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2280 - mse: 0.2280 - mae: 0.3619 - val_loss: 0.4234 - val_mse: 0.4234 - val_mae: 0.4802\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1886 - mse: 0.1886 - mae: 0.3358 - val_loss: 0.4143 - val_mse: 0.4143 - val_mae: 0.4751\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1852 - mse: 0.1852 - mae: 0.3292 - val_loss: 0.4523 - val_mse: 0.4523 - val_mae: 0.4840\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2195 - mse: 0.2195 - mae: 0.3653 - val_loss: 0.3966 - val_mse: 0.3966 - val_mae: 0.4480\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1841 - mse: 0.1841 - mae: 0.3331 - val_loss: 0.3757 - val_mse: 0.3757 - val_mae: 0.4503\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1750 - mse: 0.1750 - mae: 0.3244 - val_loss: 0.4386 - val_mse: 0.4386 - val_mae: 0.4691\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.1786 - mse: 0.1786 - mae: 0.3230 - val_loss: 0.4101 - val_mse: 0.4101 - val_mae: 0.4769\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2054 - mse: 0.2054 - mae: 0.3464 - val_loss: 0.4187 - val_mse: 0.4187 - val_mae: 0.4733\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1712 - mse: 0.1712 - mae: 0.3148 - val_loss: 0.4247 - val_mse: 0.4247 - val_mae: 0.4699\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1943 - mse: 0.1943 - mae: 0.3334 - val_loss: 0.4118 - val_mse: 0.4118 - val_mae: 0.4849\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2045 - mse: 0.2045 - mae: 0.3509 - val_loss: 0.5277 - val_mse: 0.5277 - val_mae: 0.5289\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2295 - mse: 0.2295 - mae: 0.3637 - val_loss: 0.3998 - val_mse: 0.3998 - val_mae: 0.4491\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1821 - mse: 0.1821 - mae: 0.3300 - val_loss: 0.4514 - val_mse: 0.4514 - val_mae: 0.4811\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1700 - mse: 0.1700 - mae: 0.3151 - val_loss: 0.4346 - val_mse: 0.4346 - val_mae: 0.4726\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1838 - mse: 0.1838 - mae: 0.3309 - val_loss: 0.4480 - val_mse: 0.4480 - val_mae: 0.4831\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1735 - mse: 0.1735 - mae: 0.3182 - val_loss: 0.4277 - val_mse: 0.4277 - val_mae: 0.4822\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1913 - mse: 0.1913 - mae: 0.3342 - val_loss: 0.4896 - val_mse: 0.4896 - val_mae: 0.4991\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2321 - mse: 0.2321 - mae: 0.3692 - val_loss: 0.4226 - val_mse: 0.4226 - val_mae: 0.4715\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1655 - mse: 0.1655 - mae: 0.3076 - val_loss: 0.4271 - val_mse: 0.4271 - val_mae: 0.4606\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1742 - mse: 0.1742 - mae: 0.3179 - val_loss: 0.4258 - val_mse: 0.4258 - val_mae: 0.4977\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1954 - mse: 0.1954 - mae: 0.3469 - val_loss: 0.4046 - val_mse: 0.4046 - val_mae: 0.4667\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1885 - mse: 0.1885 - mae: 0.3372 - val_loss: 0.4557 - val_mse: 0.4557 - val_mae: 0.4904\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1931 - mse: 0.1931 - mae: 0.3433 - val_loss: 0.4327 - val_mse: 0.4327 - val_mae: 0.4772\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1733 - mse: 0.1733 - mae: 0.3229 - val_loss: 0.4182 - val_mse: 0.4182 - val_mae: 0.4549\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2410 - mse: 0.2410 - mae: 0.3832 - val_loss: 0.3767 - val_mse: 0.3767 - val_mae: 0.4540\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2357 - mse: 0.2357 - mae: 0.3802 - val_loss: 0.4820 - val_mse: 0.4820 - val_mae: 0.5016\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2030 - mse: 0.2030 - mae: 0.3479 - val_loss: 0.4011 - val_mse: 0.4011 - val_mae: 0.4749\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2016 - mse: 0.2016 - mae: 0.3531 - val_loss: 0.4920 - val_mse: 0.4920 - val_mae: 0.5078\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1992 - mse: 0.1992 - mae: 0.3503 - val_loss: 0.3834 - val_mse: 0.3834 - val_mae: 0.4576\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1858 - mse: 0.1858 - mae: 0.3363 - val_loss: 0.4445 - val_mse: 0.4445 - val_mae: 0.4781\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1686 - mse: 0.1686 - mae: 0.3174 - val_loss: 0.3984 - val_mse: 0.3984 - val_mae: 0.4563\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1734 - mse: 0.1734 - mae: 0.3161 - val_loss: 0.4058 - val_mse: 0.4058 - val_mae: 0.4646\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1802 - mse: 0.1802 - mae: 0.3286 - val_loss: 0.5307 - val_mse: 0.5307 - val_mae: 0.5341\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2027 - mse: 0.2027 - mae: 0.3415 - val_loss: 0.4447 - val_mse: 0.4447 - val_mae: 0.4655\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2395 - mse: 0.2395 - mae: 0.3888 - val_loss: 0.4016 - val_mse: 0.4016 - val_mae: 0.4710\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1887 - mse: 0.1887 - mae: 0.3327 - val_loss: 0.4298 - val_mse: 0.4298 - val_mae: 0.4646\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1777 - mse: 0.1777 - mae: 0.3253 - val_loss: 0.4063 - val_mse: 0.4063 - val_mae: 0.4482\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1745 - mse: 0.1745 - mae: 0.3248 - val_loss: 0.4220 - val_mse: 0.4220 - val_mae: 0.4703\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1718 - mse: 0.1718 - mae: 0.3142 - val_loss: 0.4417 - val_mse: 0.4417 - val_mae: 0.4764\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1675 - mse: 0.1675 - mae: 0.3179 - val_loss: 0.4221 - val_mse: 0.4221 - val_mae: 0.4804\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1725 - mse: 0.1725 - mae: 0.3178 - val_loss: 0.5300 - val_mse: 0.5300 - val_mae: 0.5341\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1730 - mse: 0.1730 - mae: 0.3181 - val_loss: 0.4188 - val_mse: 0.4188 - val_mae: 0.4551\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1809 - mse: 0.1809 - mae: 0.3293 - val_loss: 0.4801 - val_mse: 0.4801 - val_mae: 0.4959\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1871 - mse: 0.1871 - mae: 0.3337 - val_loss: 0.4023 - val_mse: 0.4023 - val_mae: 0.4587\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2092 - mse: 0.2092 - mae: 0.3590 - val_loss: 0.4048 - val_mse: 0.4048 - val_mae: 0.4650\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1756 - mse: 0.1756 - mae: 0.3161 - val_loss: 0.4060 - val_mse: 0.4060 - val_mae: 0.4553\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1656 - mse: 0.1656 - mae: 0.3107 - val_loss: 0.4234 - val_mse: 0.4234 - val_mae: 0.4797\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1713 - mse: 0.1713 - mae: 0.3156 - val_loss: 0.4878 - val_mse: 0.4878 - val_mae: 0.5116\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1777 - mse: 0.1777 - mae: 0.3239 - val_loss: 0.4145 - val_mse: 0.4145 - val_mae: 0.4749\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1701 - mse: 0.1701 - mae: 0.3148 - val_loss: 0.4457 - val_mse: 0.4457 - val_mae: 0.4841\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1767 - mse: 0.1767 - mae: 0.3178 - val_loss: 0.4011 - val_mse: 0.4011 - val_mae: 0.4751\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1740 - mse: 0.1740 - mae: 0.3229 - val_loss: 0.3986 - val_mse: 0.3986 - val_mae: 0.4772\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1806 - mse: 0.1806 - mae: 0.3318 - val_loss: 0.4595 - val_mse: 0.4595 - val_mae: 0.4878\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1805 - mse: 0.1805 - mae: 0.3263 - val_loss: 0.4191 - val_mse: 0.4191 - val_mae: 0.4624\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1767 - mse: 0.1767 - mae: 0.3221 - val_loss: 0.4359 - val_mse: 0.4359 - val_mae: 0.4683\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1821 - mse: 0.1821 - mae: 0.3172 - val_loss: 0.4244 - val_mse: 0.4244 - val_mae: 0.4763\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1698 - mse: 0.1698 - mae: 0.3200 - val_loss: 0.4142 - val_mse: 0.4142 - val_mae: 0.4669\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1458 - mse: 0.1458 - mae: 0.2985 - val_loss: 0.4420 - val_mse: 0.4420 - val_mae: 0.4792\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1691 - mse: 0.1691 - mae: 0.3231 - val_loss: 0.4019 - val_mse: 0.4019 - val_mae: 0.4713\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1623 - mse: 0.1623 - mae: 0.3088 - val_loss: 0.4464 - val_mse: 0.4464 - val_mae: 0.4809\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1772 - mse: 0.1772 - mae: 0.3217 - val_loss: 0.4834 - val_mse: 0.4834 - val_mae: 0.4950\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1908 - mse: 0.1908 - mae: 0.3431 - val_loss: 0.4055 - val_mse: 0.4055 - val_mae: 0.4757\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1799 - mse: 0.1799 - mae: 0.3263 - val_loss: 0.4106 - val_mse: 0.4106 - val_mae: 0.4618\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1796 - mse: 0.1796 - mae: 0.3255 - val_loss: 0.4391 - val_mse: 0.4391 - val_mae: 0.4823\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2163 - mse: 0.2163 - mae: 0.3672 - val_loss: 0.4450 - val_mse: 0.4450 - val_mae: 0.5107\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1981 - mse: 0.1981 - mae: 0.3472 - val_loss: 0.5346 - val_mse: 0.5346 - val_mae: 0.5282\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2063 - mse: 0.2063 - mae: 0.3535 - val_loss: 0.4155 - val_mse: 0.4155 - val_mae: 0.4847\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1818 - mse: 0.1818 - mae: 0.3255 - val_loss: 0.4376 - val_mse: 0.4376 - val_mae: 0.4666\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1939 - mse: 0.1939 - mae: 0.3402 - val_loss: 0.3895 - val_mse: 0.3895 - val_mae: 0.4561\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1610 - mse: 0.1610 - mae: 0.3177 - val_loss: 0.4161 - val_mse: 0.4161 - val_mae: 0.4827\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1632 - mse: 0.1632 - mae: 0.3122 - val_loss: 0.4058 - val_mse: 0.4058 - val_mae: 0.4591\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1548 - mse: 0.1548 - mae: 0.3086 - val_loss: 0.4469 - val_mse: 0.4469 - val_mae: 0.4822\n",
            "Epoch 252/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1611 - mse: 0.1611 - mae: 0.3076 - val_loss: 0.4091 - val_mse: 0.4091 - val_mae: 0.4768\n",
            "Epoch 253/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2005 - mse: 0.2005 - mae: 0.3404 - val_loss: 0.3967 - val_mse: 0.3967 - val_mae: 0.4581\n",
            "Epoch 254/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1545 - mse: 0.1545 - mae: 0.3061 - val_loss: 0.4087 - val_mse: 0.4087 - val_mae: 0.4607\n",
            "Epoch 255/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1475 - mse: 0.1475 - mae: 0.2880 - val_loss: 0.5634 - val_mse: 0.5634 - val_mae: 0.5618\n",
            "Epoch 256/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1860 - mse: 0.1860 - mae: 0.3378 - val_loss: 0.4602 - val_mse: 0.4602 - val_mae: 0.4851\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.5521 - mse: 0.5521 - mae: 0.5379\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 16ms/step - loss: 10.8546 - mse: 10.8546 - mae: 2.7817 - val_loss: 5.8819 - val_mse: 5.8819 - val_mae: 1.8624\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 3.7201 - mse: 3.7201 - mae: 1.5613 - val_loss: 3.3032 - val_mse: 3.3032 - val_mae: 1.4656\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.5371 - mse: 2.5371 - mae: 1.2674 - val_loss: 2.5722 - val_mse: 2.5722 - val_mae: 1.2714\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 2.2911 - mse: 2.2911 - mae: 1.1790 - val_loss: 2.2822 - val_mse: 2.2822 - val_mae: 1.1797\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 2.0533 - mse: 2.0533 - mae: 1.1356 - val_loss: 2.0249 - val_mse: 2.0249 - val_mae: 1.1162\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.8524 - mse: 1.8524 - mae: 1.0721 - val_loss: 2.0405 - val_mse: 2.0405 - val_mae: 1.0739\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.7092 - mse: 1.7092 - mae: 1.0315 - val_loss: 1.9436 - val_mse: 1.9436 - val_mae: 1.0448\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.7486 - mse: 1.7486 - mae: 1.0448 - val_loss: 1.7729 - val_mse: 1.7729 - val_mae: 1.0309\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.4599 - mse: 1.4599 - mae: 0.9648 - val_loss: 1.5501 - val_mse: 1.5501 - val_mae: 0.9494\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.3817 - mse: 1.3817 - mae: 0.9382 - val_loss: 1.5802 - val_mse: 1.5802 - val_mae: 0.9429\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.3533 - mse: 1.3533 - mae: 0.9126 - val_loss: 1.5012 - val_mse: 1.5012 - val_mae: 0.9111\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 1.1933 - mse: 1.1933 - mae: 0.8767 - val_loss: 1.3756 - val_mse: 1.3756 - val_mae: 0.8810\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.1944 - mse: 1.1944 - mae: 0.8672 - val_loss: 1.3550 - val_mse: 1.3550 - val_mae: 0.8645\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.1189 - mse: 1.1189 - mae: 0.8441 - val_loss: 1.2261 - val_mse: 1.2261 - val_mae: 0.8230\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.0895 - mse: 1.0895 - mae: 0.8179 - val_loss: 1.2965 - val_mse: 1.2965 - val_mae: 0.8337\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9893 - mse: 0.9893 - mae: 0.7750 - val_loss: 1.1087 - val_mse: 1.1087 - val_mae: 0.7739\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 1.0162 - mse: 1.0162 - mae: 0.8028 - val_loss: 1.1765 - val_mse: 1.1765 - val_mae: 0.7962\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9302 - mse: 0.9302 - mae: 0.7477 - val_loss: 1.0070 - val_mse: 1.0070 - val_mae: 0.7375\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.9396 - mse: 0.9396 - mae: 0.7665 - val_loss: 1.0463 - val_mse: 1.0463 - val_mae: 0.7441\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.8967 - mse: 0.8967 - mae: 0.7388 - val_loss: 1.0928 - val_mse: 1.0928 - val_mae: 0.7615\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7456 - mse: 0.7456 - mae: 0.6846 - val_loss: 0.8829 - val_mse: 0.8829 - val_mae: 0.6940\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7627 - mse: 0.7627 - mae: 0.6761 - val_loss: 0.9304 - val_mse: 0.9304 - val_mae: 0.7043\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7451 - mse: 0.7451 - mae: 0.6722 - val_loss: 0.8244 - val_mse: 0.8244 - val_mae: 0.6734\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7245 - mse: 0.7245 - mae: 0.6659 - val_loss: 0.8245 - val_mse: 0.8245 - val_mae: 0.6816\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.8204 - mse: 0.8204 - mae: 0.6994 - val_loss: 0.8432 - val_mse: 0.8432 - val_mae: 0.6786\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.6798 - mse: 0.6798 - mae: 0.6531 - val_loss: 0.7202 - val_mse: 0.7202 - val_mae: 0.6293\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.6388 - mse: 0.6388 - mae: 0.6132 - val_loss: 0.7204 - val_mse: 0.7204 - val_mae: 0.6236\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.6447 - mse: 0.6447 - mae: 0.6242 - val_loss: 0.7853 - val_mse: 0.7853 - val_mae: 0.6483\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6055 - mse: 0.6055 - mae: 0.6099 - val_loss: 0.6735 - val_mse: 0.6735 - val_mae: 0.6054\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.6176 - mse: 0.6176 - mae: 0.6083 - val_loss: 0.7444 - val_mse: 0.7444 - val_mae: 0.6362\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6041 - mse: 0.6041 - mae: 0.6066 - val_loss: 0.6487 - val_mse: 0.6487 - val_mae: 0.6061\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5839 - mse: 0.5839 - mae: 0.5837 - val_loss: 0.6522 - val_mse: 0.6522 - val_mae: 0.6032\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6138 - mse: 0.6138 - mae: 0.6104 - val_loss: 0.7235 - val_mse: 0.7235 - val_mae: 0.6297\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5375 - mse: 0.5375 - mae: 0.5595 - val_loss: 0.6390 - val_mse: 0.6390 - val_mae: 0.5926\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5385 - mse: 0.5385 - mae: 0.5686 - val_loss: 0.7221 - val_mse: 0.7221 - val_mae: 0.6209\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5350 - mse: 0.5350 - mae: 0.5799 - val_loss: 0.5669 - val_mse: 0.5669 - val_mae: 0.5696\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4774 - mse: 0.4774 - mae: 0.5302 - val_loss: 0.5975 - val_mse: 0.5975 - val_mae: 0.5673\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4917 - mse: 0.4917 - mae: 0.5458 - val_loss: 0.5641 - val_mse: 0.5641 - val_mae: 0.5554\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4844 - mse: 0.4844 - mae: 0.5317 - val_loss: 0.5472 - val_mse: 0.5472 - val_mae: 0.5409\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4864 - mse: 0.4864 - mae: 0.5349 - val_loss: 0.5796 - val_mse: 0.5796 - val_mae: 0.5601\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4464 - mse: 0.4464 - mae: 0.5177 - val_loss: 0.5688 - val_mse: 0.5688 - val_mae: 0.5486\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4594 - mse: 0.4594 - mae: 0.5349 - val_loss: 0.5179 - val_mse: 0.5179 - val_mae: 0.5303\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4498 - mse: 0.4498 - mae: 0.5285 - val_loss: 0.5031 - val_mse: 0.5031 - val_mae: 0.5249\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4244 - mse: 0.4244 - mae: 0.5088 - val_loss: 0.5622 - val_mse: 0.5622 - val_mae: 0.5571\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4573 - mse: 0.4573 - mae: 0.5213 - val_loss: 0.5342 - val_mse: 0.5342 - val_mae: 0.5293\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4150 - mse: 0.4150 - mae: 0.5024 - val_loss: 0.5137 - val_mse: 0.5137 - val_mae: 0.5262\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4012 - mse: 0.4012 - mae: 0.4976 - val_loss: 0.4905 - val_mse: 0.4905 - val_mae: 0.5156\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4103 - mse: 0.4103 - mae: 0.4905 - val_loss: 0.5015 - val_mse: 0.5015 - val_mae: 0.5268\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4060 - mse: 0.4060 - mae: 0.4927 - val_loss: 0.5160 - val_mse: 0.5160 - val_mae: 0.5425\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4546 - mse: 0.4546 - mae: 0.5199 - val_loss: 0.5966 - val_mse: 0.5966 - val_mae: 0.5765\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4128 - mse: 0.4128 - mae: 0.4992 - val_loss: 0.4937 - val_mse: 0.4937 - val_mae: 0.5090\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4058 - mse: 0.4058 - mae: 0.4996 - val_loss: 0.5189 - val_mse: 0.5189 - val_mae: 0.5181\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3672 - mse: 0.3672 - mae: 0.4672 - val_loss: 0.4573 - val_mse: 0.4573 - val_mae: 0.4958\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3850 - mse: 0.3850 - mae: 0.4739 - val_loss: 0.4778 - val_mse: 0.4778 - val_mae: 0.5147\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.4084 - mse: 0.4084 - mae: 0.5039 - val_loss: 0.4515 - val_mse: 0.4515 - val_mae: 0.4996\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3605 - mse: 0.3605 - mae: 0.4639 - val_loss: 0.4681 - val_mse: 0.4681 - val_mae: 0.5128\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3604 - mse: 0.3604 - mae: 0.4609 - val_loss: 0.4781 - val_mse: 0.4781 - val_mae: 0.5095\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3324 - mse: 0.3324 - mae: 0.4411 - val_loss: 0.5079 - val_mse: 0.5079 - val_mae: 0.5195\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3496 - mse: 0.3496 - mae: 0.4586 - val_loss: 0.4542 - val_mse: 0.4542 - val_mae: 0.5042\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3837 - mse: 0.3837 - mae: 0.4777 - val_loss: 0.5149 - val_mse: 0.5149 - val_mae: 0.5333\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4082 - mse: 0.4082 - mae: 0.5074 - val_loss: 0.4998 - val_mse: 0.4998 - val_mae: 0.5327\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3635 - mse: 0.3635 - mae: 0.4672 - val_loss: 0.4370 - val_mse: 0.4370 - val_mae: 0.4969\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3443 - mse: 0.3443 - mae: 0.4519 - val_loss: 0.4466 - val_mse: 0.4466 - val_mae: 0.5040\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3769 - mse: 0.3769 - mae: 0.4800 - val_loss: 0.4908 - val_mse: 0.4908 - val_mae: 0.5335\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3357 - mse: 0.3357 - mae: 0.4548 - val_loss: 0.4802 - val_mse: 0.4802 - val_mae: 0.5184\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3387 - mse: 0.3387 - mae: 0.4501 - val_loss: 0.4397 - val_mse: 0.4397 - val_mae: 0.5028\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3211 - mse: 0.3211 - mae: 0.4357 - val_loss: 0.4503 - val_mse: 0.4503 - val_mae: 0.5047\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3379 - mse: 0.3379 - mae: 0.4512 - val_loss: 0.5674 - val_mse: 0.5674 - val_mae: 0.5715\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3213 - mse: 0.3213 - mae: 0.4455 - val_loss: 0.4382 - val_mse: 0.4382 - val_mae: 0.5002\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3185 - mse: 0.3185 - mae: 0.4423 - val_loss: 0.4496 - val_mse: 0.4496 - val_mae: 0.5035\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3644 - mse: 0.3644 - mae: 0.4719 - val_loss: 0.5219 - val_mse: 0.5219 - val_mae: 0.5553\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3291 - mse: 0.3291 - mae: 0.4462 - val_loss: 0.4545 - val_mse: 0.4545 - val_mae: 0.5045\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3248 - mse: 0.3248 - mae: 0.4396 - val_loss: 0.5244 - val_mse: 0.5244 - val_mae: 0.5592\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3010 - mse: 0.3010 - mae: 0.4247 - val_loss: 0.4488 - val_mse: 0.4488 - val_mae: 0.5086\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3259 - mse: 0.3259 - mae: 0.4471 - val_loss: 0.4578 - val_mse: 0.4578 - val_mae: 0.5092\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3266 - mse: 0.3266 - mae: 0.4462 - val_loss: 0.4743 - val_mse: 0.4743 - val_mae: 0.5137\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2875 - mse: 0.2875 - mae: 0.4136 - val_loss: 0.4978 - val_mse: 0.4978 - val_mae: 0.5368\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2869 - mse: 0.2869 - mae: 0.4149 - val_loss: 0.4637 - val_mse: 0.4637 - val_mae: 0.5071\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3156 - mse: 0.3156 - mae: 0.4404 - val_loss: 0.4276 - val_mse: 0.4276 - val_mae: 0.5005\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3004 - mse: 0.3004 - mae: 0.4261 - val_loss: 0.5329 - val_mse: 0.5329 - val_mae: 0.5702\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2958 - mse: 0.2958 - mae: 0.4204 - val_loss: 0.4369 - val_mse: 0.4369 - val_mae: 0.4983\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3088 - mse: 0.3088 - mae: 0.4354 - val_loss: 0.4445 - val_mse: 0.4445 - val_mae: 0.5033\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2987 - mse: 0.2987 - mae: 0.4177 - val_loss: 0.4496 - val_mse: 0.4496 - val_mae: 0.5123\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2724 - mse: 0.2724 - mae: 0.3968 - val_loss: 0.4336 - val_mse: 0.4336 - val_mae: 0.4970\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2779 - mse: 0.2779 - mae: 0.4137 - val_loss: 0.4686 - val_mse: 0.4686 - val_mae: 0.5161\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3070 - mse: 0.3070 - mae: 0.4331 - val_loss: 0.5381 - val_mse: 0.5381 - val_mae: 0.5649\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2765 - mse: 0.2765 - mae: 0.4129 - val_loss: 0.4736 - val_mse: 0.4736 - val_mae: 0.5264\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2764 - mse: 0.2764 - mae: 0.4099 - val_loss: 0.4624 - val_mse: 0.4624 - val_mae: 0.5075\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2880 - mse: 0.2880 - mae: 0.4208 - val_loss: 0.4573 - val_mse: 0.4573 - val_mae: 0.5128\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2933 - mse: 0.2933 - mae: 0.4213 - val_loss: 0.4669 - val_mse: 0.4669 - val_mae: 0.5237\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3039 - mse: 0.3039 - mae: 0.4360 - val_loss: 0.4867 - val_mse: 0.4867 - val_mae: 0.5382\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2962 - mse: 0.2962 - mae: 0.4298 - val_loss: 0.4229 - val_mse: 0.4229 - val_mae: 0.4841\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2649 - mse: 0.2649 - mae: 0.4041 - val_loss: 0.4195 - val_mse: 0.4195 - val_mae: 0.5002\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2923 - mse: 0.2923 - mae: 0.4219 - val_loss: 0.4155 - val_mse: 0.4155 - val_mae: 0.4976\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2826 - mse: 0.2826 - mae: 0.4205 - val_loss: 0.4388 - val_mse: 0.4388 - val_mae: 0.4964\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3014 - mse: 0.3014 - mae: 0.4348 - val_loss: 0.5109 - val_mse: 0.5109 - val_mae: 0.5535\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2761 - mse: 0.2761 - mae: 0.4050 - val_loss: 0.4627 - val_mse: 0.4627 - val_mae: 0.5270\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2680 - mse: 0.2680 - mae: 0.4094 - val_loss: 0.4286 - val_mse: 0.4286 - val_mae: 0.4969\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2934 - mse: 0.2934 - mae: 0.4221 - val_loss: 0.4547 - val_mse: 0.4547 - val_mae: 0.5145\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2588 - mse: 0.2588 - mae: 0.3935 - val_loss: 0.4561 - val_mse: 0.4561 - val_mae: 0.5214\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2780 - mse: 0.2780 - mae: 0.4115 - val_loss: 0.4690 - val_mse: 0.4690 - val_mae: 0.5320\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2689 - mse: 0.2689 - mae: 0.4099 - val_loss: 0.4164 - val_mse: 0.4164 - val_mae: 0.4985\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2727 - mse: 0.2727 - mae: 0.3991 - val_loss: 0.4833 - val_mse: 0.4833 - val_mae: 0.5549\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3015 - mse: 0.3015 - mae: 0.4343 - val_loss: 0.4203 - val_mse: 0.4203 - val_mae: 0.4978\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2492 - mse: 0.2492 - mae: 0.3928 - val_loss: 0.4214 - val_mse: 0.4214 - val_mae: 0.5022\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2664 - mse: 0.2664 - mae: 0.3993 - val_loss: 0.4255 - val_mse: 0.4255 - val_mae: 0.5075\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2919 - mse: 0.2919 - mae: 0.4225 - val_loss: 0.4228 - val_mse: 0.4228 - val_mae: 0.5009\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2653 - mse: 0.2653 - mae: 0.3970 - val_loss: 0.4753 - val_mse: 0.4753 - val_mae: 0.5272\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2343 - mse: 0.2343 - mae: 0.3740 - val_loss: 0.4414 - val_mse: 0.4414 - val_mae: 0.4997\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2541 - mse: 0.2541 - mae: 0.3947 - val_loss: 0.4675 - val_mse: 0.4675 - val_mae: 0.5276\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2808 - mse: 0.2808 - mae: 0.4128 - val_loss: 0.4122 - val_mse: 0.4122 - val_mae: 0.4931\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2431 - mse: 0.2431 - mae: 0.3907 - val_loss: 0.4389 - val_mse: 0.4389 - val_mae: 0.5103\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2672 - mse: 0.2672 - mae: 0.4062 - val_loss: 0.5119 - val_mse: 0.5119 - val_mae: 0.5723\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2451 - mse: 0.2451 - mae: 0.3830 - val_loss: 0.4152 - val_mse: 0.4152 - val_mae: 0.4903\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2247 - mse: 0.2247 - mae: 0.3671 - val_loss: 0.4677 - val_mse: 0.4677 - val_mae: 0.5318\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2471 - mse: 0.2471 - mae: 0.3878 - val_loss: 0.4221 - val_mse: 0.4221 - val_mae: 0.5101\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2531 - mse: 0.2531 - mae: 0.3849 - val_loss: 0.4566 - val_mse: 0.4566 - val_mae: 0.5066\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2579 - mse: 0.2579 - mae: 0.3922 - val_loss: 0.5065 - val_mse: 0.5065 - val_mae: 0.5394\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2415 - mse: 0.2415 - mae: 0.3872 - val_loss: 0.4183 - val_mse: 0.4183 - val_mae: 0.5058\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2641 - mse: 0.2641 - mae: 0.4085 - val_loss: 0.5535 - val_mse: 0.5535 - val_mae: 0.5850\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2528 - mse: 0.2528 - mae: 0.3921 - val_loss: 0.4220 - val_mse: 0.4220 - val_mae: 0.5055\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2240 - mse: 0.2240 - mae: 0.3710 - val_loss: 0.4322 - val_mse: 0.4322 - val_mae: 0.5040\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2550 - mse: 0.2550 - mae: 0.3899 - val_loss: 0.4787 - val_mse: 0.4787 - val_mae: 0.5231\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2371 - mse: 0.2371 - mae: 0.3761 - val_loss: 0.4210 - val_mse: 0.4210 - val_mae: 0.5061\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2410 - mse: 0.2410 - mae: 0.3852 - val_loss: 0.4336 - val_mse: 0.4336 - val_mae: 0.5231\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2528 - mse: 0.2528 - mae: 0.3913 - val_loss: 0.4972 - val_mse: 0.4972 - val_mae: 0.5522\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2638 - mse: 0.2638 - mae: 0.4013 - val_loss: 0.4871 - val_mse: 0.4871 - val_mae: 0.5321\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2512 - mse: 0.2512 - mae: 0.3947 - val_loss: 0.4514 - val_mse: 0.4514 - val_mae: 0.5066\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2484 - mse: 0.2484 - mae: 0.3892 - val_loss: 0.4814 - val_mse: 0.4814 - val_mae: 0.5364\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2430 - mse: 0.2430 - mae: 0.3830 - val_loss: 0.4587 - val_mse: 0.4587 - val_mae: 0.5117\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2276 - mse: 0.2276 - mae: 0.3727 - val_loss: 0.4576 - val_mse: 0.4576 - val_mae: 0.5208\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2400 - mse: 0.2400 - mae: 0.3806 - val_loss: 0.4225 - val_mse: 0.4225 - val_mae: 0.5055\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2277 - mse: 0.2277 - mae: 0.3705 - val_loss: 0.4539 - val_mse: 0.4539 - val_mae: 0.5081\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2404 - mse: 0.2404 - mae: 0.3859 - val_loss: 0.4375 - val_mse: 0.4375 - val_mae: 0.5105\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2364 - mse: 0.2364 - mae: 0.3794 - val_loss: 0.4344 - val_mse: 0.4344 - val_mae: 0.5082\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2272 - mse: 0.2272 - mae: 0.3676 - val_loss: 0.4848 - val_mse: 0.4848 - val_mae: 0.5303\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2633 - mse: 0.2633 - mae: 0.3927 - val_loss: 0.4677 - val_mse: 0.4677 - val_mae: 0.5273\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2282 - mse: 0.2282 - mae: 0.3718 - val_loss: 0.4557 - val_mse: 0.4557 - val_mae: 0.5347\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2637 - mse: 0.2637 - mae: 0.3954 - val_loss: 0.4292 - val_mse: 0.4292 - val_mae: 0.4925\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2498 - mse: 0.2498 - mae: 0.3852 - val_loss: 0.4773 - val_mse: 0.4773 - val_mae: 0.5331\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2369 - mse: 0.2369 - mae: 0.3771 - val_loss: 0.4394 - val_mse: 0.4394 - val_mae: 0.5087\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2239 - mse: 0.2239 - mae: 0.3715 - val_loss: 0.5494 - val_mse: 0.5494 - val_mae: 0.5883\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2486 - mse: 0.2486 - mae: 0.3911 - val_loss: 0.4194 - val_mse: 0.4194 - val_mae: 0.4956\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2460 - mse: 0.2460 - mae: 0.3835 - val_loss: 0.4342 - val_mse: 0.4342 - val_mae: 0.4939\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2366 - mse: 0.2366 - mae: 0.3760 - val_loss: 0.5243 - val_mse: 0.5243 - val_mae: 0.5721\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2740 - mse: 0.2740 - mae: 0.4156 - val_loss: 0.4159 - val_mse: 0.4159 - val_mae: 0.5012\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2295 - mse: 0.2295 - mae: 0.3712 - val_loss: 0.4487 - val_mse: 0.4487 - val_mae: 0.5025\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2302 - mse: 0.2302 - mae: 0.3721 - val_loss: 0.4880 - val_mse: 0.4880 - val_mae: 0.5607\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2130 - mse: 0.2130 - mae: 0.3551 - val_loss: 0.4458 - val_mse: 0.4458 - val_mae: 0.5245\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2160 - mse: 0.2160 - mae: 0.3639 - val_loss: 0.4683 - val_mse: 0.4683 - val_mae: 0.5138\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2421 - mse: 0.2421 - mae: 0.3795 - val_loss: 0.4474 - val_mse: 0.4474 - val_mae: 0.5074\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2157 - mse: 0.2157 - mae: 0.3641 - val_loss: 0.5038 - val_mse: 0.5038 - val_mae: 0.5551\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2665 - mse: 0.2665 - mae: 0.4142 - val_loss: 0.4320 - val_mse: 0.4320 - val_mae: 0.4964\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2475 - mse: 0.2475 - mae: 0.3862 - val_loss: 0.5121 - val_mse: 0.5121 - val_mae: 0.5625\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2153 - mse: 0.2153 - mae: 0.3611 - val_loss: 0.4543 - val_mse: 0.4543 - val_mae: 0.5135\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2129 - mse: 0.2129 - mae: 0.3654 - val_loss: 0.5014 - val_mse: 0.5014 - val_mae: 0.5573\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2229 - mse: 0.2229 - mae: 0.3695 - val_loss: 0.4447 - val_mse: 0.4447 - val_mae: 0.5202\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.1972 - mse: 0.1972 - mae: 0.3506 - val_loss: 0.4367 - val_mse: 0.4367 - val_mae: 0.5154\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2293 - mse: 0.2293 - mae: 0.3758 - val_loss: 0.4306 - val_mse: 0.4306 - val_mae: 0.4879\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2127 - mse: 0.2127 - mae: 0.3619 - val_loss: 0.4234 - val_mse: 0.4234 - val_mae: 0.4980\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2100 - mse: 0.2100 - mae: 0.3577 - val_loss: 0.4006 - val_mse: 0.4006 - val_mae: 0.4974\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2289 - mse: 0.2289 - mae: 0.3749 - val_loss: 0.3948 - val_mse: 0.3948 - val_mae: 0.4846\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2131 - mse: 0.2131 - mae: 0.3592 - val_loss: 0.4476 - val_mse: 0.4476 - val_mae: 0.5343\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2313 - mse: 0.2313 - mae: 0.3736 - val_loss: 0.4657 - val_mse: 0.4657 - val_mae: 0.5297\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2327 - mse: 0.2327 - mae: 0.3776 - val_loss: 0.4511 - val_mse: 0.4511 - val_mae: 0.5252\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2263 - mse: 0.2263 - mae: 0.3729 - val_loss: 0.4004 - val_mse: 0.4004 - val_mae: 0.4946\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2248 - mse: 0.2248 - mae: 0.3663 - val_loss: 0.4271 - val_mse: 0.4271 - val_mae: 0.4983\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1920 - mse: 0.1920 - mae: 0.3457 - val_loss: 0.4484 - val_mse: 0.4484 - val_mae: 0.5291\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2214 - mse: 0.2214 - mae: 0.3769 - val_loss: 0.4314 - val_mse: 0.4314 - val_mae: 0.5041\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1953 - mse: 0.1953 - mae: 0.3453 - val_loss: 0.4640 - val_mse: 0.4640 - val_mae: 0.5277\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2161 - mse: 0.2161 - mae: 0.3556 - val_loss: 0.4265 - val_mse: 0.4265 - val_mae: 0.4955\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2339 - mse: 0.2339 - mae: 0.3788 - val_loss: 0.4499 - val_mse: 0.4499 - val_mae: 0.5104\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2380 - mse: 0.2380 - mae: 0.3746 - val_loss: 0.4140 - val_mse: 0.4140 - val_mae: 0.5003\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1964 - mse: 0.1964 - mae: 0.3428 - val_loss: 0.4455 - val_mse: 0.4455 - val_mae: 0.5243\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2069 - mse: 0.2069 - mae: 0.3596 - val_loss: 0.4068 - val_mse: 0.4068 - val_mae: 0.4938\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1964 - mse: 0.1964 - mae: 0.3424 - val_loss: 0.4204 - val_mse: 0.4204 - val_mae: 0.5044\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2091 - mse: 0.2091 - mae: 0.3502 - val_loss: 0.4335 - val_mse: 0.4335 - val_mae: 0.5222\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2159 - mse: 0.2159 - mae: 0.3586 - val_loss: 0.4215 - val_mse: 0.4215 - val_mae: 0.5058\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1840 - mse: 0.1840 - mae: 0.3329 - val_loss: 0.4197 - val_mse: 0.4197 - val_mae: 0.4919\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2014 - mse: 0.2014 - mae: 0.3474 - val_loss: 0.4538 - val_mse: 0.4538 - val_mae: 0.5239\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2309 - mse: 0.2309 - mae: 0.3754 - val_loss: 0.3922 - val_mse: 0.3922 - val_mae: 0.4885\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2322 - mse: 0.2322 - mae: 0.3772 - val_loss: 0.4887 - val_mse: 0.4887 - val_mae: 0.5464\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2279 - mse: 0.2279 - mae: 0.3682 - val_loss: 0.4129 - val_mse: 0.4129 - val_mae: 0.5024\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2106 - mse: 0.2106 - mae: 0.3575 - val_loss: 0.4056 - val_mse: 0.4056 - val_mae: 0.4747\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2050 - mse: 0.2050 - mae: 0.3554 - val_loss: 0.4638 - val_mse: 0.4638 - val_mae: 0.5402\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2148 - mse: 0.2148 - mae: 0.3657 - val_loss: 0.4046 - val_mse: 0.4046 - val_mae: 0.4916\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2002 - mse: 0.2002 - mae: 0.3484 - val_loss: 0.5886 - val_mse: 0.5886 - val_mae: 0.6225\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2383 - mse: 0.2383 - mae: 0.3866 - val_loss: 0.4703 - val_mse: 0.4703 - val_mae: 0.5430\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2094 - mse: 0.2094 - mae: 0.3593 - val_loss: 0.4680 - val_mse: 0.4680 - val_mae: 0.5432\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2255 - mse: 0.2255 - mae: 0.3751 - val_loss: 0.4310 - val_mse: 0.4310 - val_mae: 0.4998\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2232 - mse: 0.2232 - mae: 0.3679 - val_loss: 0.4356 - val_mse: 0.4356 - val_mae: 0.5053\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2004 - mse: 0.2004 - mae: 0.3533 - val_loss: 0.4474 - val_mse: 0.4474 - val_mae: 0.5232\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1969 - mse: 0.1969 - mae: 0.3464 - val_loss: 0.4875 - val_mse: 0.4875 - val_mae: 0.5508\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2160 - mse: 0.2160 - mae: 0.3523 - val_loss: 0.4071 - val_mse: 0.4071 - val_mae: 0.4836\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1963 - mse: 0.1963 - mae: 0.3520 - val_loss: 0.4314 - val_mse: 0.4314 - val_mae: 0.5041\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1893 - mse: 0.1893 - mae: 0.3327 - val_loss: 0.4258 - val_mse: 0.4258 - val_mae: 0.4892\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1915 - mse: 0.1915 - mae: 0.3366 - val_loss: 0.4154 - val_mse: 0.4154 - val_mae: 0.4988\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2157 - mse: 0.2157 - mae: 0.3564 - val_loss: 0.4363 - val_mse: 0.4363 - val_mae: 0.5124\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2138 - mse: 0.2138 - mae: 0.3578 - val_loss: 0.4437 - val_mse: 0.4437 - val_mae: 0.5256\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1953 - mse: 0.1953 - mae: 0.3504 - val_loss: 0.4228 - val_mse: 0.4228 - val_mae: 0.5032\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1818 - mse: 0.1818 - mae: 0.3365 - val_loss: 0.4157 - val_mse: 0.4157 - val_mae: 0.5007\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1840 - mse: 0.1840 - mae: 0.3301 - val_loss: 0.4272 - val_mse: 0.4272 - val_mae: 0.5185\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2073 - mse: 0.2073 - mae: 0.3526 - val_loss: 0.5354 - val_mse: 0.5354 - val_mae: 0.5967\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2520 - mse: 0.2520 - mae: 0.4012 - val_loss: 0.4185 - val_mse: 0.4185 - val_mae: 0.5038\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2190 - mse: 0.2190 - mae: 0.3584 - val_loss: 0.4445 - val_mse: 0.4445 - val_mae: 0.5050\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2175 - mse: 0.2175 - mae: 0.3688 - val_loss: 0.4434 - val_mse: 0.4434 - val_mae: 0.5015\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2503 - mse: 0.2503 - mae: 0.3967 - val_loss: 0.4483 - val_mse: 0.4483 - val_mae: 0.5066\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2345 - mse: 0.2345 - mae: 0.3734 - val_loss: 0.5061 - val_mse: 0.5061 - val_mae: 0.5730\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2305 - mse: 0.2305 - mae: 0.3699 - val_loss: 0.4171 - val_mse: 0.4171 - val_mae: 0.4886\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1980 - mse: 0.1980 - mae: 0.3482 - val_loss: 0.4261 - val_mse: 0.4261 - val_mae: 0.4807\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1923 - mse: 0.1923 - mae: 0.3406 - val_loss: 0.4652 - val_mse: 0.4652 - val_mae: 0.5483\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2139 - mse: 0.2139 - mae: 0.3629 - val_loss: 0.4041 - val_mse: 0.4041 - val_mae: 0.4955\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1761 - mse: 0.1761 - mae: 0.3252 - val_loss: 0.4334 - val_mse: 0.4334 - val_mae: 0.4861\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2151 - mse: 0.2151 - mae: 0.3576 - val_loss: 0.4565 - val_mse: 0.4565 - val_mae: 0.5298\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2269 - mse: 0.2269 - mae: 0.3654 - val_loss: 0.4191 - val_mse: 0.4191 - val_mae: 0.4834\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2040 - mse: 0.2040 - mae: 0.3553 - val_loss: 0.4302 - val_mse: 0.4302 - val_mae: 0.5125\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1863 - mse: 0.1863 - mae: 0.3408 - val_loss: 0.4513 - val_mse: 0.4513 - val_mae: 0.5164\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1785 - mse: 0.1785 - mae: 0.3303 - val_loss: 0.4186 - val_mse: 0.4186 - val_mae: 0.4861\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1937 - mse: 0.1937 - mae: 0.3420 - val_loss: 0.4531 - val_mse: 0.4531 - val_mae: 0.5285\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1688 - mse: 0.1688 - mae: 0.3221 - val_loss: 0.4438 - val_mse: 0.4438 - val_mae: 0.5225\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1894 - mse: 0.1894 - mae: 0.3373 - val_loss: 0.3979 - val_mse: 0.3979 - val_mae: 0.4827\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1980 - mse: 0.1980 - mae: 0.3482 - val_loss: 0.4588 - val_mse: 0.4588 - val_mae: 0.5287\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2030 - mse: 0.2030 - mae: 0.3540 - val_loss: 0.4333 - val_mse: 0.4333 - val_mae: 0.5151\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2396 - mse: 0.2396 - mae: 0.3837 - val_loss: 0.4901 - val_mse: 0.4901 - val_mae: 0.5575\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1865 - mse: 0.1865 - mae: 0.3437 - val_loss: 0.4168 - val_mse: 0.4168 - val_mae: 0.4974\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1843 - mse: 0.1843 - mae: 0.3298 - val_loss: 0.4432 - val_mse: 0.4432 - val_mae: 0.5270\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1915 - mse: 0.1915 - mae: 0.3337 - val_loss: 0.4435 - val_mse: 0.4435 - val_mae: 0.5095\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2089 - mse: 0.2089 - mae: 0.3617 - val_loss: 0.4167 - val_mse: 0.4167 - val_mae: 0.4996\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1821 - mse: 0.1821 - mae: 0.3324 - val_loss: 0.4095 - val_mse: 0.4095 - val_mae: 0.4823\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1910 - mse: 0.1910 - mae: 0.3463 - val_loss: 0.4014 - val_mse: 0.4014 - val_mae: 0.4934\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1976 - mse: 0.1976 - mae: 0.3475 - val_loss: 0.4319 - val_mse: 0.4319 - val_mae: 0.5043\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1892 - mse: 0.1892 - mae: 0.3328 - val_loss: 0.4219 - val_mse: 0.4219 - val_mae: 0.5025\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1951 - mse: 0.1951 - mae: 0.3460 - val_loss: 0.4030 - val_mse: 0.4030 - val_mae: 0.4773\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2369 - mse: 0.2369 - mae: 0.3758 - val_loss: 0.4032 - val_mse: 0.4032 - val_mae: 0.4756\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2019 - mse: 0.2019 - mae: 0.3428 - val_loss: 0.4060 - val_mse: 0.4060 - val_mae: 0.4959\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1980 - mse: 0.1980 - mae: 0.3521 - val_loss: 0.4040 - val_mse: 0.4040 - val_mae: 0.4911\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2029 - mse: 0.2029 - mae: 0.3432 - val_loss: 0.4124 - val_mse: 0.4124 - val_mae: 0.4930\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2015 - mse: 0.2015 - mae: 0.3453 - val_loss: 0.4149 - val_mse: 0.4149 - val_mae: 0.5123\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1912 - mse: 0.1912 - mae: 0.3443 - val_loss: 0.3783 - val_mse: 0.3783 - val_mae: 0.4699\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1821 - mse: 0.1821 - mae: 0.3277 - val_loss: 0.4066 - val_mse: 0.4066 - val_mae: 0.4952\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2164 - mse: 0.2164 - mae: 0.3614 - val_loss: 0.4393 - val_mse: 0.4393 - val_mae: 0.4947\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1804 - mse: 0.1804 - mae: 0.3260 - val_loss: 0.4218 - val_mse: 0.4218 - val_mae: 0.4936\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2260 - mse: 0.2260 - mae: 0.3733 - val_loss: 0.4335 - val_mse: 0.4335 - val_mae: 0.5001\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1944 - mse: 0.1944 - mae: 0.3422 - val_loss: 0.4222 - val_mse: 0.4222 - val_mae: 0.4919\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1815 - mse: 0.1815 - mae: 0.3362 - val_loss: 0.3968 - val_mse: 0.3968 - val_mae: 0.4878\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1945 - mse: 0.1945 - mae: 0.3444 - val_loss: 0.4268 - val_mse: 0.4268 - val_mae: 0.4893\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1769 - mse: 0.1769 - mae: 0.3284 - val_loss: 0.4096 - val_mse: 0.4096 - val_mae: 0.4916\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1740 - mse: 0.1740 - mae: 0.3230 - val_loss: 0.4124 - val_mse: 0.4124 - val_mae: 0.4974\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1857 - mse: 0.1857 - mae: 0.3353 - val_loss: 0.4437 - val_mse: 0.4437 - val_mae: 0.5240\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1768 - mse: 0.1768 - mae: 0.3212 - val_loss: 0.4071 - val_mse: 0.4071 - val_mae: 0.4919\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1988 - mse: 0.1988 - mae: 0.3461 - val_loss: 0.4568 - val_mse: 0.4568 - val_mae: 0.5459\n",
            "Epoch 252/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2054 - mse: 0.2054 - mae: 0.3558 - val_loss: 0.4290 - val_mse: 0.4290 - val_mae: 0.4905\n",
            "Epoch 253/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1967 - mse: 0.1967 - mae: 0.3460 - val_loss: 0.4130 - val_mse: 0.4130 - val_mae: 0.4932\n",
            "Epoch 254/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1835 - mse: 0.1835 - mae: 0.3322 - val_loss: 0.4438 - val_mse: 0.4438 - val_mae: 0.5182\n",
            "Epoch 255/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2335 - mse: 0.2335 - mae: 0.3843 - val_loss: 0.4072 - val_mse: 0.4072 - val_mae: 0.4928\n",
            "Epoch 256/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2465 - mse: 0.2465 - mae: 0.3862 - val_loss: 0.4500 - val_mse: 0.4500 - val_mae: 0.5045\n",
            "Epoch 257/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2027 - mse: 0.2027 - mae: 0.3482 - val_loss: 0.3924 - val_mse: 0.3924 - val_mae: 0.4876\n",
            "Epoch 258/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1670 - mse: 0.1670 - mae: 0.3138 - val_loss: 0.4092 - val_mse: 0.4092 - val_mae: 0.5089\n",
            "Epoch 259/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1815 - mse: 0.1815 - mae: 0.3190 - val_loss: 0.3884 - val_mse: 0.3884 - val_mae: 0.4733\n",
            "Epoch 260/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1838 - mse: 0.1838 - mae: 0.3335 - val_loss: 0.4269 - val_mse: 0.4269 - val_mae: 0.5051\n",
            "Epoch 261/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1778 - mse: 0.1778 - mae: 0.3244 - val_loss: 0.4077 - val_mse: 0.4077 - val_mae: 0.4857\n",
            "Epoch 262/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1663 - mse: 0.1663 - mae: 0.3118 - val_loss: 0.4009 - val_mse: 0.4009 - val_mae: 0.4810\n",
            "Epoch 263/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1875 - mse: 0.1875 - mae: 0.3231 - val_loss: 0.4209 - val_mse: 0.4209 - val_mae: 0.4957\n",
            "Epoch 264/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1530 - mse: 0.1530 - mae: 0.3071 - val_loss: 0.4264 - val_mse: 0.4264 - val_mae: 0.4990\n",
            "Epoch 265/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1696 - mse: 0.1696 - mae: 0.3200 - val_loss: 0.4139 - val_mse: 0.4139 - val_mae: 0.4936\n",
            "Epoch 266/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1638 - mse: 0.1638 - mae: 0.3113 - val_loss: 0.4301 - val_mse: 0.4301 - val_mae: 0.4988\n",
            "Epoch 267/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1839 - mse: 0.1839 - mae: 0.3365 - val_loss: 0.4174 - val_mse: 0.4174 - val_mae: 0.4955\n",
            "Epoch 268/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1823 - mse: 0.1823 - mae: 0.3283 - val_loss: 0.4333 - val_mse: 0.4333 - val_mae: 0.5259\n",
            "Epoch 269/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1770 - mse: 0.1770 - mae: 0.3327 - val_loss: 0.3840 - val_mse: 0.3840 - val_mae: 0.4751\n",
            "Epoch 270/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1735 - mse: 0.1735 - mae: 0.3203 - val_loss: 0.4784 - val_mse: 0.4784 - val_mae: 0.5439\n",
            "Epoch 271/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2110 - mse: 0.2110 - mae: 0.3596 - val_loss: 0.4013 - val_mse: 0.4013 - val_mae: 0.4849\n",
            "Epoch 272/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2044 - mse: 0.2044 - mae: 0.3551 - val_loss: 0.4174 - val_mse: 0.4174 - val_mae: 0.4904\n",
            "Epoch 273/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1873 - mse: 0.1873 - mae: 0.3397 - val_loss: 0.4009 - val_mse: 0.4009 - val_mae: 0.4852\n",
            "Epoch 274/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2091 - mse: 0.2091 - mae: 0.3543 - val_loss: 0.4043 - val_mse: 0.4043 - val_mae: 0.4931\n",
            "Epoch 275/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1624 - mse: 0.1624 - mae: 0.3098 - val_loss: 0.3982 - val_mse: 0.3982 - val_mae: 0.4625\n",
            "Epoch 276/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1786 - mse: 0.1786 - mae: 0.3298 - val_loss: 0.4051 - val_mse: 0.4051 - val_mae: 0.4908\n",
            "Epoch 277/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1512 - mse: 0.1512 - mae: 0.3010 - val_loss: 0.3715 - val_mse: 0.3715 - val_mae: 0.4678\n",
            "Epoch 278/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1837 - mse: 0.1837 - mae: 0.3272 - val_loss: 0.3953 - val_mse: 0.3953 - val_mae: 0.4786\n",
            "Epoch 279/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1780 - mse: 0.1780 - mae: 0.3321 - val_loss: 0.4238 - val_mse: 0.4238 - val_mae: 0.5085\n",
            "Epoch 280/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.1777 - mse: 0.1777 - mae: 0.3228 - val_loss: 0.4065 - val_mse: 0.4065 - val_mae: 0.4716\n",
            "Epoch 281/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1825 - mse: 0.1825 - mae: 0.3266 - val_loss: 0.4403 - val_mse: 0.4403 - val_mae: 0.5192\n",
            "Epoch 282/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2011 - mse: 0.2011 - mae: 0.3515 - val_loss: 0.4449 - val_mse: 0.4449 - val_mae: 0.5056\n",
            "Epoch 283/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1799 - mse: 0.1799 - mae: 0.3240 - val_loss: 0.4438 - val_mse: 0.4438 - val_mae: 0.5258\n",
            "Epoch 284/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1905 - mse: 0.1905 - mae: 0.3414 - val_loss: 0.3942 - val_mse: 0.3942 - val_mae: 0.4751\n",
            "Epoch 285/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1719 - mse: 0.1719 - mae: 0.3278 - val_loss: 0.3931 - val_mse: 0.3931 - val_mae: 0.4695\n",
            "Epoch 286/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1749 - mse: 0.1749 - mae: 0.3252 - val_loss: 0.4046 - val_mse: 0.4046 - val_mae: 0.4905\n",
            "Epoch 287/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1791 - mse: 0.1791 - mae: 0.3306 - val_loss: 0.4012 - val_mse: 0.4012 - val_mae: 0.4894\n",
            "Epoch 288/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1587 - mse: 0.1587 - mae: 0.3059 - val_loss: 0.4095 - val_mse: 0.4095 - val_mae: 0.4916\n",
            "Epoch 289/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1793 - mse: 0.1793 - mae: 0.3251 - val_loss: 0.4513 - val_mse: 0.4513 - val_mae: 0.5135\n",
            "Epoch 290/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1726 - mse: 0.1726 - mae: 0.3214 - val_loss: 0.4530 - val_mse: 0.4530 - val_mae: 0.5320\n",
            "Epoch 291/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1783 - mse: 0.1783 - mae: 0.3231 - val_loss: 0.4509 - val_mse: 0.4509 - val_mae: 0.5163\n",
            "Epoch 292/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1718 - mse: 0.1718 - mae: 0.3267 - val_loss: 0.4244 - val_mse: 0.4244 - val_mae: 0.5125\n",
            "Epoch 293/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1765 - mse: 0.1765 - mae: 0.3255 - val_loss: 0.4010 - val_mse: 0.4010 - val_mae: 0.4861\n",
            "Epoch 294/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1488 - mse: 0.1488 - mae: 0.2971 - val_loss: 0.4105 - val_mse: 0.4105 - val_mae: 0.4996\n",
            "Epoch 295/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1642 - mse: 0.1642 - mae: 0.3152 - val_loss: 0.4259 - val_mse: 0.4259 - val_mae: 0.4956\n",
            "Epoch 296/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1755 - mse: 0.1755 - mae: 0.3137 - val_loss: 0.4868 - val_mse: 0.4868 - val_mae: 0.5525\n",
            "Epoch 297/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2100 - mse: 0.2100 - mae: 0.3594 - val_loss: 0.4149 - val_mse: 0.4149 - val_mae: 0.4986\n",
            "Epoch 298/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1901 - mse: 0.1901 - mae: 0.3293 - val_loss: 0.4294 - val_mse: 0.4294 - val_mae: 0.5060\n",
            "Epoch 299/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1764 - mse: 0.1764 - mae: 0.3283 - val_loss: 0.4034 - val_mse: 0.4034 - val_mae: 0.4904\n",
            "Epoch 300/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1539 - mse: 0.1539 - mae: 0.3062 - val_loss: 0.4122 - val_mse: 0.4122 - val_mae: 0.4919\n",
            "Epoch 301/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1653 - mse: 0.1653 - mae: 0.3132 - val_loss: 0.4063 - val_mse: 0.4063 - val_mae: 0.4845\n",
            "Epoch 302/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1534 - mse: 0.1534 - mae: 0.3000 - val_loss: 0.4653 - val_mse: 0.4653 - val_mae: 0.5318\n",
            "Epoch 303/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1775 - mse: 0.1775 - mae: 0.3264 - val_loss: 0.4314 - val_mse: 0.4314 - val_mae: 0.5074\n",
            "Epoch 304/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1787 - mse: 0.1787 - mae: 0.3269 - val_loss: 0.3961 - val_mse: 0.3961 - val_mae: 0.4904\n",
            "Epoch 305/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1506 - mse: 0.1506 - mae: 0.3056 - val_loss: 0.3895 - val_mse: 0.3895 - val_mae: 0.4868\n",
            "Epoch 306/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1970 - mse: 0.1970 - mae: 0.3365 - val_loss: 0.4066 - val_mse: 0.4066 - val_mae: 0.4979\n",
            "Epoch 307/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1714 - mse: 0.1714 - mae: 0.3148 - val_loss: 0.4220 - val_mse: 0.4220 - val_mae: 0.5089\n",
            "Epoch 308/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1602 - mse: 0.1602 - mae: 0.3090 - val_loss: 0.4128 - val_mse: 0.4128 - val_mae: 0.5037\n",
            "Epoch 309/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1675 - mse: 0.1675 - mae: 0.3136 - val_loss: 0.4109 - val_mse: 0.4109 - val_mae: 0.5036\n",
            "Epoch 310/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1840 - mse: 0.1840 - mae: 0.3366 - val_loss: 0.4503 - val_mse: 0.4503 - val_mae: 0.5177\n",
            "Epoch 311/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1892 - mse: 0.1892 - mae: 0.3359 - val_loss: 0.4809 - val_mse: 0.4809 - val_mae: 0.5520\n",
            "Epoch 312/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1934 - mse: 0.1934 - mae: 0.3405 - val_loss: 0.3927 - val_mse: 0.3927 - val_mae: 0.4831\n",
            "Epoch 313/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1696 - mse: 0.1696 - mae: 0.3209 - val_loss: 0.4088 - val_mse: 0.4088 - val_mae: 0.4860\n",
            "Epoch 314/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1736 - mse: 0.1736 - mae: 0.3255 - val_loss: 0.4351 - val_mse: 0.4351 - val_mae: 0.4969\n",
            "Epoch 315/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1842 - mse: 0.1842 - mae: 0.3407 - val_loss: 0.4457 - val_mse: 0.4457 - val_mae: 0.5116\n",
            "Epoch 316/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1685 - mse: 0.1685 - mae: 0.3193 - val_loss: 0.4205 - val_mse: 0.4205 - val_mae: 0.5021\n",
            "Epoch 317/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1521 - mse: 0.1521 - mae: 0.3034 - val_loss: 0.4332 - val_mse: 0.4332 - val_mae: 0.4976\n",
            "Epoch 318/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1566 - mse: 0.1566 - mae: 0.3132 - val_loss: 0.4332 - val_mse: 0.4332 - val_mae: 0.5083\n",
            "Epoch 319/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1508 - mse: 0.1508 - mae: 0.2972 - val_loss: 0.4712 - val_mse: 0.4712 - val_mae: 0.5417\n",
            "Epoch 320/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1667 - mse: 0.1667 - mae: 0.3181 - val_loss: 0.4777 - val_mse: 0.4777 - val_mae: 0.5400\n",
            "Epoch 321/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1790 - mse: 0.1790 - mae: 0.3223 - val_loss: 0.4544 - val_mse: 0.4544 - val_mae: 0.5200\n",
            "Epoch 322/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1617 - mse: 0.1617 - mae: 0.3130 - val_loss: 0.4399 - val_mse: 0.4399 - val_mae: 0.5127\n",
            "Epoch 323/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1738 - mse: 0.1738 - mae: 0.3158 - val_loss: 0.4428 - val_mse: 0.4428 - val_mae: 0.5110\n",
            "Epoch 324/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1624 - mse: 0.1624 - mae: 0.3078 - val_loss: 0.4695 - val_mse: 0.4695 - val_mae: 0.5432\n",
            "Epoch 325/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1727 - mse: 0.1727 - mae: 0.3211 - val_loss: 0.4204 - val_mse: 0.4204 - val_mae: 0.5013\n",
            "Epoch 326/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1696 - mse: 0.1696 - mae: 0.3175 - val_loss: 0.4064 - val_mse: 0.4064 - val_mae: 0.4945\n",
            "Epoch 327/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1523 - mse: 0.1523 - mae: 0.3066 - val_loss: 0.4187 - val_mse: 0.4187 - val_mae: 0.5121\n",
            "Epoch 328/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1640 - mse: 0.1640 - mae: 0.3085 - val_loss: 0.4093 - val_mse: 0.4093 - val_mae: 0.4912\n",
            "Epoch 329/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1731 - mse: 0.1731 - mae: 0.3249 - val_loss: 0.4641 - val_mse: 0.4641 - val_mae: 0.5283\n",
            "Epoch 330/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1760 - mse: 0.1760 - mae: 0.3175 - val_loss: 0.4438 - val_mse: 0.4438 - val_mae: 0.5197\n",
            "Epoch 331/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1700 - mse: 0.1700 - mae: 0.3176 - val_loss: 0.4217 - val_mse: 0.4217 - val_mae: 0.5024\n",
            "Epoch 332/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1530 - mse: 0.1530 - mae: 0.3032 - val_loss: 0.4152 - val_mse: 0.4152 - val_mae: 0.4867\n",
            "Epoch 333/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1524 - mse: 0.1524 - mae: 0.3076 - val_loss: 0.5339 - val_mse: 0.5339 - val_mae: 0.5750\n",
            "Epoch 334/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2255 - mse: 0.2255 - mae: 0.3716 - val_loss: 0.4221 - val_mse: 0.4221 - val_mae: 0.4890\n",
            "Epoch 335/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1979 - mse: 0.1979 - mae: 0.3488 - val_loss: 0.3991 - val_mse: 0.3991 - val_mae: 0.4828\n",
            "Epoch 336/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2395 - mse: 0.2395 - mae: 0.3806 - val_loss: 0.4375 - val_mse: 0.4375 - val_mae: 0.5150\n",
            "Epoch 337/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1639 - mse: 0.1639 - mae: 0.3114 - val_loss: 0.4701 - val_mse: 0.4701 - val_mae: 0.5462\n",
            "Epoch 338/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1590 - mse: 0.1590 - mae: 0.3067 - val_loss: 0.4586 - val_mse: 0.4586 - val_mae: 0.5392\n",
            "Epoch 339/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1556 - mse: 0.1556 - mae: 0.2980 - val_loss: 0.4107 - val_mse: 0.4107 - val_mae: 0.4939\n",
            "Epoch 340/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1756 - mse: 0.1756 - mae: 0.3164 - val_loss: 0.4263 - val_mse: 0.4263 - val_mae: 0.4943\n",
            "Epoch 341/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1943 - mse: 0.1943 - mae: 0.3375 - val_loss: 0.4344 - val_mse: 0.4344 - val_mae: 0.5244\n",
            "Epoch 342/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1557 - mse: 0.1557 - mae: 0.3009 - val_loss: 0.4240 - val_mse: 0.4240 - val_mae: 0.5095\n",
            "Epoch 343/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1586 - mse: 0.1586 - mae: 0.3085 - val_loss: 0.4114 - val_mse: 0.4114 - val_mae: 0.4933\n",
            "Epoch 344/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1573 - mse: 0.1573 - mae: 0.3092 - val_loss: 0.4157 - val_mse: 0.4157 - val_mae: 0.5009\n",
            "Epoch 345/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1364 - mse: 0.1364 - mae: 0.2805 - val_loss: 0.4690 - val_mse: 0.4690 - val_mae: 0.5518\n",
            "Epoch 346/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1832 - mse: 0.1832 - mae: 0.3355 - val_loss: 0.4242 - val_mse: 0.4242 - val_mae: 0.4951\n",
            "Epoch 347/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1652 - mse: 0.1652 - mae: 0.3116 - val_loss: 0.4113 - val_mse: 0.4113 - val_mae: 0.4965\n",
            "Epoch 348/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1642 - mse: 0.1642 - mae: 0.3177 - val_loss: 0.4230 - val_mse: 0.4230 - val_mae: 0.5028\n",
            "Epoch 349/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1811 - mse: 0.1811 - mae: 0.3267 - val_loss: 0.4679 - val_mse: 0.4679 - val_mae: 0.5498\n",
            "Epoch 350/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1687 - mse: 0.1687 - mae: 0.3195 - val_loss: 0.3974 - val_mse: 0.3974 - val_mae: 0.4817\n",
            "Epoch 351/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1846 - mse: 0.1846 - mae: 0.3305 - val_loss: 0.4269 - val_mse: 0.4269 - val_mae: 0.5171\n",
            "Epoch 352/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1827 - mse: 0.1827 - mae: 0.3251 - val_loss: 0.4574 - val_mse: 0.4574 - val_mae: 0.5273\n",
            "Epoch 353/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1718 - mse: 0.1718 - mae: 0.3155 - val_loss: 0.4127 - val_mse: 0.4127 - val_mae: 0.4938\n",
            "Epoch 354/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1440 - mse: 0.1440 - mae: 0.2945 - val_loss: 0.3955 - val_mse: 0.3955 - val_mae: 0.4864\n",
            "Epoch 355/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1347 - mse: 0.1347 - mae: 0.2855 - val_loss: 0.3878 - val_mse: 0.3878 - val_mae: 0.4896\n",
            "Epoch 356/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1602 - mse: 0.1602 - mae: 0.3061 - val_loss: 0.4166 - val_mse: 0.4166 - val_mae: 0.4969\n",
            "Epoch 357/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1387 - mse: 0.1387 - mae: 0.2869 - val_loss: 0.4328 - val_mse: 0.4328 - val_mae: 0.5151\n",
            "Epoch 358/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1543 - mse: 0.1543 - mae: 0.3010 - val_loss: 0.4930 - val_mse: 0.4930 - val_mae: 0.5621\n",
            "Epoch 359/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1655 - mse: 0.1655 - mae: 0.3158 - val_loss: 0.4404 - val_mse: 0.4404 - val_mae: 0.5043\n",
            "Epoch 360/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1493 - mse: 0.1493 - mae: 0.2908 - val_loss: 0.4232 - val_mse: 0.4232 - val_mae: 0.5193\n",
            "Epoch 361/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1534 - mse: 0.1534 - mae: 0.3049 - val_loss: 0.4500 - val_mse: 0.4500 - val_mae: 0.5324\n",
            "Epoch 362/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1761 - mse: 0.1761 - mae: 0.3329 - val_loss: 0.4233 - val_mse: 0.4233 - val_mae: 0.4906\n",
            "Epoch 363/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1629 - mse: 0.1629 - mae: 0.3133 - val_loss: 0.4488 - val_mse: 0.4488 - val_mae: 0.5129\n",
            "Epoch 364/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1486 - mse: 0.1486 - mae: 0.2977 - val_loss: 0.4155 - val_mse: 0.4155 - val_mae: 0.4989\n",
            "Epoch 365/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1645 - mse: 0.1645 - mae: 0.3152 - val_loss: 0.4020 - val_mse: 0.4020 - val_mae: 0.4971\n",
            "Epoch 366/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1402 - mse: 0.1402 - mae: 0.2828 - val_loss: 0.4155 - val_mse: 0.4155 - val_mae: 0.4982\n",
            "Epoch 367/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1512 - mse: 0.1512 - mae: 0.2929 - val_loss: 0.4571 - val_mse: 0.4571 - val_mae: 0.5310\n",
            "Epoch 368/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1615 - mse: 0.1615 - mae: 0.3084 - val_loss: 0.4149 - val_mse: 0.4149 - val_mae: 0.4932\n",
            "Epoch 369/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1590 - mse: 0.1590 - mae: 0.3086 - val_loss: 0.4289 - val_mse: 0.4289 - val_mae: 0.5067\n",
            "Epoch 370/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1497 - mse: 0.1497 - mae: 0.2991 - val_loss: 0.3953 - val_mse: 0.3953 - val_mae: 0.4868\n",
            "Epoch 371/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1724 - mse: 0.1724 - mae: 0.3177 - val_loss: 0.4413 - val_mse: 0.4413 - val_mae: 0.5090\n",
            "Epoch 372/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1699 - mse: 0.1699 - mae: 0.3188 - val_loss: 0.4250 - val_mse: 0.4250 - val_mae: 0.5025\n",
            "Epoch 373/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1435 - mse: 0.1435 - mae: 0.2919 - val_loss: 0.4676 - val_mse: 0.4676 - val_mae: 0.5378\n",
            "Epoch 374/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1675 - mse: 0.1675 - mae: 0.3155 - val_loss: 0.4129 - val_mse: 0.4129 - val_mae: 0.4979\n",
            "Epoch 375/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1661 - mse: 0.1661 - mae: 0.3167 - val_loss: 0.4241 - val_mse: 0.4241 - val_mae: 0.5073\n",
            "Epoch 376/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1643 - mse: 0.1643 - mae: 0.3106 - val_loss: 0.4040 - val_mse: 0.4040 - val_mae: 0.4879\n",
            "Epoch 377/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1782 - mse: 0.1782 - mae: 0.3259 - val_loss: 0.4164 - val_mse: 0.4164 - val_mae: 0.4987\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.4667 - mse: 0.4667 - mae: 0.4978\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 17ms/step - loss: 21.1243 - mse: 21.1243 - mae: 4.3534 - val_loss: 8.7000 - val_mse: 8.7000 - val_mae: 2.6976\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5.2398 - mse: 5.2398 - mae: 1.9244 - val_loss: 4.0552 - val_mse: 4.0552 - val_mae: 1.5763\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 3.6171 - mse: 3.6171 - mae: 1.4716 - val_loss: 3.1348 - val_mse: 3.1348 - val_mae: 1.4429\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.8576 - mse: 2.8576 - mae: 1.3529 - val_loss: 2.6998 - val_mse: 2.6998 - val_mae: 1.3519\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.5003 - mse: 2.5003 - mae: 1.2559 - val_loss: 2.4364 - val_mse: 2.4364 - val_mae: 1.2819\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.1412 - mse: 2.1412 - mae: 1.1508 - val_loss: 2.2376 - val_mse: 2.2376 - val_mae: 1.2248\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.1562 - mse: 2.1562 - mae: 1.1530 - val_loss: 2.1055 - val_mse: 2.1055 - val_mae: 1.1824\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.9566 - mse: 1.9566 - mae: 1.1110 - val_loss: 2.0158 - val_mse: 2.0158 - val_mae: 1.1527\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.8599 - mse: 1.8599 - mae: 1.0771 - val_loss: 1.8982 - val_mse: 1.8982 - val_mae: 1.1166\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.6786 - mse: 1.6786 - mae: 1.0041 - val_loss: 1.8090 - val_mse: 1.8090 - val_mae: 1.0938\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.6361 - mse: 1.6361 - mae: 1.0176 - val_loss: 1.7610 - val_mse: 1.7610 - val_mae: 1.0787\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.5071 - mse: 1.5071 - mae: 0.9671 - val_loss: 1.6807 - val_mse: 1.6807 - val_mae: 1.0509\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.4697 - mse: 1.4697 - mae: 0.9558 - val_loss: 1.6176 - val_mse: 1.6176 - val_mae: 1.0326\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.4496 - mse: 1.4496 - mae: 0.9403 - val_loss: 1.5488 - val_mse: 1.5488 - val_mae: 1.0081\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.3168 - mse: 1.3168 - mae: 0.9023 - val_loss: 1.4998 - val_mse: 1.4998 - val_mae: 0.9884\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.3011 - mse: 1.3011 - mae: 0.8917 - val_loss: 1.4385 - val_mse: 1.4385 - val_mae: 0.9760\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2648 - mse: 1.2648 - mae: 0.8793 - val_loss: 1.3716 - val_mse: 1.3716 - val_mae: 0.9427\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.2720 - mse: 1.2720 - mae: 0.8841 - val_loss: 1.3519 - val_mse: 1.3519 - val_mae: 0.9355\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.1346 - mse: 1.1346 - mae: 0.8262 - val_loss: 1.3157 - val_mse: 1.3157 - val_mae: 0.9262\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1165 - mse: 1.1165 - mae: 0.8243 - val_loss: 1.2431 - val_mse: 1.2431 - val_mae: 0.9014\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 1.0735 - mse: 1.0735 - mae: 0.8164 - val_loss: 1.2045 - val_mse: 1.2045 - val_mae: 0.8815\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0095 - mse: 1.0095 - mae: 0.7822 - val_loss: 1.1583 - val_mse: 1.1583 - val_mae: 0.8619\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.0106 - mse: 1.0106 - mae: 0.7792 - val_loss: 1.1309 - val_mse: 1.1309 - val_mae: 0.8524\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 1.0575 - mse: 1.0575 - mae: 0.8109 - val_loss: 1.1050 - val_mse: 1.1050 - val_mae: 0.8349\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.9609 - mse: 0.9609 - mae: 0.7597 - val_loss: 1.0645 - val_mse: 1.0645 - val_mae: 0.8200\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.9045 - mse: 0.9045 - mae: 0.7362 - val_loss: 1.0296 - val_mse: 1.0296 - val_mae: 0.8116\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8740 - mse: 0.8740 - mae: 0.7338 - val_loss: 1.0049 - val_mse: 1.0049 - val_mae: 0.7840\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.8762 - mse: 0.8762 - mae: 0.7230 - val_loss: 0.9737 - val_mse: 0.9737 - val_mae: 0.7870\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8227 - mse: 0.8227 - mae: 0.6990 - val_loss: 0.9363 - val_mse: 0.9363 - val_mae: 0.7610\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.8248 - mse: 0.8248 - mae: 0.7114 - val_loss: 0.9210 - val_mse: 0.9210 - val_mae: 0.7724\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7648 - mse: 0.7648 - mae: 0.6810 - val_loss: 0.9022 - val_mse: 0.9022 - val_mae: 0.7474\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.7932 - mse: 0.7932 - mae: 0.6902 - val_loss: 0.8734 - val_mse: 0.8734 - val_mae: 0.7246\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7529 - mse: 0.7529 - mae: 0.6715 - val_loss: 0.8521 - val_mse: 0.8521 - val_mae: 0.7113\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7560 - mse: 0.7560 - mae: 0.6706 - val_loss: 0.8173 - val_mse: 0.8173 - val_mae: 0.6973\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.6964 - mse: 0.6964 - mae: 0.6529 - val_loss: 0.7987 - val_mse: 0.7987 - val_mae: 0.6897\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.6495 - mse: 0.6495 - mae: 0.6263 - val_loss: 0.7878 - val_mse: 0.7878 - val_mae: 0.6937\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7369 - mse: 0.7369 - mae: 0.6645 - val_loss: 0.7790 - val_mse: 0.7790 - val_mae: 0.6968\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.6082 - mse: 0.6082 - mae: 0.6062 - val_loss: 0.7587 - val_mse: 0.7587 - val_mae: 0.6768\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.6785 - mse: 0.6785 - mae: 0.6351 - val_loss: 0.7614 - val_mse: 0.7614 - val_mae: 0.6898\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.6605 - mse: 0.6605 - mae: 0.6276 - val_loss: 0.7424 - val_mse: 0.7424 - val_mae: 0.6685\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.6721 - mse: 0.6721 - mae: 0.6476 - val_loss: 0.7266 - val_mse: 0.7266 - val_mae: 0.6746\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.5542 - mse: 0.5542 - mae: 0.5770 - val_loss: 0.7054 - val_mse: 0.7054 - val_mae: 0.6383\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5735 - mse: 0.5735 - mae: 0.5939 - val_loss: 0.7193 - val_mse: 0.7193 - val_mae: 0.6330\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5617 - mse: 0.5617 - mae: 0.5831 - val_loss: 0.6886 - val_mse: 0.6886 - val_mae: 0.6286\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5625 - mse: 0.5625 - mae: 0.5691 - val_loss: 0.6544 - val_mse: 0.6544 - val_mae: 0.6176\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5273 - mse: 0.5273 - mae: 0.5597 - val_loss: 0.6637 - val_mse: 0.6637 - val_mae: 0.6124\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5461 - mse: 0.5461 - mae: 0.5698 - val_loss: 0.6641 - val_mse: 0.6641 - val_mae: 0.6151\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4922 - mse: 0.4922 - mae: 0.5439 - val_loss: 0.6619 - val_mse: 0.6619 - val_mae: 0.6200\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4962 - mse: 0.4962 - mae: 0.5458 - val_loss: 0.6467 - val_mse: 0.6467 - val_mae: 0.6091\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5028 - mse: 0.5028 - mae: 0.5549 - val_loss: 0.6130 - val_mse: 0.6130 - val_mae: 0.5999\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5322 - mse: 0.5322 - mae: 0.5610 - val_loss: 0.6150 - val_mse: 0.6150 - val_mae: 0.6100\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5197 - mse: 0.5197 - mae: 0.5573 - val_loss: 0.6137 - val_mse: 0.6137 - val_mae: 0.5945\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4521 - mse: 0.4521 - mae: 0.5245 - val_loss: 0.6063 - val_mse: 0.6063 - val_mae: 0.5938\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4661 - mse: 0.4661 - mae: 0.5264 - val_loss: 0.5955 - val_mse: 0.5955 - val_mae: 0.5980\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4467 - mse: 0.4467 - mae: 0.5173 - val_loss: 0.5798 - val_mse: 0.5798 - val_mae: 0.5911\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4078 - mse: 0.4078 - mae: 0.5003 - val_loss: 0.5771 - val_mse: 0.5771 - val_mae: 0.5912\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4502 - mse: 0.4502 - mae: 0.5196 - val_loss: 0.5655 - val_mse: 0.5655 - val_mae: 0.5941\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4373 - mse: 0.4373 - mae: 0.5120 - val_loss: 0.5647 - val_mse: 0.5647 - val_mae: 0.5819\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4430 - mse: 0.4430 - mae: 0.5175 - val_loss: 0.5508 - val_mse: 0.5508 - val_mae: 0.5787\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4276 - mse: 0.4276 - mae: 0.5029 - val_loss: 0.5414 - val_mse: 0.5414 - val_mae: 0.5723\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4372 - mse: 0.4372 - mae: 0.5051 - val_loss: 0.5257 - val_mse: 0.5257 - val_mae: 0.5686\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4119 - mse: 0.4119 - mae: 0.4959 - val_loss: 0.5292 - val_mse: 0.5292 - val_mae: 0.5604\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4141 - mse: 0.4141 - mae: 0.4945 - val_loss: 0.5424 - val_mse: 0.5424 - val_mae: 0.5688\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3956 - mse: 0.3956 - mae: 0.4841 - val_loss: 0.5237 - val_mse: 0.5237 - val_mae: 0.5616\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4115 - mse: 0.4115 - mae: 0.5035 - val_loss: 0.5077 - val_mse: 0.5077 - val_mae: 0.5590\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4283 - mse: 0.4283 - mae: 0.5094 - val_loss: 0.5252 - val_mse: 0.5252 - val_mae: 0.5727\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4008 - mse: 0.4008 - mae: 0.4887 - val_loss: 0.5138 - val_mse: 0.5138 - val_mae: 0.5541\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3967 - mse: 0.3967 - mae: 0.4835 - val_loss: 0.5291 - val_mse: 0.5291 - val_mae: 0.5689\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4478 - mse: 0.4478 - mae: 0.5286 - val_loss: 0.4910 - val_mse: 0.4910 - val_mae: 0.5490\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4185 - mse: 0.4185 - mae: 0.5058 - val_loss: 0.5311 - val_mse: 0.5311 - val_mae: 0.5594\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4059 - mse: 0.4059 - mae: 0.4945 - val_loss: 0.4854 - val_mse: 0.4854 - val_mae: 0.5466\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3733 - mse: 0.3733 - mae: 0.4732 - val_loss: 0.4949 - val_mse: 0.4949 - val_mae: 0.5421\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3924 - mse: 0.3924 - mae: 0.4872 - val_loss: 0.4886 - val_mse: 0.4886 - val_mae: 0.5389\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3428 - mse: 0.3428 - mae: 0.4578 - val_loss: 0.4876 - val_mse: 0.4876 - val_mae: 0.5451\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3700 - mse: 0.3700 - mae: 0.4731 - val_loss: 0.4983 - val_mse: 0.4983 - val_mae: 0.5404\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3744 - mse: 0.3744 - mae: 0.4717 - val_loss: 0.5025 - val_mse: 0.5025 - val_mae: 0.5438\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4178 - mse: 0.4178 - mae: 0.4928 - val_loss: 0.4696 - val_mse: 0.4696 - val_mae: 0.5332\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3457 - mse: 0.3457 - mae: 0.4620 - val_loss: 0.4769 - val_mse: 0.4769 - val_mae: 0.5451\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3679 - mse: 0.3679 - mae: 0.4741 - val_loss: 0.4699 - val_mse: 0.4699 - val_mae: 0.5285\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3432 - mse: 0.3432 - mae: 0.4548 - val_loss: 0.4515 - val_mse: 0.4515 - val_mae: 0.5204\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3506 - mse: 0.3506 - mae: 0.4573 - val_loss: 0.4518 - val_mse: 0.4518 - val_mae: 0.5243\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3511 - mse: 0.3511 - mae: 0.4603 - val_loss: 0.4595 - val_mse: 0.4595 - val_mae: 0.5248\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3693 - mse: 0.3693 - mae: 0.4682 - val_loss: 0.4515 - val_mse: 0.4515 - val_mae: 0.5208\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3425 - mse: 0.3425 - mae: 0.4574 - val_loss: 0.4689 - val_mse: 0.4689 - val_mae: 0.5304\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3393 - mse: 0.3393 - mae: 0.4625 - val_loss: 0.4491 - val_mse: 0.4491 - val_mae: 0.5210\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3258 - mse: 0.3258 - mae: 0.4454 - val_loss: 0.4376 - val_mse: 0.4376 - val_mae: 0.5144\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3375 - mse: 0.3375 - mae: 0.4440 - val_loss: 0.4607 - val_mse: 0.4607 - val_mae: 0.5286\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3405 - mse: 0.3405 - mae: 0.4510 - val_loss: 0.4516 - val_mse: 0.4516 - val_mae: 0.5191\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3327 - mse: 0.3327 - mae: 0.4518 - val_loss: 0.4430 - val_mse: 0.4430 - val_mae: 0.5131\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3386 - mse: 0.3386 - mae: 0.4485 - val_loss: 0.4493 - val_mse: 0.4493 - val_mae: 0.5180\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3494 - mse: 0.3494 - mae: 0.4598 - val_loss: 0.5137 - val_mse: 0.5137 - val_mae: 0.5435\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3314 - mse: 0.3314 - mae: 0.4453 - val_loss: 0.4244 - val_mse: 0.4244 - val_mae: 0.5054\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3253 - mse: 0.3253 - mae: 0.4428 - val_loss: 0.4250 - val_mse: 0.4250 - val_mae: 0.5058\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3234 - mse: 0.3234 - mae: 0.4427 - val_loss: 0.4524 - val_mse: 0.4524 - val_mae: 0.5129\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.3319 - mse: 0.3319 - mae: 0.4486 - val_loss: 0.4220 - val_mse: 0.4220 - val_mae: 0.5047\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3177 - mse: 0.3177 - mae: 0.4376 - val_loss: 0.4159 - val_mse: 0.4159 - val_mae: 0.4970\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3172 - mse: 0.3172 - mae: 0.4281 - val_loss: 0.4247 - val_mse: 0.4247 - val_mae: 0.5053\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3313 - mse: 0.3313 - mae: 0.4454 - val_loss: 0.4489 - val_mse: 0.4489 - val_mae: 0.5120\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3305 - mse: 0.3305 - mae: 0.4525 - val_loss: 0.4178 - val_mse: 0.4178 - val_mae: 0.4992\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3124 - mse: 0.3124 - mae: 0.4317 - val_loss: 0.4668 - val_mse: 0.4668 - val_mae: 0.5281\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.3330 - mse: 0.3330 - mae: 0.4466 - val_loss: 0.4483 - val_mse: 0.4483 - val_mae: 0.5129\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3083 - mse: 0.3083 - mae: 0.4328 - val_loss: 0.4465 - val_mse: 0.4465 - val_mae: 0.5161\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.3401 - mse: 0.3401 - mae: 0.4551 - val_loss: 0.4785 - val_mse: 0.4785 - val_mae: 0.5318\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2774 - mse: 0.2774 - mae: 0.4075 - val_loss: 0.4328 - val_mse: 0.4328 - val_mae: 0.5089\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3293 - mse: 0.3293 - mae: 0.4521 - val_loss: 0.4696 - val_mse: 0.4696 - val_mae: 0.5227\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.3052 - mse: 0.3052 - mae: 0.4364 - val_loss: 0.4155 - val_mse: 0.4155 - val_mae: 0.4955\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3233 - mse: 0.3233 - mae: 0.4413 - val_loss: 0.4271 - val_mse: 0.4271 - val_mae: 0.5044\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2933 - mse: 0.2933 - mae: 0.4291 - val_loss: 0.4512 - val_mse: 0.4512 - val_mae: 0.5102\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3045 - mse: 0.3045 - mae: 0.4287 - val_loss: 0.4170 - val_mse: 0.4170 - val_mae: 0.4965\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2747 - mse: 0.2747 - mae: 0.4074 - val_loss: 0.4167 - val_mse: 0.4167 - val_mae: 0.4925\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3201 - mse: 0.3201 - mae: 0.4394 - val_loss: 0.4149 - val_mse: 0.4149 - val_mae: 0.5026\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2880 - mse: 0.2880 - mae: 0.4185 - val_loss: 0.4201 - val_mse: 0.4201 - val_mae: 0.4952\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2945 - mse: 0.2945 - mae: 0.4225 - val_loss: 0.4271 - val_mse: 0.4271 - val_mae: 0.5039\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3077 - mse: 0.3077 - mae: 0.4377 - val_loss: 0.4220 - val_mse: 0.4220 - val_mae: 0.4939\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2908 - mse: 0.2908 - mae: 0.4193 - val_loss: 0.4866 - val_mse: 0.4866 - val_mae: 0.5375\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2999 - mse: 0.2999 - mae: 0.4273 - val_loss: 0.4224 - val_mse: 0.4224 - val_mae: 0.4941\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3144 - mse: 0.3144 - mae: 0.4338 - val_loss: 0.4131 - val_mse: 0.4131 - val_mae: 0.4894\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2806 - mse: 0.2806 - mae: 0.4161 - val_loss: 0.4507 - val_mse: 0.4507 - val_mae: 0.5106\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2747 - mse: 0.2747 - mae: 0.4047 - val_loss: 0.4680 - val_mse: 0.4680 - val_mae: 0.5168\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2969 - mse: 0.2969 - mae: 0.4238 - val_loss: 0.4531 - val_mse: 0.4531 - val_mae: 0.5192\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2841 - mse: 0.2841 - mae: 0.4143 - val_loss: 0.4268 - val_mse: 0.4268 - val_mae: 0.5027\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3183 - mse: 0.3183 - mae: 0.4418 - val_loss: 0.5020 - val_mse: 0.5020 - val_mae: 0.5408\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3326 - mse: 0.3326 - mae: 0.4455 - val_loss: 0.4137 - val_mse: 0.4137 - val_mae: 0.4909\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2999 - mse: 0.2999 - mae: 0.4331 - val_loss: 0.4119 - val_mse: 0.4119 - val_mae: 0.4954\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2654 - mse: 0.2654 - mae: 0.4049 - val_loss: 0.4923 - val_mse: 0.4923 - val_mae: 0.5286\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2821 - mse: 0.2821 - mae: 0.4068 - val_loss: 0.4206 - val_mse: 0.4206 - val_mae: 0.5038\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2732 - mse: 0.2732 - mae: 0.3977 - val_loss: 0.4350 - val_mse: 0.4350 - val_mae: 0.5045\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2856 - mse: 0.2856 - mae: 0.4076 - val_loss: 0.4036 - val_mse: 0.4036 - val_mae: 0.4870\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2751 - mse: 0.2751 - mae: 0.4087 - val_loss: 0.4099 - val_mse: 0.4099 - val_mae: 0.4992\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2991 - mse: 0.2991 - mae: 0.4239 - val_loss: 0.4224 - val_mse: 0.4224 - val_mae: 0.4981\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2698 - mse: 0.2698 - mae: 0.4019 - val_loss: 0.4423 - val_mse: 0.4423 - val_mae: 0.5085\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2796 - mse: 0.2796 - mae: 0.4076 - val_loss: 0.4038 - val_mse: 0.4038 - val_mae: 0.4959\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2921 - mse: 0.2921 - mae: 0.4235 - val_loss: 0.4254 - val_mse: 0.4254 - val_mae: 0.4989\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2753 - mse: 0.2753 - mae: 0.4098 - val_loss: 0.4078 - val_mse: 0.4078 - val_mae: 0.4863\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2960 - mse: 0.2960 - mae: 0.4247 - val_loss: 0.4592 - val_mse: 0.4592 - val_mae: 0.5401\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2860 - mse: 0.2860 - mae: 0.4179 - val_loss: 0.4090 - val_mse: 0.4090 - val_mae: 0.4920\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2632 - mse: 0.2632 - mae: 0.3941 - val_loss: 0.4698 - val_mse: 0.4698 - val_mae: 0.5243\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2985 - mse: 0.2985 - mae: 0.4250 - val_loss: 0.4086 - val_mse: 0.4086 - val_mae: 0.4949\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2649 - mse: 0.2649 - mae: 0.4025 - val_loss: 0.4320 - val_mse: 0.4320 - val_mae: 0.5033\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2583 - mse: 0.2583 - mae: 0.3880 - val_loss: 0.4225 - val_mse: 0.4225 - val_mae: 0.4977\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2627 - mse: 0.2627 - mae: 0.4030 - val_loss: 0.3925 - val_mse: 0.3925 - val_mae: 0.4815\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2520 - mse: 0.2520 - mae: 0.3931 - val_loss: 0.4047 - val_mse: 0.4047 - val_mae: 0.4838\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2764 - mse: 0.2764 - mae: 0.4053 - val_loss: 0.4062 - val_mse: 0.4062 - val_mae: 0.4919\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2834 - mse: 0.2834 - mae: 0.4135 - val_loss: 0.3962 - val_mse: 0.3962 - val_mae: 0.4867\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2517 - mse: 0.2517 - mae: 0.3906 - val_loss: 0.4124 - val_mse: 0.4124 - val_mae: 0.4904\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2938 - mse: 0.2938 - mae: 0.4258 - val_loss: 0.3985 - val_mse: 0.3985 - val_mae: 0.4908\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2414 - mse: 0.2414 - mae: 0.3777 - val_loss: 0.4110 - val_mse: 0.4110 - val_mae: 0.5034\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2941 - mse: 0.2941 - mae: 0.4212 - val_loss: 0.3986 - val_mse: 0.3986 - val_mae: 0.4881\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2751 - mse: 0.2751 - mae: 0.4091 - val_loss: 0.3911 - val_mse: 0.3911 - val_mae: 0.4800\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2510 - mse: 0.2510 - mae: 0.3946 - val_loss: 0.4019 - val_mse: 0.4019 - val_mae: 0.4864\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2564 - mse: 0.2564 - mae: 0.3865 - val_loss: 0.4046 - val_mse: 0.4046 - val_mae: 0.4941\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2472 - mse: 0.2472 - mae: 0.3858 - val_loss: 0.3970 - val_mse: 0.3970 - val_mae: 0.4864\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2819 - mse: 0.2819 - mae: 0.4113 - val_loss: 0.4804 - val_mse: 0.4804 - val_mae: 0.5324\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2950 - mse: 0.2950 - mae: 0.4299 - val_loss: 0.4786 - val_mse: 0.4786 - val_mae: 0.5309\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2334 - mse: 0.2334 - mae: 0.3698 - val_loss: 0.3921 - val_mse: 0.3921 - val_mae: 0.4837\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2604 - mse: 0.2604 - mae: 0.3972 - val_loss: 0.4091 - val_mse: 0.4091 - val_mae: 0.4884\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2428 - mse: 0.2428 - mae: 0.3806 - val_loss: 0.4020 - val_mse: 0.4020 - val_mae: 0.4868\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2565 - mse: 0.2565 - mae: 0.3910 - val_loss: 0.4307 - val_mse: 0.4307 - val_mae: 0.4981\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2953 - mse: 0.2953 - mae: 0.4251 - val_loss: 0.4233 - val_mse: 0.4233 - val_mae: 0.4895\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2854 - mse: 0.2854 - mae: 0.4107 - val_loss: 0.3994 - val_mse: 0.3994 - val_mae: 0.4924\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2494 - mse: 0.2494 - mae: 0.3904 - val_loss: 0.4291 - val_mse: 0.4291 - val_mae: 0.4907\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2521 - mse: 0.2521 - mae: 0.3938 - val_loss: 0.4143 - val_mse: 0.4143 - val_mae: 0.5074\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2636 - mse: 0.2636 - mae: 0.3993 - val_loss: 0.3875 - val_mse: 0.3875 - val_mae: 0.4731\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2212 - mse: 0.2212 - mae: 0.3624 - val_loss: 0.3939 - val_mse: 0.3939 - val_mae: 0.4780\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2546 - mse: 0.2546 - mae: 0.3885 - val_loss: 0.4314 - val_mse: 0.4314 - val_mae: 0.4966\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2362 - mse: 0.2362 - mae: 0.3803 - val_loss: 0.4385 - val_mse: 0.4385 - val_mae: 0.4991\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2539 - mse: 0.2539 - mae: 0.3907 - val_loss: 0.4063 - val_mse: 0.4063 - val_mae: 0.4949\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3087 - mse: 0.3087 - mae: 0.4304 - val_loss: 0.4172 - val_mse: 0.4172 - val_mae: 0.4941\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2708 - mse: 0.2708 - mae: 0.4128 - val_loss: 0.4128 - val_mse: 0.4128 - val_mae: 0.4918\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2495 - mse: 0.2495 - mae: 0.3899 - val_loss: 0.3827 - val_mse: 0.3827 - val_mae: 0.4736\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2448 - mse: 0.2448 - mae: 0.3771 - val_loss: 0.4049 - val_mse: 0.4049 - val_mae: 0.4855\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2621 - mse: 0.2621 - mae: 0.4019 - val_loss: 0.3948 - val_mse: 0.3948 - val_mae: 0.4733\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2476 - mse: 0.2476 - mae: 0.3842 - val_loss: 0.4083 - val_mse: 0.4083 - val_mae: 0.4858\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2474 - mse: 0.2474 - mae: 0.3887 - val_loss: 0.4207 - val_mse: 0.4207 - val_mae: 0.4951\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2369 - mse: 0.2369 - mae: 0.3820 - val_loss: 0.4179 - val_mse: 0.4179 - val_mae: 0.5078\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2564 - mse: 0.2564 - mae: 0.3896 - val_loss: 0.3901 - val_mse: 0.3901 - val_mae: 0.4831\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2415 - mse: 0.2415 - mae: 0.3796 - val_loss: 0.4292 - val_mse: 0.4292 - val_mae: 0.4932\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2515 - mse: 0.2515 - mae: 0.3904 - val_loss: 0.4095 - val_mse: 0.4095 - val_mae: 0.4907\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2558 - mse: 0.2558 - mae: 0.3817 - val_loss: 0.4184 - val_mse: 0.4184 - val_mae: 0.4990\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2674 - mse: 0.2674 - mae: 0.3959 - val_loss: 0.4121 - val_mse: 0.4121 - val_mae: 0.5042\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2384 - mse: 0.2384 - mae: 0.3766 - val_loss: 0.3988 - val_mse: 0.3988 - val_mae: 0.4816\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2326 - mse: 0.2326 - mae: 0.3717 - val_loss: 0.3887 - val_mse: 0.3887 - val_mae: 0.4752\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2257 - mse: 0.2257 - mae: 0.3679 - val_loss: 0.4093 - val_mse: 0.4093 - val_mae: 0.4966\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2641 - mse: 0.2641 - mae: 0.3934 - val_loss: 0.4035 - val_mse: 0.4035 - val_mae: 0.4816\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2724 - mse: 0.2724 - mae: 0.4034 - val_loss: 0.4063 - val_mse: 0.4063 - val_mae: 0.4820\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2467 - mse: 0.2467 - mae: 0.3905 - val_loss: 0.3959 - val_mse: 0.3959 - val_mae: 0.4944\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2510 - mse: 0.2510 - mae: 0.3904 - val_loss: 0.3876 - val_mse: 0.3876 - val_mae: 0.4728\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2276 - mse: 0.2276 - mae: 0.3679 - val_loss: 0.4761 - val_mse: 0.4761 - val_mae: 0.5246\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3167 - mse: 0.3167 - mae: 0.4287 - val_loss: 0.4169 - val_mse: 0.4169 - val_mae: 0.4983\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2912 - mse: 0.2912 - mae: 0.4104 - val_loss: 0.4002 - val_mse: 0.4002 - val_mae: 0.4858\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2198 - mse: 0.2198 - mae: 0.3549 - val_loss: 0.3887 - val_mse: 0.3887 - val_mae: 0.4794\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2272 - mse: 0.2272 - mae: 0.3664 - val_loss: 0.3954 - val_mse: 0.3954 - val_mae: 0.4793\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2327 - mse: 0.2327 - mae: 0.3654 - val_loss: 0.4057 - val_mse: 0.4057 - val_mae: 0.4834\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2261 - mse: 0.2261 - mae: 0.3718 - val_loss: 0.4122 - val_mse: 0.4122 - val_mae: 0.4942\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2429 - mse: 0.2429 - mae: 0.3788 - val_loss: 0.4075 - val_mse: 0.4075 - val_mae: 0.4906\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2372 - mse: 0.2372 - mae: 0.3743 - val_loss: 0.4164 - val_mse: 0.4164 - val_mae: 0.4853\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2378 - mse: 0.2378 - mae: 0.3789 - val_loss: 0.4060 - val_mse: 0.4060 - val_mae: 0.4855\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2262 - mse: 0.2262 - mae: 0.3643 - val_loss: 0.4004 - val_mse: 0.4004 - val_mae: 0.4915\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2450 - mse: 0.2450 - mae: 0.3840 - val_loss: 0.3975 - val_mse: 0.3975 - val_mae: 0.4802\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2398 - mse: 0.2398 - mae: 0.3793 - val_loss: 0.3994 - val_mse: 0.3994 - val_mae: 0.4934\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2313 - mse: 0.2313 - mae: 0.3670 - val_loss: 0.4107 - val_mse: 0.4107 - val_mae: 0.4864\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2453 - mse: 0.2453 - mae: 0.3916 - val_loss: 0.4329 - val_mse: 0.4329 - val_mae: 0.5049\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2481 - mse: 0.2481 - mae: 0.3921 - val_loss: 0.4107 - val_mse: 0.4107 - val_mae: 0.5002\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2398 - mse: 0.2398 - mae: 0.3833 - val_loss: 0.4014 - val_mse: 0.4014 - val_mae: 0.4878\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2247 - mse: 0.2247 - mae: 0.3689 - val_loss: 0.4122 - val_mse: 0.4122 - val_mae: 0.4831\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2585 - mse: 0.2585 - mae: 0.3921 - val_loss: 0.4160 - val_mse: 0.4160 - val_mae: 0.4896\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2291 - mse: 0.2291 - mae: 0.3715 - val_loss: 0.4599 - val_mse: 0.4599 - val_mae: 0.5228\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2274 - mse: 0.2274 - mae: 0.3802 - val_loss: 0.4489 - val_mse: 0.4489 - val_mae: 0.5073\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2126 - mse: 0.2126 - mae: 0.3545 - val_loss: 0.4195 - val_mse: 0.4195 - val_mae: 0.4932\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2129 - mse: 0.2129 - mae: 0.3574 - val_loss: 0.4286 - val_mse: 0.4286 - val_mae: 0.4982\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2372 - mse: 0.2372 - mae: 0.3758 - val_loss: 0.4527 - val_mse: 0.4527 - val_mae: 0.5073\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2398 - mse: 0.2398 - mae: 0.3767 - val_loss: 0.4018 - val_mse: 0.4018 - val_mae: 0.4808\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2305 - mse: 0.2305 - mae: 0.3656 - val_loss: 0.3891 - val_mse: 0.3891 - val_mae: 0.4839\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2452 - mse: 0.2452 - mae: 0.3808 - val_loss: 0.4344 - val_mse: 0.4344 - val_mae: 0.5028\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2364 - mse: 0.2364 - mae: 0.3767 - val_loss: 0.4255 - val_mse: 0.4255 - val_mae: 0.5017\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2369 - mse: 0.2369 - mae: 0.3812 - val_loss: 0.3839 - val_mse: 0.3839 - val_mae: 0.4700\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2438 - mse: 0.2438 - mae: 0.3673 - val_loss: 0.4243 - val_mse: 0.4243 - val_mae: 0.4964\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2398 - mse: 0.2398 - mae: 0.3810 - val_loss: 0.4132 - val_mse: 0.4132 - val_mae: 0.4892\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2433 - mse: 0.2433 - mae: 0.3837 - val_loss: 0.4014 - val_mse: 0.4014 - val_mae: 0.4824\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2055 - mse: 0.2055 - mae: 0.3507 - val_loss: 0.4761 - val_mse: 0.4761 - val_mae: 0.5241\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2308 - mse: 0.2308 - mae: 0.3658 - val_loss: 0.3980 - val_mse: 0.3980 - val_mae: 0.4846\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2569 - mse: 0.2569 - mae: 0.3883 - val_loss: 0.4282 - val_mse: 0.4282 - val_mae: 0.4990\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2224 - mse: 0.2224 - mae: 0.3586 - val_loss: 0.3981 - val_mse: 0.3981 - val_mae: 0.4907\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2663 - mse: 0.2663 - mae: 0.4050 - val_loss: 0.4129 - val_mse: 0.4129 - val_mae: 0.4912\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2569 - mse: 0.2569 - mae: 0.3989 - val_loss: 0.4057 - val_mse: 0.4057 - val_mae: 0.4848\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2378 - mse: 0.2378 - mae: 0.3778 - val_loss: 0.4253 - val_mse: 0.4253 - val_mae: 0.5050\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2246 - mse: 0.2246 - mae: 0.3648 - val_loss: 0.4233 - val_mse: 0.4233 - val_mae: 0.4964\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2147 - mse: 0.2147 - mae: 0.3529 - val_loss: 0.4260 - val_mse: 0.4260 - val_mae: 0.4947\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2236 - mse: 0.2236 - mae: 0.3571 - val_loss: 0.3996 - val_mse: 0.3996 - val_mae: 0.4863\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2361 - mse: 0.2361 - mae: 0.3815 - val_loss: 0.4272 - val_mse: 0.4272 - val_mae: 0.4952\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2079 - mse: 0.2079 - mae: 0.3528 - val_loss: 0.4305 - val_mse: 0.4305 - val_mae: 0.5142\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2669 - mse: 0.2669 - mae: 0.3949 - val_loss: 0.4568 - val_mse: 0.4568 - val_mae: 0.5094\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2303 - mse: 0.2303 - mae: 0.3745 - val_loss: 0.4023 - val_mse: 0.4023 - val_mae: 0.4884\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2095 - mse: 0.2095 - mae: 0.3550 - val_loss: 0.3985 - val_mse: 0.3985 - val_mae: 0.4845\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2309 - mse: 0.2309 - mae: 0.3729 - val_loss: 0.4428 - val_mse: 0.4428 - val_mae: 0.5104\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2069 - mse: 0.2069 - mae: 0.3457 - val_loss: 0.4337 - val_mse: 0.4337 - val_mae: 0.5036\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2379 - mse: 0.2379 - mae: 0.3709 - val_loss: 0.4070 - val_mse: 0.4070 - val_mae: 0.4871\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2040 - mse: 0.2040 - mae: 0.3528 - val_loss: 0.4117 - val_mse: 0.4117 - val_mae: 0.4849\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2101 - mse: 0.2101 - mae: 0.3493 - val_loss: 0.4088 - val_mse: 0.4088 - val_mae: 0.4805\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2130 - mse: 0.2130 - mae: 0.3536 - val_loss: 0.4145 - val_mse: 0.4145 - val_mae: 0.4858\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2168 - mse: 0.2168 - mae: 0.3537 - val_loss: 0.4022 - val_mse: 0.4022 - val_mae: 0.4863\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2169 - mse: 0.2169 - mae: 0.3604 - val_loss: 0.4111 - val_mse: 0.4111 - val_mae: 0.4881\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2091 - mse: 0.2091 - mae: 0.3495 - val_loss: 0.4033 - val_mse: 0.4033 - val_mae: 0.4842\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1840 - mse: 0.1840 - mae: 0.3242 - val_loss: 0.3962 - val_mse: 0.3962 - val_mae: 0.4819\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2253 - mse: 0.2253 - mae: 0.3602 - val_loss: 0.4111 - val_mse: 0.4111 - val_mae: 0.4848\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2369 - mse: 0.2369 - mae: 0.3799 - val_loss: 0.3851 - val_mse: 0.3851 - val_mae: 0.4757\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2077 - mse: 0.2077 - mae: 0.3538 - val_loss: 0.3910 - val_mse: 0.3910 - val_mae: 0.4800\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2200 - mse: 0.2200 - mae: 0.3514 - val_loss: 0.4149 - val_mse: 0.4149 - val_mae: 0.4976\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2175 - mse: 0.2175 - mae: 0.3526 - val_loss: 0.4051 - val_mse: 0.4051 - val_mae: 0.4888\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2479 - mse: 0.2479 - mae: 0.3820 - val_loss: 0.4231 - val_mse: 0.4231 - val_mae: 0.5038\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2491 - mse: 0.2491 - mae: 0.3854 - val_loss: 0.4380 - val_mse: 0.4380 - val_mae: 0.5077\n",
            "Epoch 252/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2043 - mse: 0.2043 - mae: 0.3488 - val_loss: 0.3971 - val_mse: 0.3971 - val_mae: 0.4830\n",
            "Epoch 253/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2120 - mse: 0.2120 - mae: 0.3454 - val_loss: 0.4048 - val_mse: 0.4048 - val_mae: 0.4890\n",
            "Epoch 254/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2064 - mse: 0.2064 - mae: 0.3490 - val_loss: 0.4398 - val_mse: 0.4398 - val_mae: 0.5126\n",
            "Epoch 255/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2413 - mse: 0.2413 - mae: 0.3815 - val_loss: 0.4289 - val_mse: 0.4289 - val_mae: 0.5124\n",
            "Epoch 256/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2309 - mse: 0.2309 - mae: 0.3758 - val_loss: 0.4083 - val_mse: 0.4083 - val_mae: 0.4861\n",
            "Epoch 257/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1976 - mse: 0.1976 - mae: 0.3408 - val_loss: 0.4075 - val_mse: 0.4075 - val_mae: 0.4858\n",
            "Epoch 258/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2020 - mse: 0.2020 - mae: 0.3499 - val_loss: 0.4257 - val_mse: 0.4257 - val_mae: 0.4985\n",
            "Epoch 259/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2393 - mse: 0.2393 - mae: 0.3864 - val_loss: 0.4017 - val_mse: 0.4017 - val_mae: 0.4869\n",
            "Epoch 260/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2343 - mse: 0.2343 - mae: 0.3640 - val_loss: 0.3973 - val_mse: 0.3973 - val_mae: 0.4845\n",
            "Epoch 261/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2046 - mse: 0.2046 - mae: 0.3373 - val_loss: 0.4046 - val_mse: 0.4046 - val_mae: 0.4818\n",
            "Epoch 262/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2095 - mse: 0.2095 - mae: 0.3507 - val_loss: 0.4282 - val_mse: 0.4282 - val_mae: 0.5027\n",
            "Epoch 263/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2185 - mse: 0.2185 - mae: 0.3646 - val_loss: 0.3914 - val_mse: 0.3914 - val_mae: 0.4712\n",
            "Epoch 264/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2367 - mse: 0.2367 - mae: 0.3780 - val_loss: 0.4474 - val_mse: 0.4474 - val_mae: 0.4948\n",
            "Epoch 265/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2058 - mse: 0.2058 - mae: 0.3460 - val_loss: 0.4053 - val_mse: 0.4053 - val_mae: 0.4888\n",
            "Epoch 266/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1969 - mse: 0.1969 - mae: 0.3352 - val_loss: 0.4046 - val_mse: 0.4046 - val_mae: 0.4941\n",
            "Epoch 267/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2370 - mse: 0.2370 - mae: 0.3764 - val_loss: 0.4538 - val_mse: 0.4538 - val_mae: 0.5062\n",
            "Epoch 268/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2311 - mse: 0.2311 - mae: 0.3655 - val_loss: 0.4090 - val_mse: 0.4090 - val_mae: 0.4936\n",
            "Epoch 269/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2055 - mse: 0.2055 - mae: 0.3468 - val_loss: 0.4115 - val_mse: 0.4115 - val_mae: 0.5019\n",
            "Epoch 270/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2321 - mse: 0.2321 - mae: 0.3700 - val_loss: 0.4014 - val_mse: 0.4014 - val_mae: 0.4862\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.4808 - mse: 0.4808 - mae: 0.5173\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 15ms/step - loss: 18.5661 - mse: 18.5661 - mae: 3.9746 - val_loss: 7.0346 - val_mse: 7.0346 - val_mae: 2.3519\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 4.8753 - mse: 4.8753 - mae: 1.7882 - val_loss: 4.4380 - val_mse: 4.4380 - val_mae: 1.6194\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 3.2849 - mse: 3.2849 - mae: 1.4582 - val_loss: 3.6779 - val_mse: 3.6779 - val_mae: 1.5440\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.6089 - mse: 2.6089 - mae: 1.3186 - val_loss: 3.2513 - val_mse: 3.2513 - val_mae: 1.4252\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.4311 - mse: 2.4311 - mae: 1.2666 - val_loss: 2.9904 - val_mse: 2.9904 - val_mae: 1.3581\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.2843 - mse: 2.2843 - mae: 1.2173 - val_loss: 2.7779 - val_mse: 2.7779 - val_mae: 1.2993\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.1320 - mse: 2.1320 - mae: 1.1693 - val_loss: 2.5668 - val_mse: 2.5668 - val_mae: 1.2429\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.0351 - mse: 2.0351 - mae: 1.1571 - val_loss: 2.4318 - val_mse: 2.4318 - val_mae: 1.2219\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.7926 - mse: 1.7926 - mae: 1.0615 - val_loss: 2.2633 - val_mse: 2.2633 - val_mae: 1.1785\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.7640 - mse: 1.7640 - mae: 1.0589 - val_loss: 2.1579 - val_mse: 2.1579 - val_mae: 1.1609\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.6214 - mse: 1.6214 - mae: 1.0146 - val_loss: 2.0272 - val_mse: 2.0272 - val_mae: 1.1218\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.5339 - mse: 1.5339 - mae: 0.9845 - val_loss: 1.9162 - val_mse: 1.9162 - val_mae: 1.0866\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.4726 - mse: 1.4726 - mae: 0.9705 - val_loss: 1.7936 - val_mse: 1.7936 - val_mae: 1.0429\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.3653 - mse: 1.3653 - mae: 0.9242 - val_loss: 1.7089 - val_mse: 1.7089 - val_mae: 1.0356\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.3521 - mse: 1.3521 - mae: 0.9284 - val_loss: 1.6415 - val_mse: 1.6415 - val_mae: 1.0277\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.3214 - mse: 1.3214 - mae: 0.9178 - val_loss: 1.5446 - val_mse: 1.5446 - val_mae: 0.9733\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2825 - mse: 1.2825 - mae: 0.8917 - val_loss: 1.4928 - val_mse: 1.4928 - val_mae: 0.9598\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.2116 - mse: 1.2116 - mae: 0.8788 - val_loss: 1.4271 - val_mse: 1.4271 - val_mae: 0.9462\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.1327 - mse: 1.1327 - mae: 0.8342 - val_loss: 1.3734 - val_mse: 1.3734 - val_mae: 0.9158\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.1353 - mse: 1.1353 - mae: 0.8356 - val_loss: 1.3039 - val_mse: 1.3039 - val_mae: 0.9076\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0746 - mse: 1.0746 - mae: 0.8222 - val_loss: 1.2393 - val_mse: 1.2393 - val_mae: 0.8949\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.0343 - mse: 1.0343 - mae: 0.8018 - val_loss: 1.2048 - val_mse: 1.2048 - val_mae: 0.8674\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0286 - mse: 1.0286 - mae: 0.7971 - val_loss: 1.1800 - val_mse: 1.1800 - val_mae: 0.8734\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9584 - mse: 0.9584 - mae: 0.7812 - val_loss: 1.1253 - val_mse: 1.1253 - val_mae: 0.8238\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9304 - mse: 0.9304 - mae: 0.7641 - val_loss: 1.0548 - val_mse: 1.0548 - val_mae: 0.7873\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9071 - mse: 0.9071 - mae: 0.7547 - val_loss: 1.0301 - val_mse: 1.0301 - val_mae: 0.7870\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9107 - mse: 0.9107 - mae: 0.7573 - val_loss: 0.9787 - val_mse: 0.9787 - val_mae: 0.7848\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7904 - mse: 0.7904 - mae: 0.7102 - val_loss: 0.9394 - val_mse: 0.9394 - val_mae: 0.7747\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.8068 - mse: 0.8068 - mae: 0.7093 - val_loss: 0.9605 - val_mse: 0.9605 - val_mae: 0.7568\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7627 - mse: 0.7627 - mae: 0.6923 - val_loss: 0.8836 - val_mse: 0.8836 - val_mae: 0.7499\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7514 - mse: 0.7514 - mae: 0.6776 - val_loss: 0.8447 - val_mse: 0.8447 - val_mae: 0.7383\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7495 - mse: 0.7495 - mae: 0.6832 - val_loss: 0.8612 - val_mse: 0.8612 - val_mae: 0.7286\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6538 - mse: 0.6538 - mae: 0.6282 - val_loss: 0.8574 - val_mse: 0.8574 - val_mae: 0.7192\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6793 - mse: 0.6793 - mae: 0.6436 - val_loss: 0.7757 - val_mse: 0.7757 - val_mae: 0.7078\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6683 - mse: 0.6683 - mae: 0.6357 - val_loss: 0.8199 - val_mse: 0.8199 - val_mae: 0.6958\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6848 - mse: 0.6848 - mae: 0.6442 - val_loss: 0.7106 - val_mse: 0.7106 - val_mae: 0.6808\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5787 - mse: 0.5787 - mae: 0.5899 - val_loss: 0.7228 - val_mse: 0.7228 - val_mae: 0.6768\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.6457 - mse: 0.6457 - mae: 0.6401 - val_loss: 0.6991 - val_mse: 0.6991 - val_mae: 0.6901\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6130 - mse: 0.6130 - mae: 0.6099 - val_loss: 0.6483 - val_mse: 0.6483 - val_mae: 0.6558\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5942 - mse: 0.5942 - mae: 0.6035 - val_loss: 0.6532 - val_mse: 0.6532 - val_mae: 0.6611\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5631 - mse: 0.5631 - mae: 0.5805 - val_loss: 0.6467 - val_mse: 0.6467 - val_mae: 0.6426\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5433 - mse: 0.5433 - mae: 0.5756 - val_loss: 0.6550 - val_mse: 0.6550 - val_mae: 0.6362\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5642 - mse: 0.5642 - mae: 0.5929 - val_loss: 0.6572 - val_mse: 0.6572 - val_mae: 0.6416\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4924 - mse: 0.4924 - mae: 0.5500 - val_loss: 0.6266 - val_mse: 0.6266 - val_mae: 0.6125\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5229 - mse: 0.5229 - mae: 0.5541 - val_loss: 0.6233 - val_mse: 0.6233 - val_mae: 0.6323\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4932 - mse: 0.4932 - mae: 0.5544 - val_loss: 0.5846 - val_mse: 0.5846 - val_mae: 0.6172\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5128 - mse: 0.5128 - mae: 0.5615 - val_loss: 0.5935 - val_mse: 0.5935 - val_mae: 0.6062\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4843 - mse: 0.4843 - mae: 0.5437 - val_loss: 0.5486 - val_mse: 0.5486 - val_mae: 0.5909\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4433 - mse: 0.4433 - mae: 0.5248 - val_loss: 0.5779 - val_mse: 0.5779 - val_mae: 0.6008\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4635 - mse: 0.4635 - mae: 0.5310 - val_loss: 0.5586 - val_mse: 0.5586 - val_mae: 0.5798\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4766 - mse: 0.4766 - mae: 0.5406 - val_loss: 0.5389 - val_mse: 0.5389 - val_mae: 0.5738\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4632 - mse: 0.4632 - mae: 0.5249 - val_loss: 0.5207 - val_mse: 0.5207 - val_mae: 0.5808\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4442 - mse: 0.4442 - mae: 0.5101 - val_loss: 0.5235 - val_mse: 0.5235 - val_mae: 0.5836\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4537 - mse: 0.4537 - mae: 0.5213 - val_loss: 0.5551 - val_mse: 0.5551 - val_mae: 0.5683\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4560 - mse: 0.4560 - mae: 0.5281 - val_loss: 0.5286 - val_mse: 0.5286 - val_mae: 0.5505\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4391 - mse: 0.4391 - mae: 0.5158 - val_loss: 0.5138 - val_mse: 0.5138 - val_mae: 0.5568\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4391 - mse: 0.4391 - mae: 0.5128 - val_loss: 0.5203 - val_mse: 0.5203 - val_mae: 0.5701\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4410 - mse: 0.4410 - mae: 0.5211 - val_loss: 0.5932 - val_mse: 0.5932 - val_mae: 0.5810\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4378 - mse: 0.4378 - mae: 0.5143 - val_loss: 0.5190 - val_mse: 0.5190 - val_mae: 0.5729\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4243 - mse: 0.4243 - mae: 0.5137 - val_loss: 0.4969 - val_mse: 0.4969 - val_mae: 0.5370\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4284 - mse: 0.4284 - mae: 0.5149 - val_loss: 0.4770 - val_mse: 0.4770 - val_mae: 0.5408\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4152 - mse: 0.4152 - mae: 0.5047 - val_loss: 0.4580 - val_mse: 0.4580 - val_mae: 0.5455\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4183 - mse: 0.4183 - mae: 0.4959 - val_loss: 0.4788 - val_mse: 0.4788 - val_mae: 0.5502\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4042 - mse: 0.4042 - mae: 0.4855 - val_loss: 0.4461 - val_mse: 0.4461 - val_mae: 0.5347\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.4105 - mse: 0.4105 - mae: 0.4838 - val_loss: 0.4739 - val_mse: 0.4739 - val_mae: 0.5676\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3976 - mse: 0.3976 - mae: 0.4855 - val_loss: 0.4599 - val_mse: 0.4599 - val_mae: 0.5355\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3747 - mse: 0.3747 - mae: 0.4711 - val_loss: 0.4609 - val_mse: 0.4609 - val_mae: 0.5279\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3707 - mse: 0.3707 - mae: 0.4683 - val_loss: 0.4683 - val_mse: 0.4683 - val_mae: 0.5342\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3983 - mse: 0.3983 - mae: 0.4799 - val_loss: 0.4432 - val_mse: 0.4432 - val_mae: 0.5162\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4023 - mse: 0.4023 - mae: 0.4899 - val_loss: 0.4526 - val_mse: 0.4526 - val_mae: 0.5439\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3934 - mse: 0.3934 - mae: 0.4898 - val_loss: 0.4272 - val_mse: 0.4272 - val_mae: 0.5078\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3966 - mse: 0.3966 - mae: 0.4932 - val_loss: 0.4612 - val_mse: 0.4612 - val_mae: 0.5186\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3510 - mse: 0.3510 - mae: 0.4596 - val_loss: 0.4570 - val_mse: 0.4570 - val_mae: 0.5183\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3666 - mse: 0.3666 - mae: 0.4749 - val_loss: 0.4318 - val_mse: 0.4318 - val_mae: 0.5004\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3613 - mse: 0.3613 - mae: 0.4727 - val_loss: 0.4176 - val_mse: 0.4176 - val_mae: 0.5124\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3722 - mse: 0.3722 - mae: 0.4799 - val_loss: 0.4044 - val_mse: 0.4044 - val_mae: 0.5027\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3562 - mse: 0.3562 - mae: 0.4559 - val_loss: 0.4215 - val_mse: 0.4215 - val_mae: 0.4990\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3571 - mse: 0.3571 - mae: 0.4603 - val_loss: 0.4339 - val_mse: 0.4339 - val_mae: 0.5122\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3580 - mse: 0.3580 - mae: 0.4675 - val_loss: 0.4007 - val_mse: 0.4007 - val_mae: 0.5011\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3758 - mse: 0.3758 - mae: 0.4708 - val_loss: 0.4152 - val_mse: 0.4152 - val_mae: 0.4967\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3556 - mse: 0.3556 - mae: 0.4559 - val_loss: 0.3961 - val_mse: 0.3961 - val_mae: 0.4921\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3386 - mse: 0.3386 - mae: 0.4471 - val_loss: 0.3958 - val_mse: 0.3958 - val_mae: 0.4797\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3592 - mse: 0.3592 - mae: 0.4713 - val_loss: 0.4438 - val_mse: 0.4438 - val_mae: 0.5115\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3460 - mse: 0.3460 - mae: 0.4615 - val_loss: 0.3987 - val_mse: 0.3987 - val_mae: 0.4770\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3418 - mse: 0.3418 - mae: 0.4540 - val_loss: 0.3917 - val_mse: 0.3917 - val_mae: 0.4880\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3126 - mse: 0.3126 - mae: 0.4323 - val_loss: 0.3970 - val_mse: 0.3970 - val_mae: 0.4916\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3445 - mse: 0.3445 - mae: 0.4465 - val_loss: 0.4341 - val_mse: 0.4341 - val_mae: 0.5040\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3322 - mse: 0.3322 - mae: 0.4490 - val_loss: 0.4277 - val_mse: 0.4277 - val_mae: 0.5036\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3408 - mse: 0.3408 - mae: 0.4556 - val_loss: 0.4263 - val_mse: 0.4263 - val_mae: 0.5142\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3462 - mse: 0.3462 - mae: 0.4591 - val_loss: 0.3780 - val_mse: 0.3780 - val_mae: 0.4935\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3101 - mse: 0.3101 - mae: 0.4289 - val_loss: 0.3937 - val_mse: 0.3937 - val_mae: 0.5032\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3527 - mse: 0.3527 - mae: 0.4585 - val_loss: 0.5098 - val_mse: 0.5098 - val_mae: 0.5478\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3661 - mse: 0.3661 - mae: 0.4640 - val_loss: 0.4105 - val_mse: 0.4105 - val_mae: 0.4934\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3217 - mse: 0.3217 - mae: 0.4389 - val_loss: 0.4121 - val_mse: 0.4121 - val_mae: 0.4921\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3364 - mse: 0.3364 - mae: 0.4480 - val_loss: 0.3796 - val_mse: 0.3796 - val_mae: 0.4909\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3152 - mse: 0.3152 - mae: 0.4412 - val_loss: 0.3834 - val_mse: 0.3834 - val_mae: 0.4919\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3357 - mse: 0.3357 - mae: 0.4477 - val_loss: 0.4004 - val_mse: 0.4004 - val_mae: 0.4979\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3085 - mse: 0.3085 - mae: 0.4287 - val_loss: 0.3717 - val_mse: 0.3717 - val_mae: 0.4828\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3098 - mse: 0.3098 - mae: 0.4268 - val_loss: 0.3856 - val_mse: 0.3856 - val_mae: 0.4843\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3012 - mse: 0.3012 - mae: 0.4261 - val_loss: 0.3970 - val_mse: 0.3970 - val_mae: 0.4793\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3289 - mse: 0.3289 - mae: 0.4383 - val_loss: 0.3923 - val_mse: 0.3923 - val_mae: 0.4844\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3156 - mse: 0.3156 - mae: 0.4389 - val_loss: 0.3558 - val_mse: 0.3558 - val_mae: 0.4683\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3128 - mse: 0.3128 - mae: 0.4375 - val_loss: 0.3720 - val_mse: 0.3720 - val_mae: 0.4817\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3243 - mse: 0.3243 - mae: 0.4421 - val_loss: 0.3737 - val_mse: 0.3737 - val_mae: 0.4832\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3290 - mse: 0.3290 - mae: 0.4415 - val_loss: 0.3670 - val_mse: 0.3670 - val_mae: 0.4775\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3254 - mse: 0.3254 - mae: 0.4395 - val_loss: 0.3624 - val_mse: 0.3624 - val_mae: 0.4819\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3246 - mse: 0.3246 - mae: 0.4401 - val_loss: 0.4230 - val_mse: 0.4230 - val_mae: 0.5014\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2966 - mse: 0.2966 - mae: 0.4185 - val_loss: 0.3578 - val_mse: 0.3578 - val_mae: 0.4757\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3310 - mse: 0.3310 - mae: 0.4420 - val_loss: 0.3627 - val_mse: 0.3627 - val_mae: 0.4663\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3073 - mse: 0.3073 - mae: 0.4271 - val_loss: 0.3748 - val_mse: 0.3748 - val_mae: 0.4888\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3258 - mse: 0.3258 - mae: 0.4418 - val_loss: 0.3555 - val_mse: 0.3555 - val_mae: 0.4726\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3096 - mse: 0.3096 - mae: 0.4330 - val_loss: 0.3719 - val_mse: 0.3719 - val_mae: 0.4842\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3096 - mse: 0.3096 - mae: 0.4284 - val_loss: 0.3870 - val_mse: 0.3870 - val_mae: 0.4906\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3253 - mse: 0.3253 - mae: 0.4465 - val_loss: 0.3852 - val_mse: 0.3852 - val_mae: 0.4958\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2758 - mse: 0.2758 - mae: 0.4101 - val_loss: 0.4003 - val_mse: 0.4003 - val_mae: 0.4957\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3132 - mse: 0.3132 - mae: 0.4242 - val_loss: 0.3520 - val_mse: 0.3520 - val_mae: 0.4799\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3237 - mse: 0.3237 - mae: 0.4385 - val_loss: 0.3440 - val_mse: 0.3440 - val_mae: 0.4710\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2998 - mse: 0.2998 - mae: 0.4244 - val_loss: 0.3844 - val_mse: 0.3844 - val_mae: 0.4888\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3202 - mse: 0.3202 - mae: 0.4405 - val_loss: 0.3931 - val_mse: 0.3931 - val_mae: 0.5106\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3144 - mse: 0.3144 - mae: 0.4391 - val_loss: 0.3571 - val_mse: 0.3571 - val_mae: 0.4754\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2964 - mse: 0.2964 - mae: 0.4209 - val_loss: 0.3681 - val_mse: 0.3681 - val_mae: 0.4762\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3029 - mse: 0.3029 - mae: 0.4275 - val_loss: 0.3725 - val_mse: 0.3725 - val_mae: 0.4924\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2966 - mse: 0.2966 - mae: 0.4171 - val_loss: 0.3709 - val_mse: 0.3709 - val_mae: 0.4861\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2956 - mse: 0.2956 - mae: 0.4284 - val_loss: 0.3625 - val_mse: 0.3625 - val_mae: 0.4687\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3010 - mse: 0.3010 - mae: 0.4104 - val_loss: 0.3774 - val_mse: 0.3774 - val_mae: 0.4883\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2894 - mse: 0.2894 - mae: 0.4155 - val_loss: 0.3706 - val_mse: 0.3706 - val_mae: 0.4856\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3071 - mse: 0.3071 - mae: 0.4364 - val_loss: 0.3694 - val_mse: 0.3694 - val_mae: 0.4768\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2873 - mse: 0.2873 - mae: 0.4096 - val_loss: 0.3622 - val_mse: 0.3622 - val_mae: 0.4757\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2914 - mse: 0.2914 - mae: 0.4162 - val_loss: 0.4252 - val_mse: 0.4252 - val_mae: 0.5141\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2941 - mse: 0.2941 - mae: 0.4200 - val_loss: 0.3591 - val_mse: 0.3591 - val_mae: 0.4666\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2995 - mse: 0.2995 - mae: 0.4232 - val_loss: 0.4219 - val_mse: 0.4219 - val_mae: 0.5019\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2849 - mse: 0.2849 - mae: 0.4178 - val_loss: 0.3463 - val_mse: 0.3463 - val_mae: 0.4635\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2908 - mse: 0.2908 - mae: 0.4184 - val_loss: 0.3720 - val_mse: 0.3720 - val_mae: 0.4862\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2744 - mse: 0.2744 - mae: 0.4023 - val_loss: 0.3540 - val_mse: 0.3540 - val_mae: 0.4700\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2836 - mse: 0.2836 - mae: 0.4109 - val_loss: 0.3633 - val_mse: 0.3633 - val_mae: 0.4759\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2876 - mse: 0.2876 - mae: 0.4149 - val_loss: 0.4187 - val_mse: 0.4187 - val_mae: 0.5153\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3154 - mse: 0.3154 - mae: 0.4424 - val_loss: 0.3503 - val_mse: 0.3503 - val_mae: 0.4629\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2846 - mse: 0.2846 - mae: 0.4108 - val_loss: 0.3459 - val_mse: 0.3459 - val_mae: 0.4689\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3052 - mse: 0.3052 - mae: 0.4281 - val_loss: 0.3497 - val_mse: 0.3497 - val_mae: 0.4682\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2825 - mse: 0.2825 - mae: 0.4114 - val_loss: 0.3868 - val_mse: 0.3868 - val_mae: 0.4824\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2844 - mse: 0.2844 - mae: 0.4035 - val_loss: 0.3722 - val_mse: 0.3722 - val_mae: 0.4844\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2824 - mse: 0.2824 - mae: 0.4126 - val_loss: 0.3939 - val_mse: 0.3939 - val_mae: 0.4983\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2721 - mse: 0.2721 - mae: 0.4004 - val_loss: 0.3731 - val_mse: 0.3731 - val_mae: 0.4794\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2977 - mse: 0.2977 - mae: 0.4177 - val_loss: 0.3534 - val_mse: 0.3534 - val_mae: 0.4667\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2761 - mse: 0.2761 - mae: 0.4102 - val_loss: 0.3837 - val_mse: 0.3837 - val_mae: 0.4850\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2667 - mse: 0.2667 - mae: 0.4024 - val_loss: 0.3628 - val_mse: 0.3628 - val_mae: 0.4692\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2782 - mse: 0.2782 - mae: 0.4055 - val_loss: 0.3803 - val_mse: 0.3803 - val_mae: 0.4858\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2697 - mse: 0.2697 - mae: 0.4000 - val_loss: 0.3433 - val_mse: 0.3433 - val_mae: 0.4603\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2700 - mse: 0.2700 - mae: 0.3981 - val_loss: 0.3799 - val_mse: 0.3799 - val_mae: 0.4746\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2784 - mse: 0.2784 - mae: 0.4099 - val_loss: 0.3879 - val_mse: 0.3879 - val_mae: 0.4877\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2899 - mse: 0.2899 - mae: 0.4249 - val_loss: 0.3690 - val_mse: 0.3690 - val_mae: 0.4742\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2779 - mse: 0.2779 - mae: 0.3942 - val_loss: 0.3595 - val_mse: 0.3595 - val_mae: 0.4762\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2646 - mse: 0.2646 - mae: 0.3959 - val_loss: 0.3554 - val_mse: 0.3554 - val_mae: 0.4678\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2549 - mse: 0.2549 - mae: 0.3785 - val_loss: 0.4384 - val_mse: 0.4384 - val_mae: 0.5086\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2811 - mse: 0.2811 - mae: 0.4150 - val_loss: 0.4056 - val_mse: 0.4056 - val_mae: 0.4984\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2743 - mse: 0.2743 - mae: 0.3977 - val_loss: 0.3588 - val_mse: 0.3588 - val_mae: 0.4760\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2800 - mse: 0.2800 - mae: 0.4062 - val_loss: 0.3577 - val_mse: 0.3577 - val_mae: 0.4763\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2951 - mse: 0.2951 - mae: 0.4281 - val_loss: 0.4031 - val_mse: 0.4031 - val_mae: 0.4938\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2857 - mse: 0.2857 - mae: 0.4155 - val_loss: 0.3493 - val_mse: 0.3493 - val_mae: 0.4727\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2589 - mse: 0.2589 - mae: 0.3921 - val_loss: 0.3474 - val_mse: 0.3474 - val_mae: 0.4629\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2578 - mse: 0.2578 - mae: 0.3915 - val_loss: 0.3457 - val_mse: 0.3457 - val_mae: 0.4674\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2592 - mse: 0.2592 - mae: 0.3962 - val_loss: 0.3673 - val_mse: 0.3673 - val_mae: 0.4742\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2664 - mse: 0.2664 - mae: 0.3999 - val_loss: 0.3838 - val_mse: 0.3838 - val_mae: 0.4830\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2744 - mse: 0.2744 - mae: 0.4077 - val_loss: 0.3601 - val_mse: 0.3601 - val_mae: 0.4720\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2926 - mse: 0.2926 - mae: 0.4280 - val_loss: 0.3686 - val_mse: 0.3686 - val_mae: 0.4798\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2496 - mse: 0.2496 - mae: 0.3858 - val_loss: 0.3873 - val_mse: 0.3873 - val_mae: 0.4842\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2543 - mse: 0.2543 - mae: 0.3921 - val_loss: 0.3552 - val_mse: 0.3552 - val_mae: 0.4683\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2628 - mse: 0.2628 - mae: 0.3990 - val_loss: 0.3546 - val_mse: 0.3546 - val_mae: 0.4618\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2725 - mse: 0.2725 - mae: 0.4077 - val_loss: 0.3641 - val_mse: 0.3641 - val_mae: 0.4684\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2586 - mse: 0.2586 - mae: 0.3903 - val_loss: 0.3655 - val_mse: 0.3655 - val_mae: 0.4721\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2726 - mse: 0.2726 - mae: 0.3978 - val_loss: 0.3755 - val_mse: 0.3755 - val_mae: 0.4701\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2668 - mse: 0.2668 - mae: 0.4047 - val_loss: 0.3856 - val_mse: 0.3856 - val_mae: 0.4794\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2777 - mse: 0.2777 - mae: 0.4003 - val_loss: 0.4128 - val_mse: 0.4128 - val_mae: 0.5043\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2741 - mse: 0.2741 - mae: 0.4095 - val_loss: 0.3969 - val_mse: 0.3969 - val_mae: 0.4826\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2478 - mse: 0.2478 - mae: 0.3857 - val_loss: 0.3652 - val_mse: 0.3652 - val_mae: 0.4773\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2411 - mse: 0.2411 - mae: 0.3832 - val_loss: 0.3758 - val_mse: 0.3758 - val_mae: 0.4716\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2476 - mse: 0.2476 - mae: 0.3852 - val_loss: 0.3891 - val_mse: 0.3891 - val_mae: 0.4823\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2468 - mse: 0.2468 - mae: 0.3829 - val_loss: 0.3414 - val_mse: 0.3414 - val_mae: 0.4473\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2375 - mse: 0.2375 - mae: 0.3736 - val_loss: 0.3433 - val_mse: 0.3433 - val_mae: 0.4538\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2594 - mse: 0.2594 - mae: 0.3908 - val_loss: 0.3317 - val_mse: 0.3317 - val_mae: 0.4495\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2792 - mse: 0.2792 - mae: 0.4095 - val_loss: 0.4627 - val_mse: 0.4627 - val_mae: 0.5274\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2783 - mse: 0.2783 - mae: 0.4102 - val_loss: 0.3358 - val_mse: 0.3358 - val_mae: 0.4509\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2691 - mse: 0.2691 - mae: 0.4089 - val_loss: 0.3426 - val_mse: 0.3426 - val_mae: 0.4583\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2453 - mse: 0.2453 - mae: 0.3781 - val_loss: 0.3824 - val_mse: 0.3824 - val_mae: 0.4822\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2527 - mse: 0.2527 - mae: 0.3873 - val_loss: 0.3400 - val_mse: 0.3400 - val_mae: 0.4611\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2711 - mse: 0.2711 - mae: 0.4047 - val_loss: 0.3487 - val_mse: 0.3487 - val_mae: 0.4669\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2670 - mse: 0.2670 - mae: 0.3997 - val_loss: 0.3355 - val_mse: 0.3355 - val_mae: 0.4542\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2583 - mse: 0.2583 - mae: 0.3955 - val_loss: 0.3590 - val_mse: 0.3590 - val_mae: 0.4716\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2456 - mse: 0.2456 - mae: 0.3890 - val_loss: 0.3624 - val_mse: 0.3624 - val_mae: 0.4803\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2602 - mse: 0.2602 - mae: 0.3939 - val_loss: 0.3512 - val_mse: 0.3512 - val_mae: 0.4733\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2400 - mse: 0.2400 - mae: 0.3779 - val_loss: 0.3434 - val_mse: 0.3434 - val_mae: 0.4581\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2431 - mse: 0.2431 - mae: 0.3795 - val_loss: 0.3653 - val_mse: 0.3653 - val_mae: 0.4674\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2835 - mse: 0.2835 - mae: 0.4125 - val_loss: 0.3811 - val_mse: 0.3811 - val_mae: 0.4735\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2499 - mse: 0.2499 - mae: 0.3859 - val_loss: 0.3453 - val_mse: 0.3453 - val_mae: 0.4598\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2658 - mse: 0.2658 - mae: 0.4004 - val_loss: 0.3829 - val_mse: 0.3829 - val_mae: 0.4790\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2536 - mse: 0.2536 - mae: 0.3912 - val_loss: 0.3768 - val_mse: 0.3768 - val_mae: 0.4833\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2603 - mse: 0.2603 - mae: 0.3944 - val_loss: 0.3648 - val_mse: 0.3648 - val_mae: 0.4651\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2290 - mse: 0.2290 - mae: 0.3687 - val_loss: 0.3448 - val_mse: 0.3448 - val_mae: 0.4515\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2587 - mse: 0.2587 - mae: 0.3958 - val_loss: 0.3760 - val_mse: 0.3760 - val_mae: 0.4842\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2388 - mse: 0.2388 - mae: 0.3724 - val_loss: 0.3544 - val_mse: 0.3544 - val_mae: 0.4666\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2580 - mse: 0.2580 - mae: 0.3931 - val_loss: 0.3404 - val_mse: 0.3404 - val_mae: 0.4580\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2511 - mse: 0.2511 - mae: 0.3876 - val_loss: 0.3520 - val_mse: 0.3520 - val_mae: 0.4636\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2725 - mse: 0.2725 - mae: 0.4066 - val_loss: 0.3661 - val_mse: 0.3661 - val_mae: 0.4733\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2418 - mse: 0.2418 - mae: 0.3827 - val_loss: 0.3680 - val_mse: 0.3680 - val_mae: 0.4716\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2413 - mse: 0.2413 - mae: 0.3728 - val_loss: 0.3455 - val_mse: 0.3455 - val_mae: 0.4569\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2600 - mse: 0.2600 - mae: 0.3970 - val_loss: 0.3375 - val_mse: 0.3375 - val_mae: 0.4488\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2455 - mse: 0.2455 - mae: 0.3900 - val_loss: 0.3732 - val_mse: 0.3732 - val_mae: 0.4706\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2327 - mse: 0.2327 - mae: 0.3684 - val_loss: 0.3456 - val_mse: 0.3456 - val_mae: 0.4603\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2321 - mse: 0.2321 - mae: 0.3748 - val_loss: 0.3577 - val_mse: 0.3577 - val_mae: 0.4631\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2414 - mse: 0.2414 - mae: 0.3770 - val_loss: 0.3740 - val_mse: 0.3740 - val_mae: 0.4795\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2309 - mse: 0.2309 - mae: 0.3717 - val_loss: 0.3446 - val_mse: 0.3446 - val_mae: 0.4537\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2343 - mse: 0.2343 - mae: 0.3694 - val_loss: 0.3562 - val_mse: 0.3562 - val_mae: 0.4661\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2331 - mse: 0.2331 - mae: 0.3716 - val_loss: 0.3436 - val_mse: 0.3436 - val_mae: 0.4594\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2238 - mse: 0.2238 - mae: 0.3632 - val_loss: 0.3505 - val_mse: 0.3505 - val_mae: 0.4663\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2452 - mse: 0.2452 - mae: 0.3807 - val_loss: 0.3658 - val_mse: 0.3658 - val_mae: 0.4723\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2350 - mse: 0.2350 - mae: 0.3748 - val_loss: 0.3532 - val_mse: 0.3532 - val_mae: 0.4643\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2341 - mse: 0.2341 - mae: 0.3711 - val_loss: 0.3642 - val_mse: 0.3642 - val_mae: 0.4650\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2335 - mse: 0.2335 - mae: 0.3785 - val_loss: 0.3543 - val_mse: 0.3543 - val_mae: 0.4615\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2393 - mse: 0.2393 - mae: 0.3781 - val_loss: 0.3504 - val_mse: 0.3504 - val_mae: 0.4574\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2424 - mse: 0.2424 - mae: 0.3869 - val_loss: 0.3429 - val_mse: 0.3429 - val_mae: 0.4511\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2316 - mse: 0.2316 - mae: 0.3660 - val_loss: 0.3790 - val_mse: 0.3790 - val_mae: 0.4855\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2818 - mse: 0.2818 - mae: 0.4144 - val_loss: 0.3612 - val_mse: 0.3612 - val_mae: 0.4714\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2230 - mse: 0.2230 - mae: 0.3693 - val_loss: 0.3424 - val_mse: 0.3424 - val_mae: 0.4517\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2205 - mse: 0.2205 - mae: 0.3601 - val_loss: 0.3591 - val_mse: 0.3591 - val_mae: 0.4595\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2276 - mse: 0.2276 - mae: 0.3661 - val_loss: 0.3603 - val_mse: 0.3603 - val_mae: 0.4607\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2400 - mse: 0.2400 - mae: 0.3767 - val_loss: 0.3871 - val_mse: 0.3871 - val_mae: 0.4851\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2221 - mse: 0.2221 - mae: 0.3598 - val_loss: 0.3417 - val_mse: 0.3417 - val_mae: 0.4468\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2324 - mse: 0.2324 - mae: 0.3720 - val_loss: 0.3484 - val_mse: 0.3484 - val_mae: 0.4514\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2505 - mse: 0.2505 - mae: 0.3888 - val_loss: 0.3448 - val_mse: 0.3448 - val_mae: 0.4589\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2119 - mse: 0.2119 - mae: 0.3526 - val_loss: 0.3401 - val_mse: 0.3401 - val_mae: 0.4468\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2164 - mse: 0.2164 - mae: 0.3557 - val_loss: 0.3608 - val_mse: 0.3608 - val_mae: 0.4631\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2349 - mse: 0.2349 - mae: 0.3823 - val_loss: 0.3444 - val_mse: 0.3444 - val_mae: 0.4516\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2298 - mse: 0.2298 - mae: 0.3725 - val_loss: 0.4153 - val_mse: 0.4153 - val_mae: 0.4941\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2530 - mse: 0.2530 - mae: 0.3913 - val_loss: 0.3495 - val_mse: 0.3495 - val_mae: 0.4498\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2352 - mse: 0.2352 - mae: 0.3675 - val_loss: 0.3595 - val_mse: 0.3595 - val_mae: 0.4554\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2418 - mse: 0.2418 - mae: 0.3806 - val_loss: 0.3640 - val_mse: 0.3640 - val_mae: 0.4660\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2380 - mse: 0.2380 - mae: 0.3776 - val_loss: 0.3591 - val_mse: 0.3591 - val_mae: 0.4554\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2206 - mse: 0.2206 - mae: 0.3620 - val_loss: 0.3956 - val_mse: 0.3956 - val_mae: 0.4752\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2345 - mse: 0.2345 - mae: 0.3767 - val_loss: 0.3727 - val_mse: 0.3727 - val_mae: 0.4696\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2223 - mse: 0.2223 - mae: 0.3650 - val_loss: 0.3839 - val_mse: 0.3839 - val_mae: 0.4864\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2431 - mse: 0.2431 - mae: 0.3894 - val_loss: 0.3442 - val_mse: 0.3442 - val_mae: 0.4476\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2296 - mse: 0.2296 - mae: 0.3669 - val_loss: 0.3597 - val_mse: 0.3597 - val_mae: 0.4684\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2215 - mse: 0.2215 - mae: 0.3600 - val_loss: 0.3603 - val_mse: 0.3603 - val_mae: 0.4629\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2376 - mse: 0.2376 - mae: 0.3699 - val_loss: 0.4036 - val_mse: 0.4036 - val_mae: 0.4820\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2289 - mse: 0.2289 - mae: 0.3677 - val_loss: 0.3571 - val_mse: 0.3571 - val_mae: 0.4624\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2295 - mse: 0.2295 - mae: 0.3703 - val_loss: 0.3315 - val_mse: 0.3315 - val_mae: 0.4486\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2431 - mse: 0.2431 - mae: 0.3866 - val_loss: 0.3681 - val_mse: 0.3681 - val_mae: 0.4657\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2278 - mse: 0.2278 - mae: 0.3703 - val_loss: 0.3775 - val_mse: 0.3775 - val_mae: 0.4672\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2189 - mse: 0.2189 - mae: 0.3673 - val_loss: 0.3703 - val_mse: 0.3703 - val_mae: 0.4663\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2475 - mse: 0.2475 - mae: 0.3788 - val_loss: 0.3484 - val_mse: 0.3484 - val_mae: 0.4569\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2296 - mse: 0.2296 - mae: 0.3725 - val_loss: 0.3541 - val_mse: 0.3541 - val_mae: 0.4461\n",
            "Epoch 252/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1999 - mse: 0.1999 - mae: 0.3488 - val_loss: 0.3568 - val_mse: 0.3568 - val_mae: 0.4565\n",
            "Epoch 253/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2155 - mse: 0.2155 - mae: 0.3572 - val_loss: 0.3474 - val_mse: 0.3474 - val_mae: 0.4466\n",
            "Epoch 254/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2380 - mse: 0.2380 - mae: 0.3760 - val_loss: 0.3717 - val_mse: 0.3717 - val_mae: 0.4658\n",
            "Epoch 255/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2344 - mse: 0.2344 - mae: 0.3742 - val_loss: 0.3585 - val_mse: 0.3585 - val_mae: 0.4564\n",
            "Epoch 256/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2271 - mse: 0.2271 - mae: 0.3662 - val_loss: 0.3468 - val_mse: 0.3468 - val_mae: 0.4513\n",
            "Epoch 257/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2271 - mse: 0.2271 - mae: 0.3675 - val_loss: 0.3478 - val_mse: 0.3478 - val_mae: 0.4506\n",
            "Epoch 258/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2364 - mse: 0.2364 - mae: 0.3759 - val_loss: 0.3963 - val_mse: 0.3963 - val_mae: 0.4850\n",
            "Epoch 259/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2304 - mse: 0.2304 - mae: 0.3715 - val_loss: 0.3646 - val_mse: 0.3646 - val_mae: 0.4566\n",
            "Epoch 260/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2027 - mse: 0.2027 - mae: 0.3437 - val_loss: 0.3563 - val_mse: 0.3563 - val_mae: 0.4463\n",
            "Epoch 261/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1923 - mse: 0.1923 - mae: 0.3328 - val_loss: 0.3725 - val_mse: 0.3725 - val_mae: 0.4671\n",
            "Epoch 262/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2026 - mse: 0.2026 - mae: 0.3436 - val_loss: 0.3411 - val_mse: 0.3411 - val_mae: 0.4440\n",
            "Epoch 263/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2123 - mse: 0.2123 - mae: 0.3521 - val_loss: 0.3596 - val_mse: 0.3596 - val_mae: 0.4701\n",
            "Epoch 264/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2327 - mse: 0.2327 - mae: 0.3710 - val_loss: 0.3735 - val_mse: 0.3735 - val_mae: 0.4700\n",
            "Epoch 265/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2153 - mse: 0.2153 - mae: 0.3541 - val_loss: 0.3674 - val_mse: 0.3674 - val_mae: 0.4616\n",
            "Epoch 266/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2306 - mse: 0.2306 - mae: 0.3661 - val_loss: 0.3551 - val_mse: 0.3551 - val_mae: 0.4583\n",
            "Epoch 267/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2077 - mse: 0.2077 - mae: 0.3504 - val_loss: 0.3483 - val_mse: 0.3483 - val_mae: 0.4435\n",
            "Epoch 268/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2214 - mse: 0.2214 - mae: 0.3588 - val_loss: 0.3528 - val_mse: 0.3528 - val_mae: 0.4593\n",
            "Epoch 269/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2647 - mse: 0.2647 - mae: 0.3942 - val_loss: 0.3632 - val_mse: 0.3632 - val_mae: 0.4592\n",
            "Epoch 270/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2088 - mse: 0.2088 - mae: 0.3604 - val_loss: 0.3810 - val_mse: 0.3810 - val_mae: 0.4792\n",
            "Epoch 271/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2362 - mse: 0.2362 - mae: 0.3850 - val_loss: 0.3591 - val_mse: 0.3591 - val_mae: 0.4535\n",
            "Epoch 272/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2007 - mse: 0.2007 - mae: 0.3487 - val_loss: 0.3618 - val_mse: 0.3618 - val_mae: 0.4587\n",
            "Epoch 273/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2023 - mse: 0.2023 - mae: 0.3424 - val_loss: 0.3741 - val_mse: 0.3741 - val_mae: 0.4677\n",
            "Epoch 274/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2378 - mse: 0.2378 - mae: 0.3824 - val_loss: 0.3668 - val_mse: 0.3668 - val_mae: 0.4681\n",
            "Epoch 275/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2291 - mse: 0.2291 - mae: 0.3746 - val_loss: 0.3736 - val_mse: 0.3736 - val_mae: 0.4671\n",
            "Epoch 276/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2156 - mse: 0.2156 - mae: 0.3506 - val_loss: 0.3697 - val_mse: 0.3697 - val_mae: 0.4591\n",
            "Epoch 277/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2173 - mse: 0.2173 - mae: 0.3638 - val_loss: 0.3479 - val_mse: 0.3479 - val_mae: 0.4558\n",
            "Epoch 278/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2158 - mse: 0.2158 - mae: 0.3566 - val_loss: 0.3684 - val_mse: 0.3684 - val_mae: 0.4607\n",
            "Epoch 279/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2099 - mse: 0.2099 - mae: 0.3475 - val_loss: 0.3612 - val_mse: 0.3612 - val_mae: 0.4621\n",
            "Epoch 280/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2056 - mse: 0.2056 - mae: 0.3498 - val_loss: 0.3835 - val_mse: 0.3835 - val_mae: 0.4780\n",
            "Epoch 281/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2117 - mse: 0.2117 - mae: 0.3595 - val_loss: 0.3709 - val_mse: 0.3709 - val_mae: 0.4698\n",
            "Epoch 282/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2179 - mse: 0.2179 - mae: 0.3611 - val_loss: 0.3817 - val_mse: 0.3817 - val_mae: 0.4708\n",
            "Epoch 283/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2242 - mse: 0.2242 - mae: 0.3598 - val_loss: 0.3550 - val_mse: 0.3550 - val_mae: 0.4528\n",
            "Epoch 284/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2043 - mse: 0.2043 - mae: 0.3457 - val_loss: 0.3554 - val_mse: 0.3554 - val_mae: 0.4520\n",
            "Epoch 285/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2286 - mse: 0.2286 - mae: 0.3661 - val_loss: 0.3513 - val_mse: 0.3513 - val_mae: 0.4451\n",
            "Epoch 286/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2192 - mse: 0.2192 - mae: 0.3608 - val_loss: 0.3816 - val_mse: 0.3816 - val_mae: 0.4790\n",
            "Epoch 287/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2214 - mse: 0.2214 - mae: 0.3640 - val_loss: 0.3764 - val_mse: 0.3764 - val_mae: 0.4782\n",
            "Epoch 288/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2311 - mse: 0.2311 - mae: 0.3675 - val_loss: 0.3618 - val_mse: 0.3618 - val_mae: 0.4725\n",
            "Epoch 289/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2234 - mse: 0.2234 - mae: 0.3777 - val_loss: 0.3859 - val_mse: 0.3859 - val_mae: 0.4772\n",
            "Epoch 290/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2217 - mse: 0.2217 - mae: 0.3612 - val_loss: 0.3524 - val_mse: 0.3524 - val_mae: 0.4461\n",
            "Epoch 291/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1963 - mse: 0.1963 - mae: 0.3384 - val_loss: 0.3950 - val_mse: 0.3950 - val_mae: 0.4766\n",
            "Epoch 292/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2088 - mse: 0.2088 - mae: 0.3508 - val_loss: 0.3547 - val_mse: 0.3547 - val_mae: 0.4519\n",
            "Epoch 293/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1855 - mse: 0.1855 - mae: 0.3268 - val_loss: 0.3574 - val_mse: 0.3574 - val_mae: 0.4539\n",
            "Epoch 294/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2098 - mse: 0.2098 - mae: 0.3497 - val_loss: 0.3631 - val_mse: 0.3631 - val_mae: 0.4594\n",
            "Epoch 295/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2023 - mse: 0.2023 - mae: 0.3403 - val_loss: 0.3756 - val_mse: 0.3756 - val_mae: 0.4650\n",
            "Epoch 296/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2077 - mse: 0.2077 - mae: 0.3481 - val_loss: 0.3452 - val_mse: 0.3452 - val_mae: 0.4409\n",
            "Epoch 297/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1914 - mse: 0.1914 - mae: 0.3370 - val_loss: 0.3777 - val_mse: 0.3777 - val_mae: 0.4673\n",
            "Epoch 298/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2112 - mse: 0.2112 - mae: 0.3614 - val_loss: 0.3567 - val_mse: 0.3567 - val_mae: 0.4554\n",
            "Epoch 299/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2175 - mse: 0.2175 - mae: 0.3609 - val_loss: 0.3986 - val_mse: 0.3986 - val_mae: 0.4832\n",
            "Epoch 300/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2042 - mse: 0.2042 - mae: 0.3463 - val_loss: 0.3890 - val_mse: 0.3890 - val_mae: 0.4745\n",
            "Epoch 301/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2078 - mse: 0.2078 - mae: 0.3466 - val_loss: 0.3618 - val_mse: 0.3618 - val_mae: 0.4521\n",
            "Epoch 302/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2085 - mse: 0.2085 - mae: 0.3510 - val_loss: 0.4124 - val_mse: 0.4124 - val_mae: 0.5011\n",
            "Epoch 303/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2178 - mse: 0.2178 - mae: 0.3736 - val_loss: 0.3714 - val_mse: 0.3714 - val_mae: 0.4703\n",
            "Epoch 304/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2022 - mse: 0.2022 - mae: 0.3453 - val_loss: 0.3887 - val_mse: 0.3887 - val_mae: 0.4756\n",
            "Epoch 305/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2168 - mse: 0.2168 - mae: 0.3600 - val_loss: 0.3715 - val_mse: 0.3715 - val_mae: 0.4633\n",
            "Epoch 306/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2292 - mse: 0.2292 - mae: 0.3669 - val_loss: 0.3664 - val_mse: 0.3664 - val_mae: 0.4614\n",
            "Epoch 307/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.1883 - mse: 0.1883 - mae: 0.3290 - val_loss: 0.3694 - val_mse: 0.3694 - val_mae: 0.4602\n",
            "Epoch 308/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1971 - mse: 0.1971 - mae: 0.3438 - val_loss: 0.3545 - val_mse: 0.3545 - val_mae: 0.4587\n",
            "Epoch 309/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1928 - mse: 0.1928 - mae: 0.3423 - val_loss: 0.3472 - val_mse: 0.3472 - val_mae: 0.4503\n",
            "Epoch 310/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2175 - mse: 0.2175 - mae: 0.3648 - val_loss: 0.3609 - val_mse: 0.3609 - val_mae: 0.4602\n",
            "Epoch 311/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2156 - mse: 0.2156 - mae: 0.3621 - val_loss: 0.3692 - val_mse: 0.3692 - val_mae: 0.4605\n",
            "Epoch 312/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2097 - mse: 0.2097 - mae: 0.3526 - val_loss: 0.3608 - val_mse: 0.3608 - val_mae: 0.4608\n",
            "Epoch 313/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1978 - mse: 0.1978 - mae: 0.3388 - val_loss: 0.3773 - val_mse: 0.3773 - val_mae: 0.4661\n",
            "Epoch 314/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1982 - mse: 0.1982 - mae: 0.3505 - val_loss: 0.3467 - val_mse: 0.3467 - val_mae: 0.4546\n",
            "Epoch 315/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1839 - mse: 0.1839 - mae: 0.3350 - val_loss: 0.3540 - val_mse: 0.3540 - val_mae: 0.4570\n",
            "Epoch 316/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2062 - mse: 0.2062 - mae: 0.3557 - val_loss: 0.4088 - val_mse: 0.4088 - val_mae: 0.4782\n",
            "Epoch 317/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2082 - mse: 0.2082 - mae: 0.3545 - val_loss: 0.3622 - val_mse: 0.3622 - val_mae: 0.4588\n",
            "Epoch 318/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2263 - mse: 0.2263 - mae: 0.3661 - val_loss: 0.3855 - val_mse: 0.3855 - val_mae: 0.4690\n",
            "Epoch 319/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2126 - mse: 0.2126 - mae: 0.3572 - val_loss: 0.3705 - val_mse: 0.3705 - val_mae: 0.4678\n",
            "Epoch 320/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2074 - mse: 0.2074 - mae: 0.3566 - val_loss: 0.4114 - val_mse: 0.4114 - val_mae: 0.4788\n",
            "Epoch 321/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2027 - mse: 0.2027 - mae: 0.3489 - val_loss: 0.3734 - val_mse: 0.3734 - val_mae: 0.4723\n",
            "Epoch 322/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2166 - mse: 0.2166 - mae: 0.3527 - val_loss: 0.4175 - val_mse: 0.4175 - val_mae: 0.4955\n",
            "Epoch 323/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2158 - mse: 0.2158 - mae: 0.3640 - val_loss: 0.3854 - val_mse: 0.3854 - val_mae: 0.4763\n",
            "Epoch 324/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2109 - mse: 0.2109 - mae: 0.3559 - val_loss: 0.3564 - val_mse: 0.3564 - val_mae: 0.4553\n",
            "Epoch 325/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2001 - mse: 0.2001 - mae: 0.3476 - val_loss: 0.3844 - val_mse: 0.3844 - val_mae: 0.4724\n",
            "Epoch 326/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2068 - mse: 0.2068 - mae: 0.3550 - val_loss: 0.3630 - val_mse: 0.3630 - val_mae: 0.4578\n",
            "Epoch 327/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2165 - mse: 0.2165 - mae: 0.3574 - val_loss: 0.3555 - val_mse: 0.3555 - val_mae: 0.4566\n",
            "Epoch 328/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2119 - mse: 0.2119 - mae: 0.3526 - val_loss: 0.3676 - val_mse: 0.3676 - val_mae: 0.4618\n",
            "Epoch 329/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1890 - mse: 0.1890 - mae: 0.3369 - val_loss: 0.4044 - val_mse: 0.4044 - val_mae: 0.4838\n",
            "Epoch 330/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1883 - mse: 0.1883 - mae: 0.3329 - val_loss: 0.3902 - val_mse: 0.3902 - val_mae: 0.4755\n",
            "Epoch 331/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2037 - mse: 0.2037 - mae: 0.3396 - val_loss: 0.3780 - val_mse: 0.3780 - val_mae: 0.4780\n",
            "Epoch 332/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.1955 - mse: 0.1955 - mae: 0.3433 - val_loss: 0.3852 - val_mse: 0.3852 - val_mae: 0.4618\n",
            "Epoch 333/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1994 - mse: 0.1994 - mae: 0.3393 - val_loss: 0.3780 - val_mse: 0.3780 - val_mae: 0.4653\n",
            "Epoch 334/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2269 - mse: 0.2269 - mae: 0.3615 - val_loss: 0.3703 - val_mse: 0.3703 - val_mae: 0.4608\n",
            "Epoch 335/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1993 - mse: 0.1993 - mae: 0.3488 - val_loss: 0.3601 - val_mse: 0.3601 - val_mae: 0.4545\n",
            "Epoch 336/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1915 - mse: 0.1915 - mae: 0.3386 - val_loss: 0.4136 - val_mse: 0.4136 - val_mae: 0.4897\n",
            "Epoch 337/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2269 - mse: 0.2269 - mae: 0.3704 - val_loss: 0.3586 - val_mse: 0.3586 - val_mae: 0.4579\n",
            "Epoch 338/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2212 - mse: 0.2212 - mae: 0.3622 - val_loss: 0.3488 - val_mse: 0.3488 - val_mae: 0.4520\n",
            "Epoch 339/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2142 - mse: 0.2142 - mae: 0.3542 - val_loss: 0.3596 - val_mse: 0.3596 - val_mae: 0.4609\n",
            "Epoch 340/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2064 - mse: 0.2064 - mae: 0.3478 - val_loss: 0.3652 - val_mse: 0.3652 - val_mae: 0.4554\n",
            "Epoch 341/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1991 - mse: 0.1991 - mae: 0.3376 - val_loss: 0.3515 - val_mse: 0.3515 - val_mae: 0.4534\n",
            "Epoch 342/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.1924 - mse: 0.1924 - mae: 0.3399 - val_loss: 0.3793 - val_mse: 0.3793 - val_mae: 0.4620\n",
            "Epoch 343/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.1869 - mse: 0.1869 - mae: 0.3383 - val_loss: 0.3555 - val_mse: 0.3555 - val_mae: 0.4546\n",
            "Epoch 344/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2244 - mse: 0.2244 - mae: 0.3573 - val_loss: 0.3690 - val_mse: 0.3690 - val_mae: 0.4621\n",
            "Epoch 345/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.1996 - mse: 0.1996 - mae: 0.3412 - val_loss: 0.4171 - val_mse: 0.4171 - val_mae: 0.4912\n",
            "Epoch 346/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2133 - mse: 0.2133 - mae: 0.3592 - val_loss: 0.3892 - val_mse: 0.3892 - val_mae: 0.4883\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.6018 - mse: 0.6018 - mae: 0.5855\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 14ms/step - loss: 18.1085 - mse: 18.1085 - mae: 3.9283 - val_loss: 6.1373 - val_mse: 6.1373 - val_mae: 2.1844\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 4.1167 - mse: 4.1167 - mae: 1.6375 - val_loss: 3.4452 - val_mse: 3.4452 - val_mae: 1.4249\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 3.2166 - mse: 3.2166 - mae: 1.4486 - val_loss: 2.8037 - val_mse: 2.8037 - val_mae: 1.3543\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 2.7465 - mse: 2.7465 - mae: 1.3473 - val_loss: 2.3901 - val_mse: 2.3901 - val_mae: 1.2167\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 2.4181 - mse: 2.4181 - mae: 1.2377 - val_loss: 2.1554 - val_mse: 2.1554 - val_mae: 1.1548\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.2079 - mse: 2.2079 - mae: 1.2025 - val_loss: 2.0782 - val_mse: 2.0782 - val_mae: 1.1388\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.9637 - mse: 1.9637 - mae: 1.1381 - val_loss: 1.9854 - val_mse: 1.9854 - val_mae: 1.1094\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.8661 - mse: 1.8661 - mae: 1.0932 - val_loss: 1.8254 - val_mse: 1.8254 - val_mae: 1.0553\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.8520 - mse: 1.8520 - mae: 1.0837 - val_loss: 1.6933 - val_mse: 1.6933 - val_mae: 1.0199\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.6571 - mse: 1.6571 - mae: 1.0396 - val_loss: 1.7101 - val_mse: 1.7101 - val_mae: 1.0132\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.6404 - mse: 1.6404 - mae: 1.0271 - val_loss: 1.5959 - val_mse: 1.5959 - val_mae: 0.9849\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.5135 - mse: 1.5135 - mae: 0.9725 - val_loss: 1.5269 - val_mse: 1.5269 - val_mae: 0.9485\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.3829 - mse: 1.3829 - mae: 0.9376 - val_loss: 1.4502 - val_mse: 1.4502 - val_mae: 0.9185\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.3285 - mse: 1.3285 - mae: 0.9213 - val_loss: 1.3945 - val_mse: 1.3945 - val_mae: 0.8962\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.2335 - mse: 1.2335 - mae: 0.8856 - val_loss: 1.2789 - val_mse: 1.2789 - val_mae: 0.8594\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.1672 - mse: 1.1672 - mae: 0.8642 - val_loss: 1.1758 - val_mse: 1.1758 - val_mae: 0.8291\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0937 - mse: 1.0937 - mae: 0.8267 - val_loss: 1.1517 - val_mse: 1.1517 - val_mae: 0.8129\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.1070 - mse: 1.1070 - mae: 0.8261 - val_loss: 1.1137 - val_mse: 1.1137 - val_mae: 0.7993\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.0458 - mse: 1.0458 - mae: 0.8114 - val_loss: 1.1378 - val_mse: 1.1378 - val_mae: 0.8028\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9468 - mse: 0.9468 - mae: 0.7738 - val_loss: 1.0521 - val_mse: 1.0521 - val_mae: 0.7666\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9662 - mse: 0.9662 - mae: 0.7780 - val_loss: 0.9909 - val_mse: 0.9909 - val_mae: 0.7514\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.9243 - mse: 0.9243 - mae: 0.7537 - val_loss: 0.9979 - val_mse: 0.9979 - val_mae: 0.7431\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.8748 - mse: 0.8748 - mae: 0.7303 - val_loss: 0.9326 - val_mse: 0.9326 - val_mae: 0.7297\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.8174 - mse: 0.8174 - mae: 0.6984 - val_loss: 0.8998 - val_mse: 0.8998 - val_mae: 0.7203\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7769 - mse: 0.7769 - mae: 0.6920 - val_loss: 0.8909 - val_mse: 0.8909 - val_mae: 0.7195\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7958 - mse: 0.7958 - mae: 0.7028 - val_loss: 0.8706 - val_mse: 0.8706 - val_mae: 0.7097\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7105 - mse: 0.7105 - mae: 0.6781 - val_loss: 0.8564 - val_mse: 0.8564 - val_mae: 0.6963\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7208 - mse: 0.7208 - mae: 0.6656 - val_loss: 0.8442 - val_mse: 0.8442 - val_mae: 0.6966\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7628 - mse: 0.7628 - mae: 0.6936 - val_loss: 0.7928 - val_mse: 0.7928 - val_mae: 0.6798\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.6853 - mse: 0.6853 - mae: 0.6617 - val_loss: 0.8078 - val_mse: 0.8078 - val_mae: 0.6811\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.7671 - mse: 0.7671 - mae: 0.6884 - val_loss: 0.7854 - val_mse: 0.7854 - val_mae: 0.6677\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.6682 - mse: 0.6682 - mae: 0.6474 - val_loss: 0.7762 - val_mse: 0.7762 - val_mae: 0.6630\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.6257 - mse: 0.6257 - mae: 0.6239 - val_loss: 0.7382 - val_mse: 0.7382 - val_mae: 0.6429\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.6172 - mse: 0.6172 - mae: 0.6202 - val_loss: 0.7609 - val_mse: 0.7609 - val_mae: 0.6550\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.6350 - mse: 0.6350 - mae: 0.6255 - val_loss: 0.7173 - val_mse: 0.7173 - val_mae: 0.6382\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.6289 - mse: 0.6289 - mae: 0.6204 - val_loss: 0.7186 - val_mse: 0.7186 - val_mae: 0.6346\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.6026 - mse: 0.6026 - mae: 0.6087 - val_loss: 0.7318 - val_mse: 0.7318 - val_mae: 0.6486\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.5635 - mse: 0.5635 - mae: 0.5942 - val_loss: 0.7086 - val_mse: 0.7086 - val_mae: 0.6425\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.5790 - mse: 0.5790 - mae: 0.5971 - val_loss: 0.6859 - val_mse: 0.6859 - val_mae: 0.6206\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.5613 - mse: 0.5613 - mae: 0.5873 - val_loss: 0.6936 - val_mse: 0.6936 - val_mae: 0.6268\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5324 - mse: 0.5324 - mae: 0.5739 - val_loss: 0.6503 - val_mse: 0.6503 - val_mae: 0.6069\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.5215 - mse: 0.5215 - mae: 0.5624 - val_loss: 0.6849 - val_mse: 0.6849 - val_mae: 0.6319\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.5421 - mse: 0.5421 - mae: 0.5763 - val_loss: 0.6590 - val_mse: 0.6590 - val_mae: 0.6192\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4886 - mse: 0.4886 - mae: 0.5475 - val_loss: 0.5894 - val_mse: 0.5894 - val_mae: 0.5817\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.5031 - mse: 0.5031 - mae: 0.5607 - val_loss: 0.6269 - val_mse: 0.6269 - val_mae: 0.5923\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4988 - mse: 0.4988 - mae: 0.5617 - val_loss: 0.6507 - val_mse: 0.6507 - val_mae: 0.6151\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4673 - mse: 0.4673 - mae: 0.5308 - val_loss: 0.6148 - val_mse: 0.6148 - val_mae: 0.5871\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4799 - mse: 0.4799 - mae: 0.5443 - val_loss: 0.6358 - val_mse: 0.6358 - val_mae: 0.6055\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4908 - mse: 0.4908 - mae: 0.5465 - val_loss: 0.6365 - val_mse: 0.6365 - val_mae: 0.6013\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4552 - mse: 0.4552 - mae: 0.5267 - val_loss: 0.6272 - val_mse: 0.6272 - val_mae: 0.6080\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4594 - mse: 0.4594 - mae: 0.5384 - val_loss: 0.6253 - val_mse: 0.6253 - val_mae: 0.5907\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4537 - mse: 0.4537 - mae: 0.5317 - val_loss: 0.5578 - val_mse: 0.5578 - val_mae: 0.5645\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4351 - mse: 0.4351 - mae: 0.5146 - val_loss: 0.6124 - val_mse: 0.6124 - val_mae: 0.5804\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4521 - mse: 0.4521 - mae: 0.5298 - val_loss: 0.5796 - val_mse: 0.5796 - val_mae: 0.5715\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4575 - mse: 0.4575 - mae: 0.5237 - val_loss: 0.5549 - val_mse: 0.5549 - val_mae: 0.5536\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4798 - mse: 0.4798 - mae: 0.5363 - val_loss: 0.5728 - val_mse: 0.5728 - val_mae: 0.5668\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.4137 - mse: 0.4137 - mae: 0.5090 - val_loss: 0.5275 - val_mse: 0.5275 - val_mae: 0.5437\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4129 - mse: 0.4129 - mae: 0.5006 - val_loss: 0.5811 - val_mse: 0.5811 - val_mae: 0.5738\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4263 - mse: 0.4263 - mae: 0.5091 - val_loss: 0.5440 - val_mse: 0.5440 - val_mae: 0.5481\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4043 - mse: 0.4043 - mae: 0.4953 - val_loss: 0.5430 - val_mse: 0.5430 - val_mae: 0.5502\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4134 - mse: 0.4134 - mae: 0.4977 - val_loss: 0.5583 - val_mse: 0.5583 - val_mae: 0.5592\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3894 - mse: 0.3894 - mae: 0.4920 - val_loss: 0.5362 - val_mse: 0.5362 - val_mae: 0.5423\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4204 - mse: 0.4204 - mae: 0.5098 - val_loss: 0.5640 - val_mse: 0.5640 - val_mae: 0.5649\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3929 - mse: 0.3929 - mae: 0.4847 - val_loss: 0.5399 - val_mse: 0.5399 - val_mae: 0.5443\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3980 - mse: 0.3980 - mae: 0.4953 - val_loss: 0.5183 - val_mse: 0.5183 - val_mae: 0.5393\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3985 - mse: 0.3985 - mae: 0.4923 - val_loss: 0.6025 - val_mse: 0.6025 - val_mae: 0.5974\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3851 - mse: 0.3851 - mae: 0.4842 - val_loss: 0.5365 - val_mse: 0.5365 - val_mae: 0.5557\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3764 - mse: 0.3764 - mae: 0.4784 - val_loss: 0.5341 - val_mse: 0.5341 - val_mae: 0.5432\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3771 - mse: 0.3771 - mae: 0.4753 - val_loss: 0.5984 - val_mse: 0.5984 - val_mae: 0.5955\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4078 - mse: 0.4078 - mae: 0.4998 - val_loss: 0.5804 - val_mse: 0.5804 - val_mae: 0.5911\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3939 - mse: 0.3939 - mae: 0.4917 - val_loss: 0.5061 - val_mse: 0.5061 - val_mae: 0.5307\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3796 - mse: 0.3796 - mae: 0.4745 - val_loss: 0.5209 - val_mse: 0.5209 - val_mae: 0.5359\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3675 - mse: 0.3675 - mae: 0.4730 - val_loss: 0.5161 - val_mse: 0.5161 - val_mae: 0.5368\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3897 - mse: 0.3897 - mae: 0.4894 - val_loss: 0.4965 - val_mse: 0.4965 - val_mae: 0.5287\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3555 - mse: 0.3555 - mae: 0.4659 - val_loss: 0.5053 - val_mse: 0.5053 - val_mae: 0.5350\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3300 - mse: 0.3300 - mae: 0.4471 - val_loss: 0.5181 - val_mse: 0.5181 - val_mae: 0.5263\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3608 - mse: 0.3608 - mae: 0.4720 - val_loss: 0.5230 - val_mse: 0.5230 - val_mae: 0.5451\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3698 - mse: 0.3698 - mae: 0.4760 - val_loss: 0.5031 - val_mse: 0.5031 - val_mae: 0.5348\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3605 - mse: 0.3605 - mae: 0.4715 - val_loss: 0.5053 - val_mse: 0.5053 - val_mae: 0.5312\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3943 - mse: 0.3943 - mae: 0.4928 - val_loss: 0.5510 - val_mse: 0.5510 - val_mae: 0.5688\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3567 - mse: 0.3567 - mae: 0.4665 - val_loss: 0.4894 - val_mse: 0.4894 - val_mae: 0.5280\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3521 - mse: 0.3521 - mae: 0.4591 - val_loss: 0.5077 - val_mse: 0.5077 - val_mae: 0.5299\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3387 - mse: 0.3387 - mae: 0.4532 - val_loss: 0.4777 - val_mse: 0.4777 - val_mae: 0.5169\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3775 - mse: 0.3775 - mae: 0.4819 - val_loss: 0.4895 - val_mse: 0.4895 - val_mae: 0.5180\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3258 - mse: 0.3258 - mae: 0.4450 - val_loss: 0.4927 - val_mse: 0.4927 - val_mae: 0.5139\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3559 - mse: 0.3559 - mae: 0.4614 - val_loss: 0.4847 - val_mse: 0.4847 - val_mae: 0.5312\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3591 - mse: 0.3591 - mae: 0.4653 - val_loss: 0.4760 - val_mse: 0.4760 - val_mae: 0.5156\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3316 - mse: 0.3316 - mae: 0.4462 - val_loss: 0.4903 - val_mse: 0.4903 - val_mae: 0.5202\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3477 - mse: 0.3477 - mae: 0.4674 - val_loss: 0.4786 - val_mse: 0.4786 - val_mae: 0.5190\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3664 - mse: 0.3664 - mae: 0.4720 - val_loss: 0.5419 - val_mse: 0.5419 - val_mae: 0.5745\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3300 - mse: 0.3300 - mae: 0.4437 - val_loss: 0.4672 - val_mse: 0.4672 - val_mae: 0.5158\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3299 - mse: 0.3299 - mae: 0.4495 - val_loss: 0.5045 - val_mse: 0.5045 - val_mae: 0.5274\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3470 - mse: 0.3470 - mae: 0.4633 - val_loss: 0.4958 - val_mse: 0.4958 - val_mae: 0.5177\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3388 - mse: 0.3388 - mae: 0.4528 - val_loss: 0.4485 - val_mse: 0.4485 - val_mae: 0.5007\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.3191 - mse: 0.3191 - mae: 0.4380 - val_loss: 0.4924 - val_mse: 0.4924 - val_mae: 0.5212\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3282 - mse: 0.3282 - mae: 0.4423 - val_loss: 0.4681 - val_mse: 0.4681 - val_mae: 0.5248\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3343 - mse: 0.3343 - mae: 0.4515 - val_loss: 0.4828 - val_mse: 0.4828 - val_mae: 0.5130\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3290 - mse: 0.3290 - mae: 0.4463 - val_loss: 0.4885 - val_mse: 0.4885 - val_mae: 0.5199\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3092 - mse: 0.3092 - mae: 0.4361 - val_loss: 0.4719 - val_mse: 0.4719 - val_mae: 0.5201\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3307 - mse: 0.3307 - mae: 0.4505 - val_loss: 0.4623 - val_mse: 0.4623 - val_mae: 0.5113\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3357 - mse: 0.3357 - mae: 0.4499 - val_loss: 0.5337 - val_mse: 0.5337 - val_mae: 0.5631\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3124 - mse: 0.3124 - mae: 0.4284 - val_loss: 0.4838 - val_mse: 0.4838 - val_mae: 0.5378\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3478 - mse: 0.3478 - mae: 0.4658 - val_loss: 0.4899 - val_mse: 0.4899 - val_mae: 0.5288\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3404 - mse: 0.3404 - mae: 0.4473 - val_loss: 0.4647 - val_mse: 0.4647 - val_mae: 0.5098\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3265 - mse: 0.3265 - mae: 0.4472 - val_loss: 0.5017 - val_mse: 0.5017 - val_mae: 0.5193\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3256 - mse: 0.3256 - mae: 0.4462 - val_loss: 0.4850 - val_mse: 0.4850 - val_mae: 0.5434\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3157 - mse: 0.3157 - mae: 0.4357 - val_loss: 0.4992 - val_mse: 0.4992 - val_mae: 0.5203\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3157 - mse: 0.3157 - mae: 0.4305 - val_loss: 0.4780 - val_mse: 0.4780 - val_mae: 0.5409\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3033 - mse: 0.3033 - mae: 0.4243 - val_loss: 0.4694 - val_mse: 0.4694 - val_mae: 0.5131\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3046 - mse: 0.3046 - mae: 0.4239 - val_loss: 0.4889 - val_mse: 0.4889 - val_mae: 0.5195\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3188 - mse: 0.3188 - mae: 0.4414 - val_loss: 0.4382 - val_mse: 0.4382 - val_mae: 0.5076\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.3305 - mse: 0.3305 - mae: 0.4497 - val_loss: 0.4878 - val_mse: 0.4878 - val_mae: 0.5444\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3012 - mse: 0.3012 - mae: 0.4288 - val_loss: 0.5246 - val_mse: 0.5246 - val_mae: 0.5421\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3405 - mse: 0.3405 - mae: 0.4715 - val_loss: 0.4599 - val_mse: 0.4599 - val_mae: 0.5074\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3064 - mse: 0.3064 - mae: 0.4308 - val_loss: 0.5069 - val_mse: 0.5069 - val_mae: 0.5496\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3428 - mse: 0.3428 - mae: 0.4555 - val_loss: 0.4722 - val_mse: 0.4722 - val_mae: 0.5104\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3252 - mse: 0.3252 - mae: 0.4451 - val_loss: 0.4568 - val_mse: 0.4568 - val_mae: 0.5103\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3018 - mse: 0.3018 - mae: 0.4357 - val_loss: 0.4731 - val_mse: 0.4731 - val_mae: 0.5176\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2944 - mse: 0.2944 - mae: 0.4197 - val_loss: 0.4721 - val_mse: 0.4721 - val_mae: 0.5109\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2868 - mse: 0.2868 - mae: 0.4219 - val_loss: 0.4467 - val_mse: 0.4467 - val_mae: 0.5180\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3032 - mse: 0.3032 - mae: 0.4254 - val_loss: 0.4915 - val_mse: 0.4915 - val_mae: 0.5226\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3183 - mse: 0.3183 - mae: 0.4458 - val_loss: 0.4192 - val_mse: 0.4192 - val_mae: 0.4955\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2978 - mse: 0.2978 - mae: 0.4318 - val_loss: 0.4788 - val_mse: 0.4788 - val_mae: 0.5346\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3215 - mse: 0.3215 - mae: 0.4417 - val_loss: 0.4941 - val_mse: 0.4941 - val_mae: 0.5434\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2938 - mse: 0.2938 - mae: 0.4302 - val_loss: 0.4327 - val_mse: 0.4327 - val_mae: 0.5008\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2885 - mse: 0.2885 - mae: 0.4132 - val_loss: 0.4608 - val_mse: 0.4608 - val_mae: 0.5059\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2969 - mse: 0.2969 - mae: 0.4215 - val_loss: 0.4579 - val_mse: 0.4579 - val_mae: 0.5058\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2980 - mse: 0.2980 - mae: 0.4255 - val_loss: 0.4290 - val_mse: 0.4290 - val_mae: 0.5055\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2863 - mse: 0.2863 - mae: 0.4076 - val_loss: 0.4864 - val_mse: 0.4864 - val_mae: 0.5127\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2847 - mse: 0.2847 - mae: 0.4107 - val_loss: 0.4753 - val_mse: 0.4753 - val_mae: 0.5405\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3082 - mse: 0.3082 - mae: 0.4339 - val_loss: 0.4322 - val_mse: 0.4322 - val_mae: 0.5034\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2812 - mse: 0.2812 - mae: 0.4202 - val_loss: 0.4268 - val_mse: 0.4268 - val_mae: 0.5004\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2869 - mse: 0.2869 - mae: 0.4168 - val_loss: 0.4112 - val_mse: 0.4112 - val_mae: 0.4849\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2850 - mse: 0.2850 - mae: 0.4154 - val_loss: 0.4213 - val_mse: 0.4213 - val_mae: 0.4921\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3047 - mse: 0.3047 - mae: 0.4347 - val_loss: 0.4526 - val_mse: 0.4526 - val_mae: 0.5275\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3070 - mse: 0.3070 - mae: 0.4370 - val_loss: 0.4888 - val_mse: 0.4888 - val_mae: 0.5332\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2880 - mse: 0.2880 - mae: 0.4146 - val_loss: 0.4852 - val_mse: 0.4852 - val_mae: 0.5440\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2839 - mse: 0.2839 - mae: 0.4142 - val_loss: 0.4889 - val_mse: 0.4889 - val_mae: 0.5243\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3046 - mse: 0.3046 - mae: 0.4169 - val_loss: 0.4730 - val_mse: 0.4730 - val_mae: 0.5413\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2927 - mse: 0.2927 - mae: 0.4242 - val_loss: 0.5262 - val_mse: 0.5262 - val_mae: 0.5682\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3250 - mse: 0.3250 - mae: 0.4516 - val_loss: 0.4507 - val_mse: 0.4507 - val_mae: 0.5073\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2931 - mse: 0.2931 - mae: 0.4186 - val_loss: 0.4406 - val_mse: 0.4406 - val_mae: 0.5159\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2760 - mse: 0.2760 - mae: 0.4082 - val_loss: 0.4425 - val_mse: 0.4425 - val_mae: 0.5028\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2990 - mse: 0.2990 - mae: 0.4163 - val_loss: 0.4526 - val_mse: 0.4526 - val_mae: 0.5143\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2653 - mse: 0.2653 - mae: 0.4097 - val_loss: 0.4380 - val_mse: 0.4380 - val_mae: 0.5159\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2968 - mse: 0.2968 - mae: 0.4216 - val_loss: 0.4973 - val_mse: 0.4973 - val_mae: 0.5252\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2788 - mse: 0.2788 - mae: 0.4176 - val_loss: 0.4264 - val_mse: 0.4264 - val_mae: 0.4968\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2863 - mse: 0.2863 - mae: 0.4152 - val_loss: 0.4805 - val_mse: 0.4805 - val_mae: 0.5194\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2784 - mse: 0.2784 - mae: 0.4112 - val_loss: 0.4554 - val_mse: 0.4554 - val_mae: 0.5176\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2794 - mse: 0.2794 - mae: 0.4114 - val_loss: 0.4761 - val_mse: 0.4761 - val_mae: 0.5383\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2720 - mse: 0.2720 - mae: 0.4051 - val_loss: 0.4751 - val_mse: 0.4751 - val_mae: 0.5361\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2845 - mse: 0.2845 - mae: 0.4105 - val_loss: 0.4469 - val_mse: 0.4469 - val_mae: 0.5277\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2757 - mse: 0.2757 - mae: 0.4106 - val_loss: 0.4530 - val_mse: 0.4530 - val_mae: 0.5112\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2750 - mse: 0.2750 - mae: 0.4062 - val_loss: 0.4733 - val_mse: 0.4733 - val_mae: 0.5157\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3134 - mse: 0.3134 - mae: 0.4286 - val_loss: 0.4602 - val_mse: 0.4602 - val_mae: 0.5054\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2828 - mse: 0.2828 - mae: 0.4165 - val_loss: 0.4552 - val_mse: 0.4552 - val_mae: 0.5198\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2848 - mse: 0.2848 - mae: 0.4142 - val_loss: 0.4816 - val_mse: 0.4816 - val_mae: 0.5346\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2865 - mse: 0.2865 - mae: 0.4258 - val_loss: 0.4404 - val_mse: 0.4404 - val_mae: 0.5137\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2985 - mse: 0.2985 - mae: 0.4157 - val_loss: 0.4928 - val_mse: 0.4928 - val_mae: 0.5312\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3067 - mse: 0.3067 - mae: 0.4326 - val_loss: 0.4937 - val_mse: 0.4937 - val_mae: 0.5556\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2850 - mse: 0.2850 - mae: 0.4151 - val_loss: 0.4202 - val_mse: 0.4202 - val_mae: 0.4935\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2598 - mse: 0.2598 - mae: 0.3973 - val_loss: 0.4383 - val_mse: 0.4383 - val_mae: 0.4940\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2650 - mse: 0.2650 - mae: 0.4082 - val_loss: 0.4113 - val_mse: 0.4113 - val_mae: 0.4981\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2833 - mse: 0.2833 - mae: 0.4066 - val_loss: 0.4573 - val_mse: 0.4573 - val_mae: 0.5229\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2793 - mse: 0.2793 - mae: 0.4141 - val_loss: 0.4410 - val_mse: 0.4410 - val_mae: 0.5047\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3337 - mse: 0.3337 - mae: 0.4462 - val_loss: 0.4232 - val_mse: 0.4232 - val_mae: 0.5013\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2719 - mse: 0.2719 - mae: 0.4101 - val_loss: 0.4613 - val_mse: 0.4613 - val_mae: 0.5378\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2646 - mse: 0.2646 - mae: 0.4059 - val_loss: 0.4316 - val_mse: 0.4316 - val_mae: 0.4954\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2649 - mse: 0.2649 - mae: 0.4045 - val_loss: 0.4693 - val_mse: 0.4693 - val_mae: 0.5403\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2854 - mse: 0.2854 - mae: 0.4181 - val_loss: 0.4422 - val_mse: 0.4422 - val_mae: 0.5263\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2662 - mse: 0.2662 - mae: 0.3954 - val_loss: 0.4488 - val_mse: 0.4488 - val_mae: 0.5086\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2633 - mse: 0.2633 - mae: 0.3990 - val_loss: 0.4049 - val_mse: 0.4049 - val_mae: 0.4922\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2475 - mse: 0.2475 - mae: 0.3874 - val_loss: 0.4317 - val_mse: 0.4317 - val_mae: 0.5006\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2521 - mse: 0.2521 - mae: 0.3922 - val_loss: 0.4110 - val_mse: 0.4110 - val_mae: 0.4828\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2540 - mse: 0.2540 - mae: 0.3896 - val_loss: 0.4251 - val_mse: 0.4251 - val_mae: 0.4901\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2911 - mse: 0.2911 - mae: 0.4277 - val_loss: 0.4819 - val_mse: 0.4819 - val_mae: 0.5469\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2759 - mse: 0.2759 - mae: 0.4102 - val_loss: 0.4244 - val_mse: 0.4244 - val_mae: 0.4892\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2801 - mse: 0.2801 - mae: 0.4205 - val_loss: 0.4133 - val_mse: 0.4133 - val_mae: 0.4940\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2811 - mse: 0.2811 - mae: 0.4090 - val_loss: 0.5135 - val_mse: 0.5135 - val_mae: 0.5643\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3064 - mse: 0.3064 - mae: 0.4331 - val_loss: 0.4810 - val_mse: 0.4810 - val_mae: 0.5408\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2843 - mse: 0.2843 - mae: 0.4165 - val_loss: 0.4433 - val_mse: 0.4433 - val_mae: 0.5077\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2722 - mse: 0.2722 - mae: 0.4053 - val_loss: 0.4513 - val_mse: 0.4513 - val_mae: 0.5246\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2460 - mse: 0.2460 - mae: 0.3893 - val_loss: 0.4267 - val_mse: 0.4267 - val_mae: 0.5040\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2677 - mse: 0.2677 - mae: 0.4092 - val_loss: 0.4201 - val_mse: 0.4201 - val_mae: 0.4918\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2796 - mse: 0.2796 - mae: 0.4098 - val_loss: 0.4258 - val_mse: 0.4258 - val_mae: 0.5038\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2627 - mse: 0.2627 - mae: 0.3998 - val_loss: 0.4351 - val_mse: 0.4351 - val_mae: 0.4996\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2667 - mse: 0.2667 - mae: 0.3954 - val_loss: 0.4263 - val_mse: 0.4263 - val_mae: 0.5056\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2560 - mse: 0.2560 - mae: 0.3982 - val_loss: 0.4558 - val_mse: 0.4558 - val_mae: 0.5108\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2409 - mse: 0.2409 - mae: 0.3785 - val_loss: 0.4237 - val_mse: 0.4237 - val_mae: 0.5079\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2480 - mse: 0.2480 - mae: 0.3900 - val_loss: 0.4226 - val_mse: 0.4226 - val_mae: 0.5043\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2580 - mse: 0.2580 - mae: 0.3944 - val_loss: 0.4489 - val_mse: 0.4489 - val_mae: 0.5241\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2489 - mse: 0.2489 - mae: 0.3938 - val_loss: 0.4126 - val_mse: 0.4126 - val_mae: 0.4991\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2526 - mse: 0.2526 - mae: 0.3879 - val_loss: 0.4211 - val_mse: 0.4211 - val_mae: 0.5107\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2609 - mse: 0.2609 - mae: 0.3977 - val_loss: 0.3825 - val_mse: 0.3825 - val_mae: 0.4754\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2573 - mse: 0.2573 - mae: 0.3915 - val_loss: 0.4426 - val_mse: 0.4426 - val_mae: 0.5212\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2580 - mse: 0.2580 - mae: 0.3956 - val_loss: 0.4085 - val_mse: 0.4085 - val_mae: 0.4907\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2300 - mse: 0.2300 - mae: 0.3649 - val_loss: 0.4190 - val_mse: 0.4190 - val_mae: 0.4987\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2366 - mse: 0.2366 - mae: 0.3760 - val_loss: 0.3885 - val_mse: 0.3885 - val_mae: 0.4759\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2348 - mse: 0.2348 - mae: 0.3735 - val_loss: 0.3973 - val_mse: 0.3973 - val_mae: 0.4829\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2524 - mse: 0.2524 - mae: 0.3928 - val_loss: 0.4000 - val_mse: 0.4000 - val_mae: 0.4844\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2580 - mse: 0.2580 - mae: 0.3989 - val_loss: 0.4198 - val_mse: 0.4198 - val_mae: 0.4984\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2628 - mse: 0.2628 - mae: 0.3923 - val_loss: 0.4114 - val_mse: 0.4114 - val_mae: 0.4944\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2566 - mse: 0.2566 - mae: 0.3915 - val_loss: 0.4058 - val_mse: 0.4058 - val_mae: 0.4889\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2405 - mse: 0.2405 - mae: 0.3799 - val_loss: 0.4191 - val_mse: 0.4191 - val_mae: 0.4950\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2397 - mse: 0.2397 - mae: 0.3769 - val_loss: 0.4380 - val_mse: 0.4380 - val_mae: 0.5001\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2609 - mse: 0.2609 - mae: 0.3983 - val_loss: 0.4384 - val_mse: 0.4384 - val_mae: 0.5053\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2468 - mse: 0.2468 - mae: 0.3848 - val_loss: 0.4113 - val_mse: 0.4113 - val_mae: 0.4930\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2548 - mse: 0.2548 - mae: 0.3963 - val_loss: 0.4740 - val_mse: 0.4740 - val_mae: 0.5185\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2800 - mse: 0.2800 - mae: 0.4170 - val_loss: 0.4873 - val_mse: 0.4873 - val_mae: 0.5358\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2522 - mse: 0.2522 - mae: 0.3974 - val_loss: 0.4442 - val_mse: 0.4442 - val_mae: 0.5162\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2429 - mse: 0.2429 - mae: 0.3832 - val_loss: 0.4100 - val_mse: 0.4100 - val_mae: 0.4856\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2477 - mse: 0.2477 - mae: 0.3913 - val_loss: 0.4220 - val_mse: 0.4220 - val_mae: 0.4981\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2412 - mse: 0.2412 - mae: 0.3801 - val_loss: 0.4794 - val_mse: 0.4794 - val_mae: 0.5395\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2707 - mse: 0.2707 - mae: 0.4142 - val_loss: 0.4392 - val_mse: 0.4392 - val_mae: 0.5018\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2498 - mse: 0.2498 - mae: 0.3876 - val_loss: 0.4452 - val_mse: 0.4452 - val_mae: 0.5047\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2460 - mse: 0.2460 - mae: 0.3809 - val_loss: 0.4328 - val_mse: 0.4328 - val_mae: 0.4991\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2627 - mse: 0.2627 - mae: 0.3906 - val_loss: 0.4341 - val_mse: 0.4341 - val_mae: 0.5041\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2510 - mse: 0.2510 - mae: 0.3908 - val_loss: 0.4222 - val_mse: 0.4222 - val_mae: 0.4989\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2248 - mse: 0.2248 - mae: 0.3675 - val_loss: 0.4239 - val_mse: 0.4239 - val_mae: 0.4982\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2471 - mse: 0.2471 - mae: 0.3816 - val_loss: 0.5127 - val_mse: 0.5127 - val_mae: 0.5689\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2718 - mse: 0.2718 - mae: 0.4028 - val_loss: 0.4575 - val_mse: 0.4575 - val_mae: 0.5132\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2513 - mse: 0.2513 - mae: 0.3885 - val_loss: 0.4334 - val_mse: 0.4334 - val_mae: 0.4990\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2391 - mse: 0.2391 - mae: 0.3806 - val_loss: 0.4609 - val_mse: 0.4609 - val_mae: 0.5209\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2484 - mse: 0.2484 - mae: 0.3841 - val_loss: 0.4357 - val_mse: 0.4357 - val_mae: 0.4989\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2436 - mse: 0.2436 - mae: 0.3872 - val_loss: 0.4218 - val_mse: 0.4218 - val_mae: 0.5031\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2551 - mse: 0.2551 - mae: 0.3940 - val_loss: 0.4266 - val_mse: 0.4266 - val_mae: 0.4937\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2258 - mse: 0.2258 - mae: 0.3721 - val_loss: 0.4962 - val_mse: 0.4962 - val_mae: 0.5630\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2295 - mse: 0.2295 - mae: 0.3755 - val_loss: 0.4327 - val_mse: 0.4327 - val_mae: 0.5081\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2405 - mse: 0.2405 - mae: 0.3877 - val_loss: 0.4354 - val_mse: 0.4354 - val_mae: 0.5099\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2375 - mse: 0.2375 - mae: 0.3856 - val_loss: 0.4439 - val_mse: 0.4439 - val_mae: 0.5162\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2482 - mse: 0.2482 - mae: 0.3888 - val_loss: 0.4455 - val_mse: 0.4455 - val_mae: 0.5112\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2303 - mse: 0.2303 - mae: 0.3771 - val_loss: 0.4510 - val_mse: 0.4510 - val_mae: 0.5208\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2566 - mse: 0.2566 - mae: 0.3929 - val_loss: 0.4178 - val_mse: 0.4178 - val_mae: 0.4955\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2517 - mse: 0.2517 - mae: 0.3942 - val_loss: 0.4604 - val_mse: 0.4604 - val_mae: 0.5217\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2234 - mse: 0.2234 - mae: 0.3664 - val_loss: 0.4536 - val_mse: 0.4536 - val_mae: 0.5207\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2379 - mse: 0.2379 - mae: 0.3793 - val_loss: 0.4438 - val_mse: 0.4438 - val_mae: 0.5157\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2606 - mse: 0.2606 - mae: 0.3979 - val_loss: 0.4404 - val_mse: 0.4404 - val_mae: 0.5121\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2363 - mse: 0.2363 - mae: 0.3776 - val_loss: 0.4494 - val_mse: 0.4494 - val_mae: 0.5174\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2268 - mse: 0.2268 - mae: 0.3700 - val_loss: 0.4146 - val_mse: 0.4146 - val_mae: 0.5005\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2439 - mse: 0.2439 - mae: 0.3789 - val_loss: 0.4130 - val_mse: 0.4130 - val_mae: 0.4995\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2318 - mse: 0.2318 - mae: 0.3777 - val_loss: 0.4460 - val_mse: 0.4460 - val_mae: 0.5064\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2133 - mse: 0.2133 - mae: 0.3557 - val_loss: 0.4378 - val_mse: 0.4378 - val_mae: 0.5087\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2402 - mse: 0.2402 - mae: 0.3816 - val_loss: 0.4298 - val_mse: 0.4298 - val_mae: 0.5011\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2417 - mse: 0.2417 - mae: 0.3855 - val_loss: 0.4322 - val_mse: 0.4322 - val_mae: 0.5064\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2368 - mse: 0.2368 - mae: 0.3785 - val_loss: 0.4322 - val_mse: 0.4322 - val_mae: 0.4994\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2559 - mse: 0.2559 - mae: 0.3930 - val_loss: 0.4356 - val_mse: 0.4356 - val_mae: 0.5059\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2292 - mse: 0.2292 - mae: 0.3705 - val_loss: 0.4676 - val_mse: 0.4676 - val_mae: 0.5295\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2383 - mse: 0.2383 - mae: 0.3827 - val_loss: 0.4264 - val_mse: 0.4264 - val_mae: 0.5039\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2329 - mse: 0.2329 - mae: 0.3775 - val_loss: 0.4322 - val_mse: 0.4322 - val_mae: 0.5054\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2276 - mse: 0.2276 - mae: 0.3747 - val_loss: 0.4427 - val_mse: 0.4427 - val_mae: 0.5120\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2351 - mse: 0.2351 - mae: 0.3785 - val_loss: 0.4571 - val_mse: 0.4571 - val_mae: 0.5131\n",
            "Epoch 252/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2105 - mse: 0.2105 - mae: 0.3574 - val_loss: 0.4174 - val_mse: 0.4174 - val_mae: 0.4874\n",
            "Epoch 253/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2203 - mse: 0.2203 - mae: 0.3683 - val_loss: 0.4744 - val_mse: 0.4744 - val_mae: 0.5207\n",
            "Epoch 254/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2330 - mse: 0.2330 - mae: 0.3780 - val_loss: 0.4424 - val_mse: 0.4424 - val_mae: 0.5081\n",
            "Epoch 255/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2384 - mse: 0.2384 - mae: 0.3825 - val_loss: 0.4496 - val_mse: 0.4496 - val_mae: 0.5127\n",
            "Epoch 256/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.2360 - mse: 0.2360 - mae: 0.3784 - val_loss: 0.4365 - val_mse: 0.4365 - val_mae: 0.4988\n",
            "Epoch 257/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2444 - mse: 0.2444 - mae: 0.3834 - val_loss: 0.4019 - val_mse: 0.4019 - val_mae: 0.4902\n",
            "Epoch 258/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2488 - mse: 0.2488 - mae: 0.3891 - val_loss: 0.4950 - val_mse: 0.4950 - val_mae: 0.5512\n",
            "Epoch 259/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2525 - mse: 0.2525 - mae: 0.3918 - val_loss: 0.4351 - val_mse: 0.4351 - val_mae: 0.5082\n",
            "Epoch 260/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2251 - mse: 0.2251 - mae: 0.3653 - val_loss: 0.4717 - val_mse: 0.4717 - val_mae: 0.5492\n",
            "Epoch 261/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2210 - mse: 0.2210 - mae: 0.3700 - val_loss: 0.4189 - val_mse: 0.4189 - val_mae: 0.5015\n",
            "Epoch 262/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2159 - mse: 0.2159 - mae: 0.3654 - val_loss: 0.4333 - val_mse: 0.4333 - val_mae: 0.5052\n",
            "Epoch 263/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2156 - mse: 0.2156 - mae: 0.3637 - val_loss: 0.4351 - val_mse: 0.4351 - val_mae: 0.5102\n",
            "Epoch 264/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2453 - mse: 0.2453 - mae: 0.3841 - val_loss: 0.4293 - val_mse: 0.4293 - val_mae: 0.5072\n",
            "Epoch 265/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2190 - mse: 0.2190 - mae: 0.3680 - val_loss: 0.4468 - val_mse: 0.4468 - val_mae: 0.5043\n",
            "Epoch 266/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2260 - mse: 0.2260 - mae: 0.3735 - val_loss: 0.4399 - val_mse: 0.4399 - val_mae: 0.5102\n",
            "Epoch 267/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2328 - mse: 0.2328 - mae: 0.3698 - val_loss: 0.4298 - val_mse: 0.4298 - val_mae: 0.4977\n",
            "Epoch 268/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2377 - mse: 0.2377 - mae: 0.3832 - val_loss: 0.4504 - val_mse: 0.4504 - val_mae: 0.5088\n",
            "Epoch 269/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2218 - mse: 0.2218 - mae: 0.3670 - val_loss: 0.4443 - val_mse: 0.4443 - val_mae: 0.5029\n",
            "Epoch 270/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2161 - mse: 0.2161 - mae: 0.3604 - val_loss: 0.4246 - val_mse: 0.4246 - val_mae: 0.4911\n",
            "Epoch 271/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2112 - mse: 0.2112 - mae: 0.3597 - val_loss: 0.4865 - val_mse: 0.4865 - val_mae: 0.5267\n",
            "Epoch 272/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2293 - mse: 0.2293 - mae: 0.3665 - val_loss: 0.4573 - val_mse: 0.4573 - val_mae: 0.5193\n",
            "Epoch 273/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2179 - mse: 0.2179 - mae: 0.3683 - val_loss: 0.4169 - val_mse: 0.4169 - val_mae: 0.4999\n",
            "Epoch 274/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2218 - mse: 0.2218 - mae: 0.3727 - val_loss: 0.4334 - val_mse: 0.4334 - val_mae: 0.4971\n",
            "Epoch 275/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2375 - mse: 0.2375 - mae: 0.3840 - val_loss: 0.4362 - val_mse: 0.4362 - val_mae: 0.5090\n",
            "Epoch 276/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2267 - mse: 0.2267 - mae: 0.3781 - val_loss: 0.4676 - val_mse: 0.4676 - val_mae: 0.5317\n",
            "Epoch 277/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2443 - mse: 0.2443 - mae: 0.3802 - val_loss: 0.4455 - val_mse: 0.4455 - val_mae: 0.5011\n",
            "Epoch 278/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2534 - mse: 0.2534 - mae: 0.3867 - val_loss: 0.4188 - val_mse: 0.4188 - val_mae: 0.5035\n",
            "Epoch 279/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2368 - mse: 0.2368 - mae: 0.3841 - val_loss: 0.4218 - val_mse: 0.4218 - val_mae: 0.4971\n",
            "Epoch 280/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2236 - mse: 0.2236 - mae: 0.3673 - val_loss: 0.4568 - val_mse: 0.4568 - val_mae: 0.5268\n",
            "Epoch 281/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2158 - mse: 0.2158 - mae: 0.3614 - val_loss: 0.4430 - val_mse: 0.4430 - val_mae: 0.5065\n",
            "Epoch 282/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2225 - mse: 0.2225 - mae: 0.3667 - val_loss: 0.4238 - val_mse: 0.4238 - val_mae: 0.5010\n",
            "Epoch 283/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2167 - mse: 0.2167 - mae: 0.3672 - val_loss: 0.4257 - val_mse: 0.4257 - val_mae: 0.5048\n",
            "Epoch 284/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2265 - mse: 0.2265 - mae: 0.3656 - val_loss: 0.4190 - val_mse: 0.4190 - val_mae: 0.4994\n",
            "Epoch 285/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2320 - mse: 0.2320 - mae: 0.3762 - val_loss: 0.4338 - val_mse: 0.4338 - val_mae: 0.5022\n",
            "Epoch 286/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2084 - mse: 0.2084 - mae: 0.3534 - val_loss: 0.4189 - val_mse: 0.4189 - val_mae: 0.5002\n",
            "Epoch 287/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2101 - mse: 0.2101 - mae: 0.3503 - val_loss: 0.4465 - val_mse: 0.4465 - val_mae: 0.5136\n",
            "Epoch 288/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2258 - mse: 0.2258 - mae: 0.3727 - val_loss: 0.4406 - val_mse: 0.4406 - val_mae: 0.5151\n",
            "Epoch 289/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2306 - mse: 0.2306 - mae: 0.3727 - val_loss: 0.4779 - val_mse: 0.4779 - val_mae: 0.5351\n",
            "Epoch 290/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2170 - mse: 0.2170 - mae: 0.3666 - val_loss: 0.4486 - val_mse: 0.4486 - val_mae: 0.5083\n",
            "Epoch 291/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2103 - mse: 0.2103 - mae: 0.3580 - val_loss: 0.4529 - val_mse: 0.4529 - val_mae: 0.5054\n",
            "Epoch 292/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2314 - mse: 0.2314 - mae: 0.3742 - val_loss: 0.4407 - val_mse: 0.4407 - val_mae: 0.5136\n",
            "Epoch 293/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2316 - mse: 0.2316 - mae: 0.3779 - val_loss: 0.4160 - val_mse: 0.4160 - val_mae: 0.4986\n",
            "Epoch 294/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2059 - mse: 0.2059 - mae: 0.3537 - val_loss: 0.4281 - val_mse: 0.4281 - val_mae: 0.5058\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 0.4659 - mse: 0.4659 - mae: 0.5116\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 15ms/step - loss: 30.6268 - mse: 30.6268 - mae: 5.3952 - val_loss: 22.3642 - val_mse: 22.3642 - val_mae: 4.5732\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 17.0533 - mse: 17.0533 - mae: 3.9160 - val_loss: 12.3632 - val_mse: 12.3632 - val_mae: 3.3013\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 9.0613 - mse: 9.0613 - mae: 2.7316 - val_loss: 7.1381 - val_mse: 7.1381 - val_mae: 2.3479\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 5.3663 - mse: 5.3663 - mae: 1.9893 - val_loss: 4.8209 - val_mse: 4.8209 - val_mae: 1.7927\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 3.7008 - mse: 3.7008 - mae: 1.5953 - val_loss: 3.9640 - val_mse: 3.9640 - val_mae: 1.5639\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 3.2274 - mse: 3.2274 - mae: 1.4608 - val_loss: 3.5047 - val_mse: 3.5047 - val_mae: 1.4586\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.8826 - mse: 2.8826 - mae: 1.3711 - val_loss: 3.2431 - val_mse: 3.2431 - val_mae: 1.4076\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 2.7106 - mse: 2.7106 - mae: 1.3314 - val_loss: 3.0021 - val_mse: 3.0021 - val_mae: 1.3585\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.5568 - mse: 2.5568 - mae: 1.3023 - val_loss: 2.8369 - val_mse: 2.8369 - val_mae: 1.3156\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.2341 - mse: 2.2341 - mae: 1.1969 - val_loss: 2.6807 - val_mse: 2.6807 - val_mae: 1.2836\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.3356 - mse: 2.3356 - mae: 1.2288 - val_loss: 2.5421 - val_mse: 2.5421 - val_mae: 1.2526\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 2.1296 - mse: 2.1296 - mae: 1.1529 - val_loss: 2.4256 - val_mse: 2.4256 - val_mae: 1.2216\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 2.0997 - mse: 2.0997 - mae: 1.1626 - val_loss: 2.3190 - val_mse: 2.3190 - val_mae: 1.1945\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.1004 - mse: 2.1004 - mae: 1.1547 - val_loss: 2.2008 - val_mse: 2.2008 - val_mae: 1.1756\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.0416 - mse: 2.0416 - mae: 1.1361 - val_loss: 2.1316 - val_mse: 2.1316 - val_mae: 1.1590\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.9157 - mse: 1.9157 - mae: 1.1080 - val_loss: 2.0615 - val_mse: 2.0615 - val_mae: 1.1441\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.8326 - mse: 1.8326 - mae: 1.0741 - val_loss: 2.0172 - val_mse: 2.0172 - val_mae: 1.1260\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.8390 - mse: 1.8390 - mae: 1.0850 - val_loss: 1.9511 - val_mse: 1.9511 - val_mae: 1.1105\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.7341 - mse: 1.7341 - mae: 1.0556 - val_loss: 1.8895 - val_mse: 1.8895 - val_mae: 1.0965\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.8312 - mse: 1.8312 - mae: 1.0746 - val_loss: 1.8209 - val_mse: 1.8209 - val_mae: 1.0786\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.6430 - mse: 1.6430 - mae: 1.0179 - val_loss: 1.7860 - val_mse: 1.7860 - val_mae: 1.0697\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.6381 - mse: 1.6381 - mae: 1.0174 - val_loss: 1.7010 - val_mse: 1.7010 - val_mae: 1.0500\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.5357 - mse: 1.5357 - mae: 0.9899 - val_loss: 1.6468 - val_mse: 1.6468 - val_mae: 1.0330\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.4995 - mse: 1.4995 - mae: 0.9721 - val_loss: 1.6186 - val_mse: 1.6186 - val_mae: 1.0179\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.5080 - mse: 1.5080 - mae: 0.9654 - val_loss: 1.5853 - val_mse: 1.5853 - val_mae: 1.0058\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.4759 - mse: 1.4759 - mae: 0.9617 - val_loss: 1.5460 - val_mse: 1.5460 - val_mae: 1.0041\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.3986 - mse: 1.3986 - mae: 0.9538 - val_loss: 1.5071 - val_mse: 1.5071 - val_mae: 0.9819\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.4984 - mse: 1.4984 - mae: 0.9734 - val_loss: 1.4910 - val_mse: 1.4910 - val_mae: 0.9764\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.3226 - mse: 1.3226 - mae: 0.9006 - val_loss: 1.4353 - val_mse: 1.4353 - val_mae: 0.9643\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.4546 - mse: 1.4546 - mae: 0.9604 - val_loss: 1.3927 - val_mse: 1.3927 - val_mae: 0.9505\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.2285 - mse: 1.2285 - mae: 0.8788 - val_loss: 1.3664 - val_mse: 1.3664 - val_mae: 0.9387\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.3005 - mse: 1.3005 - mae: 0.9069 - val_loss: 1.3456 - val_mse: 1.3456 - val_mae: 0.9323\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1922 - mse: 1.1922 - mae: 0.8571 - val_loss: 1.3098 - val_mse: 1.3098 - val_mae: 0.9209\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.2086 - mse: 1.2086 - mae: 0.8686 - val_loss: 1.2859 - val_mse: 1.2859 - val_mae: 0.9063\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1895 - mse: 1.1895 - mae: 0.8693 - val_loss: 1.2615 - val_mse: 1.2615 - val_mae: 0.9025\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.1680 - mse: 1.1680 - mae: 0.8582 - val_loss: 1.2245 - val_mse: 1.2245 - val_mae: 0.8907\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1494 - mse: 1.1494 - mae: 0.8501 - val_loss: 1.1878 - val_mse: 1.1878 - val_mae: 0.8765\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1241 - mse: 1.1241 - mae: 0.8280 - val_loss: 1.1545 - val_mse: 1.1545 - val_mae: 0.8599\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0279 - mse: 1.0279 - mae: 0.8072 - val_loss: 1.1247 - val_mse: 1.1247 - val_mae: 0.8501\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0494 - mse: 1.0494 - mae: 0.8168 - val_loss: 1.1068 - val_mse: 1.1068 - val_mae: 0.8432\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.0350 - mse: 1.0350 - mae: 0.7996 - val_loss: 1.0849 - val_mse: 1.0849 - val_mae: 0.8364\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9838 - mse: 0.9838 - mae: 0.7766 - val_loss: 1.0690 - val_mse: 1.0690 - val_mae: 0.8237\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.0007 - mse: 1.0007 - mae: 0.7863 - val_loss: 1.0513 - val_mse: 1.0513 - val_mae: 0.8151\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9732 - mse: 0.9732 - mae: 0.7807 - val_loss: 1.0144 - val_mse: 1.0144 - val_mae: 0.8035\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9859 - mse: 0.9859 - mae: 0.7726 - val_loss: 0.9939 - val_mse: 0.9939 - val_mae: 0.7942\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.8613 - mse: 0.8613 - mae: 0.7266 - val_loss: 0.9717 - val_mse: 0.9717 - val_mae: 0.7852\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9407 - mse: 0.9407 - mae: 0.7690 - val_loss: 0.9531 - val_mse: 0.9531 - val_mae: 0.7773\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9257 - mse: 0.9257 - mae: 0.7475 - val_loss: 0.9203 - val_mse: 0.9203 - val_mae: 0.7660\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8740 - mse: 0.8740 - mae: 0.7407 - val_loss: 0.8971 - val_mse: 0.8971 - val_mae: 0.7554\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8970 - mse: 0.8970 - mae: 0.7515 - val_loss: 0.8860 - val_mse: 0.8860 - val_mae: 0.7445\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8352 - mse: 0.8352 - mae: 0.7156 - val_loss: 0.8621 - val_mse: 0.8621 - val_mae: 0.7390\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7703 - mse: 0.7703 - mae: 0.6917 - val_loss: 0.8516 - val_mse: 0.8516 - val_mae: 0.7369\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7734 - mse: 0.7734 - mae: 0.6985 - val_loss: 0.8293 - val_mse: 0.8293 - val_mae: 0.7296\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7772 - mse: 0.7772 - mae: 0.6952 - val_loss: 0.8101 - val_mse: 0.8101 - val_mae: 0.7212\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7308 - mse: 0.7308 - mae: 0.6643 - val_loss: 0.7863 - val_mse: 0.7863 - val_mae: 0.7020\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7779 - mse: 0.7779 - mae: 0.6880 - val_loss: 0.7811 - val_mse: 0.7811 - val_mae: 0.7076\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7509 - mse: 0.7509 - mae: 0.6719 - val_loss: 0.7621 - val_mse: 0.7621 - val_mae: 0.6941\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7165 - mse: 0.7165 - mae: 0.6702 - val_loss: 0.7410 - val_mse: 0.7410 - val_mae: 0.6807\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7180 - mse: 0.7180 - mae: 0.6616 - val_loss: 0.7351 - val_mse: 0.7351 - val_mae: 0.6768\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7486 - mse: 0.7486 - mae: 0.6700 - val_loss: 0.7189 - val_mse: 0.7189 - val_mae: 0.6744\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6903 - mse: 0.6903 - mae: 0.6514 - val_loss: 0.7034 - val_mse: 0.7034 - val_mae: 0.6641\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6712 - mse: 0.6712 - mae: 0.6391 - val_loss: 0.6811 - val_mse: 0.6811 - val_mae: 0.6544\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6846 - mse: 0.6846 - mae: 0.6525 - val_loss: 0.6815 - val_mse: 0.6815 - val_mae: 0.6560\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6715 - mse: 0.6715 - mae: 0.6357 - val_loss: 0.6563 - val_mse: 0.6563 - val_mae: 0.6398\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.6942 - mse: 0.6942 - mae: 0.6500 - val_loss: 0.6448 - val_mse: 0.6448 - val_mae: 0.6341\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6169 - mse: 0.6169 - mae: 0.6084 - val_loss: 0.6307 - val_mse: 0.6307 - val_mae: 0.6257\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6044 - mse: 0.6044 - mae: 0.6018 - val_loss: 0.6243 - val_mse: 0.6243 - val_mae: 0.6218\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5921 - mse: 0.5921 - mae: 0.5925 - val_loss: 0.6176 - val_mse: 0.6176 - val_mae: 0.6186\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6281 - mse: 0.6281 - mae: 0.6159 - val_loss: 0.6069 - val_mse: 0.6069 - val_mae: 0.6141\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5648 - mse: 0.5648 - mae: 0.5832 - val_loss: 0.6032 - val_mse: 0.6032 - val_mae: 0.6150\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5976 - mse: 0.5976 - mae: 0.6002 - val_loss: 0.5858 - val_mse: 0.5858 - val_mae: 0.6027\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.5731 - mse: 0.5731 - mae: 0.5952 - val_loss: 0.5825 - val_mse: 0.5825 - val_mae: 0.6016\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5813 - mse: 0.5813 - mae: 0.5935 - val_loss: 0.5741 - val_mse: 0.5741 - val_mae: 0.5952\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5181 - mse: 0.5181 - mae: 0.5619 - val_loss: 0.5633 - val_mse: 0.5633 - val_mae: 0.5893\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5489 - mse: 0.5489 - mae: 0.5813 - val_loss: 0.5531 - val_mse: 0.5531 - val_mae: 0.5831\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5394 - mse: 0.5394 - mae: 0.5683 - val_loss: 0.5534 - val_mse: 0.5534 - val_mae: 0.5843\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5434 - mse: 0.5434 - mae: 0.5771 - val_loss: 0.5403 - val_mse: 0.5403 - val_mae: 0.5761\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.5434 - mse: 0.5434 - mae: 0.5645 - val_loss: 0.5297 - val_mse: 0.5297 - val_mae: 0.5718\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5223 - mse: 0.5223 - mae: 0.5557 - val_loss: 0.5216 - val_mse: 0.5216 - val_mae: 0.5689\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5353 - mse: 0.5353 - mae: 0.5747 - val_loss: 0.5180 - val_mse: 0.5180 - val_mae: 0.5675\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5205 - mse: 0.5205 - mae: 0.5687 - val_loss: 0.5088 - val_mse: 0.5088 - val_mae: 0.5597\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5096 - mse: 0.5096 - mae: 0.5498 - val_loss: 0.5053 - val_mse: 0.5053 - val_mae: 0.5567\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5129 - mse: 0.5129 - mae: 0.5561 - val_loss: 0.5095 - val_mse: 0.5095 - val_mae: 0.5596\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4987 - mse: 0.4987 - mae: 0.5473 - val_loss: 0.4948 - val_mse: 0.4948 - val_mae: 0.5533\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4958 - mse: 0.4958 - mae: 0.5380 - val_loss: 0.4791 - val_mse: 0.4791 - val_mae: 0.5441\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5118 - mse: 0.5118 - mae: 0.5544 - val_loss: 0.4780 - val_mse: 0.4780 - val_mae: 0.5426\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4834 - mse: 0.4834 - mae: 0.5402 - val_loss: 0.4759 - val_mse: 0.4759 - val_mae: 0.5424\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4830 - mse: 0.4830 - mae: 0.5410 - val_loss: 0.4612 - val_mse: 0.4612 - val_mae: 0.5331\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4677 - mse: 0.4677 - mae: 0.5359 - val_loss: 0.4593 - val_mse: 0.4593 - val_mae: 0.5355\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4602 - mse: 0.4602 - mae: 0.5295 - val_loss: 0.4623 - val_mse: 0.4623 - val_mae: 0.5361\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4845 - mse: 0.4845 - mae: 0.5493 - val_loss: 0.4509 - val_mse: 0.4509 - val_mae: 0.5328\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4521 - mse: 0.4521 - mae: 0.5200 - val_loss: 0.4582 - val_mse: 0.4582 - val_mae: 0.5332\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4488 - mse: 0.4488 - mae: 0.5190 - val_loss: 0.4482 - val_mse: 0.4482 - val_mae: 0.5299\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4554 - mse: 0.4554 - mae: 0.5275 - val_loss: 0.4428 - val_mse: 0.4428 - val_mae: 0.5252\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4526 - mse: 0.4526 - mae: 0.5238 - val_loss: 0.4440 - val_mse: 0.4440 - val_mae: 0.5255\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4673 - mse: 0.4673 - mae: 0.5313 - val_loss: 0.4370 - val_mse: 0.4370 - val_mae: 0.5226\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4425 - mse: 0.4425 - mae: 0.5155 - val_loss: 0.4297 - val_mse: 0.4297 - val_mae: 0.5184\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4347 - mse: 0.4347 - mae: 0.5085 - val_loss: 0.4331 - val_mse: 0.4331 - val_mae: 0.5225\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4424 - mse: 0.4424 - mae: 0.5198 - val_loss: 0.4301 - val_mse: 0.4301 - val_mae: 0.5200\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4331 - mse: 0.4331 - mae: 0.5078 - val_loss: 0.4286 - val_mse: 0.4286 - val_mae: 0.5201\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4356 - mse: 0.4356 - mae: 0.5092 - val_loss: 0.4213 - val_mse: 0.4213 - val_mae: 0.5171\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4203 - mse: 0.4203 - mae: 0.5018 - val_loss: 0.4179 - val_mse: 0.4179 - val_mae: 0.5133\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4223 - mse: 0.4223 - mae: 0.5052 - val_loss: 0.4112 - val_mse: 0.4112 - val_mae: 0.5091\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4227 - mse: 0.4227 - mae: 0.5027 - val_loss: 0.4113 - val_mse: 0.4113 - val_mae: 0.5094\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4499 - mse: 0.4499 - mae: 0.5172 - val_loss: 0.4258 - val_mse: 0.4258 - val_mae: 0.5150\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4100 - mse: 0.4100 - mae: 0.4967 - val_loss: 0.4051 - val_mse: 0.4051 - val_mae: 0.5059\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4041 - mse: 0.4041 - mae: 0.4894 - val_loss: 0.4152 - val_mse: 0.4152 - val_mae: 0.5112\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4154 - mse: 0.4154 - mae: 0.4996 - val_loss: 0.4032 - val_mse: 0.4032 - val_mae: 0.5030\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4002 - mse: 0.4002 - mae: 0.4941 - val_loss: 0.3989 - val_mse: 0.3989 - val_mae: 0.5007\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4184 - mse: 0.4184 - mae: 0.4996 - val_loss: 0.4042 - val_mse: 0.4042 - val_mae: 0.5030\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4010 - mse: 0.4010 - mae: 0.4902 - val_loss: 0.4008 - val_mse: 0.4008 - val_mae: 0.5011\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3991 - mse: 0.3991 - mae: 0.4898 - val_loss: 0.3988 - val_mse: 0.3988 - val_mae: 0.4979\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4005 - mse: 0.4005 - mae: 0.4938 - val_loss: 0.3999 - val_mse: 0.3999 - val_mae: 0.4992\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4113 - mse: 0.4113 - mae: 0.5024 - val_loss: 0.3930 - val_mse: 0.3930 - val_mae: 0.4953\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4222 - mse: 0.4222 - mae: 0.5127 - val_loss: 0.3881 - val_mse: 0.3881 - val_mae: 0.4928\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4003 - mse: 0.4003 - mae: 0.4932 - val_loss: 0.4065 - val_mse: 0.4065 - val_mae: 0.5051\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3933 - mse: 0.3933 - mae: 0.4856 - val_loss: 0.3864 - val_mse: 0.3864 - val_mae: 0.4920\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3863 - mse: 0.3863 - mae: 0.4761 - val_loss: 0.3906 - val_mse: 0.3906 - val_mae: 0.4935\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3701 - mse: 0.3701 - mae: 0.4706 - val_loss: 0.3832 - val_mse: 0.3832 - val_mae: 0.4880\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4019 - mse: 0.4019 - mae: 0.4895 - val_loss: 0.3848 - val_mse: 0.3848 - val_mae: 0.4897\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3937 - mse: 0.3937 - mae: 0.4880 - val_loss: 0.3982 - val_mse: 0.3982 - val_mae: 0.4974\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3812 - mse: 0.3812 - mae: 0.4800 - val_loss: 0.3877 - val_mse: 0.3877 - val_mae: 0.4880\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4040 - mse: 0.4040 - mae: 0.4984 - val_loss: 0.3916 - val_mse: 0.3916 - val_mae: 0.4957\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3989 - mse: 0.3989 - mae: 0.4875 - val_loss: 0.3808 - val_mse: 0.3808 - val_mae: 0.4898\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3949 - mse: 0.3949 - mae: 0.4876 - val_loss: 0.3794 - val_mse: 0.3794 - val_mae: 0.4875\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4038 - mse: 0.4038 - mae: 0.4953 - val_loss: 0.3925 - val_mse: 0.3925 - val_mae: 0.4973\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3651 - mse: 0.3651 - mae: 0.4629 - val_loss: 0.3769 - val_mse: 0.3769 - val_mae: 0.4885\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3748 - mse: 0.3748 - mae: 0.4808 - val_loss: 0.3763 - val_mse: 0.3763 - val_mae: 0.4863\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3638 - mse: 0.3638 - mae: 0.4682 - val_loss: 0.3852 - val_mse: 0.3852 - val_mae: 0.4917\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3800 - mse: 0.3800 - mae: 0.4806 - val_loss: 0.3662 - val_mse: 0.3662 - val_mae: 0.4783\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3574 - mse: 0.3574 - mae: 0.4671 - val_loss: 0.3916 - val_mse: 0.3916 - val_mae: 0.4949\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3732 - mse: 0.3732 - mae: 0.4739 - val_loss: 0.3729 - val_mse: 0.3729 - val_mae: 0.4845\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3750 - mse: 0.3750 - mae: 0.4727 - val_loss: 0.3754 - val_mse: 0.3754 - val_mae: 0.4842\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3563 - mse: 0.3563 - mae: 0.4661 - val_loss: 0.3754 - val_mse: 0.3754 - val_mae: 0.4836\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3824 - mse: 0.3824 - mae: 0.4823 - val_loss: 0.3777 - val_mse: 0.3777 - val_mae: 0.4858\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3776 - mse: 0.3776 - mae: 0.4766 - val_loss: 0.3611 - val_mse: 0.3611 - val_mae: 0.4759\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3553 - mse: 0.3553 - mae: 0.4616 - val_loss: 0.3681 - val_mse: 0.3681 - val_mae: 0.4794\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3678 - mse: 0.3678 - mae: 0.4673 - val_loss: 0.3629 - val_mse: 0.3629 - val_mae: 0.4760\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3608 - mse: 0.3608 - mae: 0.4644 - val_loss: 0.3707 - val_mse: 0.3707 - val_mae: 0.4820\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3727 - mse: 0.3727 - mae: 0.4689 - val_loss: 0.3642 - val_mse: 0.3642 - val_mae: 0.4765\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3699 - mse: 0.3699 - mae: 0.4707 - val_loss: 0.3637 - val_mse: 0.3637 - val_mae: 0.4741\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3543 - mse: 0.3543 - mae: 0.4593 - val_loss: 0.3615 - val_mse: 0.3615 - val_mae: 0.4741\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3683 - mse: 0.3683 - mae: 0.4692 - val_loss: 0.3698 - val_mse: 0.3698 - val_mae: 0.4819\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3638 - mse: 0.3638 - mae: 0.4660 - val_loss: 0.3661 - val_mse: 0.3661 - val_mae: 0.4781\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3668 - mse: 0.3668 - mae: 0.4638 - val_loss: 0.3610 - val_mse: 0.3610 - val_mae: 0.4746\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3486 - mse: 0.3486 - mae: 0.4591 - val_loss: 0.3590 - val_mse: 0.3590 - val_mae: 0.4715\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3591 - mse: 0.3591 - mae: 0.4529 - val_loss: 0.3667 - val_mse: 0.3667 - val_mae: 0.4789\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3474 - mse: 0.3474 - mae: 0.4538 - val_loss: 0.3552 - val_mse: 0.3552 - val_mae: 0.4687\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3461 - mse: 0.3461 - mae: 0.4574 - val_loss: 0.3600 - val_mse: 0.3600 - val_mae: 0.4736\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3557 - mse: 0.3557 - mae: 0.4626 - val_loss: 0.3574 - val_mse: 0.3574 - val_mae: 0.4726\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3449 - mse: 0.3449 - mae: 0.4628 - val_loss: 0.3704 - val_mse: 0.3704 - val_mae: 0.4820\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3479 - mse: 0.3479 - mae: 0.4536 - val_loss: 0.3642 - val_mse: 0.3642 - val_mae: 0.4767\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3407 - mse: 0.3407 - mae: 0.4511 - val_loss: 0.3828 - val_mse: 0.3828 - val_mae: 0.4882\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3287 - mse: 0.3287 - mae: 0.4461 - val_loss: 0.3552 - val_mse: 0.3552 - val_mae: 0.4706\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3393 - mse: 0.3393 - mae: 0.4482 - val_loss: 0.3767 - val_mse: 0.3767 - val_mae: 0.4849\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3628 - mse: 0.3628 - mae: 0.4646 - val_loss: 0.3609 - val_mse: 0.3609 - val_mae: 0.4747\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3643 - mse: 0.3643 - mae: 0.4728 - val_loss: 0.3570 - val_mse: 0.3570 - val_mae: 0.4712\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3419 - mse: 0.3419 - mae: 0.4514 - val_loss: 0.3594 - val_mse: 0.3594 - val_mae: 0.4704\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3362 - mse: 0.3362 - mae: 0.4437 - val_loss: 0.3661 - val_mse: 0.3661 - val_mae: 0.4774\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3531 - mse: 0.3531 - mae: 0.4638 - val_loss: 0.3545 - val_mse: 0.3545 - val_mae: 0.4685\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3490 - mse: 0.3490 - mae: 0.4540 - val_loss: 0.3671 - val_mse: 0.3671 - val_mae: 0.4758\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3313 - mse: 0.3313 - mae: 0.4430 - val_loss: 0.3452 - val_mse: 0.3452 - val_mae: 0.4632\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3447 - mse: 0.3447 - mae: 0.4584 - val_loss: 0.3546 - val_mse: 0.3546 - val_mae: 0.4682\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3409 - mse: 0.3409 - mae: 0.4567 - val_loss: 0.3492 - val_mse: 0.3492 - val_mae: 0.4624\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3203 - mse: 0.3203 - mae: 0.4365 - val_loss: 0.3540 - val_mse: 0.3540 - val_mae: 0.4671\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3507 - mse: 0.3507 - mae: 0.4608 - val_loss: 0.3547 - val_mse: 0.3547 - val_mae: 0.4691\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3309 - mse: 0.3309 - mae: 0.4507 - val_loss: 0.3534 - val_mse: 0.3534 - val_mae: 0.4687\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3452 - mse: 0.3452 - mae: 0.4541 - val_loss: 0.3454 - val_mse: 0.3454 - val_mae: 0.4623\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3480 - mse: 0.3480 - mae: 0.4573 - val_loss: 0.3551 - val_mse: 0.3551 - val_mae: 0.4748\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3426 - mse: 0.3426 - mae: 0.4567 - val_loss: 0.3581 - val_mse: 0.3581 - val_mae: 0.4754\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3268 - mse: 0.3268 - mae: 0.4426 - val_loss: 0.3597 - val_mse: 0.3597 - val_mae: 0.4733\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3527 - mse: 0.3527 - mae: 0.4547 - val_loss: 0.3539 - val_mse: 0.3539 - val_mae: 0.4710\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3278 - mse: 0.3278 - mae: 0.4420 - val_loss: 0.3537 - val_mse: 0.3537 - val_mae: 0.4691\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3324 - mse: 0.3324 - mae: 0.4428 - val_loss: 0.3449 - val_mse: 0.3449 - val_mae: 0.4632\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3400 - mse: 0.3400 - mae: 0.4564 - val_loss: 0.3544 - val_mse: 0.3544 - val_mae: 0.4709\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3235 - mse: 0.3235 - mae: 0.4418 - val_loss: 0.3474 - val_mse: 0.3474 - val_mae: 0.4642\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3319 - mse: 0.3319 - mae: 0.4494 - val_loss: 0.3519 - val_mse: 0.3519 - val_mae: 0.4688\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3234 - mse: 0.3234 - mae: 0.4473 - val_loss: 0.3588 - val_mse: 0.3588 - val_mae: 0.4734\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3397 - mse: 0.3397 - mae: 0.4575 - val_loss: 0.3488 - val_mse: 0.3488 - val_mae: 0.4689\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3424 - mse: 0.3424 - mae: 0.4560 - val_loss: 0.3865 - val_mse: 0.3865 - val_mae: 0.4897\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3484 - mse: 0.3484 - mae: 0.4591 - val_loss: 0.3463 - val_mse: 0.3463 - val_mae: 0.4640\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3383 - mse: 0.3383 - mae: 0.4528 - val_loss: 0.3609 - val_mse: 0.3609 - val_mae: 0.4737\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3478 - mse: 0.3478 - mae: 0.4466 - val_loss: 0.3425 - val_mse: 0.3425 - val_mae: 0.4628\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3184 - mse: 0.3184 - mae: 0.4385 - val_loss: 0.3414 - val_mse: 0.3414 - val_mae: 0.4614\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3235 - mse: 0.3235 - mae: 0.4389 - val_loss: 0.3508 - val_mse: 0.3508 - val_mae: 0.4648\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3215 - mse: 0.3215 - mae: 0.4433 - val_loss: 0.3395 - val_mse: 0.3395 - val_mae: 0.4582\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3356 - mse: 0.3356 - mae: 0.4491 - val_loss: 0.3494 - val_mse: 0.3494 - val_mae: 0.4676\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3194 - mse: 0.3194 - mae: 0.4347 - val_loss: 0.3467 - val_mse: 0.3467 - val_mae: 0.4633\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3150 - mse: 0.3150 - mae: 0.4400 - val_loss: 0.3423 - val_mse: 0.3423 - val_mae: 0.4573\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3174 - mse: 0.3174 - mae: 0.4355 - val_loss: 0.3601 - val_mse: 0.3601 - val_mae: 0.4696\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3108 - mse: 0.3108 - mae: 0.4267 - val_loss: 0.3460 - val_mse: 0.3460 - val_mae: 0.4591\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3144 - mse: 0.3144 - mae: 0.4349 - val_loss: 0.3582 - val_mse: 0.3582 - val_mae: 0.4696\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3507 - mse: 0.3507 - mae: 0.4609 - val_loss: 0.3403 - val_mse: 0.3403 - val_mae: 0.4602\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3175 - mse: 0.3175 - mae: 0.4331 - val_loss: 0.3784 - val_mse: 0.3784 - val_mae: 0.4833\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3104 - mse: 0.3104 - mae: 0.4265 - val_loss: 0.3393 - val_mse: 0.3393 - val_mae: 0.4574\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3319 - mse: 0.3319 - mae: 0.4383 - val_loss: 0.3460 - val_mse: 0.3460 - val_mae: 0.4627\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3337 - mse: 0.3337 - mae: 0.4486 - val_loss: 0.3488 - val_mse: 0.3488 - val_mae: 0.4637\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3252 - mse: 0.3252 - mae: 0.4401 - val_loss: 0.3667 - val_mse: 0.3667 - val_mae: 0.4743\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3124 - mse: 0.3124 - mae: 0.4319 - val_loss: 0.3344 - val_mse: 0.3344 - val_mae: 0.4554\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3176 - mse: 0.3176 - mae: 0.4336 - val_loss: 0.3434 - val_mse: 0.3434 - val_mae: 0.4569\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3154 - mse: 0.3154 - mae: 0.4320 - val_loss: 0.3378 - val_mse: 0.3378 - val_mae: 0.4542\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3202 - mse: 0.3202 - mae: 0.4339 - val_loss: 0.3444 - val_mse: 0.3444 - val_mae: 0.4603\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3164 - mse: 0.3164 - mae: 0.4288 - val_loss: 0.3407 - val_mse: 0.3407 - val_mae: 0.4580\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3286 - mse: 0.3286 - mae: 0.4426 - val_loss: 0.3580 - val_mse: 0.3580 - val_mae: 0.4718\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3195 - mse: 0.3195 - mae: 0.4362 - val_loss: 0.3425 - val_mse: 0.3425 - val_mae: 0.4594\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3087 - mse: 0.3087 - mae: 0.4300 - val_loss: 0.3597 - val_mse: 0.3597 - val_mae: 0.4707\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3344 - mse: 0.3344 - mae: 0.4464 - val_loss: 0.3400 - val_mse: 0.3400 - val_mae: 0.4574\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2996 - mse: 0.2996 - mae: 0.4298 - val_loss: 0.3448 - val_mse: 0.3448 - val_mae: 0.4628\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3133 - mse: 0.3133 - mae: 0.4312 - val_loss: 0.3669 - val_mse: 0.3669 - val_mae: 0.4754\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3046 - mse: 0.3046 - mae: 0.4234 - val_loss: 0.3406 - val_mse: 0.3406 - val_mae: 0.4558\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3202 - mse: 0.3202 - mae: 0.4450 - val_loss: 0.3351 - val_mse: 0.3351 - val_mae: 0.4571\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3053 - mse: 0.3053 - mae: 0.4270 - val_loss: 0.3634 - val_mse: 0.3634 - val_mae: 0.4767\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3261 - mse: 0.3261 - mae: 0.4396 - val_loss: 0.3537 - val_mse: 0.3537 - val_mae: 0.4708\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3140 - mse: 0.3140 - mae: 0.4309 - val_loss: 0.3385 - val_mse: 0.3385 - val_mae: 0.4580\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3087 - mse: 0.3087 - mae: 0.4287 - val_loss: 0.3374 - val_mse: 0.3374 - val_mae: 0.4561\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3088 - mse: 0.3088 - mae: 0.4285 - val_loss: 0.3600 - val_mse: 0.3600 - val_mae: 0.4726\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3144 - mse: 0.3144 - mae: 0.4335 - val_loss: 0.3969 - val_mse: 0.3969 - val_mae: 0.5006\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3379 - mse: 0.3379 - mae: 0.4519 - val_loss: 0.3407 - val_mse: 0.3407 - val_mae: 0.4601\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2918 - mse: 0.2918 - mae: 0.4171 - val_loss: 0.3464 - val_mse: 0.3464 - val_mae: 0.4633\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3078 - mse: 0.3078 - mae: 0.4321 - val_loss: 0.3632 - val_mse: 0.3632 - val_mae: 0.4740\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3350 - mse: 0.3350 - mae: 0.4472 - val_loss: 0.3331 - val_mse: 0.3331 - val_mae: 0.4557\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3100 - mse: 0.3100 - mae: 0.4277 - val_loss: 0.3448 - val_mse: 0.3448 - val_mae: 0.4605\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3054 - mse: 0.3054 - mae: 0.4269 - val_loss: 0.3642 - val_mse: 0.3642 - val_mae: 0.4762\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3191 - mse: 0.3191 - mae: 0.4352 - val_loss: 0.3541 - val_mse: 0.3541 - val_mae: 0.4697\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3024 - mse: 0.3024 - mae: 0.4256 - val_loss: 0.3402 - val_mse: 0.3402 - val_mae: 0.4581\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3084 - mse: 0.3084 - mae: 0.4254 - val_loss: 0.3401 - val_mse: 0.3401 - val_mae: 0.4581\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3134 - mse: 0.3134 - mae: 0.4260 - val_loss: 0.3530 - val_mse: 0.3530 - val_mae: 0.4631\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3008 - mse: 0.3008 - mae: 0.4288 - val_loss: 0.3522 - val_mse: 0.3522 - val_mae: 0.4638\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2977 - mse: 0.2977 - mae: 0.4252 - val_loss: 0.3433 - val_mse: 0.3433 - val_mae: 0.4570\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3231 - mse: 0.3231 - mae: 0.4407 - val_loss: 0.3475 - val_mse: 0.3475 - val_mae: 0.4634\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3230 - mse: 0.3230 - mae: 0.4404 - val_loss: 0.3466 - val_mse: 0.3466 - val_mae: 0.4629\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3107 - mse: 0.3107 - mae: 0.4360 - val_loss: 0.3432 - val_mse: 0.3432 - val_mae: 0.4565\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3219 - mse: 0.3219 - mae: 0.4329 - val_loss: 0.3483 - val_mse: 0.3483 - val_mae: 0.4608\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3013 - mse: 0.3013 - mae: 0.4205 - val_loss: 0.3521 - val_mse: 0.3521 - val_mae: 0.4659\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3149 - mse: 0.3149 - mae: 0.4303 - val_loss: 0.3479 - val_mse: 0.3479 - val_mae: 0.4598\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3012 - mse: 0.3012 - mae: 0.4233 - val_loss: 0.3432 - val_mse: 0.3432 - val_mae: 0.4572\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3147 - mse: 0.3147 - mae: 0.4329 - val_loss: 0.3477 - val_mse: 0.3477 - val_mae: 0.4650\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3029 - mse: 0.3029 - mae: 0.4314 - val_loss: 0.3445 - val_mse: 0.3445 - val_mae: 0.4608\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3060 - mse: 0.3060 - mae: 0.4280 - val_loss: 0.3802 - val_mse: 0.3802 - val_mae: 0.4838\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2855 - mse: 0.2855 - mae: 0.4149 - val_loss: 0.3536 - val_mse: 0.3536 - val_mae: 0.4688\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3112 - mse: 0.3112 - mae: 0.4267 - val_loss: 0.3651 - val_mse: 0.3651 - val_mae: 0.4718\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2981 - mse: 0.2981 - mae: 0.4255 - val_loss: 0.3377 - val_mse: 0.3377 - val_mae: 0.4539\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3067 - mse: 0.3067 - mae: 0.4284 - val_loss: 0.3699 - val_mse: 0.3699 - val_mae: 0.4770\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2983 - mse: 0.2983 - mae: 0.4290 - val_loss: 0.3433 - val_mse: 0.3433 - val_mae: 0.4595\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3139 - mse: 0.3139 - mae: 0.4320 - val_loss: 0.3529 - val_mse: 0.3529 - val_mae: 0.4695\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3206 - mse: 0.3206 - mae: 0.4428 - val_loss: 0.3408 - val_mse: 0.3408 - val_mae: 0.4602\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3094 - mse: 0.3094 - mae: 0.4333 - val_loss: 0.3406 - val_mse: 0.3406 - val_mae: 0.4605\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3045 - mse: 0.3045 - mae: 0.4276 - val_loss: 0.3571 - val_mse: 0.3571 - val_mae: 0.4724\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2895 - mse: 0.2895 - mae: 0.4160 - val_loss: 0.3654 - val_mse: 0.3654 - val_mae: 0.4757\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2871 - mse: 0.2871 - mae: 0.4155 - val_loss: 0.3422 - val_mse: 0.3422 - val_mae: 0.4583\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3186 - mse: 0.3186 - mae: 0.4338 - val_loss: 0.3571 - val_mse: 0.3571 - val_mae: 0.4723\n",
            "Epoch 252/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2834 - mse: 0.2834 - mae: 0.4120 - val_loss: 0.3506 - val_mse: 0.3506 - val_mae: 0.4648\n",
            "Epoch 253/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2870 - mse: 0.2870 - mae: 0.4159 - val_loss: 0.3393 - val_mse: 0.3393 - val_mae: 0.4551\n",
            "Epoch 254/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3119 - mse: 0.3119 - mae: 0.4305 - val_loss: 0.3519 - val_mse: 0.3519 - val_mae: 0.4612\n",
            "Epoch 255/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3059 - mse: 0.3059 - mae: 0.4257 - val_loss: 0.3597 - val_mse: 0.3597 - val_mae: 0.4712\n",
            "Epoch 256/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2932 - mse: 0.2932 - mae: 0.4165 - val_loss: 0.3573 - val_mse: 0.3573 - val_mae: 0.4717\n",
            "Epoch 257/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2966 - mse: 0.2966 - mae: 0.4235 - val_loss: 0.3427 - val_mse: 0.3427 - val_mae: 0.4540\n",
            "Epoch 258/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2906 - mse: 0.2906 - mae: 0.4160 - val_loss: 0.3410 - val_mse: 0.3410 - val_mae: 0.4530\n",
            "Epoch 259/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2944 - mse: 0.2944 - mae: 0.4212 - val_loss: 0.3598 - val_mse: 0.3598 - val_mae: 0.4713\n",
            "Epoch 260/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2921 - mse: 0.2921 - mae: 0.4175 - val_loss: 0.3703 - val_mse: 0.3703 - val_mae: 0.4808\n",
            "Epoch 261/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3169 - mse: 0.3169 - mae: 0.4358 - val_loss: 0.3462 - val_mse: 0.3462 - val_mae: 0.4568\n",
            "Epoch 262/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2984 - mse: 0.2984 - mae: 0.4233 - val_loss: 0.3444 - val_mse: 0.3444 - val_mae: 0.4567\n",
            "Epoch 263/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2967 - mse: 0.2967 - mae: 0.4231 - val_loss: 0.3414 - val_mse: 0.3414 - val_mae: 0.4605\n",
            "Epoch 264/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3104 - mse: 0.3104 - mae: 0.4310 - val_loss: 0.3388 - val_mse: 0.3388 - val_mae: 0.4586\n",
            "Epoch 265/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2880 - mse: 0.2880 - mae: 0.4077 - val_loss: 0.3499 - val_mse: 0.3499 - val_mae: 0.4654\n",
            "Epoch 266/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2912 - mse: 0.2912 - mae: 0.4231 - val_loss: 0.3629 - val_mse: 0.3629 - val_mae: 0.4762\n",
            "Epoch 267/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2871 - mse: 0.2871 - mae: 0.4136 - val_loss: 0.3494 - val_mse: 0.3494 - val_mae: 0.4665\n",
            "Epoch 268/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2909 - mse: 0.2909 - mae: 0.4108 - val_loss: 0.3467 - val_mse: 0.3467 - val_mae: 0.4662\n",
            "Epoch 269/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2942 - mse: 0.2942 - mae: 0.4180 - val_loss: 0.3434 - val_mse: 0.3434 - val_mae: 0.4618\n",
            "Epoch 270/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2984 - mse: 0.2984 - mae: 0.4232 - val_loss: 0.3863 - val_mse: 0.3863 - val_mae: 0.4950\n",
            "Epoch 271/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2853 - mse: 0.2853 - mae: 0.4166 - val_loss: 0.3565 - val_mse: 0.3565 - val_mae: 0.4686\n",
            "Epoch 272/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3064 - mse: 0.3064 - mae: 0.4295 - val_loss: 0.3440 - val_mse: 0.3440 - val_mae: 0.4535\n",
            "Epoch 273/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3152 - mse: 0.3152 - mae: 0.4310 - val_loss: 0.3451 - val_mse: 0.3451 - val_mae: 0.4555\n",
            "Epoch 274/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2821 - mse: 0.2821 - mae: 0.4108 - val_loss: 0.3648 - val_mse: 0.3648 - val_mae: 0.4766\n",
            "Epoch 275/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2847 - mse: 0.2847 - mae: 0.4174 - val_loss: 0.3587 - val_mse: 0.3587 - val_mae: 0.4746\n",
            "Epoch 276/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2917 - mse: 0.2917 - mae: 0.4218 - val_loss: 0.3526 - val_mse: 0.3526 - val_mae: 0.4607\n",
            "Epoch 277/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2887 - mse: 0.2887 - mae: 0.4219 - val_loss: 0.3394 - val_mse: 0.3394 - val_mae: 0.4509\n",
            "Epoch 278/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2856 - mse: 0.2856 - mae: 0.4097 - val_loss: 0.3342 - val_mse: 0.3342 - val_mae: 0.4499\n",
            "Epoch 279/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2831 - mse: 0.2831 - mae: 0.4162 - val_loss: 0.3643 - val_mse: 0.3643 - val_mae: 0.4762\n",
            "Epoch 280/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2755 - mse: 0.2755 - mae: 0.4065 - val_loss: 0.3559 - val_mse: 0.3559 - val_mae: 0.4667\n",
            "Epoch 281/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2849 - mse: 0.2849 - mae: 0.4110 - val_loss: 0.3648 - val_mse: 0.3648 - val_mae: 0.4708\n",
            "Epoch 282/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2828 - mse: 0.2828 - mae: 0.4084 - val_loss: 0.3475 - val_mse: 0.3475 - val_mae: 0.4605\n",
            "Epoch 283/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2934 - mse: 0.2934 - mae: 0.4200 - val_loss: 0.3474 - val_mse: 0.3474 - val_mae: 0.4612\n",
            "Epoch 284/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2634 - mse: 0.2634 - mae: 0.4048 - val_loss: 0.3471 - val_mse: 0.3471 - val_mae: 0.4611\n",
            "Epoch 285/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3088 - mse: 0.3088 - mae: 0.4385 - val_loss: 0.3706 - val_mse: 0.3706 - val_mae: 0.4836\n",
            "Epoch 286/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2910 - mse: 0.2910 - mae: 0.4152 - val_loss: 0.3658 - val_mse: 0.3658 - val_mae: 0.4726\n",
            "Epoch 287/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2834 - mse: 0.2834 - mae: 0.4015 - val_loss: 0.3440 - val_mse: 0.3440 - val_mae: 0.4623\n",
            "Epoch 288/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2843 - mse: 0.2843 - mae: 0.4097 - val_loss: 0.3459 - val_mse: 0.3459 - val_mae: 0.4547\n",
            "Epoch 289/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2768 - mse: 0.2768 - mae: 0.4124 - val_loss: 0.3513 - val_mse: 0.3513 - val_mae: 0.4645\n",
            "Epoch 290/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2859 - mse: 0.2859 - mae: 0.4127 - val_loss: 0.3415 - val_mse: 0.3415 - val_mae: 0.4573\n",
            "Epoch 291/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2803 - mse: 0.2803 - mae: 0.4116 - val_loss: 0.3497 - val_mse: 0.3497 - val_mae: 0.4662\n",
            "Epoch 292/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2810 - mse: 0.2810 - mae: 0.4068 - val_loss: 0.3521 - val_mse: 0.3521 - val_mae: 0.4696\n",
            "Epoch 293/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2953 - mse: 0.2953 - mae: 0.4274 - val_loss: 0.3674 - val_mse: 0.3674 - val_mae: 0.4820\n",
            "Epoch 294/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2746 - mse: 0.2746 - mae: 0.4079 - val_loss: 0.3617 - val_mse: 0.3617 - val_mae: 0.4737\n",
            "Epoch 295/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2895 - mse: 0.2895 - mae: 0.4188 - val_loss: 0.3452 - val_mse: 0.3452 - val_mae: 0.4599\n",
            "Epoch 296/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2913 - mse: 0.2913 - mae: 0.4189 - val_loss: 0.3467 - val_mse: 0.3467 - val_mae: 0.4614\n",
            "Epoch 297/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2908 - mse: 0.2908 - mae: 0.4144 - val_loss: 0.3682 - val_mse: 0.3682 - val_mae: 0.4769\n",
            "Epoch 298/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2828 - mse: 0.2828 - mae: 0.4171 - val_loss: 0.3564 - val_mse: 0.3564 - val_mae: 0.4741\n",
            "Epoch 299/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2884 - mse: 0.2884 - mae: 0.4143 - val_loss: 0.3627 - val_mse: 0.3627 - val_mae: 0.4795\n",
            "Epoch 300/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2946 - mse: 0.2946 - mae: 0.4206 - val_loss: 0.3497 - val_mse: 0.3497 - val_mae: 0.4667\n",
            "Epoch 301/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2672 - mse: 0.2672 - mae: 0.3940 - val_loss: 0.3572 - val_mse: 0.3572 - val_mae: 0.4698\n",
            "Epoch 302/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2606 - mse: 0.2606 - mae: 0.3979 - val_loss: 0.3518 - val_mse: 0.3518 - val_mae: 0.4614\n",
            "Epoch 303/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2747 - mse: 0.2747 - mae: 0.4069 - val_loss: 0.3410 - val_mse: 0.3410 - val_mae: 0.4641\n",
            "Epoch 304/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2849 - mse: 0.2849 - mae: 0.4117 - val_loss: 0.3618 - val_mse: 0.3618 - val_mae: 0.4688\n",
            "Epoch 305/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2925 - mse: 0.2925 - mae: 0.4163 - val_loss: 0.3607 - val_mse: 0.3607 - val_mae: 0.4761\n",
            "Epoch 306/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2808 - mse: 0.2808 - mae: 0.4096 - val_loss: 0.3551 - val_mse: 0.3551 - val_mae: 0.4755\n",
            "Epoch 307/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2707 - mse: 0.2707 - mae: 0.4071 - val_loss: 0.3664 - val_mse: 0.3664 - val_mae: 0.4718\n",
            "Epoch 308/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2877 - mse: 0.2877 - mae: 0.4123 - val_loss: 0.3532 - val_mse: 0.3532 - val_mae: 0.4649\n",
            "Epoch 309/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2948 - mse: 0.2948 - mae: 0.4233 - val_loss: 0.3739 - val_mse: 0.3739 - val_mae: 0.4785\n",
            "Epoch 310/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3008 - mse: 0.3008 - mae: 0.4280 - val_loss: 0.3584 - val_mse: 0.3584 - val_mae: 0.4731\n",
            "Epoch 311/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2968 - mse: 0.2968 - mae: 0.4229 - val_loss: 0.3576 - val_mse: 0.3576 - val_mae: 0.4694\n",
            "Epoch 312/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2998 - mse: 0.2998 - mae: 0.4250 - val_loss: 0.3515 - val_mse: 0.3515 - val_mae: 0.4625\n",
            "Epoch 313/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2854 - mse: 0.2854 - mae: 0.4102 - val_loss: 0.3523 - val_mse: 0.3523 - val_mae: 0.4600\n",
            "Epoch 314/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2729 - mse: 0.2729 - mae: 0.4020 - val_loss: 0.3493 - val_mse: 0.3493 - val_mae: 0.4618\n",
            "Epoch 315/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2608 - mse: 0.2608 - mae: 0.3914 - val_loss: 0.3480 - val_mse: 0.3480 - val_mae: 0.4599\n",
            "Epoch 316/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2713 - mse: 0.2713 - mae: 0.3951 - val_loss: 0.3768 - val_mse: 0.3768 - val_mae: 0.4806\n",
            "Epoch 317/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2597 - mse: 0.2597 - mae: 0.3931 - val_loss: 0.3612 - val_mse: 0.3612 - val_mae: 0.4739\n",
            "Epoch 318/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2655 - mse: 0.2655 - mae: 0.3957 - val_loss: 0.3722 - val_mse: 0.3722 - val_mae: 0.4826\n",
            "Epoch 319/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2686 - mse: 0.2686 - mae: 0.4032 - val_loss: 0.3472 - val_mse: 0.3472 - val_mae: 0.4686\n",
            "Epoch 320/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2889 - mse: 0.2889 - mae: 0.4213 - val_loss: 0.3591 - val_mse: 0.3591 - val_mae: 0.4694\n",
            "Epoch 321/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2755 - mse: 0.2755 - mae: 0.4078 - val_loss: 0.3508 - val_mse: 0.3508 - val_mae: 0.4650\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.4591 - mse: 0.4591 - mae: 0.5147\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 14ms/step - loss: 30.5131 - mse: 30.5131 - mae: 5.3971 - val_loss: 23.8481 - val_mse: 23.8481 - val_mae: 4.7501\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 16.4180 - mse: 16.4180 - mae: 3.8561 - val_loss: 12.8889 - val_mse: 12.8889 - val_mae: 3.3915\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 8.3164 - mse: 8.3164 - mae: 2.5950 - val_loss: 7.0074 - val_mse: 7.0074 - val_mae: 2.3926\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 4.5081 - mse: 4.5081 - mae: 1.8042 - val_loss: 4.5163 - val_mse: 4.5163 - val_mae: 1.7989\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 3.4001 - mse: 3.4001 - mae: 1.5292 - val_loss: 3.6453 - val_mse: 3.6453 - val_mae: 1.5605\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 2.8825 - mse: 2.8825 - mae: 1.3694 - val_loss: 3.2378 - val_mse: 3.2378 - val_mae: 1.4505\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.5347 - mse: 2.5347 - mae: 1.2623 - val_loss: 2.9977 - val_mse: 2.9977 - val_mae: 1.3860\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.5240 - mse: 2.5240 - mae: 1.2688 - val_loss: 2.7791 - val_mse: 2.7791 - val_mae: 1.3395\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.3615 - mse: 2.3615 - mae: 1.2320 - val_loss: 2.5801 - val_mse: 2.5801 - val_mae: 1.2885\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.1397 - mse: 2.1397 - mae: 1.1690 - val_loss: 2.4379 - val_mse: 2.4379 - val_mae: 1.2451\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.9658 - mse: 1.9658 - mae: 1.1314 - val_loss: 2.2993 - val_mse: 2.2993 - val_mae: 1.2047\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.9099 - mse: 1.9099 - mae: 1.1058 - val_loss: 2.2021 - val_mse: 2.2021 - val_mae: 1.1732\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.8819 - mse: 1.8819 - mae: 1.1118 - val_loss: 2.0995 - val_mse: 2.0995 - val_mae: 1.1434\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.7758 - mse: 1.7758 - mae: 1.0728 - val_loss: 2.0254 - val_mse: 2.0254 - val_mae: 1.1194\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.6919 - mse: 1.6919 - mae: 1.0528 - val_loss: 1.9712 - val_mse: 1.9712 - val_mae: 1.1038\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.6626 - mse: 1.6626 - mae: 1.0444 - val_loss: 1.9043 - val_mse: 1.9043 - val_mae: 1.0837\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.5905 - mse: 1.5905 - mae: 1.0117 - val_loss: 1.8253 - val_mse: 1.8253 - val_mae: 1.0581\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.3947 - mse: 1.3947 - mae: 0.9579 - val_loss: 1.7822 - val_mse: 1.7822 - val_mae: 1.0416\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.4533 - mse: 1.4533 - mae: 0.9734 - val_loss: 1.7351 - val_mse: 1.7351 - val_mae: 1.0288\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.4117 - mse: 1.4117 - mae: 0.9516 - val_loss: 1.6818 - val_mse: 1.6818 - val_mae: 1.0100\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.3203 - mse: 1.3203 - mae: 0.9312 - val_loss: 1.6426 - val_mse: 1.6426 - val_mae: 0.9972\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.3031 - mse: 1.3031 - mae: 0.9093 - val_loss: 1.6141 - val_mse: 1.6141 - val_mae: 0.9811\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2816 - mse: 1.2816 - mae: 0.9059 - val_loss: 1.5906 - val_mse: 1.5906 - val_mae: 0.9720\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.2056 - mse: 1.2056 - mae: 0.8671 - val_loss: 1.5346 - val_mse: 1.5346 - val_mae: 0.9574\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1627 - mse: 1.1627 - mae: 0.8603 - val_loss: 1.4985 - val_mse: 1.4985 - val_mae: 0.9429\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1672 - mse: 1.1672 - mae: 0.8715 - val_loss: 1.4708 - val_mse: 1.4708 - val_mae: 0.9343\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1217 - mse: 1.1217 - mae: 0.8473 - val_loss: 1.4652 - val_mse: 1.4652 - val_mae: 0.9257\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.1180 - mse: 1.1180 - mae: 0.8370 - val_loss: 1.4064 - val_mse: 1.4064 - val_mae: 0.9085\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.0595 - mse: 1.0595 - mae: 0.8258 - val_loss: 1.3398 - val_mse: 1.3398 - val_mae: 0.8946\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.0859 - mse: 1.0859 - mae: 0.8393 - val_loss: 1.3178 - val_mse: 1.3178 - val_mae: 0.8789\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.0009 - mse: 1.0009 - mae: 0.7939 - val_loss: 1.2830 - val_mse: 1.2830 - val_mae: 0.8665\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.0291 - mse: 1.0291 - mae: 0.8098 - val_loss: 1.2763 - val_mse: 1.2763 - val_mae: 0.8601\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.9747 - mse: 0.9747 - mae: 0.7783 - val_loss: 1.2310 - val_mse: 1.2310 - val_mae: 0.8464\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9716 - mse: 0.9716 - mae: 0.7904 - val_loss: 1.1835 - val_mse: 1.1835 - val_mae: 0.8281\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.8952 - mse: 0.8952 - mae: 0.7598 - val_loss: 1.1507 - val_mse: 1.1507 - val_mae: 0.8215\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9001 - mse: 0.9001 - mae: 0.7610 - val_loss: 1.1372 - val_mse: 1.1372 - val_mae: 0.8089\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.8798 - mse: 0.8798 - mae: 0.7442 - val_loss: 1.1193 - val_mse: 1.1193 - val_mae: 0.8033\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9213 - mse: 0.9213 - mae: 0.7625 - val_loss: 1.0983 - val_mse: 1.0983 - val_mae: 0.7943\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9026 - mse: 0.9026 - mae: 0.7433 - val_loss: 1.0367 - val_mse: 1.0367 - val_mae: 0.7783\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.8136 - mse: 0.8136 - mae: 0.7035 - val_loss: 1.0557 - val_mse: 1.0557 - val_mae: 0.7691\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.8320 - mse: 0.8320 - mae: 0.7228 - val_loss: 1.0255 - val_mse: 1.0255 - val_mae: 0.7616\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.8255 - mse: 0.8255 - mae: 0.7228 - val_loss: 1.0083 - val_mse: 1.0083 - val_mae: 0.7590\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.8139 - mse: 0.8139 - mae: 0.7115 - val_loss: 0.9874 - val_mse: 0.9874 - val_mae: 0.7498\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.7874 - mse: 0.7874 - mae: 0.6928 - val_loss: 0.9526 - val_mse: 0.9526 - val_mae: 0.7386\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7636 - mse: 0.7636 - mae: 0.6946 - val_loss: 0.9536 - val_mse: 0.9536 - val_mae: 0.7331\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7811 - mse: 0.7811 - mae: 0.7006 - val_loss: 0.9443 - val_mse: 0.9443 - val_mae: 0.7278\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7095 - mse: 0.7095 - mae: 0.6668 - val_loss: 0.9154 - val_mse: 0.9154 - val_mae: 0.7193\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7464 - mse: 0.7464 - mae: 0.6893 - val_loss: 0.9104 - val_mse: 0.9104 - val_mae: 0.7160\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7118 - mse: 0.7118 - mae: 0.6517 - val_loss: 0.8944 - val_mse: 0.8944 - val_mae: 0.7088\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7067 - mse: 0.7067 - mae: 0.6625 - val_loss: 0.8651 - val_mse: 0.8651 - val_mae: 0.7037\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7037 - mse: 0.7037 - mae: 0.6663 - val_loss: 0.8494 - val_mse: 0.8494 - val_mae: 0.6946\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6772 - mse: 0.6772 - mae: 0.6473 - val_loss: 0.8080 - val_mse: 0.8080 - val_mae: 0.6790\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.6968 - mse: 0.6968 - mae: 0.6637 - val_loss: 0.8334 - val_mse: 0.8334 - val_mae: 0.6860\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6488 - mse: 0.6488 - mae: 0.6277 - val_loss: 0.8087 - val_mse: 0.8087 - val_mae: 0.6766\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6668 - mse: 0.6668 - mae: 0.6447 - val_loss: 0.8071 - val_mse: 0.8071 - val_mae: 0.6650\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6516 - mse: 0.6516 - mae: 0.6301 - val_loss: 0.7847 - val_mse: 0.7847 - val_mae: 0.6685\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6573 - mse: 0.6573 - mae: 0.6331 - val_loss: 0.7795 - val_mse: 0.7795 - val_mae: 0.6626\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6262 - mse: 0.6262 - mae: 0.6192 - val_loss: 0.7602 - val_mse: 0.7602 - val_mae: 0.6581\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6425 - mse: 0.6425 - mae: 0.6331 - val_loss: 0.7420 - val_mse: 0.7420 - val_mae: 0.6531\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6334 - mse: 0.6334 - mae: 0.6243 - val_loss: 0.7513 - val_mse: 0.7513 - val_mae: 0.6505\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5842 - mse: 0.5842 - mae: 0.6004 - val_loss: 0.7321 - val_mse: 0.7321 - val_mae: 0.6402\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6047 - mse: 0.6047 - mae: 0.6134 - val_loss: 0.7343 - val_mse: 0.7343 - val_mae: 0.6402\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6087 - mse: 0.6087 - mae: 0.6061 - val_loss: 0.7315 - val_mse: 0.7315 - val_mae: 0.6372\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6236 - mse: 0.6236 - mae: 0.6041 - val_loss: 0.6904 - val_mse: 0.6904 - val_mae: 0.6240\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5611 - mse: 0.5611 - mae: 0.5842 - val_loss: 0.7030 - val_mse: 0.7030 - val_mae: 0.6234\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6017 - mse: 0.6017 - mae: 0.6117 - val_loss: 0.6648 - val_mse: 0.6648 - val_mae: 0.6154\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5706 - mse: 0.5706 - mae: 0.5914 - val_loss: 0.6671 - val_mse: 0.6671 - val_mae: 0.6131\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5677 - mse: 0.5677 - mae: 0.5859 - val_loss: 0.6703 - val_mse: 0.6703 - val_mae: 0.6091\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5692 - mse: 0.5692 - mae: 0.5799 - val_loss: 0.6619 - val_mse: 0.6619 - val_mae: 0.6103\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5813 - mse: 0.5813 - mae: 0.5982 - val_loss: 0.6500 - val_mse: 0.6500 - val_mae: 0.5993\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5215 - mse: 0.5215 - mae: 0.5712 - val_loss: 0.6476 - val_mse: 0.6476 - val_mae: 0.5999\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5423 - mse: 0.5423 - mae: 0.5790 - val_loss: 0.6313 - val_mse: 0.6313 - val_mae: 0.5945\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5390 - mse: 0.5390 - mae: 0.5750 - val_loss: 0.6238 - val_mse: 0.6238 - val_mae: 0.5932\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.5307 - mse: 0.5307 - mae: 0.5674 - val_loss: 0.6238 - val_mse: 0.6238 - val_mae: 0.5944\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5180 - mse: 0.5180 - mae: 0.5691 - val_loss: 0.6263 - val_mse: 0.6263 - val_mae: 0.5872\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5243 - mse: 0.5243 - mae: 0.5661 - val_loss: 0.5973 - val_mse: 0.5973 - val_mae: 0.5781\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5152 - mse: 0.5152 - mae: 0.5663 - val_loss: 0.6117 - val_mse: 0.6117 - val_mae: 0.5809\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5274 - mse: 0.5274 - mae: 0.5718 - val_loss: 0.5936 - val_mse: 0.5936 - val_mae: 0.5803\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5205 - mse: 0.5205 - mae: 0.5635 - val_loss: 0.5914 - val_mse: 0.5914 - val_mae: 0.5744\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5286 - mse: 0.5286 - mae: 0.5684 - val_loss: 0.5932 - val_mse: 0.5932 - val_mae: 0.5700\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4925 - mse: 0.4925 - mae: 0.5510 - val_loss: 0.5729 - val_mse: 0.5729 - val_mae: 0.5735\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5078 - mse: 0.5078 - mae: 0.5574 - val_loss: 0.5769 - val_mse: 0.5769 - val_mae: 0.5670\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4841 - mse: 0.4841 - mae: 0.5403 - val_loss: 0.5757 - val_mse: 0.5757 - val_mae: 0.5625\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4750 - mse: 0.4750 - mae: 0.5454 - val_loss: 0.5562 - val_mse: 0.5562 - val_mae: 0.5633\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4897 - mse: 0.4897 - mae: 0.5423 - val_loss: 0.5527 - val_mse: 0.5527 - val_mae: 0.5582\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4764 - mse: 0.4764 - mae: 0.5362 - val_loss: 0.5585 - val_mse: 0.5585 - val_mae: 0.5601\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4858 - mse: 0.4858 - mae: 0.5395 - val_loss: 0.5541 - val_mse: 0.5541 - val_mae: 0.5571\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4522 - mse: 0.4522 - mae: 0.5318 - val_loss: 0.5469 - val_mse: 0.5469 - val_mae: 0.5555\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4704 - mse: 0.4704 - mae: 0.5358 - val_loss: 0.5510 - val_mse: 0.5510 - val_mae: 0.5545\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4715 - mse: 0.4715 - mae: 0.5362 - val_loss: 0.5227 - val_mse: 0.5227 - val_mae: 0.5467\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4658 - mse: 0.4658 - mae: 0.5354 - val_loss: 0.5424 - val_mse: 0.5424 - val_mae: 0.5468\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4450 - mse: 0.4450 - mae: 0.5265 - val_loss: 0.5205 - val_mse: 0.5205 - val_mae: 0.5410\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4814 - mse: 0.4814 - mae: 0.5445 - val_loss: 0.5195 - val_mse: 0.5195 - val_mae: 0.5415\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.4700 - mse: 0.4700 - mae: 0.5339 - val_loss: 0.5234 - val_mse: 0.5234 - val_mae: 0.5385\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4663 - mse: 0.4663 - mae: 0.5394 - val_loss: 0.5082 - val_mse: 0.5082 - val_mae: 0.5392\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4485 - mse: 0.4485 - mae: 0.5207 - val_loss: 0.5268 - val_mse: 0.5268 - val_mae: 0.5371\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4435 - mse: 0.4435 - mae: 0.5183 - val_loss: 0.5015 - val_mse: 0.5015 - val_mae: 0.5330\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4369 - mse: 0.4369 - mae: 0.5185 - val_loss: 0.4997 - val_mse: 0.4997 - val_mae: 0.5365\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4437 - mse: 0.4437 - mae: 0.5170 - val_loss: 0.5100 - val_mse: 0.5100 - val_mae: 0.5373\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4508 - mse: 0.4508 - mae: 0.5226 - val_loss: 0.4902 - val_mse: 0.4902 - val_mae: 0.5296\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4330 - mse: 0.4330 - mae: 0.5114 - val_loss: 0.4921 - val_mse: 0.4921 - val_mae: 0.5319\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4149 - mse: 0.4149 - mae: 0.5033 - val_loss: 0.5075 - val_mse: 0.5075 - val_mae: 0.5327\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4279 - mse: 0.4279 - mae: 0.5142 - val_loss: 0.4847 - val_mse: 0.4847 - val_mae: 0.5242\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4224 - mse: 0.4224 - mae: 0.5067 - val_loss: 0.5084 - val_mse: 0.5084 - val_mae: 0.5286\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4529 - mse: 0.4529 - mae: 0.5232 - val_loss: 0.5145 - val_mse: 0.5145 - val_mae: 0.5328\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4338 - mse: 0.4338 - mae: 0.5147 - val_loss: 0.4707 - val_mse: 0.4707 - val_mae: 0.5218\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.4324 - mse: 0.4324 - mae: 0.5104 - val_loss: 0.4745 - val_mse: 0.4745 - val_mae: 0.5189\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4083 - mse: 0.4083 - mae: 0.5051 - val_loss: 0.4715 - val_mse: 0.4715 - val_mae: 0.5180\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4493 - mse: 0.4493 - mae: 0.5211 - val_loss: 0.4769 - val_mse: 0.4769 - val_mae: 0.5145\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4147 - mse: 0.4147 - mae: 0.5074 - val_loss: 0.4660 - val_mse: 0.4660 - val_mae: 0.5146\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4140 - mse: 0.4140 - mae: 0.5078 - val_loss: 0.4513 - val_mse: 0.4513 - val_mae: 0.5152\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4135 - mse: 0.4135 - mae: 0.5037 - val_loss: 0.4553 - val_mse: 0.4553 - val_mae: 0.5082\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4019 - mse: 0.4019 - mae: 0.4893 - val_loss: 0.4602 - val_mse: 0.4602 - val_mae: 0.5109\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.4172 - mse: 0.4172 - mae: 0.5036 - val_loss: 0.4548 - val_mse: 0.4548 - val_mae: 0.5084\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4105 - mse: 0.4105 - mae: 0.5005 - val_loss: 0.4540 - val_mse: 0.4540 - val_mae: 0.5109\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4199 - mse: 0.4199 - mae: 0.5026 - val_loss: 0.4575 - val_mse: 0.4575 - val_mae: 0.5045\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3936 - mse: 0.3936 - mae: 0.4885 - val_loss: 0.4565 - val_mse: 0.4565 - val_mae: 0.5073\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3978 - mse: 0.3978 - mae: 0.4866 - val_loss: 0.4537 - val_mse: 0.4537 - val_mae: 0.5037\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4146 - mse: 0.4146 - mae: 0.4896 - val_loss: 0.4447 - val_mse: 0.4447 - val_mae: 0.5020\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3906 - mse: 0.3906 - mae: 0.4853 - val_loss: 0.4505 - val_mse: 0.4505 - val_mae: 0.5000\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3926 - mse: 0.3926 - mae: 0.4802 - val_loss: 0.4825 - val_mse: 0.4825 - val_mae: 0.5191\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4208 - mse: 0.4208 - mae: 0.5058 - val_loss: 0.4382 - val_mse: 0.4382 - val_mae: 0.4992\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3815 - mse: 0.3815 - mae: 0.4730 - val_loss: 0.4442 - val_mse: 0.4442 - val_mae: 0.5028\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3786 - mse: 0.3786 - mae: 0.4768 - val_loss: 0.4302 - val_mse: 0.4302 - val_mae: 0.4953\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3766 - mse: 0.3766 - mae: 0.4662 - val_loss: 0.4441 - val_mse: 0.4441 - val_mae: 0.4987\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3849 - mse: 0.3849 - mae: 0.4817 - val_loss: 0.4258 - val_mse: 0.4258 - val_mae: 0.4988\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3779 - mse: 0.3779 - mae: 0.4779 - val_loss: 0.4343 - val_mse: 0.4343 - val_mae: 0.4989\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3843 - mse: 0.3843 - mae: 0.4804 - val_loss: 0.4342 - val_mse: 0.4342 - val_mae: 0.4998\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3960 - mse: 0.3960 - mae: 0.4848 - val_loss: 0.4368 - val_mse: 0.4368 - val_mae: 0.5033\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3921 - mse: 0.3921 - mae: 0.4837 - val_loss: 0.4605 - val_mse: 0.4605 - val_mae: 0.5074\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3654 - mse: 0.3654 - mae: 0.4740 - val_loss: 0.4277 - val_mse: 0.4277 - val_mae: 0.4920\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3719 - mse: 0.3719 - mae: 0.4783 - val_loss: 0.4445 - val_mse: 0.4445 - val_mae: 0.5028\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3927 - mse: 0.3927 - mae: 0.4895 - val_loss: 0.4257 - val_mse: 0.4257 - val_mae: 0.4919\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3656 - mse: 0.3656 - mae: 0.4670 - val_loss: 0.4171 - val_mse: 0.4171 - val_mae: 0.4939\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3715 - mse: 0.3715 - mae: 0.4758 - val_loss: 0.4827 - val_mse: 0.4827 - val_mae: 0.5214\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3707 - mse: 0.3707 - mae: 0.4697 - val_loss: 0.4164 - val_mse: 0.4164 - val_mae: 0.4868\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3677 - mse: 0.3677 - mae: 0.4763 - val_loss: 0.4126 - val_mse: 0.4126 - val_mae: 0.4889\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3774 - mse: 0.3774 - mae: 0.4773 - val_loss: 0.4072 - val_mse: 0.4072 - val_mae: 0.4821\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3775 - mse: 0.3775 - mae: 0.4733 - val_loss: 0.4213 - val_mse: 0.4213 - val_mae: 0.4906\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3909 - mse: 0.3909 - mae: 0.4880 - val_loss: 0.4426 - val_mse: 0.4426 - val_mae: 0.5030\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3847 - mse: 0.3847 - mae: 0.4851 - val_loss: 0.4364 - val_mse: 0.4364 - val_mae: 0.4999\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3801 - mse: 0.3801 - mae: 0.4841 - val_loss: 0.4172 - val_mse: 0.4172 - val_mae: 0.4876\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3793 - mse: 0.3793 - mae: 0.4725 - val_loss: 0.4158 - val_mse: 0.4158 - val_mae: 0.4879\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3799 - mse: 0.3799 - mae: 0.4804 - val_loss: 0.4105 - val_mse: 0.4105 - val_mae: 0.4853\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3708 - mse: 0.3708 - mae: 0.4683 - val_loss: 0.4026 - val_mse: 0.4026 - val_mae: 0.4882\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3783 - mse: 0.3783 - mae: 0.4865 - val_loss: 0.4007 - val_mse: 0.4007 - val_mae: 0.4845\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3708 - mse: 0.3708 - mae: 0.4717 - val_loss: 0.4035 - val_mse: 0.4035 - val_mae: 0.4828\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3736 - mse: 0.3736 - mae: 0.4761 - val_loss: 0.4112 - val_mse: 0.4112 - val_mae: 0.4849\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3668 - mse: 0.3668 - mae: 0.4729 - val_loss: 0.4226 - val_mse: 0.4226 - val_mae: 0.4921\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3534 - mse: 0.3534 - mae: 0.4621 - val_loss: 0.4304 - val_mse: 0.4304 - val_mae: 0.4970\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3438 - mse: 0.3438 - mae: 0.4549 - val_loss: 0.3957 - val_mse: 0.3957 - val_mae: 0.4799\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3540 - mse: 0.3540 - mae: 0.4607 - val_loss: 0.3994 - val_mse: 0.3994 - val_mae: 0.4804\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3418 - mse: 0.3418 - mae: 0.4563 - val_loss: 0.4007 - val_mse: 0.4007 - val_mae: 0.4819\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3536 - mse: 0.3536 - mae: 0.4574 - val_loss: 0.4129 - val_mse: 0.4129 - val_mae: 0.4891\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3486 - mse: 0.3486 - mae: 0.4592 - val_loss: 0.4199 - val_mse: 0.4199 - val_mae: 0.4919\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3463 - mse: 0.3463 - mae: 0.4607 - val_loss: 0.3987 - val_mse: 0.3987 - val_mae: 0.4808\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3619 - mse: 0.3619 - mae: 0.4678 - val_loss: 0.4159 - val_mse: 0.4159 - val_mae: 0.4905\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3400 - mse: 0.3400 - mae: 0.4489 - val_loss: 0.3968 - val_mse: 0.3968 - val_mae: 0.4832\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3506 - mse: 0.3506 - mae: 0.4622 - val_loss: 0.3960 - val_mse: 0.3960 - val_mae: 0.4779\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3671 - mse: 0.3671 - mae: 0.4739 - val_loss: 0.4118 - val_mse: 0.4118 - val_mae: 0.4877\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3559 - mse: 0.3559 - mae: 0.4606 - val_loss: 0.4245 - val_mse: 0.4245 - val_mae: 0.4962\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3506 - mse: 0.3506 - mae: 0.4586 - val_loss: 0.3980 - val_mse: 0.3980 - val_mae: 0.4820\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3633 - mse: 0.3633 - mae: 0.4631 - val_loss: 0.3999 - val_mse: 0.3999 - val_mae: 0.4832\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3534 - mse: 0.3534 - mae: 0.4611 - val_loss: 0.3916 - val_mse: 0.3916 - val_mae: 0.4740\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3497 - mse: 0.3497 - mae: 0.4583 - val_loss: 0.4353 - val_mse: 0.4353 - val_mae: 0.5014\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3962 - mse: 0.3962 - mae: 0.4880 - val_loss: 0.4411 - val_mse: 0.4411 - val_mae: 0.5011\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3627 - mse: 0.3627 - mae: 0.4710 - val_loss: 0.3838 - val_mse: 0.3838 - val_mae: 0.4753\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3488 - mse: 0.3488 - mae: 0.4588 - val_loss: 0.3862 - val_mse: 0.3862 - val_mae: 0.4731\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3413 - mse: 0.3413 - mae: 0.4526 - val_loss: 0.3866 - val_mse: 0.3866 - val_mae: 0.4710\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3457 - mse: 0.3457 - mae: 0.4573 - val_loss: 0.3893 - val_mse: 0.3893 - val_mae: 0.4779\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3267 - mse: 0.3267 - mae: 0.4369 - val_loss: 0.3991 - val_mse: 0.3991 - val_mae: 0.4832\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3288 - mse: 0.3288 - mae: 0.4455 - val_loss: 0.3893 - val_mse: 0.3893 - val_mae: 0.4733\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3596 - mse: 0.3596 - mae: 0.4621 - val_loss: 0.4056 - val_mse: 0.4056 - val_mae: 0.4866\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3469 - mse: 0.3469 - mae: 0.4559 - val_loss: 0.4251 - val_mse: 0.4251 - val_mae: 0.4950\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3490 - mse: 0.3490 - mae: 0.4589 - val_loss: 0.4051 - val_mse: 0.4051 - val_mae: 0.4848\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3389 - mse: 0.3389 - mae: 0.4515 - val_loss: 0.4180 - val_mse: 0.4180 - val_mae: 0.4909\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3510 - mse: 0.3510 - mae: 0.4620 - val_loss: 0.4084 - val_mse: 0.4084 - val_mae: 0.4905\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3449 - mse: 0.3449 - mae: 0.4581 - val_loss: 0.3850 - val_mse: 0.3850 - val_mae: 0.4730\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3391 - mse: 0.3391 - mae: 0.4523 - val_loss: 0.3816 - val_mse: 0.3816 - val_mae: 0.4707\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3336 - mse: 0.3336 - mae: 0.4461 - val_loss: 0.3895 - val_mse: 0.3895 - val_mae: 0.4745\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3327 - mse: 0.3327 - mae: 0.4496 - val_loss: 0.3728 - val_mse: 0.3728 - val_mae: 0.4670\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3458 - mse: 0.3458 - mae: 0.4549 - val_loss: 0.3866 - val_mse: 0.3866 - val_mae: 0.4735\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3383 - mse: 0.3383 - mae: 0.4518 - val_loss: 0.3857 - val_mse: 0.3857 - val_mae: 0.4730\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3422 - mse: 0.3422 - mae: 0.4539 - val_loss: 0.3959 - val_mse: 0.3959 - val_mae: 0.4841\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3332 - mse: 0.3332 - mae: 0.4465 - val_loss: 0.3791 - val_mse: 0.3791 - val_mae: 0.4667\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3418 - mse: 0.3418 - mae: 0.4562 - val_loss: 0.3725 - val_mse: 0.3725 - val_mae: 0.4629\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3558 - mse: 0.3558 - mae: 0.4648 - val_loss: 0.3718 - val_mse: 0.3718 - val_mae: 0.4656\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3524 - mse: 0.3524 - mae: 0.4614 - val_loss: 0.3926 - val_mse: 0.3926 - val_mae: 0.4780\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3465 - mse: 0.3465 - mae: 0.4549 - val_loss: 0.3790 - val_mse: 0.3790 - val_mae: 0.4646\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3253 - mse: 0.3253 - mae: 0.4426 - val_loss: 0.3777 - val_mse: 0.3777 - val_mae: 0.4691\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3148 - mse: 0.3148 - mae: 0.4425 - val_loss: 0.3774 - val_mse: 0.3774 - val_mae: 0.4690\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3365 - mse: 0.3365 - mae: 0.4439 - val_loss: 0.3773 - val_mse: 0.3773 - val_mae: 0.4645\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3231 - mse: 0.3231 - mae: 0.4400 - val_loss: 0.3706 - val_mse: 0.3706 - val_mae: 0.4610\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3478 - mse: 0.3478 - mae: 0.4527 - val_loss: 0.3808 - val_mse: 0.3808 - val_mae: 0.4679\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3319 - mse: 0.3319 - mae: 0.4458 - val_loss: 0.3904 - val_mse: 0.3904 - val_mae: 0.4766\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3405 - mse: 0.3405 - mae: 0.4508 - val_loss: 0.4042 - val_mse: 0.4042 - val_mae: 0.4892\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3279 - mse: 0.3279 - mae: 0.4456 - val_loss: 0.3830 - val_mse: 0.3830 - val_mae: 0.4716\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3127 - mse: 0.3127 - mae: 0.4343 - val_loss: 0.3802 - val_mse: 0.3802 - val_mae: 0.4673\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3224 - mse: 0.3224 - mae: 0.4338 - val_loss: 0.3960 - val_mse: 0.3960 - val_mae: 0.4783\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3332 - mse: 0.3332 - mae: 0.4488 - val_loss: 0.3761 - val_mse: 0.3761 - val_mae: 0.4687\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3271 - mse: 0.3271 - mae: 0.4425 - val_loss: 0.3834 - val_mse: 0.3834 - val_mae: 0.4722\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3169 - mse: 0.3169 - mae: 0.4365 - val_loss: 0.3782 - val_mse: 0.3782 - val_mae: 0.4663\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3154 - mse: 0.3154 - mae: 0.4333 - val_loss: 0.4022 - val_mse: 0.4022 - val_mae: 0.4854\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3436 - mse: 0.3436 - mae: 0.4550 - val_loss: 0.3829 - val_mse: 0.3829 - val_mae: 0.4677\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3201 - mse: 0.3201 - mae: 0.4374 - val_loss: 0.3763 - val_mse: 0.3763 - val_mae: 0.4721\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3327 - mse: 0.3327 - mae: 0.4454 - val_loss: 0.3699 - val_mse: 0.3699 - val_mae: 0.4643\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3592 - mse: 0.3592 - mae: 0.4675 - val_loss: 0.3847 - val_mse: 0.3847 - val_mae: 0.4733\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3297 - mse: 0.3297 - mae: 0.4424 - val_loss: 0.3827 - val_mse: 0.3827 - val_mae: 0.4676\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3344 - mse: 0.3344 - mae: 0.4454 - val_loss: 0.3797 - val_mse: 0.3797 - val_mae: 0.4723\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3137 - mse: 0.3137 - mae: 0.4338 - val_loss: 0.3839 - val_mse: 0.3839 - val_mae: 0.4800\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3203 - mse: 0.3203 - mae: 0.4428 - val_loss: 0.3706 - val_mse: 0.3706 - val_mae: 0.4647\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3058 - mse: 0.3058 - mae: 0.4265 - val_loss: 0.3767 - val_mse: 0.3767 - val_mae: 0.4669\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3236 - mse: 0.3236 - mae: 0.4444 - val_loss: 0.3878 - val_mse: 0.3878 - val_mae: 0.4734\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3057 - mse: 0.3057 - mae: 0.4286 - val_loss: 0.3708 - val_mse: 0.3708 - val_mae: 0.4677\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3213 - mse: 0.3213 - mae: 0.4409 - val_loss: 0.3754 - val_mse: 0.3754 - val_mae: 0.4684\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3171 - mse: 0.3171 - mae: 0.4401 - val_loss: 0.3704 - val_mse: 0.3704 - val_mae: 0.4624\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3155 - mse: 0.3155 - mae: 0.4341 - val_loss: 0.3644 - val_mse: 0.3644 - val_mae: 0.4652\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3324 - mse: 0.3324 - mae: 0.4432 - val_loss: 0.3879 - val_mse: 0.3879 - val_mae: 0.4757\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3483 - mse: 0.3483 - mae: 0.4585 - val_loss: 0.3848 - val_mse: 0.3848 - val_mae: 0.4710\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3316 - mse: 0.3316 - mae: 0.4333 - val_loss: 0.3682 - val_mse: 0.3682 - val_mae: 0.4606\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3251 - mse: 0.3251 - mae: 0.4443 - val_loss: 0.4123 - val_mse: 0.4123 - val_mae: 0.4924\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2993 - mse: 0.2993 - mae: 0.4323 - val_loss: 0.3786 - val_mse: 0.3786 - val_mae: 0.4698\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3336 - mse: 0.3336 - mae: 0.4493 - val_loss: 0.3750 - val_mse: 0.3750 - val_mae: 0.4664\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.3155 - mse: 0.3155 - mae: 0.4393 - val_loss: 0.3718 - val_mse: 0.3718 - val_mae: 0.4699\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3348 - mse: 0.3348 - mae: 0.4506 - val_loss: 0.3674 - val_mse: 0.3674 - val_mae: 0.4635\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3122 - mse: 0.3122 - mae: 0.4336 - val_loss: 0.4246 - val_mse: 0.4246 - val_mae: 0.4983\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3284 - mse: 0.3284 - mae: 0.4469 - val_loss: 0.3680 - val_mse: 0.3680 - val_mae: 0.4605\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3125 - mse: 0.3125 - mae: 0.4313 - val_loss: 0.3798 - val_mse: 0.3798 - val_mae: 0.4688\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3109 - mse: 0.3109 - mae: 0.4339 - val_loss: 0.3723 - val_mse: 0.3723 - val_mae: 0.4607\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3020 - mse: 0.3020 - mae: 0.4253 - val_loss: 0.3710 - val_mse: 0.3710 - val_mae: 0.4647\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3092 - mse: 0.3092 - mae: 0.4252 - val_loss: 0.3672 - val_mse: 0.3672 - val_mae: 0.4619\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.3239 - mse: 0.3239 - mae: 0.4445 - val_loss: 0.3729 - val_mse: 0.3729 - val_mae: 0.4642\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.3196 - mse: 0.3196 - mae: 0.4417 - val_loss: 0.3741 - val_mse: 0.3741 - val_mae: 0.4636\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3286 - mse: 0.3286 - mae: 0.4449 - val_loss: 0.3689 - val_mse: 0.3689 - val_mae: 0.4703\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3106 - mse: 0.3106 - mae: 0.4268 - val_loss: 0.3766 - val_mse: 0.3766 - val_mae: 0.4663\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3020 - mse: 0.3020 - mae: 0.4261 - val_loss: 0.3784 - val_mse: 0.3784 - val_mae: 0.4716\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3057 - mse: 0.3057 - mae: 0.4275 - val_loss: 0.3653 - val_mse: 0.3653 - val_mae: 0.4617\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3033 - mse: 0.3033 - mae: 0.4228 - val_loss: 0.3807 - val_mse: 0.3807 - val_mae: 0.4668\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3233 - mse: 0.3233 - mae: 0.4394 - val_loss: 0.3784 - val_mse: 0.3784 - val_mae: 0.4732\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3152 - mse: 0.3152 - mae: 0.4349 - val_loss: 0.3874 - val_mse: 0.3874 - val_mae: 0.4702\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2974 - mse: 0.2974 - mae: 0.4226 - val_loss: 0.3751 - val_mse: 0.3751 - val_mae: 0.4643\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3056 - mse: 0.3056 - mae: 0.4291 - val_loss: 0.3733 - val_mse: 0.3733 - val_mae: 0.4671\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3025 - mse: 0.3025 - mae: 0.4270 - val_loss: 0.3719 - val_mse: 0.3719 - val_mae: 0.4681\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3104 - mse: 0.3104 - mae: 0.4303 - val_loss: 0.3818 - val_mse: 0.3818 - val_mae: 0.4697\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3014 - mse: 0.3014 - mae: 0.4212 - val_loss: 0.3967 - val_mse: 0.3967 - val_mae: 0.4728\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3292 - mse: 0.3292 - mae: 0.4362 - val_loss: 0.3835 - val_mse: 0.3835 - val_mae: 0.4692\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2995 - mse: 0.2995 - mae: 0.4220 - val_loss: 0.3859 - val_mse: 0.3859 - val_mae: 0.4871\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3221 - mse: 0.3221 - mae: 0.4382 - val_loss: 0.3741 - val_mse: 0.3741 - val_mae: 0.4670\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3010 - mse: 0.3010 - mae: 0.4245 - val_loss: 0.3764 - val_mse: 0.3764 - val_mae: 0.4670\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2946 - mse: 0.2946 - mae: 0.4130 - val_loss: 0.3879 - val_mse: 0.3879 - val_mae: 0.4689\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3090 - mse: 0.3090 - mae: 0.4250 - val_loss: 0.3611 - val_mse: 0.3611 - val_mae: 0.4589\n",
            "Epoch 252/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3309 - mse: 0.3309 - mae: 0.4436 - val_loss: 0.3698 - val_mse: 0.3698 - val_mae: 0.4682\n",
            "Epoch 253/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3024 - mse: 0.3024 - mae: 0.4343 - val_loss: 0.3608 - val_mse: 0.3608 - val_mae: 0.4610\n",
            "Epoch 254/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3052 - mse: 0.3052 - mae: 0.4300 - val_loss: 0.3693 - val_mse: 0.3693 - val_mae: 0.4597\n",
            "Epoch 255/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3095 - mse: 0.3095 - mae: 0.4292 - val_loss: 0.3611 - val_mse: 0.3611 - val_mae: 0.4578\n",
            "Epoch 256/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2957 - mse: 0.2957 - mae: 0.4120 - val_loss: 0.3675 - val_mse: 0.3675 - val_mae: 0.4601\n",
            "Epoch 257/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3256 - mse: 0.3256 - mae: 0.4463 - val_loss: 0.3641 - val_mse: 0.3641 - val_mae: 0.4577\n",
            "Epoch 258/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2985 - mse: 0.2985 - mae: 0.4179 - val_loss: 0.3800 - val_mse: 0.3800 - val_mae: 0.4685\n",
            "Epoch 259/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3256 - mse: 0.3256 - mae: 0.4383 - val_loss: 0.3901 - val_mse: 0.3901 - val_mae: 0.4720\n",
            "Epoch 260/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3042 - mse: 0.3042 - mae: 0.4246 - val_loss: 0.3678 - val_mse: 0.3678 - val_mae: 0.4622\n",
            "Epoch 261/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3181 - mse: 0.3181 - mae: 0.4383 - val_loss: 0.3618 - val_mse: 0.3618 - val_mae: 0.4583\n",
            "Epoch 262/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3007 - mse: 0.3007 - mae: 0.4261 - val_loss: 0.3643 - val_mse: 0.3643 - val_mae: 0.4615\n",
            "Epoch 263/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3086 - mse: 0.3086 - mae: 0.4207 - val_loss: 0.3600 - val_mse: 0.3600 - val_mae: 0.4562\n",
            "Epoch 264/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3040 - mse: 0.3040 - mae: 0.4316 - val_loss: 0.3724 - val_mse: 0.3724 - val_mae: 0.4638\n",
            "Epoch 265/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2998 - mse: 0.2998 - mae: 0.4216 - val_loss: 0.3606 - val_mse: 0.3606 - val_mae: 0.4552\n",
            "Epoch 266/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2931 - mse: 0.2931 - mae: 0.4097 - val_loss: 0.3690 - val_mse: 0.3690 - val_mae: 0.4714\n",
            "Epoch 267/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2888 - mse: 0.2888 - mae: 0.4115 - val_loss: 0.3751 - val_mse: 0.3751 - val_mae: 0.4654\n",
            "Epoch 268/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3047 - mse: 0.3047 - mae: 0.4285 - val_loss: 0.3939 - val_mse: 0.3939 - val_mae: 0.4776\n",
            "Epoch 269/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3088 - mse: 0.3088 - mae: 0.4245 - val_loss: 0.3845 - val_mse: 0.3845 - val_mae: 0.4701\n",
            "Epoch 270/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2920 - mse: 0.2920 - mae: 0.4167 - val_loss: 0.3668 - val_mse: 0.3668 - val_mae: 0.4638\n",
            "Epoch 271/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2984 - mse: 0.2984 - mae: 0.4198 - val_loss: 0.3791 - val_mse: 0.3791 - val_mae: 0.4709\n",
            "Epoch 272/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2901 - mse: 0.2901 - mae: 0.4177 - val_loss: 0.3842 - val_mse: 0.3842 - val_mae: 0.4697\n",
            "Epoch 273/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3038 - mse: 0.3038 - mae: 0.4223 - val_loss: 0.3876 - val_mse: 0.3876 - val_mae: 0.4733\n",
            "Epoch 274/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3004 - mse: 0.3004 - mae: 0.4211 - val_loss: 0.3683 - val_mse: 0.3683 - val_mae: 0.4567\n",
            "Epoch 275/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2920 - mse: 0.2920 - mae: 0.4155 - val_loss: 0.3598 - val_mse: 0.3598 - val_mae: 0.4552\n",
            "Epoch 276/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2837 - mse: 0.2837 - mae: 0.4112 - val_loss: 0.3673 - val_mse: 0.3673 - val_mae: 0.4566\n",
            "Epoch 277/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3039 - mse: 0.3039 - mae: 0.4200 - val_loss: 0.3874 - val_mse: 0.3874 - val_mae: 0.4702\n",
            "Epoch 278/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2956 - mse: 0.2956 - mae: 0.4137 - val_loss: 0.4136 - val_mse: 0.4136 - val_mae: 0.4911\n",
            "Epoch 279/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3201 - mse: 0.3201 - mae: 0.4371 - val_loss: 0.3816 - val_mse: 0.3816 - val_mae: 0.4695\n",
            "Epoch 280/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2943 - mse: 0.2943 - mae: 0.4218 - val_loss: 0.3789 - val_mse: 0.3789 - val_mae: 0.4701\n",
            "Epoch 281/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2845 - mse: 0.2845 - mae: 0.4066 - val_loss: 0.3593 - val_mse: 0.3593 - val_mae: 0.4555\n",
            "Epoch 282/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2911 - mse: 0.2911 - mae: 0.4122 - val_loss: 0.3707 - val_mse: 0.3707 - val_mae: 0.4635\n",
            "Epoch 283/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2905 - mse: 0.2905 - mae: 0.4134 - val_loss: 0.3780 - val_mse: 0.3780 - val_mae: 0.4663\n",
            "Epoch 284/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2833 - mse: 0.2833 - mae: 0.4164 - val_loss: 0.3826 - val_mse: 0.3826 - val_mae: 0.4675\n",
            "Epoch 285/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2902 - mse: 0.2902 - mae: 0.4169 - val_loss: 0.3930 - val_mse: 0.3930 - val_mae: 0.4745\n",
            "Epoch 286/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2851 - mse: 0.2851 - mae: 0.4101 - val_loss: 0.3795 - val_mse: 0.3795 - val_mae: 0.4642\n",
            "Epoch 287/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2864 - mse: 0.2864 - mae: 0.4037 - val_loss: 0.3552 - val_mse: 0.3552 - val_mae: 0.4580\n",
            "Epoch 288/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2830 - mse: 0.2830 - mae: 0.4052 - val_loss: 0.3714 - val_mse: 0.3714 - val_mae: 0.4663\n",
            "Epoch 289/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2831 - mse: 0.2831 - mae: 0.4136 - val_loss: 0.3668 - val_mse: 0.3668 - val_mae: 0.4563\n",
            "Epoch 290/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2779 - mse: 0.2779 - mae: 0.4066 - val_loss: 0.3728 - val_mse: 0.3728 - val_mae: 0.4623\n",
            "Epoch 291/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2725 - mse: 0.2725 - mae: 0.3993 - val_loss: 0.3566 - val_mse: 0.3566 - val_mae: 0.4570\n",
            "Epoch 292/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2771 - mse: 0.2771 - mae: 0.4016 - val_loss: 0.3603 - val_mse: 0.3603 - val_mae: 0.4580\n",
            "Epoch 293/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2781 - mse: 0.2781 - mae: 0.4064 - val_loss: 0.3601 - val_mse: 0.3601 - val_mae: 0.4570\n",
            "Epoch 294/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2930 - mse: 0.2930 - mae: 0.4136 - val_loss: 0.3618 - val_mse: 0.3618 - val_mae: 0.4616\n",
            "Epoch 295/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3022 - mse: 0.3022 - mae: 0.4224 - val_loss: 0.3713 - val_mse: 0.3713 - val_mae: 0.4588\n",
            "Epoch 296/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2889 - mse: 0.2889 - mae: 0.4094 - val_loss: 0.3587 - val_mse: 0.3587 - val_mae: 0.4600\n",
            "Epoch 297/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3011 - mse: 0.3011 - mae: 0.4262 - val_loss: 0.3584 - val_mse: 0.3584 - val_mae: 0.4593\n",
            "Epoch 298/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2983 - mse: 0.2983 - mae: 0.4260 - val_loss: 0.3631 - val_mse: 0.3631 - val_mae: 0.4612\n",
            "Epoch 299/1000\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2930 - mse: 0.2930 - mae: 0.4263 - val_loss: 0.3576 - val_mse: 0.3576 - val_mae: 0.4617\n",
            "Epoch 300/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2823 - mse: 0.2823 - mae: 0.4023 - val_loss: 0.3763 - val_mse: 0.3763 - val_mae: 0.4619\n",
            "Epoch 301/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2802 - mse: 0.2802 - mae: 0.4020 - val_loss: 0.3828 - val_mse: 0.3828 - val_mae: 0.4657\n",
            "Epoch 302/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3011 - mse: 0.3011 - mae: 0.4158 - val_loss: 0.3890 - val_mse: 0.3890 - val_mae: 0.4721\n",
            "Epoch 303/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2791 - mse: 0.2791 - mae: 0.4082 - val_loss: 0.3804 - val_mse: 0.3804 - val_mae: 0.4706\n",
            "Epoch 304/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2844 - mse: 0.2844 - mae: 0.4128 - val_loss: 0.3612 - val_mse: 0.3612 - val_mae: 0.4596\n",
            "Epoch 305/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3052 - mse: 0.3052 - mae: 0.4239 - val_loss: 0.3569 - val_mse: 0.3569 - val_mae: 0.4534\n",
            "Epoch 306/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2806 - mse: 0.2806 - mae: 0.4058 - val_loss: 0.3958 - val_mse: 0.3958 - val_mae: 0.4769\n",
            "Epoch 307/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2933 - mse: 0.2933 - mae: 0.4187 - val_loss: 0.3602 - val_mse: 0.3602 - val_mae: 0.4506\n",
            "Epoch 308/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2854 - mse: 0.2854 - mae: 0.4137 - val_loss: 0.3567 - val_mse: 0.3567 - val_mae: 0.4623\n",
            "Epoch 309/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3082 - mse: 0.3082 - mae: 0.4236 - val_loss: 0.3524 - val_mse: 0.3524 - val_mae: 0.4586\n",
            "Epoch 310/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2973 - mse: 0.2973 - mae: 0.4182 - val_loss: 0.4231 - val_mse: 0.4231 - val_mae: 0.4957\n",
            "Epoch 311/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3001 - mse: 0.3001 - mae: 0.4194 - val_loss: 0.3664 - val_mse: 0.3664 - val_mae: 0.4550\n",
            "Epoch 312/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2690 - mse: 0.2690 - mae: 0.3976 - val_loss: 0.3617 - val_mse: 0.3617 - val_mae: 0.4559\n",
            "Epoch 313/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2801 - mse: 0.2801 - mae: 0.4140 - val_loss: 0.3570 - val_mse: 0.3570 - val_mae: 0.4594\n",
            "Epoch 314/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2799 - mse: 0.2799 - mae: 0.4116 - val_loss: 0.3570 - val_mse: 0.3570 - val_mae: 0.4610\n",
            "Epoch 315/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2744 - mse: 0.2744 - mae: 0.4037 - val_loss: 0.3740 - val_mse: 0.3740 - val_mae: 0.4636\n",
            "Epoch 316/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2946 - mse: 0.2946 - mae: 0.4151 - val_loss: 0.3513 - val_mse: 0.3513 - val_mae: 0.4544\n",
            "Epoch 317/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3098 - mse: 0.3098 - mae: 0.4316 - val_loss: 0.3842 - val_mse: 0.3842 - val_mae: 0.4718\n",
            "Epoch 318/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2867 - mse: 0.2867 - mae: 0.4163 - val_loss: 0.3631 - val_mse: 0.3631 - val_mae: 0.4558\n",
            "Epoch 319/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2779 - mse: 0.2779 - mae: 0.4089 - val_loss: 0.3564 - val_mse: 0.3564 - val_mae: 0.4503\n",
            "Epoch 320/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2842 - mse: 0.2842 - mae: 0.4103 - val_loss: 0.3586 - val_mse: 0.3586 - val_mae: 0.4550\n",
            "Epoch 321/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2870 - mse: 0.2870 - mae: 0.4109 - val_loss: 0.3526 - val_mse: 0.3526 - val_mae: 0.4578\n",
            "Epoch 322/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2915 - mse: 0.2915 - mae: 0.4211 - val_loss: 0.3452 - val_mse: 0.3452 - val_mae: 0.4446\n",
            "Epoch 323/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2785 - mse: 0.2785 - mae: 0.4111 - val_loss: 0.3941 - val_mse: 0.3941 - val_mae: 0.4724\n",
            "Epoch 324/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3094 - mse: 0.3094 - mae: 0.4351 - val_loss: 0.3872 - val_mse: 0.3872 - val_mae: 0.4719\n",
            "Epoch 325/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3020 - mse: 0.3020 - mae: 0.4191 - val_loss: 0.3607 - val_mse: 0.3607 - val_mae: 0.4487\n",
            "Epoch 326/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2698 - mse: 0.2698 - mae: 0.4005 - val_loss: 0.3933 - val_mse: 0.3933 - val_mae: 0.4726\n",
            "Epoch 327/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2798 - mse: 0.2798 - mae: 0.4049 - val_loss: 0.3767 - val_mse: 0.3767 - val_mae: 0.4624\n",
            "Epoch 328/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2861 - mse: 0.2861 - mae: 0.4147 - val_loss: 0.4500 - val_mse: 0.4500 - val_mae: 0.5095\n",
            "Epoch 329/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3129 - mse: 0.3129 - mae: 0.4324 - val_loss: 0.4240 - val_mse: 0.4240 - val_mae: 0.4918\n",
            "Epoch 330/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2808 - mse: 0.2808 - mae: 0.4037 - val_loss: 0.3621 - val_mse: 0.3621 - val_mae: 0.4659\n",
            "Epoch 331/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2809 - mse: 0.2809 - mae: 0.4108 - val_loss: 0.4016 - val_mse: 0.4016 - val_mae: 0.4771\n",
            "Epoch 332/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2797 - mse: 0.2797 - mae: 0.4095 - val_loss: 0.3830 - val_mse: 0.3830 - val_mae: 0.4659\n",
            "Epoch 333/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2645 - mse: 0.2645 - mae: 0.3953 - val_loss: 0.3567 - val_mse: 0.3567 - val_mae: 0.4530\n",
            "Epoch 334/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2844 - mse: 0.2844 - mae: 0.4093 - val_loss: 0.3549 - val_mse: 0.3549 - val_mae: 0.4487\n",
            "Epoch 335/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2675 - mse: 0.2675 - mae: 0.3935 - val_loss: 0.3718 - val_mse: 0.3718 - val_mae: 0.4683\n",
            "Epoch 336/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2752 - mse: 0.2752 - mae: 0.4117 - val_loss: 0.3637 - val_mse: 0.3637 - val_mae: 0.4529\n",
            "Epoch 337/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2831 - mse: 0.2831 - mae: 0.4181 - val_loss: 0.3586 - val_mse: 0.3586 - val_mae: 0.4579\n",
            "Epoch 338/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2899 - mse: 0.2899 - mae: 0.4186 - val_loss: 0.3542 - val_mse: 0.3542 - val_mae: 0.4464\n",
            "Epoch 339/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2668 - mse: 0.2668 - mae: 0.3949 - val_loss: 0.3647 - val_mse: 0.3647 - val_mae: 0.4511\n",
            "Epoch 340/1000\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2691 - mse: 0.2691 - mae: 0.3976 - val_loss: 0.3562 - val_mse: 0.3562 - val_mae: 0.4571\n",
            "Epoch 341/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2739 - mse: 0.2739 - mae: 0.4048 - val_loss: 0.3550 - val_mse: 0.3550 - val_mae: 0.4495\n",
            "Epoch 342/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2908 - mse: 0.2908 - mae: 0.4170 - val_loss: 0.3744 - val_mse: 0.3744 - val_mae: 0.4595\n",
            "Epoch 343/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2875 - mse: 0.2875 - mae: 0.4094 - val_loss: 0.3634 - val_mse: 0.3634 - val_mae: 0.4548\n",
            "Epoch 344/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2817 - mse: 0.2817 - mae: 0.4114 - val_loss: 0.3587 - val_mse: 0.3587 - val_mae: 0.4581\n",
            "Epoch 345/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2659 - mse: 0.2659 - mae: 0.3953 - val_loss: 0.3772 - val_mse: 0.3772 - val_mae: 0.4642\n",
            "Epoch 346/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2732 - mse: 0.2732 - mae: 0.4003 - val_loss: 0.3847 - val_mse: 0.3847 - val_mae: 0.4673\n",
            "Epoch 347/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2823 - mse: 0.2823 - mae: 0.4122 - val_loss: 0.3480 - val_mse: 0.3480 - val_mae: 0.4503\n",
            "Epoch 348/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2909 - mse: 0.2909 - mae: 0.4162 - val_loss: 0.3580 - val_mse: 0.3580 - val_mae: 0.4559\n",
            "Epoch 349/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2606 - mse: 0.2606 - mae: 0.3901 - val_loss: 0.3638 - val_mse: 0.3638 - val_mae: 0.4604\n",
            "Epoch 350/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2993 - mse: 0.2993 - mae: 0.4164 - val_loss: 0.3538 - val_mse: 0.3538 - val_mae: 0.4532\n",
            "Epoch 351/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2872 - mse: 0.2872 - mae: 0.4137 - val_loss: 0.3629 - val_mse: 0.3629 - val_mae: 0.4589\n",
            "Epoch 352/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2690 - mse: 0.2690 - mae: 0.3995 - val_loss: 0.3547 - val_mse: 0.3547 - val_mae: 0.4529\n",
            "Epoch 353/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2669 - mse: 0.2669 - mae: 0.3996 - val_loss: 0.3663 - val_mse: 0.3663 - val_mae: 0.4550\n",
            "Epoch 354/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2731 - mse: 0.2731 - mae: 0.3997 - val_loss: 0.3631 - val_mse: 0.3631 - val_mae: 0.4613\n",
            "Epoch 355/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2628 - mse: 0.2628 - mae: 0.3998 - val_loss: 0.3532 - val_mse: 0.3532 - val_mae: 0.4556\n",
            "Epoch 356/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2668 - mse: 0.2668 - mae: 0.4069 - val_loss: 0.3876 - val_mse: 0.3876 - val_mae: 0.4701\n",
            "Epoch 357/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2824 - mse: 0.2824 - mae: 0.4119 - val_loss: 0.3576 - val_mse: 0.3576 - val_mae: 0.4568\n",
            "Epoch 358/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2737 - mse: 0.2737 - mae: 0.4030 - val_loss: 0.3670 - val_mse: 0.3670 - val_mae: 0.4552\n",
            "Epoch 359/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2658 - mse: 0.2658 - mae: 0.3907 - val_loss: 0.4501 - val_mse: 0.4501 - val_mae: 0.5061\n",
            "Epoch 360/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2820 - mse: 0.2820 - mae: 0.4006 - val_loss: 0.3848 - val_mse: 0.3848 - val_mae: 0.4626\n",
            "Epoch 361/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2793 - mse: 0.2793 - mae: 0.4050 - val_loss: 0.4007 - val_mse: 0.4007 - val_mae: 0.4759\n",
            "Epoch 362/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2780 - mse: 0.2780 - mae: 0.4101 - val_loss: 0.3637 - val_mse: 0.3637 - val_mae: 0.4588\n",
            "Epoch 363/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2844 - mse: 0.2844 - mae: 0.4037 - val_loss: 0.3677 - val_mse: 0.3677 - val_mae: 0.4612\n",
            "Epoch 364/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2741 - mse: 0.2741 - mae: 0.4049 - val_loss: 0.3737 - val_mse: 0.3737 - val_mae: 0.4593\n",
            "Epoch 365/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2557 - mse: 0.2557 - mae: 0.3891 - val_loss: 0.3593 - val_mse: 0.3593 - val_mae: 0.4506\n",
            "Epoch 366/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2657 - mse: 0.2657 - mae: 0.4025 - val_loss: 0.3442 - val_mse: 0.3442 - val_mae: 0.4443\n",
            "Epoch 367/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2827 - mse: 0.2827 - mae: 0.4093 - val_loss: 0.3577 - val_mse: 0.3577 - val_mae: 0.4540\n",
            "Epoch 368/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3011 - mse: 0.3011 - mae: 0.4244 - val_loss: 0.3600 - val_mse: 0.3600 - val_mae: 0.4506\n",
            "Epoch 369/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3091 - mse: 0.3091 - mae: 0.4296 - val_loss: 0.3439 - val_mse: 0.3439 - val_mae: 0.4452\n",
            "Epoch 370/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2704 - mse: 0.2704 - mae: 0.4005 - val_loss: 0.3741 - val_mse: 0.3741 - val_mae: 0.4656\n",
            "Epoch 371/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2703 - mse: 0.2703 - mae: 0.3951 - val_loss: 0.3617 - val_mse: 0.3617 - val_mae: 0.4548\n",
            "Epoch 372/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2682 - mse: 0.2682 - mae: 0.4003 - val_loss: 0.3526 - val_mse: 0.3526 - val_mae: 0.4543\n",
            "Epoch 373/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2908 - mse: 0.2908 - mae: 0.4204 - val_loss: 0.3534 - val_mse: 0.3534 - val_mae: 0.4455\n",
            "Epoch 374/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2790 - mse: 0.2790 - mae: 0.4030 - val_loss: 0.3659 - val_mse: 0.3659 - val_mae: 0.4665\n",
            "Epoch 375/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2635 - mse: 0.2635 - mae: 0.3984 - val_loss: 0.3677 - val_mse: 0.3677 - val_mae: 0.4569\n",
            "Epoch 376/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2811 - mse: 0.2811 - mae: 0.4050 - val_loss: 0.3712 - val_mse: 0.3712 - val_mae: 0.4645\n",
            "Epoch 377/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2702 - mse: 0.2702 - mae: 0.4041 - val_loss: 0.3997 - val_mse: 0.3997 - val_mae: 0.4763\n",
            "Epoch 378/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2612 - mse: 0.2612 - mae: 0.3930 - val_loss: 0.3464 - val_mse: 0.3464 - val_mae: 0.4486\n",
            "Epoch 379/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2779 - mse: 0.2779 - mae: 0.3964 - val_loss: 0.3609 - val_mse: 0.3609 - val_mae: 0.4541\n",
            "Epoch 380/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2738 - mse: 0.2738 - mae: 0.3945 - val_loss: 0.3669 - val_mse: 0.3669 - val_mae: 0.4610\n",
            "Epoch 381/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2722 - mse: 0.2722 - mae: 0.4032 - val_loss: 0.3717 - val_mse: 0.3717 - val_mae: 0.4583\n",
            "Epoch 382/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2756 - mse: 0.2756 - mae: 0.3994 - val_loss: 0.3466 - val_mse: 0.3466 - val_mae: 0.4505\n",
            "Epoch 383/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2698 - mse: 0.2698 - mae: 0.3945 - val_loss: 0.3903 - val_mse: 0.3903 - val_mae: 0.4678\n",
            "Epoch 384/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2770 - mse: 0.2770 - mae: 0.4107 - val_loss: 0.3741 - val_mse: 0.3741 - val_mae: 0.4630\n",
            "Epoch 385/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2490 - mse: 0.2490 - mae: 0.3849 - val_loss: 0.3582 - val_mse: 0.3582 - val_mae: 0.4473\n",
            "Epoch 386/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2645 - mse: 0.2645 - mae: 0.3913 - val_loss: 0.3552 - val_mse: 0.3552 - val_mae: 0.4449\n",
            "Epoch 387/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2558 - mse: 0.2558 - mae: 0.3874 - val_loss: 0.3648 - val_mse: 0.3648 - val_mae: 0.4602\n",
            "Epoch 388/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2669 - mse: 0.2669 - mae: 0.3935 - val_loss: 0.3832 - val_mse: 0.3832 - val_mae: 0.4728\n",
            "Epoch 389/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2851 - mse: 0.2851 - mae: 0.4056 - val_loss: 0.3667 - val_mse: 0.3667 - val_mae: 0.4563\n",
            "Epoch 390/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2607 - mse: 0.2607 - mae: 0.3943 - val_loss: 0.3567 - val_mse: 0.3567 - val_mae: 0.4532\n",
            "Epoch 391/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2673 - mse: 0.2673 - mae: 0.4064 - val_loss: 0.3531 - val_mse: 0.3531 - val_mae: 0.4485\n",
            "Epoch 392/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2623 - mse: 0.2623 - mae: 0.3886 - val_loss: 0.3567 - val_mse: 0.3567 - val_mae: 0.4505\n",
            "Epoch 393/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2707 - mse: 0.2707 - mae: 0.4031 - val_loss: 0.3450 - val_mse: 0.3450 - val_mae: 0.4451\n",
            "Epoch 394/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2642 - mse: 0.2642 - mae: 0.3961 - val_loss: 0.3498 - val_mse: 0.3498 - val_mae: 0.4477\n",
            "Epoch 395/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2801 - mse: 0.2801 - mae: 0.4123 - val_loss: 0.3705 - val_mse: 0.3705 - val_mae: 0.4597\n",
            "Epoch 396/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2654 - mse: 0.2654 - mae: 0.3985 - val_loss: 0.3538 - val_mse: 0.3538 - val_mae: 0.4505\n",
            "Epoch 397/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2713 - mse: 0.2713 - mae: 0.3944 - val_loss: 0.3516 - val_mse: 0.3516 - val_mae: 0.4459\n",
            "Epoch 398/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2598 - mse: 0.2598 - mae: 0.3920 - val_loss: 0.3509 - val_mse: 0.3509 - val_mae: 0.4530\n",
            "Epoch 399/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2498 - mse: 0.2498 - mae: 0.3839 - val_loss: 0.3934 - val_mse: 0.3934 - val_mae: 0.4714\n",
            "Epoch 400/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2587 - mse: 0.2587 - mae: 0.3961 - val_loss: 0.3930 - val_mse: 0.3930 - val_mae: 0.4746\n",
            "Epoch 401/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2840 - mse: 0.2840 - mae: 0.4031 - val_loss: 0.3489 - val_mse: 0.3489 - val_mae: 0.4522\n",
            "Epoch 402/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2555 - mse: 0.2555 - mae: 0.3884 - val_loss: 0.3669 - val_mse: 0.3669 - val_mae: 0.4558\n",
            "Epoch 403/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2529 - mse: 0.2529 - mae: 0.3875 - val_loss: 0.3863 - val_mse: 0.3863 - val_mae: 0.4698\n",
            "Epoch 404/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2770 - mse: 0.2770 - mae: 0.4079 - val_loss: 0.3672 - val_mse: 0.3672 - val_mae: 0.4579\n",
            "Epoch 405/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2597 - mse: 0.2597 - mae: 0.3923 - val_loss: 0.4110 - val_mse: 0.4110 - val_mae: 0.4857\n",
            "Epoch 406/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2570 - mse: 0.2570 - mae: 0.3945 - val_loss: 0.3644 - val_mse: 0.3644 - val_mae: 0.4528\n",
            "Epoch 407/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2485 - mse: 0.2485 - mae: 0.3818 - val_loss: 0.4181 - val_mse: 0.4181 - val_mae: 0.4871\n",
            "Epoch 408/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2735 - mse: 0.2735 - mae: 0.4070 - val_loss: 0.3901 - val_mse: 0.3901 - val_mae: 0.4773\n",
            "Epoch 409/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2796 - mse: 0.2796 - mae: 0.4066 - val_loss: 0.3651 - val_mse: 0.3651 - val_mae: 0.4560\n",
            "Epoch 410/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2545 - mse: 0.2545 - mae: 0.3887 - val_loss: 0.3706 - val_mse: 0.3706 - val_mae: 0.4610\n",
            "Epoch 411/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2662 - mse: 0.2662 - mae: 0.3952 - val_loss: 0.3541 - val_mse: 0.3541 - val_mae: 0.4530\n",
            "Epoch 412/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2841 - mse: 0.2841 - mae: 0.4081 - val_loss: 0.3618 - val_mse: 0.3618 - val_mae: 0.4539\n",
            "Epoch 413/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2645 - mse: 0.2645 - mae: 0.3948 - val_loss: 0.3645 - val_mse: 0.3645 - val_mae: 0.4585\n",
            "Epoch 414/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2661 - mse: 0.2661 - mae: 0.3931 - val_loss: 0.3596 - val_mse: 0.3596 - val_mae: 0.4608\n",
            "Epoch 415/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2584 - mse: 0.2584 - mae: 0.3916 - val_loss: 0.3499 - val_mse: 0.3499 - val_mae: 0.4467\n",
            "Epoch 416/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2573 - mse: 0.2573 - mae: 0.3893 - val_loss: 0.3768 - val_mse: 0.3768 - val_mae: 0.4663\n",
            "Epoch 417/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2831 - mse: 0.2831 - mae: 0.4101 - val_loss: 0.4139 - val_mse: 0.4139 - val_mae: 0.4845\n",
            "Epoch 418/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2695 - mse: 0.2695 - mae: 0.3945 - val_loss: 0.3753 - val_mse: 0.3753 - val_mae: 0.4642\n",
            "Epoch 419/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2697 - mse: 0.2697 - mae: 0.3986 - val_loss: 0.3467 - val_mse: 0.3467 - val_mae: 0.4424\n",
            "Epoch 420/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2685 - mse: 0.2685 - mae: 0.3951 - val_loss: 0.4024 - val_mse: 0.4024 - val_mae: 0.4817\n",
            "Epoch 421/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2627 - mse: 0.2627 - mae: 0.3980 - val_loss: 0.3580 - val_mse: 0.3580 - val_mae: 0.4598\n",
            "Epoch 422/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2518 - mse: 0.2518 - mae: 0.3803 - val_loss: 0.3522 - val_mse: 0.3522 - val_mae: 0.4524\n",
            "Epoch 423/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2599 - mse: 0.2599 - mae: 0.3934 - val_loss: 0.3663 - val_mse: 0.3663 - val_mae: 0.4586\n",
            "Epoch 424/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2669 - mse: 0.2669 - mae: 0.3996 - val_loss: 0.3661 - val_mse: 0.3661 - val_mae: 0.4596\n",
            "Epoch 425/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2548 - mse: 0.2548 - mae: 0.3844 - val_loss: 0.3503 - val_mse: 0.3503 - val_mae: 0.4465\n",
            "Epoch 426/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2689 - mse: 0.2689 - mae: 0.3971 - val_loss: 0.3474 - val_mse: 0.3474 - val_mae: 0.4491\n",
            "Epoch 427/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2462 - mse: 0.2462 - mae: 0.3798 - val_loss: 0.3513 - val_mse: 0.3513 - val_mae: 0.4481\n",
            "Epoch 428/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2528 - mse: 0.2528 - mae: 0.3795 - val_loss: 0.3563 - val_mse: 0.3563 - val_mae: 0.4542\n",
            "Epoch 429/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2607 - mse: 0.2607 - mae: 0.3937 - val_loss: 0.3665 - val_mse: 0.3665 - val_mae: 0.4546\n",
            "Epoch 430/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2583 - mse: 0.2583 - mae: 0.3894 - val_loss: 0.3620 - val_mse: 0.3620 - val_mae: 0.4582\n",
            "Epoch 431/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2777 - mse: 0.2777 - mae: 0.4061 - val_loss: 0.3670 - val_mse: 0.3670 - val_mae: 0.4683\n",
            "Epoch 432/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2826 - mse: 0.2826 - mae: 0.4120 - val_loss: 0.3568 - val_mse: 0.3568 - val_mae: 0.4588\n",
            "Epoch 433/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2545 - mse: 0.2545 - mae: 0.3875 - val_loss: 0.3619 - val_mse: 0.3619 - val_mae: 0.4520\n",
            "Epoch 434/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2689 - mse: 0.2689 - mae: 0.3982 - val_loss: 0.3836 - val_mse: 0.3836 - val_mae: 0.4662\n",
            "Epoch 435/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2619 - mse: 0.2619 - mae: 0.3923 - val_loss: 0.3549 - val_mse: 0.3549 - val_mae: 0.4554\n",
            "Epoch 436/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2536 - mse: 0.2536 - mae: 0.3834 - val_loss: 0.3500 - val_mse: 0.3500 - val_mae: 0.4424\n",
            "Epoch 437/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2723 - mse: 0.2723 - mae: 0.4007 - val_loss: 0.3596 - val_mse: 0.3596 - val_mae: 0.4521\n",
            "Epoch 438/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2553 - mse: 0.2553 - mae: 0.3968 - val_loss: 0.3473 - val_mse: 0.3473 - val_mae: 0.4455\n",
            "Epoch 439/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2544 - mse: 0.2544 - mae: 0.3916 - val_loss: 0.3751 - val_mse: 0.3751 - val_mae: 0.4667\n",
            "Epoch 440/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2635 - mse: 0.2635 - mae: 0.3857 - val_loss: 0.3654 - val_mse: 0.3654 - val_mae: 0.4544\n",
            "Epoch 441/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2668 - mse: 0.2668 - mae: 0.4029 - val_loss: 0.3609 - val_mse: 0.3609 - val_mae: 0.4518\n",
            "Epoch 442/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2607 - mse: 0.2607 - mae: 0.3853 - val_loss: 0.3478 - val_mse: 0.3478 - val_mae: 0.4492\n",
            "Epoch 443/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2544 - mse: 0.2544 - mae: 0.3916 - val_loss: 0.3476 - val_mse: 0.3476 - val_mae: 0.4487\n",
            "Epoch 444/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2536 - mse: 0.2536 - mae: 0.3861 - val_loss: 0.3565 - val_mse: 0.3565 - val_mae: 0.4482\n",
            "Epoch 445/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2543 - mse: 0.2543 - mae: 0.3844 - val_loss: 0.3649 - val_mse: 0.3649 - val_mae: 0.4575\n",
            "Epoch 446/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2416 - mse: 0.2416 - mae: 0.3791 - val_loss: 0.3457 - val_mse: 0.3457 - val_mae: 0.4445\n",
            "Epoch 447/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2526 - mse: 0.2526 - mae: 0.3836 - val_loss: 0.3639 - val_mse: 0.3639 - val_mae: 0.4626\n",
            "Epoch 448/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2592 - mse: 0.2592 - mae: 0.3965 - val_loss: 0.3491 - val_mse: 0.3491 - val_mae: 0.4487\n",
            "Epoch 449/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2550 - mse: 0.2550 - mae: 0.3884 - val_loss: 0.3787 - val_mse: 0.3787 - val_mae: 0.4608\n",
            "Epoch 450/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2500 - mse: 0.2500 - mae: 0.3856 - val_loss: 0.3536 - val_mse: 0.3536 - val_mae: 0.4505\n",
            "Epoch 451/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2333 - mse: 0.2333 - mae: 0.3763 - val_loss: 0.3995 - val_mse: 0.3995 - val_mae: 0.4799\n",
            "Epoch 452/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2531 - mse: 0.2531 - mae: 0.3854 - val_loss: 0.3576 - val_mse: 0.3576 - val_mae: 0.4501\n",
            "Epoch 453/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2581 - mse: 0.2581 - mae: 0.3924 - val_loss: 0.3602 - val_mse: 0.3602 - val_mae: 0.4510\n",
            "Epoch 454/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.2769 - mse: 0.2769 - mae: 0.4054 - val_loss: 0.3554 - val_mse: 0.3554 - val_mae: 0.4457\n",
            "Epoch 455/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2577 - mse: 0.2577 - mae: 0.3939 - val_loss: 0.3524 - val_mse: 0.3524 - val_mae: 0.4464\n",
            "Epoch 456/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2547 - mse: 0.2547 - mae: 0.3895 - val_loss: 0.3529 - val_mse: 0.3529 - val_mae: 0.4554\n",
            "Epoch 457/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2869 - mse: 0.2869 - mae: 0.4194 - val_loss: 0.3683 - val_mse: 0.3683 - val_mae: 0.4740\n",
            "Epoch 458/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2802 - mse: 0.2802 - mae: 0.4119 - val_loss: 0.3561 - val_mse: 0.3561 - val_mae: 0.4527\n",
            "Epoch 459/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2517 - mse: 0.2517 - mae: 0.3877 - val_loss: 0.3520 - val_mse: 0.3520 - val_mae: 0.4514\n",
            "Epoch 460/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2600 - mse: 0.2600 - mae: 0.3941 - val_loss: 0.3703 - val_mse: 0.3703 - val_mae: 0.4589\n",
            "Epoch 461/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2474 - mse: 0.2474 - mae: 0.3762 - val_loss: 0.3692 - val_mse: 0.3692 - val_mae: 0.4552\n",
            "Epoch 462/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2720 - mse: 0.2720 - mae: 0.4082 - val_loss: 0.3609 - val_mse: 0.3609 - val_mae: 0.4530\n",
            "Epoch 463/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2433 - mse: 0.2433 - mae: 0.3785 - val_loss: 0.3530 - val_mse: 0.3530 - val_mae: 0.4477\n",
            "Epoch 464/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2373 - mse: 0.2373 - mae: 0.3703 - val_loss: 0.3448 - val_mse: 0.3448 - val_mae: 0.4388\n",
            "Epoch 465/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2596 - mse: 0.2596 - mae: 0.3926 - val_loss: 0.3753 - val_mse: 0.3753 - val_mae: 0.4689\n",
            "Epoch 466/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2578 - mse: 0.2578 - mae: 0.3910 - val_loss: 0.3635 - val_mse: 0.3635 - val_mae: 0.4539\n",
            "Epoch 467/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2624 - mse: 0.2624 - mae: 0.3947 - val_loss: 0.3983 - val_mse: 0.3983 - val_mae: 0.4785\n",
            "Epoch 468/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2758 - mse: 0.2758 - mae: 0.4029 - val_loss: 0.3751 - val_mse: 0.3751 - val_mae: 0.4649\n",
            "Epoch 469/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2653 - mse: 0.2653 - mae: 0.3975 - val_loss: 0.3492 - val_mse: 0.3492 - val_mae: 0.4477\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.5037 - mse: 0.5037 - mae: 0.5396\n",
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 1s 15ms/step - loss: 31.4081 - mse: 31.4081 - mae: 5.4826 - val_loss: 23.3153 - val_mse: 23.3153 - val_mae: 4.6745\n",
            "Epoch 2/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 17.2689 - mse: 17.2689 - mae: 3.9441 - val_loss: 12.8278 - val_mse: 12.8278 - val_mae: 3.3469\n",
            "Epoch 3/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 9.0463 - mse: 9.0463 - mae: 2.7227 - val_loss: 7.2810 - val_mse: 7.2810 - val_mae: 2.4105\n",
            "Epoch 4/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 5.3646 - mse: 5.3646 - mae: 1.9838 - val_loss: 4.8339 - val_mse: 4.8339 - val_mae: 1.8617\n",
            "Epoch 5/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 3.8511 - mse: 3.8511 - mae: 1.6464 - val_loss: 3.8122 - val_mse: 3.8122 - val_mae: 1.5991\n",
            "Epoch 6/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 3.2795 - mse: 3.2795 - mae: 1.4789 - val_loss: 3.2273 - val_mse: 3.2273 - val_mae: 1.4527\n",
            "Epoch 7/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 2.9393 - mse: 2.9393 - mae: 1.3722 - val_loss: 2.8693 - val_mse: 2.8693 - val_mae: 1.3649\n",
            "Epoch 8/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.6092 - mse: 2.6092 - mae: 1.3068 - val_loss: 2.5904 - val_mse: 2.5904 - val_mae: 1.2969\n",
            "Epoch 9/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 2.4231 - mse: 2.4231 - mae: 1.2352 - val_loss: 2.3962 - val_mse: 2.3962 - val_mae: 1.2393\n",
            "Epoch 10/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.2530 - mse: 2.2530 - mae: 1.1884 - val_loss: 2.2548 - val_mse: 2.2548 - val_mae: 1.1977\n",
            "Epoch 11/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.0752 - mse: 2.0752 - mae: 1.1530 - val_loss: 2.1299 - val_mse: 2.1299 - val_mae: 1.1570\n",
            "Epoch 12/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 2.0633 - mse: 2.0633 - mae: 1.1416 - val_loss: 2.0312 - val_mse: 2.0312 - val_mae: 1.1276\n",
            "Epoch 13/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.9597 - mse: 1.9597 - mae: 1.1065 - val_loss: 1.9120 - val_mse: 1.9120 - val_mae: 1.0924\n",
            "Epoch 14/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.8013 - mse: 1.8013 - mae: 1.0543 - val_loss: 1.8154 - val_mse: 1.8154 - val_mae: 1.0567\n",
            "Epoch 15/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.7590 - mse: 1.7590 - mae: 1.0597 - val_loss: 1.7427 - val_mse: 1.7427 - val_mae: 1.0310\n",
            "Epoch 16/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.7247 - mse: 1.7247 - mae: 1.0350 - val_loss: 1.6768 - val_mse: 1.6768 - val_mae: 1.0109\n",
            "Epoch 17/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.6112 - mse: 1.6112 - mae: 1.0063 - val_loss: 1.6580 - val_mse: 1.6580 - val_mae: 1.0051\n",
            "Epoch 18/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.5126 - mse: 1.5126 - mae: 0.9784 - val_loss: 1.6022 - val_mse: 1.6022 - val_mae: 0.9823\n",
            "Epoch 19/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.4675 - mse: 1.4675 - mae: 0.9502 - val_loss: 1.5217 - val_mse: 1.5217 - val_mae: 0.9542\n",
            "Epoch 20/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.4732 - mse: 1.4732 - mae: 0.9519 - val_loss: 1.4498 - val_mse: 1.4498 - val_mae: 0.9273\n",
            "Epoch 21/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.3296 - mse: 1.3296 - mae: 0.9240 - val_loss: 1.4282 - val_mse: 1.4282 - val_mae: 0.9269\n",
            "Epoch 22/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.3062 - mse: 1.3062 - mae: 0.9003 - val_loss: 1.3939 - val_mse: 1.3939 - val_mae: 0.9140\n",
            "Epoch 23/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.3463 - mse: 1.3463 - mae: 0.9154 - val_loss: 1.3400 - val_mse: 1.3400 - val_mae: 0.8915\n",
            "Epoch 24/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2317 - mse: 1.2317 - mae: 0.8902 - val_loss: 1.3118 - val_mse: 1.3118 - val_mae: 0.8864\n",
            "Epoch 25/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2352 - mse: 1.2352 - mae: 0.8744 - val_loss: 1.2560 - val_mse: 1.2560 - val_mae: 0.8655\n",
            "Epoch 26/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.2169 - mse: 1.2169 - mae: 0.8662 - val_loss: 1.2183 - val_mse: 1.2183 - val_mae: 0.8458\n",
            "Epoch 27/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1664 - mse: 1.1664 - mae: 0.8441 - val_loss: 1.1938 - val_mse: 1.1938 - val_mae: 0.8370\n",
            "Epoch 28/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 1.1325 - mse: 1.1325 - mae: 0.8393 - val_loss: 1.1842 - val_mse: 1.1842 - val_mae: 0.8333\n",
            "Epoch 29/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.0896 - mse: 1.0896 - mae: 0.8209 - val_loss: 1.1271 - val_mse: 1.1271 - val_mae: 0.8164\n",
            "Epoch 30/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.0457 - mse: 1.0457 - mae: 0.8118 - val_loss: 1.1109 - val_mse: 1.1109 - val_mae: 0.8065\n",
            "Epoch 31/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.0520 - mse: 1.0520 - mae: 0.8155 - val_loss: 1.0945 - val_mse: 1.0945 - val_mae: 0.8023\n",
            "Epoch 32/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9900 - mse: 0.9900 - mae: 0.7854 - val_loss: 1.0837 - val_mse: 1.0837 - val_mae: 0.7912\n",
            "Epoch 33/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9780 - mse: 0.9780 - mae: 0.7878 - val_loss: 1.0434 - val_mse: 1.0434 - val_mae: 0.7833\n",
            "Epoch 34/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9682 - mse: 0.9682 - mae: 0.7699 - val_loss: 1.0052 - val_mse: 1.0052 - val_mae: 0.7685\n",
            "Epoch 35/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9252 - mse: 0.9252 - mae: 0.7641 - val_loss: 0.9968 - val_mse: 0.9968 - val_mae: 0.7661\n",
            "Epoch 36/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.9182 - mse: 0.9182 - mae: 0.7479 - val_loss: 0.9657 - val_mse: 0.9657 - val_mae: 0.7519\n",
            "Epoch 37/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.8580 - mse: 0.8580 - mae: 0.7331 - val_loss: 0.9578 - val_mse: 0.9578 - val_mae: 0.7474\n",
            "Epoch 38/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8495 - mse: 0.8495 - mae: 0.7252 - val_loss: 0.9320 - val_mse: 0.9320 - val_mae: 0.7406\n",
            "Epoch 39/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.8361 - mse: 0.8361 - mae: 0.7267 - val_loss: 0.9077 - val_mse: 0.9077 - val_mae: 0.7297\n",
            "Epoch 40/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.8631 - mse: 0.8631 - mae: 0.7372 - val_loss: 0.8675 - val_mse: 0.8675 - val_mae: 0.7155\n",
            "Epoch 41/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8499 - mse: 0.8499 - mae: 0.7159 - val_loss: 0.8591 - val_mse: 0.8591 - val_mae: 0.7123\n",
            "Epoch 42/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7708 - mse: 0.7708 - mae: 0.6854 - val_loss: 0.8528 - val_mse: 0.8528 - val_mae: 0.7084\n",
            "Epoch 43/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.8280 - mse: 0.8280 - mae: 0.7228 - val_loss: 0.8301 - val_mse: 0.8301 - val_mae: 0.6989\n",
            "Epoch 44/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.8151 - mse: 0.8151 - mae: 0.7049 - val_loss: 0.8125 - val_mse: 0.8125 - val_mae: 0.6940\n",
            "Epoch 45/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7581 - mse: 0.7581 - mae: 0.6861 - val_loss: 0.8029 - val_mse: 0.8029 - val_mae: 0.6856\n",
            "Epoch 46/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7367 - mse: 0.7367 - mae: 0.6860 - val_loss: 0.7941 - val_mse: 0.7941 - val_mae: 0.6857\n",
            "Epoch 47/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.7395 - mse: 0.7395 - mae: 0.6805 - val_loss: 0.7671 - val_mse: 0.7671 - val_mae: 0.6692\n",
            "Epoch 48/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7365 - mse: 0.7365 - mae: 0.6777 - val_loss: 0.7496 - val_mse: 0.7496 - val_mae: 0.6645\n",
            "Epoch 49/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7180 - mse: 0.7180 - mae: 0.6728 - val_loss: 0.7343 - val_mse: 0.7343 - val_mae: 0.6569\n",
            "Epoch 50/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7444 - mse: 0.7444 - mae: 0.6905 - val_loss: 0.7255 - val_mse: 0.7255 - val_mae: 0.6553\n",
            "Epoch 51/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6680 - mse: 0.6680 - mae: 0.6468 - val_loss: 0.6932 - val_mse: 0.6932 - val_mae: 0.6412\n",
            "Epoch 52/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.7357 - mse: 0.7357 - mae: 0.6777 - val_loss: 0.7140 - val_mse: 0.7140 - val_mae: 0.6482\n",
            "Epoch 53/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6779 - mse: 0.6779 - mae: 0.6535 - val_loss: 0.7026 - val_mse: 0.7026 - val_mae: 0.6398\n",
            "Epoch 54/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.6793 - mse: 0.6793 - mae: 0.6499 - val_loss: 0.6926 - val_mse: 0.6926 - val_mae: 0.6341\n",
            "Epoch 55/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6604 - mse: 0.6604 - mae: 0.6403 - val_loss: 0.6657 - val_mse: 0.6657 - val_mae: 0.6264\n",
            "Epoch 56/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6493 - mse: 0.6493 - mae: 0.6379 - val_loss: 0.6732 - val_mse: 0.6732 - val_mae: 0.6329\n",
            "Epoch 57/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.6416 - mse: 0.6416 - mae: 0.6345 - val_loss: 0.6810 - val_mse: 0.6810 - val_mae: 0.6332\n",
            "Epoch 58/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6178 - mse: 0.6178 - mae: 0.6092 - val_loss: 0.6618 - val_mse: 0.6618 - val_mae: 0.6273\n",
            "Epoch 59/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6201 - mse: 0.6201 - mae: 0.6202 - val_loss: 0.6694 - val_mse: 0.6694 - val_mae: 0.6285\n",
            "Epoch 60/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6234 - mse: 0.6234 - mae: 0.6229 - val_loss: 0.6329 - val_mse: 0.6329 - val_mae: 0.6169\n",
            "Epoch 61/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.6095 - mse: 0.6095 - mae: 0.6126 - val_loss: 0.6288 - val_mse: 0.6288 - val_mae: 0.6161\n",
            "Epoch 62/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.6601 - mse: 0.6601 - mae: 0.6440 - val_loss: 0.6032 - val_mse: 0.6032 - val_mae: 0.5983\n",
            "Epoch 63/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5726 - mse: 0.5726 - mae: 0.5931 - val_loss: 0.6328 - val_mse: 0.6328 - val_mae: 0.6122\n",
            "Epoch 64/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5755 - mse: 0.5755 - mae: 0.6047 - val_loss: 0.6022 - val_mse: 0.6022 - val_mae: 0.6024\n",
            "Epoch 65/1000\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5648 - mse: 0.5648 - mae: 0.5949 - val_loss: 0.6084 - val_mse: 0.6084 - val_mae: 0.5993\n",
            "Epoch 66/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5616 - mse: 0.5616 - mae: 0.5981 - val_loss: 0.6039 - val_mse: 0.6039 - val_mae: 0.5992\n",
            "Epoch 67/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5604 - mse: 0.5604 - mae: 0.5914 - val_loss: 0.6060 - val_mse: 0.6060 - val_mae: 0.6030\n",
            "Epoch 68/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.5774 - mse: 0.5774 - mae: 0.6097 - val_loss: 0.5742 - val_mse: 0.5742 - val_mae: 0.5929\n",
            "Epoch 69/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5583 - mse: 0.5583 - mae: 0.5981 - val_loss: 0.5681 - val_mse: 0.5681 - val_mae: 0.5849\n",
            "Epoch 70/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5654 - mse: 0.5654 - mae: 0.6038 - val_loss: 0.5703 - val_mse: 0.5703 - val_mae: 0.5872\n",
            "Epoch 71/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5518 - mse: 0.5518 - mae: 0.5845 - val_loss: 0.5675 - val_mse: 0.5675 - val_mae: 0.5860\n",
            "Epoch 72/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5323 - mse: 0.5323 - mae: 0.5825 - val_loss: 0.5793 - val_mse: 0.5793 - val_mae: 0.5953\n",
            "Epoch 73/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5547 - mse: 0.5547 - mae: 0.5954 - val_loss: 0.5620 - val_mse: 0.5620 - val_mae: 0.5854\n",
            "Epoch 74/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.5134 - mse: 0.5134 - mae: 0.5731 - val_loss: 0.5495 - val_mse: 0.5495 - val_mae: 0.5812\n",
            "Epoch 75/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5260 - mse: 0.5260 - mae: 0.5749 - val_loss: 0.5605 - val_mse: 0.5605 - val_mae: 0.5810\n",
            "Epoch 76/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.5161 - mse: 0.5161 - mae: 0.5727 - val_loss: 0.5392 - val_mse: 0.5392 - val_mae: 0.5733\n",
            "Epoch 77/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4928 - mse: 0.4928 - mae: 0.5569 - val_loss: 0.5364 - val_mse: 0.5364 - val_mae: 0.5721\n",
            "Epoch 78/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5018 - mse: 0.5018 - mae: 0.5548 - val_loss: 0.5443 - val_mse: 0.5443 - val_mae: 0.5760\n",
            "Epoch 79/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.5372 - mse: 0.5372 - mae: 0.5883 - val_loss: 0.5326 - val_mse: 0.5326 - val_mae: 0.5721\n",
            "Epoch 80/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4917 - mse: 0.4917 - mae: 0.5495 - val_loss: 0.5381 - val_mse: 0.5381 - val_mae: 0.5731\n",
            "Epoch 81/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4721 - mse: 0.4721 - mae: 0.5432 - val_loss: 0.5214 - val_mse: 0.5214 - val_mae: 0.5679\n",
            "Epoch 82/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4878 - mse: 0.4878 - mae: 0.5545 - val_loss: 0.5280 - val_mse: 0.5280 - val_mae: 0.5688\n",
            "Epoch 83/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4959 - mse: 0.4959 - mae: 0.5574 - val_loss: 0.5300 - val_mse: 0.5300 - val_mae: 0.5660\n",
            "Epoch 84/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5019 - mse: 0.5019 - mae: 0.5644 - val_loss: 0.5215 - val_mse: 0.5215 - val_mae: 0.5551\n",
            "Epoch 85/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4657 - mse: 0.4657 - mae: 0.5440 - val_loss: 0.5253 - val_mse: 0.5253 - val_mae: 0.5603\n",
            "Epoch 86/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4845 - mse: 0.4845 - mae: 0.5485 - val_loss: 0.5085 - val_mse: 0.5085 - val_mae: 0.5531\n",
            "Epoch 87/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4806 - mse: 0.4806 - mae: 0.5525 - val_loss: 0.5207 - val_mse: 0.5207 - val_mae: 0.5651\n",
            "Epoch 88/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4953 - mse: 0.4953 - mae: 0.5521 - val_loss: 0.5023 - val_mse: 0.5023 - val_mae: 0.5454\n",
            "Epoch 89/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4700 - mse: 0.4700 - mae: 0.5504 - val_loss: 0.5164 - val_mse: 0.5164 - val_mae: 0.5521\n",
            "Epoch 90/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4797 - mse: 0.4797 - mae: 0.5472 - val_loss: 0.5069 - val_mse: 0.5069 - val_mae: 0.5475\n",
            "Epoch 91/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4990 - mse: 0.4990 - mae: 0.5604 - val_loss: 0.5106 - val_mse: 0.5106 - val_mae: 0.5587\n",
            "Epoch 92/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4540 - mse: 0.4540 - mae: 0.5359 - val_loss: 0.5093 - val_mse: 0.5093 - val_mae: 0.5578\n",
            "Epoch 93/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4368 - mse: 0.4368 - mae: 0.5271 - val_loss: 0.5021 - val_mse: 0.5021 - val_mae: 0.5565\n",
            "Epoch 94/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4565 - mse: 0.4565 - mae: 0.5357 - val_loss: 0.4968 - val_mse: 0.4968 - val_mae: 0.5450\n",
            "Epoch 95/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4734 - mse: 0.4734 - mae: 0.5419 - val_loss: 0.5157 - val_mse: 0.5157 - val_mae: 0.5549\n",
            "Epoch 96/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4388 - mse: 0.4388 - mae: 0.5332 - val_loss: 0.4919 - val_mse: 0.4919 - val_mae: 0.5433\n",
            "Epoch 97/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4447 - mse: 0.4447 - mae: 0.5234 - val_loss: 0.4964 - val_mse: 0.4964 - val_mae: 0.5538\n",
            "Epoch 98/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4432 - mse: 0.4432 - mae: 0.5247 - val_loss: 0.4705 - val_mse: 0.4705 - val_mae: 0.5319\n",
            "Epoch 99/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4366 - mse: 0.4366 - mae: 0.5302 - val_loss: 0.5059 - val_mse: 0.5059 - val_mae: 0.5559\n",
            "Epoch 100/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4467 - mse: 0.4467 - mae: 0.5355 - val_loss: 0.4703 - val_mse: 0.4703 - val_mae: 0.5406\n",
            "Epoch 101/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4493 - mse: 0.4493 - mae: 0.5281 - val_loss: 0.4922 - val_mse: 0.4922 - val_mae: 0.5434\n",
            "Epoch 102/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4553 - mse: 0.4553 - mae: 0.5358 - val_loss: 0.4952 - val_mse: 0.4952 - val_mae: 0.5498\n",
            "Epoch 103/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4507 - mse: 0.4507 - mae: 0.5379 - val_loss: 0.4865 - val_mse: 0.4865 - val_mae: 0.5406\n",
            "Epoch 104/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4172 - mse: 0.4172 - mae: 0.5116 - val_loss: 0.4928 - val_mse: 0.4928 - val_mae: 0.5491\n",
            "Epoch 105/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4240 - mse: 0.4240 - mae: 0.5209 - val_loss: 0.4860 - val_mse: 0.4860 - val_mae: 0.5323\n",
            "Epoch 106/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4190 - mse: 0.4190 - mae: 0.5018 - val_loss: 0.4937 - val_mse: 0.4937 - val_mae: 0.5481\n",
            "Epoch 107/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4431 - mse: 0.4431 - mae: 0.5293 - val_loss: 0.4924 - val_mse: 0.4924 - val_mae: 0.5452\n",
            "Epoch 108/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4417 - mse: 0.4417 - mae: 0.5155 - val_loss: 0.4952 - val_mse: 0.4952 - val_mae: 0.5458\n",
            "Epoch 109/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4302 - mse: 0.4302 - mae: 0.5333 - val_loss: 0.4664 - val_mse: 0.4664 - val_mae: 0.5325\n",
            "Epoch 110/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4042 - mse: 0.4042 - mae: 0.5072 - val_loss: 0.4782 - val_mse: 0.4782 - val_mae: 0.5407\n",
            "Epoch 111/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4356 - mse: 0.4356 - mae: 0.5250 - val_loss: 0.4616 - val_mse: 0.4616 - val_mae: 0.5199\n",
            "Epoch 112/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4266 - mse: 0.4266 - mae: 0.5158 - val_loss: 0.4993 - val_mse: 0.4993 - val_mae: 0.5506\n",
            "Epoch 113/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4111 - mse: 0.4111 - mae: 0.5119 - val_loss: 0.4628 - val_mse: 0.4628 - val_mae: 0.5302\n",
            "Epoch 114/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4264 - mse: 0.4264 - mae: 0.5140 - val_loss: 0.4788 - val_mse: 0.4788 - val_mae: 0.5388\n",
            "Epoch 115/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4380 - mse: 0.4380 - mae: 0.5246 - val_loss: 0.4646 - val_mse: 0.4646 - val_mae: 0.5308\n",
            "Epoch 116/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4126 - mse: 0.4126 - mae: 0.5224 - val_loss: 0.4772 - val_mse: 0.4772 - val_mae: 0.5373\n",
            "Epoch 117/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3938 - mse: 0.3938 - mae: 0.4899 - val_loss: 0.4625 - val_mse: 0.4625 - val_mae: 0.5245\n",
            "Epoch 118/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4082 - mse: 0.4082 - mae: 0.5068 - val_loss: 0.4822 - val_mse: 0.4822 - val_mae: 0.5492\n",
            "Epoch 119/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4171 - mse: 0.4171 - mae: 0.5124 - val_loss: 0.4553 - val_mse: 0.4553 - val_mae: 0.5207\n",
            "Epoch 120/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3914 - mse: 0.3914 - mae: 0.5001 - val_loss: 0.4590 - val_mse: 0.4590 - val_mae: 0.5261\n",
            "Epoch 121/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3922 - mse: 0.3922 - mae: 0.4965 - val_loss: 0.4729 - val_mse: 0.4729 - val_mae: 0.5313\n",
            "Epoch 122/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.4022 - mse: 0.4022 - mae: 0.4984 - val_loss: 0.4541 - val_mse: 0.4541 - val_mae: 0.5248\n",
            "Epoch 123/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.4078 - mse: 0.4078 - mae: 0.5109 - val_loss: 0.4596 - val_mse: 0.4596 - val_mae: 0.5203\n",
            "Epoch 124/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4386 - mse: 0.4386 - mae: 0.5175 - val_loss: 0.4672 - val_mse: 0.4672 - val_mae: 0.5306\n",
            "Epoch 125/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3854 - mse: 0.3854 - mae: 0.4857 - val_loss: 0.4663 - val_mse: 0.4663 - val_mae: 0.5231\n",
            "Epoch 126/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3988 - mse: 0.3988 - mae: 0.4968 - val_loss: 0.4618 - val_mse: 0.4618 - val_mae: 0.5281\n",
            "Epoch 127/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3959 - mse: 0.3959 - mae: 0.5003 - val_loss: 0.4775 - val_mse: 0.4775 - val_mae: 0.5350\n",
            "Epoch 128/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3720 - mse: 0.3720 - mae: 0.4808 - val_loss: 0.4490 - val_mse: 0.4490 - val_mae: 0.5226\n",
            "Epoch 129/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3785 - mse: 0.3785 - mae: 0.4904 - val_loss: 0.4514 - val_mse: 0.4514 - val_mae: 0.5199\n",
            "Epoch 130/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3939 - mse: 0.3939 - mae: 0.5036 - val_loss: 0.4916 - val_mse: 0.4916 - val_mae: 0.5447\n",
            "Epoch 131/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3787 - mse: 0.3787 - mae: 0.4901 - val_loss: 0.4571 - val_mse: 0.4571 - val_mae: 0.5151\n",
            "Epoch 132/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3736 - mse: 0.3736 - mae: 0.4876 - val_loss: 0.4581 - val_mse: 0.4581 - val_mae: 0.5276\n",
            "Epoch 133/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3877 - mse: 0.3877 - mae: 0.4940 - val_loss: 0.4564 - val_mse: 0.4564 - val_mae: 0.5287\n",
            "Epoch 134/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3844 - mse: 0.3844 - mae: 0.4891 - val_loss: 0.4640 - val_mse: 0.4640 - val_mae: 0.5232\n",
            "Epoch 135/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3804 - mse: 0.3804 - mae: 0.4856 - val_loss: 0.4600 - val_mse: 0.4600 - val_mae: 0.5254\n",
            "Epoch 136/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3662 - mse: 0.3662 - mae: 0.4797 - val_loss: 0.4541 - val_mse: 0.4541 - val_mae: 0.5148\n",
            "Epoch 137/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3824 - mse: 0.3824 - mae: 0.4921 - val_loss: 0.4527 - val_mse: 0.4527 - val_mae: 0.5200\n",
            "Epoch 138/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3905 - mse: 0.3905 - mae: 0.4906 - val_loss: 0.4618 - val_mse: 0.4618 - val_mae: 0.5239\n",
            "Epoch 139/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3772 - mse: 0.3772 - mae: 0.4806 - val_loss: 0.4667 - val_mse: 0.4667 - val_mae: 0.5277\n",
            "Epoch 140/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3832 - mse: 0.3832 - mae: 0.4845 - val_loss: 0.4449 - val_mse: 0.4449 - val_mae: 0.5104\n",
            "Epoch 141/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3753 - mse: 0.3753 - mae: 0.4846 - val_loss: 0.4610 - val_mse: 0.4610 - val_mae: 0.5184\n",
            "Epoch 142/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3917 - mse: 0.3917 - mae: 0.4929 - val_loss: 0.4639 - val_mse: 0.4639 - val_mae: 0.5304\n",
            "Epoch 143/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3759 - mse: 0.3759 - mae: 0.4775 - val_loss: 0.4486 - val_mse: 0.4486 - val_mae: 0.5140\n",
            "Epoch 144/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3938 - mse: 0.3938 - mae: 0.4937 - val_loss: 0.4614 - val_mse: 0.4614 - val_mae: 0.5283\n",
            "Epoch 145/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3766 - mse: 0.3766 - mae: 0.4868 - val_loss: 0.4665 - val_mse: 0.4665 - val_mae: 0.5209\n",
            "Epoch 146/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3995 - mse: 0.3995 - mae: 0.4937 - val_loss: 0.4457 - val_mse: 0.4457 - val_mae: 0.5206\n",
            "Epoch 147/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3802 - mse: 0.3802 - mae: 0.4844 - val_loss: 0.4875 - val_mse: 0.4875 - val_mae: 0.5447\n",
            "Epoch 148/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3659 - mse: 0.3659 - mae: 0.4827 - val_loss: 0.4493 - val_mse: 0.4493 - val_mae: 0.5173\n",
            "Epoch 149/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3642 - mse: 0.3642 - mae: 0.4754 - val_loss: 0.4653 - val_mse: 0.4653 - val_mae: 0.5387\n",
            "Epoch 150/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3542 - mse: 0.3542 - mae: 0.4727 - val_loss: 0.4702 - val_mse: 0.4702 - val_mae: 0.5355\n",
            "Epoch 151/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3766 - mse: 0.3766 - mae: 0.4875 - val_loss: 0.4402 - val_mse: 0.4402 - val_mae: 0.5159\n",
            "Epoch 152/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3531 - mse: 0.3531 - mae: 0.4669 - val_loss: 0.4414 - val_mse: 0.4414 - val_mae: 0.5107\n",
            "Epoch 153/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3701 - mse: 0.3701 - mae: 0.4814 - val_loss: 0.4488 - val_mse: 0.4488 - val_mae: 0.5140\n",
            "Epoch 154/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3574 - mse: 0.3574 - mae: 0.4692 - val_loss: 0.4528 - val_mse: 0.4528 - val_mae: 0.5277\n",
            "Epoch 155/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3742 - mse: 0.3742 - mae: 0.4827 - val_loss: 0.4510 - val_mse: 0.4510 - val_mae: 0.5298\n",
            "Epoch 156/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3860 - mse: 0.3860 - mae: 0.4940 - val_loss: 0.4453 - val_mse: 0.4453 - val_mae: 0.5199\n",
            "Epoch 157/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3573 - mse: 0.3573 - mae: 0.4739 - val_loss: 0.4445 - val_mse: 0.4445 - val_mae: 0.5199\n",
            "Epoch 158/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3626 - mse: 0.3626 - mae: 0.4766 - val_loss: 0.4330 - val_mse: 0.4330 - val_mae: 0.5125\n",
            "Epoch 159/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3597 - mse: 0.3597 - mae: 0.4725 - val_loss: 0.4431 - val_mse: 0.4431 - val_mae: 0.5140\n",
            "Epoch 160/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3464 - mse: 0.3464 - mae: 0.4589 - val_loss: 0.4591 - val_mse: 0.4591 - val_mae: 0.5248\n",
            "Epoch 161/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3678 - mse: 0.3678 - mae: 0.4797 - val_loss: 0.4764 - val_mse: 0.4764 - val_mae: 0.5414\n",
            "Epoch 162/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3700 - mse: 0.3700 - mae: 0.4803 - val_loss: 0.4453 - val_mse: 0.4453 - val_mae: 0.5149\n",
            "Epoch 163/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3795 - mse: 0.3795 - mae: 0.4770 - val_loss: 0.4466 - val_mse: 0.4466 - val_mae: 0.5188\n",
            "Epoch 164/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3609 - mse: 0.3609 - mae: 0.4700 - val_loss: 0.4392 - val_mse: 0.4392 - val_mae: 0.5140\n",
            "Epoch 165/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3533 - mse: 0.3533 - mae: 0.4695 - val_loss: 0.4439 - val_mse: 0.4439 - val_mae: 0.5158\n",
            "Epoch 166/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3648 - mse: 0.3648 - mae: 0.4777 - val_loss: 0.4353 - val_mse: 0.4353 - val_mae: 0.5107\n",
            "Epoch 167/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3589 - mse: 0.3589 - mae: 0.4766 - val_loss: 0.4386 - val_mse: 0.4386 - val_mae: 0.5107\n",
            "Epoch 168/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3614 - mse: 0.3614 - mae: 0.4757 - val_loss: 0.4429 - val_mse: 0.4429 - val_mae: 0.5035\n",
            "Epoch 169/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3496 - mse: 0.3496 - mae: 0.4637 - val_loss: 0.4329 - val_mse: 0.4329 - val_mae: 0.5176\n",
            "Epoch 170/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3561 - mse: 0.3561 - mae: 0.4682 - val_loss: 0.4496 - val_mse: 0.4496 - val_mae: 0.5230\n",
            "Epoch 171/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3508 - mse: 0.3508 - mae: 0.4623 - val_loss: 0.4612 - val_mse: 0.4612 - val_mae: 0.5355\n",
            "Epoch 172/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3595 - mse: 0.3595 - mae: 0.4686 - val_loss: 0.4491 - val_mse: 0.4491 - val_mae: 0.5173\n",
            "Epoch 173/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3449 - mse: 0.3449 - mae: 0.4668 - val_loss: 0.4477 - val_mse: 0.4477 - val_mae: 0.5279\n",
            "Epoch 174/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3410 - mse: 0.3410 - mae: 0.4592 - val_loss: 0.4424 - val_mse: 0.4424 - val_mae: 0.5129\n",
            "Epoch 175/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3466 - mse: 0.3466 - mae: 0.4642 - val_loss: 0.4572 - val_mse: 0.4572 - val_mae: 0.5244\n",
            "Epoch 176/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3405 - mse: 0.3405 - mae: 0.4599 - val_loss: 0.4420 - val_mse: 0.4420 - val_mae: 0.5116\n",
            "Epoch 177/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3552 - mse: 0.3552 - mae: 0.4710 - val_loss: 0.4434 - val_mse: 0.4434 - val_mae: 0.5231\n",
            "Epoch 178/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3496 - mse: 0.3496 - mae: 0.4612 - val_loss: 0.4700 - val_mse: 0.4700 - val_mae: 0.5196\n",
            "Epoch 179/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3589 - mse: 0.3589 - mae: 0.4782 - val_loss: 0.4362 - val_mse: 0.4362 - val_mae: 0.5061\n",
            "Epoch 180/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3426 - mse: 0.3426 - mae: 0.4645 - val_loss: 0.4523 - val_mse: 0.4523 - val_mae: 0.5222\n",
            "Epoch 181/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3476 - mse: 0.3476 - mae: 0.4590 - val_loss: 0.4383 - val_mse: 0.4383 - val_mae: 0.5149\n",
            "Epoch 182/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3477 - mse: 0.3477 - mae: 0.4666 - val_loss: 0.4445 - val_mse: 0.4445 - val_mae: 0.5225\n",
            "Epoch 183/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3363 - mse: 0.3363 - mae: 0.4530 - val_loss: 0.4499 - val_mse: 0.4499 - val_mae: 0.5213\n",
            "Epoch 184/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3652 - mse: 0.3652 - mae: 0.4762 - val_loss: 0.4596 - val_mse: 0.4596 - val_mae: 0.5356\n",
            "Epoch 185/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3408 - mse: 0.3408 - mae: 0.4590 - val_loss: 0.4350 - val_mse: 0.4350 - val_mae: 0.5148\n",
            "Epoch 186/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3564 - mse: 0.3564 - mae: 0.4619 - val_loss: 0.4412 - val_mse: 0.4412 - val_mae: 0.5256\n",
            "Epoch 187/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3509 - mse: 0.3509 - mae: 0.4628 - val_loss: 0.4555 - val_mse: 0.4555 - val_mae: 0.5315\n",
            "Epoch 188/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3461 - mse: 0.3461 - mae: 0.4597 - val_loss: 0.4558 - val_mse: 0.4558 - val_mae: 0.5337\n",
            "Epoch 189/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3516 - mse: 0.3516 - mae: 0.4669 - val_loss: 0.4344 - val_mse: 0.4344 - val_mae: 0.5111\n",
            "Epoch 190/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3407 - mse: 0.3407 - mae: 0.4580 - val_loss: 0.4499 - val_mse: 0.4499 - val_mae: 0.5304\n",
            "Epoch 191/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3421 - mse: 0.3421 - mae: 0.4615 - val_loss: 0.4390 - val_mse: 0.4390 - val_mae: 0.5138\n",
            "Epoch 192/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3408 - mse: 0.3408 - mae: 0.4558 - val_loss: 0.4434 - val_mse: 0.4434 - val_mae: 0.5083\n",
            "Epoch 193/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3475 - mse: 0.3475 - mae: 0.4645 - val_loss: 0.4401 - val_mse: 0.4401 - val_mae: 0.5164\n",
            "Epoch 194/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3188 - mse: 0.3188 - mae: 0.4483 - val_loss: 0.4516 - val_mse: 0.4516 - val_mae: 0.5203\n",
            "Epoch 195/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3409 - mse: 0.3409 - mae: 0.4606 - val_loss: 0.4393 - val_mse: 0.4393 - val_mae: 0.5189\n",
            "Epoch 196/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3452 - mse: 0.3452 - mae: 0.4581 - val_loss: 0.4430 - val_mse: 0.4430 - val_mae: 0.5095\n",
            "Epoch 197/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3426 - mse: 0.3426 - mae: 0.4607 - val_loss: 0.4417 - val_mse: 0.4417 - val_mae: 0.5159\n",
            "Epoch 198/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3275 - mse: 0.3275 - mae: 0.4471 - val_loss: 0.4557 - val_mse: 0.4557 - val_mae: 0.5288\n",
            "Epoch 199/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3454 - mse: 0.3454 - mae: 0.4578 - val_loss: 0.4345 - val_mse: 0.4345 - val_mae: 0.5129\n",
            "Epoch 200/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3463 - mse: 0.3463 - mae: 0.4639 - val_loss: 0.4333 - val_mse: 0.4333 - val_mae: 0.5111\n",
            "Epoch 201/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3308 - mse: 0.3308 - mae: 0.4546 - val_loss: 0.4316 - val_mse: 0.4316 - val_mae: 0.5105\n",
            "Epoch 202/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3409 - mse: 0.3409 - mae: 0.4558 - val_loss: 0.4446 - val_mse: 0.4446 - val_mae: 0.5230\n",
            "Epoch 203/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3407 - mse: 0.3407 - mae: 0.4605 - val_loss: 0.4360 - val_mse: 0.4360 - val_mae: 0.5204\n",
            "Epoch 204/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3274 - mse: 0.3274 - mae: 0.4566 - val_loss: 0.4345 - val_mse: 0.4345 - val_mae: 0.5190\n",
            "Epoch 205/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3306 - mse: 0.3306 - mae: 0.4511 - val_loss: 0.4525 - val_mse: 0.4525 - val_mae: 0.5273\n",
            "Epoch 206/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3268 - mse: 0.3268 - mae: 0.4570 - val_loss: 0.4278 - val_mse: 0.4278 - val_mae: 0.5036\n",
            "Epoch 207/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3367 - mse: 0.3367 - mae: 0.4556 - val_loss: 0.4443 - val_mse: 0.4443 - val_mae: 0.5162\n",
            "Epoch 208/1000\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.3424 - mse: 0.3424 - mae: 0.4644 - val_loss: 0.4371 - val_mse: 0.4371 - val_mae: 0.5183\n",
            "Epoch 209/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3240 - mse: 0.3240 - mae: 0.4483 - val_loss: 0.4382 - val_mse: 0.4382 - val_mae: 0.5157\n",
            "Epoch 210/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3338 - mse: 0.3338 - mae: 0.4570 - val_loss: 0.4370 - val_mse: 0.4370 - val_mae: 0.5141\n",
            "Epoch 211/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3221 - mse: 0.3221 - mae: 0.4448 - val_loss: 0.4339 - val_mse: 0.4339 - val_mae: 0.5097\n",
            "Epoch 212/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3317 - mse: 0.3317 - mae: 0.4451 - val_loss: 0.4413 - val_mse: 0.4413 - val_mae: 0.5184\n",
            "Epoch 213/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3343 - mse: 0.3343 - mae: 0.4469 - val_loss: 0.4412 - val_mse: 0.4412 - val_mae: 0.5146\n",
            "Epoch 214/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3140 - mse: 0.3140 - mae: 0.4398 - val_loss: 0.4380 - val_mse: 0.4380 - val_mae: 0.5220\n",
            "Epoch 215/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3181 - mse: 0.3181 - mae: 0.4456 - val_loss: 0.4455 - val_mse: 0.4455 - val_mae: 0.5185\n",
            "Epoch 216/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3140 - mse: 0.3140 - mae: 0.4360 - val_loss: 0.4368 - val_mse: 0.4368 - val_mae: 0.5165\n",
            "Epoch 217/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3264 - mse: 0.3264 - mae: 0.4491 - val_loss: 0.4437 - val_mse: 0.4437 - val_mae: 0.5073\n",
            "Epoch 218/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3352 - mse: 0.3352 - mae: 0.4532 - val_loss: 0.4488 - val_mse: 0.4488 - val_mae: 0.5146\n",
            "Epoch 219/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3299 - mse: 0.3299 - mae: 0.4461 - val_loss: 0.4383 - val_mse: 0.4383 - val_mae: 0.5067\n",
            "Epoch 220/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3107 - mse: 0.3107 - mae: 0.4417 - val_loss: 0.4574 - val_mse: 0.4574 - val_mae: 0.5332\n",
            "Epoch 221/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3147 - mse: 0.3147 - mae: 0.4385 - val_loss: 0.4549 - val_mse: 0.4549 - val_mae: 0.5294\n",
            "Epoch 222/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3244 - mse: 0.3244 - mae: 0.4494 - val_loss: 0.4607 - val_mse: 0.4607 - val_mae: 0.5218\n",
            "Epoch 223/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3232 - mse: 0.3232 - mae: 0.4486 - val_loss: 0.4738 - val_mse: 0.4738 - val_mae: 0.5393\n",
            "Epoch 224/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3289 - mse: 0.3289 - mae: 0.4472 - val_loss: 0.4625 - val_mse: 0.4625 - val_mae: 0.5339\n",
            "Epoch 225/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3385 - mse: 0.3385 - mae: 0.4580 - val_loss: 0.4491 - val_mse: 0.4491 - val_mae: 0.5081\n",
            "Epoch 226/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3295 - mse: 0.3295 - mae: 0.4582 - val_loss: 0.4803 - val_mse: 0.4803 - val_mae: 0.5194\n",
            "Epoch 227/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3314 - mse: 0.3314 - mae: 0.4595 - val_loss: 0.4548 - val_mse: 0.4548 - val_mae: 0.5217\n",
            "Epoch 228/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3161 - mse: 0.3161 - mae: 0.4382 - val_loss: 0.4577 - val_mse: 0.4577 - val_mae: 0.5115\n",
            "Epoch 229/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3192 - mse: 0.3192 - mae: 0.4445 - val_loss: 0.4411 - val_mse: 0.4411 - val_mae: 0.5204\n",
            "Epoch 230/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3200 - mse: 0.3200 - mae: 0.4475 - val_loss: 0.4526 - val_mse: 0.4526 - val_mae: 0.5221\n",
            "Epoch 231/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3360 - mse: 0.3360 - mae: 0.4563 - val_loss: 0.4480 - val_mse: 0.4480 - val_mae: 0.5165\n",
            "Epoch 232/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3182 - mse: 0.3182 - mae: 0.4463 - val_loss: 0.4395 - val_mse: 0.4395 - val_mae: 0.5074\n",
            "Epoch 233/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3078 - mse: 0.3078 - mae: 0.4310 - val_loss: 0.4498 - val_mse: 0.4498 - val_mae: 0.5139\n",
            "Epoch 234/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3248 - mse: 0.3248 - mae: 0.4489 - val_loss: 0.4501 - val_mse: 0.4501 - val_mae: 0.5165\n",
            "Epoch 235/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3134 - mse: 0.3134 - mae: 0.4359 - val_loss: 0.4524 - val_mse: 0.4524 - val_mae: 0.5171\n",
            "Epoch 236/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3278 - mse: 0.3278 - mae: 0.4515 - val_loss: 0.4471 - val_mse: 0.4471 - val_mae: 0.5203\n",
            "Epoch 237/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3275 - mse: 0.3275 - mae: 0.4571 - val_loss: 0.4741 - val_mse: 0.4741 - val_mae: 0.5390\n",
            "Epoch 238/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3211 - mse: 0.3211 - mae: 0.4380 - val_loss: 0.4713 - val_mse: 0.4713 - val_mae: 0.5365\n",
            "Epoch 239/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3182 - mse: 0.3182 - mae: 0.4333 - val_loss: 0.4487 - val_mse: 0.4487 - val_mae: 0.5093\n",
            "Epoch 240/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3337 - mse: 0.3337 - mae: 0.4548 - val_loss: 0.4630 - val_mse: 0.4630 - val_mae: 0.5110\n",
            "Epoch 241/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3231 - mse: 0.3231 - mae: 0.4476 - val_loss: 0.4461 - val_mse: 0.4461 - val_mae: 0.5157\n",
            "Epoch 242/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3234 - mse: 0.3234 - mae: 0.4395 - val_loss: 0.4486 - val_mse: 0.4486 - val_mae: 0.5136\n",
            "Epoch 243/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2990 - mse: 0.2990 - mae: 0.4238 - val_loss: 0.4500 - val_mse: 0.4500 - val_mae: 0.5240\n",
            "Epoch 244/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2980 - mse: 0.2980 - mae: 0.4317 - val_loss: 0.4365 - val_mse: 0.4365 - val_mae: 0.5056\n",
            "Epoch 245/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3068 - mse: 0.3068 - mae: 0.4346 - val_loss: 0.4386 - val_mse: 0.4386 - val_mae: 0.5087\n",
            "Epoch 246/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3149 - mse: 0.3149 - mae: 0.4399 - val_loss: 0.4396 - val_mse: 0.4396 - val_mae: 0.5117\n",
            "Epoch 247/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3240 - mse: 0.3240 - mae: 0.4478 - val_loss: 0.4541 - val_mse: 0.4541 - val_mae: 0.5206\n",
            "Epoch 248/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3120 - mse: 0.3120 - mae: 0.4395 - val_loss: 0.4344 - val_mse: 0.4344 - val_mae: 0.5029\n",
            "Epoch 249/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3051 - mse: 0.3051 - mae: 0.4340 - val_loss: 0.4383 - val_mse: 0.4383 - val_mae: 0.5107\n",
            "Epoch 250/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3160 - mse: 0.3160 - mae: 0.4409 - val_loss: 0.4381 - val_mse: 0.4381 - val_mae: 0.5091\n",
            "Epoch 251/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3100 - mse: 0.3100 - mae: 0.4361 - val_loss: 0.4550 - val_mse: 0.4550 - val_mae: 0.5128\n",
            "Epoch 252/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3062 - mse: 0.3062 - mae: 0.4329 - val_loss: 0.4722 - val_mse: 0.4722 - val_mae: 0.5460\n",
            "Epoch 253/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3267 - mse: 0.3267 - mae: 0.4486 - val_loss: 0.4427 - val_mse: 0.4427 - val_mae: 0.5202\n",
            "Epoch 254/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3125 - mse: 0.3125 - mae: 0.4433 - val_loss: 0.4480 - val_mse: 0.4480 - val_mae: 0.5247\n",
            "Epoch 255/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3058 - mse: 0.3058 - mae: 0.4392 - val_loss: 0.4390 - val_mse: 0.4390 - val_mae: 0.5132\n",
            "Epoch 256/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3060 - mse: 0.3060 - mae: 0.4336 - val_loss: 0.4537 - val_mse: 0.4537 - val_mae: 0.5073\n",
            "Epoch 257/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3096 - mse: 0.3096 - mae: 0.4389 - val_loss: 0.4803 - val_mse: 0.4803 - val_mae: 0.5189\n",
            "Epoch 258/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3109 - mse: 0.3109 - mae: 0.4394 - val_loss: 0.4437 - val_mse: 0.4437 - val_mae: 0.5172\n",
            "Epoch 259/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3140 - mse: 0.3140 - mae: 0.4394 - val_loss: 0.4505 - val_mse: 0.4505 - val_mae: 0.5228\n",
            "Epoch 260/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3078 - mse: 0.3078 - mae: 0.4409 - val_loss: 0.4556 - val_mse: 0.4556 - val_mae: 0.5164\n",
            "Epoch 261/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3032 - mse: 0.3032 - mae: 0.4279 - val_loss: 0.4480 - val_mse: 0.4480 - val_mae: 0.5123\n",
            "Epoch 262/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2894 - mse: 0.2894 - mae: 0.4242 - val_loss: 0.4505 - val_mse: 0.4505 - val_mae: 0.5136\n",
            "Epoch 263/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3123 - mse: 0.3123 - mae: 0.4389 - val_loss: 0.4405 - val_mse: 0.4405 - val_mae: 0.5122\n",
            "Epoch 264/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3136 - mse: 0.3136 - mae: 0.4467 - val_loss: 0.4568 - val_mse: 0.4568 - val_mae: 0.5251\n",
            "Epoch 265/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3008 - mse: 0.3008 - mae: 0.4330 - val_loss: 0.4494 - val_mse: 0.4494 - val_mae: 0.5113\n",
            "Epoch 266/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3012 - mse: 0.3012 - mae: 0.4336 - val_loss: 0.4547 - val_mse: 0.4547 - val_mae: 0.5163\n",
            "Epoch 267/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3274 - mse: 0.3274 - mae: 0.4500 - val_loss: 0.4472 - val_mse: 0.4472 - val_mae: 0.5247\n",
            "Epoch 268/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3150 - mse: 0.3150 - mae: 0.4430 - val_loss: 0.4640 - val_mse: 0.4640 - val_mae: 0.5174\n",
            "Epoch 269/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3077 - mse: 0.3077 - mae: 0.4302 - val_loss: 0.4386 - val_mse: 0.4386 - val_mae: 0.5116\n",
            "Epoch 270/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3007 - mse: 0.3007 - mae: 0.4295 - val_loss: 0.4532 - val_mse: 0.4532 - val_mae: 0.5213\n",
            "Epoch 271/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3132 - mse: 0.3132 - mae: 0.4425 - val_loss: 0.4576 - val_mse: 0.4576 - val_mae: 0.5262\n",
            "Epoch 272/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3096 - mse: 0.3096 - mae: 0.4356 - val_loss: 0.4475 - val_mse: 0.4475 - val_mae: 0.5152\n",
            "Epoch 273/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2869 - mse: 0.2869 - mae: 0.4230 - val_loss: 0.4714 - val_mse: 0.4714 - val_mae: 0.5160\n",
            "Epoch 274/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2977 - mse: 0.2977 - mae: 0.4210 - val_loss: 0.4506 - val_mse: 0.4506 - val_mae: 0.5248\n",
            "Epoch 275/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3311 - mse: 0.3311 - mae: 0.4527 - val_loss: 0.4586 - val_mse: 0.4586 - val_mae: 0.5179\n",
            "Epoch 276/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3142 - mse: 0.3142 - mae: 0.4360 - val_loss: 0.4616 - val_mse: 0.4616 - val_mae: 0.5157\n",
            "Epoch 277/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3046 - mse: 0.3046 - mae: 0.4317 - val_loss: 0.4522 - val_mse: 0.4522 - val_mae: 0.5159\n",
            "Epoch 278/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2952 - mse: 0.2952 - mae: 0.4274 - val_loss: 0.4497 - val_mse: 0.4497 - val_mae: 0.5197\n",
            "Epoch 279/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3048 - mse: 0.3048 - mae: 0.4322 - val_loss: 0.4546 - val_mse: 0.4546 - val_mae: 0.5251\n",
            "Epoch 280/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2971 - mse: 0.2971 - mae: 0.4324 - val_loss: 0.4560 - val_mse: 0.4560 - val_mae: 0.5164\n",
            "Epoch 281/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3164 - mse: 0.3164 - mae: 0.4373 - val_loss: 0.4521 - val_mse: 0.4521 - val_mae: 0.5196\n",
            "Epoch 282/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3103 - mse: 0.3103 - mae: 0.4399 - val_loss: 0.4574 - val_mse: 0.4574 - val_mae: 0.5259\n",
            "Epoch 283/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3084 - mse: 0.3084 - mae: 0.4365 - val_loss: 0.4765 - val_mse: 0.4765 - val_mae: 0.5339\n",
            "Epoch 284/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3042 - mse: 0.3042 - mae: 0.4343 - val_loss: 0.4497 - val_mse: 0.4497 - val_mae: 0.5075\n",
            "Epoch 285/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2846 - mse: 0.2846 - mae: 0.4206 - val_loss: 0.4557 - val_mse: 0.4557 - val_mae: 0.5090\n",
            "Epoch 286/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3140 - mse: 0.3140 - mae: 0.4421 - val_loss: 0.4565 - val_mse: 0.4565 - val_mae: 0.5196\n",
            "Epoch 287/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3004 - mse: 0.3004 - mae: 0.4339 - val_loss: 0.4483 - val_mse: 0.4483 - val_mae: 0.5033\n",
            "Epoch 288/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2959 - mse: 0.2959 - mae: 0.4244 - val_loss: 0.4590 - val_mse: 0.4590 - val_mae: 0.5049\n",
            "Epoch 289/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.3033 - mse: 0.3033 - mae: 0.4332 - val_loss: 0.4586 - val_mse: 0.4586 - val_mae: 0.5261\n",
            "Epoch 290/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3164 - mse: 0.3164 - mae: 0.4363 - val_loss: 0.4581 - val_mse: 0.4581 - val_mae: 0.5249\n",
            "Epoch 291/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2966 - mse: 0.2966 - mae: 0.4263 - val_loss: 0.4417 - val_mse: 0.4417 - val_mae: 0.5063\n",
            "Epoch 292/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3006 - mse: 0.3006 - mae: 0.4265 - val_loss: 0.4610 - val_mse: 0.4610 - val_mae: 0.5224\n",
            "Epoch 293/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3010 - mse: 0.3010 - mae: 0.4252 - val_loss: 0.4510 - val_mse: 0.4510 - val_mae: 0.5142\n",
            "Epoch 294/1000\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.2959 - mse: 0.2959 - mae: 0.4276 - val_loss: 0.4550 - val_mse: 0.4550 - val_mae: 0.5082\n",
            "Epoch 295/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2998 - mse: 0.2998 - mae: 0.4285 - val_loss: 0.4524 - val_mse: 0.4524 - val_mae: 0.5165\n",
            "Epoch 296/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2817 - mse: 0.2817 - mae: 0.4120 - val_loss: 0.4680 - val_mse: 0.4680 - val_mae: 0.5101\n",
            "Epoch 297/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2802 - mse: 0.2802 - mae: 0.4184 - val_loss: 0.4551 - val_mse: 0.4551 - val_mae: 0.5137\n",
            "Epoch 298/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2954 - mse: 0.2954 - mae: 0.4269 - val_loss: 0.4634 - val_mse: 0.4634 - val_mae: 0.5060\n",
            "Epoch 299/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2968 - mse: 0.2968 - mae: 0.4229 - val_loss: 0.4487 - val_mse: 0.4487 - val_mae: 0.5031\n",
            "Epoch 300/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3026 - mse: 0.3026 - mae: 0.4315 - val_loss: 0.4648 - val_mse: 0.4648 - val_mae: 0.5336\n",
            "Epoch 301/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2865 - mse: 0.2865 - mae: 0.4223 - val_loss: 0.4625 - val_mse: 0.4625 - val_mae: 0.5260\n",
            "Epoch 302/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.3094 - mse: 0.3094 - mae: 0.4363 - val_loss: 0.4661 - val_mse: 0.4661 - val_mae: 0.5303\n",
            "Epoch 303/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2779 - mse: 0.2779 - mae: 0.4106 - val_loss: 0.4552 - val_mse: 0.4552 - val_mae: 0.5269\n",
            "Epoch 304/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.3056 - mse: 0.3056 - mae: 0.4343 - val_loss: 0.4568 - val_mse: 0.4568 - val_mae: 0.5079\n",
            "Epoch 305/1000\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.2888 - mse: 0.2888 - mae: 0.4250 - val_loss: 0.4622 - val_mse: 0.4622 - val_mae: 0.5285\n",
            "Epoch 306/1000\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.2928 - mse: 0.2928 - mae: 0.4259 - val_loss: 0.4557 - val_mse: 0.4557 - val_mae: 0.5166\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.4525 - mse: 0.4525 - mae: 0.5014\n",
            "Epoch 1/1000\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 18.5618 - mse: 18.5618 - mae: 4.0879 - val_loss: 10.7151 - val_mse: 10.7151 - val_mae: 3.0511\n",
            "Epoch 2/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 7.2324 - mse: 7.2324 - mae: 2.3748 - val_loss: 4.7375 - val_mse: 4.7375 - val_mae: 1.8605\n",
            "Epoch 3/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 3.9233 - mse: 3.9233 - mae: 1.6471 - val_loss: 3.2401 - val_mse: 3.2401 - val_mae: 1.4482\n",
            "Epoch 4/1000\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 2.9193 - mse: 2.9193 - mae: 1.3883 - val_loss: 2.7074 - val_mse: 2.7074 - val_mae: 1.3218\n",
            "Epoch 5/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2.4501 - mse: 2.4501 - mae: 1.2694 - val_loss: 2.4693 - val_mse: 2.4693 - val_mae: 1.2595\n",
            "Epoch 6/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2.2877 - mse: 2.2877 - mae: 1.2249 - val_loss: 2.3053 - val_mse: 2.3053 - val_mae: 1.2130\n",
            "Epoch 7/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2.1175 - mse: 2.1175 - mae: 1.1660 - val_loss: 2.1565 - val_mse: 2.1565 - val_mae: 1.1685\n",
            "Epoch 8/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1.9813 - mse: 1.9813 - mae: 1.1282 - val_loss: 2.0367 - val_mse: 2.0367 - val_mae: 1.1368\n",
            "Epoch 9/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1.8998 - mse: 1.8998 - mae: 1.1131 - val_loss: 1.9141 - val_mse: 1.9141 - val_mae: 1.0992\n",
            "Epoch 10/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1.7571 - mse: 1.7571 - mae: 1.0628 - val_loss: 1.8248 - val_mse: 1.8248 - val_mae: 1.0724\n",
            "Epoch 11/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1.7134 - mse: 1.7134 - mae: 1.0381 - val_loss: 1.7510 - val_mse: 1.7510 - val_mae: 1.0460\n",
            "Epoch 12/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1.6399 - mse: 1.6399 - mae: 1.0223 - val_loss: 1.6747 - val_mse: 1.6747 - val_mae: 1.0228\n",
            "Epoch 13/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1.4974 - mse: 1.4974 - mae: 0.9737 - val_loss: 1.6102 - val_mse: 1.6102 - val_mae: 0.9993\n",
            "Epoch 14/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1.5011 - mse: 1.5011 - mae: 0.9756 - val_loss: 1.5356 - val_mse: 1.5356 - val_mae: 0.9752\n",
            "Epoch 15/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1.4775 - mse: 1.4775 - mae: 0.9786 - val_loss: 1.4692 - val_mse: 1.4692 - val_mae: 0.9534\n",
            "Epoch 16/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1.3483 - mse: 1.3483 - mae: 0.9285 - val_loss: 1.4014 - val_mse: 1.4014 - val_mae: 0.9273\n",
            "Epoch 17/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1.3294 - mse: 1.3294 - mae: 0.9204 - val_loss: 1.3751 - val_mse: 1.3751 - val_mae: 0.9201\n",
            "Epoch 18/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1.2830 - mse: 1.2830 - mae: 0.8948 - val_loss: 1.3136 - val_mse: 1.3136 - val_mae: 0.8924\n",
            "Epoch 19/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1.3237 - mse: 1.3237 - mae: 0.9194 - val_loss: 1.2552 - val_mse: 1.2552 - val_mae: 0.8713\n",
            "Epoch 20/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1.1615 - mse: 1.1615 - mae: 0.8690 - val_loss: 1.1962 - val_mse: 1.1962 - val_mae: 0.8451\n",
            "Epoch 21/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1.0977 - mse: 1.0977 - mae: 0.8276 - val_loss: 1.1569 - val_mse: 1.1569 - val_mae: 0.8269\n",
            "Epoch 22/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1.1119 - mse: 1.1119 - mae: 0.8427 - val_loss: 1.1177 - val_mse: 1.1177 - val_mae: 0.8166\n",
            "Epoch 23/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1.0684 - mse: 1.0684 - mae: 0.8134 - val_loss: 1.0739 - val_mse: 1.0739 - val_mae: 0.8001\n",
            "Epoch 24/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.9801 - mse: 0.9801 - mae: 0.7861 - val_loss: 1.0380 - val_mse: 1.0380 - val_mae: 0.7875\n",
            "Epoch 25/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1.0325 - mse: 1.0325 - mae: 0.8175 - val_loss: 1.0024 - val_mse: 1.0024 - val_mae: 0.7726\n",
            "Epoch 26/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.9605 - mse: 0.9605 - mae: 0.7809 - val_loss: 0.9622 - val_mse: 0.9622 - val_mae: 0.7561\n",
            "Epoch 27/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.9394 - mse: 0.9394 - mae: 0.7709 - val_loss: 0.9375 - val_mse: 0.9375 - val_mae: 0.7486\n",
            "Epoch 28/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.9228 - mse: 0.9228 - mae: 0.7684 - val_loss: 0.9066 - val_mse: 0.9066 - val_mae: 0.7324\n",
            "Epoch 29/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.8743 - mse: 0.8743 - mae: 0.7417 - val_loss: 0.8725 - val_mse: 0.8725 - val_mae: 0.7176\n",
            "Epoch 30/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.8492 - mse: 0.8492 - mae: 0.7292 - val_loss: 0.8452 - val_mse: 0.8452 - val_mae: 0.7095\n",
            "Epoch 31/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7821 - mse: 0.7821 - mae: 0.7039 - val_loss: 0.8161 - val_mse: 0.8161 - val_mae: 0.6893\n",
            "Epoch 32/1000\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7920 - mse: 0.7920 - mae: 0.7054 - val_loss: 0.7872 - val_mse: 0.7872 - val_mae: 0.6761\n",
            "Epoch 33/1000\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7798 - mse: 0.7798 - mae: 0.7002 - val_loss: 0.7649 - val_mse: 0.7649 - val_mae: 0.6723\n",
            "Epoch 34/1000\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7437 - mse: 0.7437 - mae: 0.6830 - val_loss: 0.7370 - val_mse: 0.7370 - val_mae: 0.6498\n",
            "Epoch 35/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7301 - mse: 0.7301 - mae: 0.6752 - val_loss: 0.7214 - val_mse: 0.7214 - val_mae: 0.6473\n",
            "Epoch 36/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7128 - mse: 0.7128 - mae: 0.6688 - val_loss: 0.6991 - val_mse: 0.6991 - val_mae: 0.6390\n",
            "Epoch 37/1000\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6738 - mse: 0.6738 - mae: 0.6557 - val_loss: 0.6786 - val_mse: 0.6786 - val_mae: 0.6260\n",
            "Epoch 38/1000\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6823 - mse: 0.6823 - mae: 0.6500 - val_loss: 0.6595 - val_mse: 0.6595 - val_mae: 0.6153\n",
            "Epoch 39/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6737 - mse: 0.6737 - mae: 0.6462 - val_loss: 0.6497 - val_mse: 0.6497 - val_mae: 0.6176\n",
            "Epoch 40/1000\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6459 - mse: 0.6459 - mae: 0.6333 - val_loss: 0.6322 - val_mse: 0.6322 - val_mae: 0.5969\n",
            "Epoch 41/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6208 - mse: 0.6208 - mae: 0.6192 - val_loss: 0.6120 - val_mse: 0.6120 - val_mae: 0.5955\n",
            "Epoch 42/1000\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6124 - mse: 0.6124 - mae: 0.6116 - val_loss: 0.5907 - val_mse: 0.5907 - val_mae: 0.5803\n",
            "Epoch 43/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.5844 - mse: 0.5844 - mae: 0.6020 - val_loss: 0.5794 - val_mse: 0.5794 - val_mae: 0.5762\n",
            "Epoch 44/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.5779 - mse: 0.5779 - mae: 0.6040 - val_loss: 0.5681 - val_mse: 0.5681 - val_mae: 0.5743\n",
            "Epoch 45/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.5698 - mse: 0.5698 - mae: 0.5949 - val_loss: 0.5561 - val_mse: 0.5561 - val_mae: 0.5681\n",
            "Epoch 46/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.5649 - mse: 0.5649 - mae: 0.5916 - val_loss: 0.5525 - val_mse: 0.5525 - val_mae: 0.5621\n",
            "Epoch 47/1000\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.5594 - mse: 0.5594 - mae: 0.5838 - val_loss: 0.5339 - val_mse: 0.5339 - val_mae: 0.5530\n",
            "Epoch 48/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.5309 - mse: 0.5309 - mae: 0.5647 - val_loss: 0.5225 - val_mse: 0.5225 - val_mae: 0.5486\n",
            "Epoch 49/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.5400 - mse: 0.5400 - mae: 0.5778 - val_loss: 0.5206 - val_mse: 0.5206 - val_mae: 0.5531\n",
            "Epoch 50/1000\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.5305 - mse: 0.5305 - mae: 0.5726 - val_loss: 0.5143 - val_mse: 0.5143 - val_mae: 0.5419\n",
            "Epoch 51/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.4903 - mse: 0.4903 - mae: 0.5446 - val_loss: 0.4984 - val_mse: 0.4984 - val_mae: 0.5402\n",
            "Epoch 52/1000\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.4987 - mse: 0.4987 - mae: 0.5495 - val_loss: 0.5039 - val_mse: 0.5039 - val_mae: 0.5348\n",
            "Epoch 53/1000\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.5018 - mse: 0.5018 - mae: 0.5585 - val_loss: 0.4803 - val_mse: 0.4803 - val_mae: 0.5226\n",
            "Epoch 54/1000\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.4683 - mse: 0.4683 - mae: 0.5375 - val_loss: 0.4762 - val_mse: 0.4762 - val_mae: 0.5250\n",
            "Epoch 55/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.4754 - mse: 0.4754 - mae: 0.5441 - val_loss: 0.4761 - val_mse: 0.4761 - val_mae: 0.5295\n",
            "Epoch 56/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.4779 - mse: 0.4779 - mae: 0.5522 - val_loss: 0.4683 - val_mse: 0.4683 - val_mae: 0.5230\n",
            "Epoch 57/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.4724 - mse: 0.4724 - mae: 0.5429 - val_loss: 0.4663 - val_mse: 0.4663 - val_mae: 0.5174\n",
            "Epoch 58/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.4729 - mse: 0.4729 - mae: 0.5398 - val_loss: 0.4575 - val_mse: 0.4575 - val_mae: 0.5159\n",
            "Epoch 59/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.4715 - mse: 0.4715 - mae: 0.5414 - val_loss: 0.4554 - val_mse: 0.4554 - val_mae: 0.5144\n",
            "Epoch 60/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.4535 - mse: 0.4535 - mae: 0.5252 - val_loss: 0.4430 - val_mse: 0.4430 - val_mae: 0.5057\n",
            "Epoch 61/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.4343 - mse: 0.4343 - mae: 0.5226 - val_loss: 0.4422 - val_mse: 0.4422 - val_mae: 0.5034\n",
            "Epoch 62/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.4526 - mse: 0.4526 - mae: 0.5273 - val_loss: 0.4384 - val_mse: 0.4384 - val_mae: 0.5020\n",
            "Epoch 63/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.4232 - mse: 0.4232 - mae: 0.5138 - val_loss: 0.4313 - val_mse: 0.4313 - val_mae: 0.4980\n",
            "Epoch 64/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.4422 - mse: 0.4422 - mae: 0.5180 - val_loss: 0.4362 - val_mse: 0.4362 - val_mae: 0.4967\n",
            "Epoch 65/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.4180 - mse: 0.4180 - mae: 0.5041 - val_loss: 0.4411 - val_mse: 0.4411 - val_mae: 0.5007\n",
            "Epoch 66/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.4612 - mse: 0.4612 - mae: 0.5291 - val_loss: 0.4314 - val_mse: 0.4314 - val_mae: 0.5031\n",
            "Epoch 67/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.4253 - mse: 0.4253 - mae: 0.5095 - val_loss: 0.4181 - val_mse: 0.4181 - val_mae: 0.4922\n",
            "Epoch 68/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.4338 - mse: 0.4338 - mae: 0.5227 - val_loss: 0.4206 - val_mse: 0.4206 - val_mae: 0.4936\n",
            "Epoch 69/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.4205 - mse: 0.4205 - mae: 0.5102 - val_loss: 0.4169 - val_mse: 0.4169 - val_mae: 0.4882\n",
            "Epoch 70/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.4271 - mse: 0.4271 - mae: 0.5076 - val_loss: 0.4139 - val_mse: 0.4139 - val_mae: 0.4875\n",
            "Epoch 71/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.4104 - mse: 0.4104 - mae: 0.4963 - val_loss: 0.4082 - val_mse: 0.4082 - val_mae: 0.4808\n",
            "Epoch 72/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.4302 - mse: 0.4302 - mae: 0.5186 - val_loss: 0.4075 - val_mse: 0.4075 - val_mae: 0.4889\n",
            "Epoch 73/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.4065 - mse: 0.4065 - mae: 0.4978 - val_loss: 0.4054 - val_mse: 0.4054 - val_mae: 0.4806\n",
            "Epoch 74/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.4217 - mse: 0.4217 - mae: 0.5042 - val_loss: 0.4100 - val_mse: 0.4100 - val_mae: 0.4869\n",
            "Epoch 75/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.4081 - mse: 0.4081 - mae: 0.4998 - val_loss: 0.4004 - val_mse: 0.4004 - val_mae: 0.4772\n",
            "Epoch 76/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.4114 - mse: 0.4114 - mae: 0.4997 - val_loss: 0.4074 - val_mse: 0.4074 - val_mae: 0.4819\n",
            "Epoch 77/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.4366 - mse: 0.4366 - mae: 0.5188 - val_loss: 0.3976 - val_mse: 0.3976 - val_mae: 0.4798\n",
            "Epoch 78/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.4197 - mse: 0.4197 - mae: 0.5043 - val_loss: 0.3973 - val_mse: 0.3973 - val_mae: 0.4806\n",
            "Epoch 79/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3954 - mse: 0.3954 - mae: 0.4962 - val_loss: 0.3903 - val_mse: 0.3903 - val_mae: 0.4738\n",
            "Epoch 80/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3836 - mse: 0.3836 - mae: 0.4891 - val_loss: 0.3917 - val_mse: 0.3917 - val_mae: 0.4761\n",
            "Epoch 81/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3887 - mse: 0.3887 - mae: 0.4858 - val_loss: 0.3887 - val_mse: 0.3887 - val_mae: 0.4729\n",
            "Epoch 82/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3929 - mse: 0.3929 - mae: 0.4944 - val_loss: 0.3981 - val_mse: 0.3981 - val_mae: 0.4837\n",
            "Epoch 83/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3978 - mse: 0.3978 - mae: 0.4934 - val_loss: 0.3860 - val_mse: 0.3860 - val_mae: 0.4722\n",
            "Epoch 84/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3932 - mse: 0.3932 - mae: 0.4852 - val_loss: 0.3993 - val_mse: 0.3993 - val_mae: 0.4846\n",
            "Epoch 85/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3820 - mse: 0.3820 - mae: 0.4830 - val_loss: 0.3822 - val_mse: 0.3822 - val_mae: 0.4676\n",
            "Epoch 86/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.3848 - mse: 0.3848 - mae: 0.4915 - val_loss: 0.3916 - val_mse: 0.3916 - val_mae: 0.4770\n",
            "Epoch 87/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3790 - mse: 0.3790 - mae: 0.4764 - val_loss: 0.3871 - val_mse: 0.3871 - val_mae: 0.4776\n",
            "Epoch 88/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3726 - mse: 0.3726 - mae: 0.4775 - val_loss: 0.3807 - val_mse: 0.3807 - val_mae: 0.4674\n",
            "Epoch 89/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3946 - mse: 0.3946 - mae: 0.4968 - val_loss: 0.3826 - val_mse: 0.3826 - val_mae: 0.4706\n",
            "Epoch 90/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3840 - mse: 0.3840 - mae: 0.4854 - val_loss: 0.3802 - val_mse: 0.3802 - val_mae: 0.4693\n",
            "Epoch 91/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3917 - mse: 0.3917 - mae: 0.4895 - val_loss: 0.4047 - val_mse: 0.4047 - val_mae: 0.4925\n",
            "Epoch 92/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3794 - mse: 0.3794 - mae: 0.4792 - val_loss: 0.3736 - val_mse: 0.3736 - val_mae: 0.4630\n",
            "Epoch 93/1000\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.3751 - mse: 0.3751 - mae: 0.4806 - val_loss: 0.3756 - val_mse: 0.3756 - val_mae: 0.4668\n",
            "Epoch 94/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.3821 - mse: 0.3821 - mae: 0.4796 - val_loss: 0.3817 - val_mse: 0.3817 - val_mae: 0.4750\n",
            "Epoch 95/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.3819 - mse: 0.3819 - mae: 0.4828 - val_loss: 0.3817 - val_mse: 0.3817 - val_mae: 0.4704\n",
            "Epoch 96/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.3989 - mse: 0.3989 - mae: 0.4865 - val_loss: 0.3902 - val_mse: 0.3902 - val_mae: 0.4742\n",
            "Epoch 97/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3823 - mse: 0.3823 - mae: 0.4809 - val_loss: 0.3809 - val_mse: 0.3809 - val_mae: 0.4672\n",
            "Epoch 98/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3802 - mse: 0.3802 - mae: 0.4827 - val_loss: 0.3820 - val_mse: 0.3820 - val_mae: 0.4686\n",
            "Epoch 99/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3767 - mse: 0.3767 - mae: 0.4781 - val_loss: 0.3726 - val_mse: 0.3726 - val_mae: 0.4646\n",
            "Epoch 100/1000\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.3814 - mse: 0.3814 - mae: 0.4770 - val_loss: 0.3704 - val_mse: 0.3704 - val_mae: 0.4592\n",
            "Epoch 101/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3915 - mse: 0.3915 - mae: 0.4869 - val_loss: 0.3691 - val_mse: 0.3691 - val_mae: 0.4565\n",
            "Epoch 102/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3785 - mse: 0.3785 - mae: 0.4789 - val_loss: 0.3681 - val_mse: 0.3681 - val_mae: 0.4637\n",
            "Epoch 103/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3851 - mse: 0.3851 - mae: 0.4880 - val_loss: 0.3678 - val_mse: 0.3678 - val_mae: 0.4570\n",
            "Epoch 104/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3751 - mse: 0.3751 - mae: 0.4745 - val_loss: 0.3684 - val_mse: 0.3684 - val_mae: 0.4584\n",
            "Epoch 105/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.3766 - mse: 0.3766 - mae: 0.4791 - val_loss: 0.3659 - val_mse: 0.3659 - val_mae: 0.4618\n",
            "Epoch 106/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3591 - mse: 0.3591 - mae: 0.4689 - val_loss: 0.3819 - val_mse: 0.3819 - val_mae: 0.4774\n",
            "Epoch 107/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3687 - mse: 0.3687 - mae: 0.4809 - val_loss: 0.3622 - val_mse: 0.3622 - val_mae: 0.4569\n",
            "Epoch 108/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3716 - mse: 0.3716 - mae: 0.4768 - val_loss: 0.3719 - val_mse: 0.3719 - val_mae: 0.4677\n",
            "Epoch 109/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3799 - mse: 0.3799 - mae: 0.4769 - val_loss: 0.3639 - val_mse: 0.3639 - val_mae: 0.4612\n",
            "Epoch 110/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3827 - mse: 0.3827 - mae: 0.4814 - val_loss: 0.3656 - val_mse: 0.3656 - val_mae: 0.4665\n",
            "Epoch 111/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3584 - mse: 0.3584 - mae: 0.4722 - val_loss: 0.3618 - val_mse: 0.3618 - val_mae: 0.4590\n",
            "Epoch 112/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3585 - mse: 0.3585 - mae: 0.4638 - val_loss: 0.3658 - val_mse: 0.3658 - val_mae: 0.4628\n",
            "Epoch 113/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.3663 - mse: 0.3663 - mae: 0.4712 - val_loss: 0.3729 - val_mse: 0.3729 - val_mae: 0.4703\n",
            "Epoch 114/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3764 - mse: 0.3764 - mae: 0.4781 - val_loss: 0.3921 - val_mse: 0.3921 - val_mae: 0.4822\n",
            "Epoch 115/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3840 - mse: 0.3840 - mae: 0.4830 - val_loss: 0.3809 - val_mse: 0.3809 - val_mae: 0.4737\n",
            "Epoch 116/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3580 - mse: 0.3580 - mae: 0.4709 - val_loss: 0.3582 - val_mse: 0.3582 - val_mae: 0.4537\n",
            "Epoch 117/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3630 - mse: 0.3630 - mae: 0.4680 - val_loss: 0.3624 - val_mse: 0.3624 - val_mae: 0.4614\n",
            "Epoch 118/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3616 - mse: 0.3616 - mae: 0.4686 - val_loss: 0.3624 - val_mse: 0.3624 - val_mae: 0.4557\n",
            "Epoch 119/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3593 - mse: 0.3593 - mae: 0.4690 - val_loss: 0.3676 - val_mse: 0.3676 - val_mae: 0.4651\n",
            "Epoch 120/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3579 - mse: 0.3579 - mae: 0.4738 - val_loss: 0.3584 - val_mse: 0.3584 - val_mae: 0.4589\n",
            "Epoch 121/1000\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.3607 - mse: 0.3607 - mae: 0.4660 - val_loss: 0.3660 - val_mse: 0.3660 - val_mae: 0.4606\n",
            "Epoch 122/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3629 - mse: 0.3629 - mae: 0.4697 - val_loss: 0.3699 - val_mse: 0.3699 - val_mae: 0.4647\n",
            "Epoch 123/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3410 - mse: 0.3410 - mae: 0.4583 - val_loss: 0.3791 - val_mse: 0.3791 - val_mae: 0.4741\n",
            "Epoch 124/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.3647 - mse: 0.3647 - mae: 0.4681 - val_loss: 0.3709 - val_mse: 0.3709 - val_mae: 0.4703\n",
            "Epoch 125/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.3497 - mse: 0.3497 - mae: 0.4609 - val_loss: 0.3649 - val_mse: 0.3649 - val_mae: 0.4563\n",
            "Epoch 126/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3615 - mse: 0.3615 - mae: 0.4716 - val_loss: 0.3810 - val_mse: 0.3810 - val_mae: 0.4784\n",
            "Epoch 127/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3617 - mse: 0.3617 - mae: 0.4610 - val_loss: 0.3643 - val_mse: 0.3643 - val_mae: 0.4589\n",
            "Epoch 128/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3658 - mse: 0.3658 - mae: 0.4714 - val_loss: 0.3566 - val_mse: 0.3566 - val_mae: 0.4562\n",
            "Epoch 129/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3615 - mse: 0.3615 - mae: 0.4656 - val_loss: 0.3606 - val_mse: 0.3606 - val_mae: 0.4595\n",
            "Epoch 130/1000\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.3516 - mse: 0.3516 - mae: 0.4628 - val_loss: 0.3596 - val_mse: 0.3596 - val_mae: 0.4593\n",
            "Epoch 131/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3596 - mse: 0.3596 - mae: 0.4677 - val_loss: 0.3560 - val_mse: 0.3560 - val_mae: 0.4536\n",
            "Epoch 132/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3666 - mse: 0.3666 - mae: 0.4769 - val_loss: 0.3523 - val_mse: 0.3523 - val_mae: 0.4500\n",
            "Epoch 133/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3485 - mse: 0.3485 - mae: 0.4557 - val_loss: 0.3544 - val_mse: 0.3544 - val_mae: 0.4539\n",
            "Epoch 134/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3637 - mse: 0.3637 - mae: 0.4715 - val_loss: 0.3554 - val_mse: 0.3554 - val_mae: 0.4582\n",
            "Epoch 135/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3581 - mse: 0.3581 - mae: 0.4619 - val_loss: 0.3591 - val_mse: 0.3591 - val_mae: 0.4583\n",
            "Epoch 136/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3581 - mse: 0.3581 - mae: 0.4642 - val_loss: 0.3598 - val_mse: 0.3598 - val_mae: 0.4553\n",
            "Epoch 137/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3539 - mse: 0.3539 - mae: 0.4577 - val_loss: 0.3472 - val_mse: 0.3472 - val_mae: 0.4474\n",
            "Epoch 138/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3721 - mse: 0.3721 - mae: 0.4725 - val_loss: 0.3568 - val_mse: 0.3568 - val_mae: 0.4576\n",
            "Epoch 139/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3533 - mse: 0.3533 - mae: 0.4653 - val_loss: 0.3622 - val_mse: 0.3622 - val_mae: 0.4652\n",
            "Epoch 140/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3381 - mse: 0.3381 - mae: 0.4570 - val_loss: 0.3762 - val_mse: 0.3762 - val_mae: 0.4759\n",
            "Epoch 141/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.3510 - mse: 0.3510 - mae: 0.4584 - val_loss: 0.3558 - val_mse: 0.3558 - val_mae: 0.4561\n",
            "Epoch 142/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3524 - mse: 0.3524 - mae: 0.4594 - val_loss: 0.3524 - val_mse: 0.3524 - val_mae: 0.4526\n",
            "Epoch 143/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3382 - mse: 0.3382 - mae: 0.4552 - val_loss: 0.3501 - val_mse: 0.3501 - val_mae: 0.4506\n",
            "Epoch 144/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3604 - mse: 0.3604 - mae: 0.4649 - val_loss: 0.3755 - val_mse: 0.3755 - val_mae: 0.4707\n",
            "Epoch 145/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3369 - mse: 0.3369 - mae: 0.4529 - val_loss: 0.3526 - val_mse: 0.3526 - val_mae: 0.4536\n",
            "Epoch 146/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3581 - mse: 0.3581 - mae: 0.4689 - val_loss: 0.3505 - val_mse: 0.3505 - val_mae: 0.4477\n",
            "Epoch 147/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3509 - mse: 0.3509 - mae: 0.4624 - val_loss: 0.3491 - val_mse: 0.3491 - val_mae: 0.4470\n",
            "Epoch 148/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3435 - mse: 0.3435 - mae: 0.4546 - val_loss: 0.3531 - val_mse: 0.3531 - val_mae: 0.4497\n",
            "Epoch 149/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3636 - mse: 0.3636 - mae: 0.4676 - val_loss: 0.3746 - val_mse: 0.3746 - val_mae: 0.4728\n",
            "Epoch 150/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3463 - mse: 0.3463 - mae: 0.4593 - val_loss: 0.3499 - val_mse: 0.3499 - val_mae: 0.4458\n",
            "Epoch 151/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3384 - mse: 0.3384 - mae: 0.4561 - val_loss: 0.3697 - val_mse: 0.3697 - val_mae: 0.4683\n",
            "Epoch 152/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3500 - mse: 0.3500 - mae: 0.4612 - val_loss: 0.3556 - val_mse: 0.3556 - val_mae: 0.4540\n",
            "Epoch 153/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3576 - mse: 0.3576 - mae: 0.4643 - val_loss: 0.3523 - val_mse: 0.3523 - val_mae: 0.4472\n",
            "Epoch 154/1000\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.3311 - mse: 0.3311 - mae: 0.4454 - val_loss: 0.3495 - val_mse: 0.3495 - val_mae: 0.4452\n",
            "Epoch 155/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3336 - mse: 0.3336 - mae: 0.4502 - val_loss: 0.3650 - val_mse: 0.3650 - val_mae: 0.4667\n",
            "Epoch 156/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3269 - mse: 0.3269 - mae: 0.4405 - val_loss: 0.3579 - val_mse: 0.3579 - val_mae: 0.4538\n",
            "Epoch 157/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3403 - mse: 0.3403 - mae: 0.4591 - val_loss: 0.3541 - val_mse: 0.3541 - val_mae: 0.4578\n",
            "Epoch 158/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3199 - mse: 0.3199 - mae: 0.4382 - val_loss: 0.3572 - val_mse: 0.3572 - val_mae: 0.4541\n",
            "Epoch 159/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3480 - mse: 0.3480 - mae: 0.4627 - val_loss: 0.3646 - val_mse: 0.3646 - val_mae: 0.4689\n",
            "Epoch 160/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.3416 - mse: 0.3416 - mae: 0.4563 - val_loss: 0.3478 - val_mse: 0.3478 - val_mae: 0.4470\n",
            "Epoch 161/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3321 - mse: 0.3321 - mae: 0.4476 - val_loss: 0.3546 - val_mse: 0.3546 - val_mae: 0.4552\n",
            "Epoch 162/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.3475 - mse: 0.3475 - mae: 0.4587 - val_loss: 0.3690 - val_mse: 0.3690 - val_mae: 0.4750\n",
            "Epoch 163/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3368 - mse: 0.3368 - mae: 0.4502 - val_loss: 0.3441 - val_mse: 0.3441 - val_mae: 0.4483\n",
            "Epoch 164/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3292 - mse: 0.3292 - mae: 0.4457 - val_loss: 0.3406 - val_mse: 0.3406 - val_mae: 0.4430\n",
            "Epoch 165/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3363 - mse: 0.3363 - mae: 0.4548 - val_loss: 0.3683 - val_mse: 0.3683 - val_mae: 0.4697\n",
            "Epoch 166/1000\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.3341 - mse: 0.3341 - mae: 0.4482 - val_loss: 0.3512 - val_mse: 0.3512 - val_mae: 0.4479\n",
            "Epoch 167/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3243 - mse: 0.3243 - mae: 0.4453 - val_loss: 0.3473 - val_mse: 0.3473 - val_mae: 0.4531\n",
            "Epoch 168/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3316 - mse: 0.3316 - mae: 0.4478 - val_loss: 0.3506 - val_mse: 0.3506 - val_mae: 0.4497\n",
            "Epoch 169/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3411 - mse: 0.3411 - mae: 0.4487 - val_loss: 0.3600 - val_mse: 0.3600 - val_mae: 0.4578\n",
            "Epoch 170/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3334 - mse: 0.3334 - mae: 0.4583 - val_loss: 0.3587 - val_mse: 0.3587 - val_mae: 0.4608\n",
            "Epoch 171/1000\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.3328 - mse: 0.3328 - mae: 0.4481 - val_loss: 0.3478 - val_mse: 0.3478 - val_mae: 0.4505\n",
            "Epoch 172/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3410 - mse: 0.3410 - mae: 0.4507 - val_loss: 0.3625 - val_mse: 0.3625 - val_mae: 0.4654\n",
            "Epoch 173/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3396 - mse: 0.3396 - mae: 0.4538 - val_loss: 0.3604 - val_mse: 0.3604 - val_mae: 0.4613\n",
            "Epoch 174/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3518 - mse: 0.3518 - mae: 0.4655 - val_loss: 0.3555 - val_mse: 0.3555 - val_mae: 0.4560\n",
            "Epoch 175/1000\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.3350 - mse: 0.3350 - mae: 0.4510 - val_loss: 0.3536 - val_mse: 0.3536 - val_mae: 0.4507\n",
            "Epoch 176/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.3348 - mse: 0.3348 - mae: 0.4475 - val_loss: 0.3508 - val_mse: 0.3508 - val_mae: 0.4532\n",
            "Epoch 177/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3293 - mse: 0.3293 - mae: 0.4440 - val_loss: 0.3606 - val_mse: 0.3606 - val_mae: 0.4640\n",
            "Epoch 178/1000\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.3398 - mse: 0.3398 - mae: 0.4560 - val_loss: 0.3522 - val_mse: 0.3522 - val_mae: 0.4525\n",
            "Epoch 179/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3319 - mse: 0.3319 - mae: 0.4415 - val_loss: 0.3504 - val_mse: 0.3504 - val_mae: 0.4461\n",
            "Epoch 180/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3364 - mse: 0.3364 - mae: 0.4528 - val_loss: 0.3489 - val_mse: 0.3489 - val_mae: 0.4520\n",
            "Epoch 181/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3292 - mse: 0.3292 - mae: 0.4498 - val_loss: 0.3482 - val_mse: 0.3482 - val_mae: 0.4504\n",
            "Epoch 182/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3402 - mse: 0.3402 - mae: 0.4575 - val_loss: 0.3588 - val_mse: 0.3588 - val_mae: 0.4590\n",
            "Epoch 183/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.3404 - mse: 0.3404 - mae: 0.4568 - val_loss: 0.3541 - val_mse: 0.3541 - val_mae: 0.4533\n",
            "Epoch 184/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3293 - mse: 0.3293 - mae: 0.4446 - val_loss: 0.3486 - val_mse: 0.3486 - val_mae: 0.4489\n",
            "Epoch 185/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3174 - mse: 0.3174 - mae: 0.4377 - val_loss: 0.3438 - val_mse: 0.3438 - val_mae: 0.4422\n",
            "Epoch 186/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3511 - mse: 0.3511 - mae: 0.4573 - val_loss: 0.3587 - val_mse: 0.3587 - val_mae: 0.4577\n",
            "Epoch 187/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3538 - mse: 0.3538 - mae: 0.4693 - val_loss: 0.3692 - val_mse: 0.3692 - val_mae: 0.4672\n",
            "Epoch 188/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3483 - mse: 0.3483 - mae: 0.4626 - val_loss: 0.3479 - val_mse: 0.3479 - val_mae: 0.4548\n",
            "Epoch 189/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3398 - mse: 0.3398 - mae: 0.4603 - val_loss: 0.3596 - val_mse: 0.3596 - val_mae: 0.4675\n",
            "Epoch 190/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3354 - mse: 0.3354 - mae: 0.4509 - val_loss: 0.3457 - val_mse: 0.3457 - val_mae: 0.4446\n",
            "Epoch 191/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3299 - mse: 0.3299 - mae: 0.4486 - val_loss: 0.3494 - val_mse: 0.3494 - val_mae: 0.4526\n",
            "Epoch 192/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3180 - mse: 0.3180 - mae: 0.4402 - val_loss: 0.3455 - val_mse: 0.3455 - val_mae: 0.4510\n",
            "Epoch 193/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3268 - mse: 0.3268 - mae: 0.4424 - val_loss: 0.3510 - val_mse: 0.3510 - val_mae: 0.4505\n",
            "Epoch 194/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3239 - mse: 0.3239 - mae: 0.4392 - val_loss: 0.3532 - val_mse: 0.3532 - val_mae: 0.4603\n",
            "Epoch 195/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3174 - mse: 0.3174 - mae: 0.4384 - val_loss: 0.3491 - val_mse: 0.3491 - val_mae: 0.4497\n",
            "Epoch 196/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3208 - mse: 0.3208 - mae: 0.4419 - val_loss: 0.3508 - val_mse: 0.3508 - val_mae: 0.4531\n",
            "Epoch 197/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3390 - mse: 0.3390 - mae: 0.4482 - val_loss: 0.3664 - val_mse: 0.3664 - val_mae: 0.4695\n",
            "Epoch 198/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.3065 - mse: 0.3065 - mae: 0.4259 - val_loss: 0.3649 - val_mse: 0.3649 - val_mae: 0.4610\n",
            "Epoch 199/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3262 - mse: 0.3262 - mae: 0.4460 - val_loss: 0.3495 - val_mse: 0.3495 - val_mae: 0.4498\n",
            "Epoch 200/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.3372 - mse: 0.3372 - mae: 0.4395 - val_loss: 0.3485 - val_mse: 0.3485 - val_mae: 0.4526\n",
            "Epoch 201/1000\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.3270 - mse: 0.3270 - mae: 0.4423 - val_loss: 0.3415 - val_mse: 0.3415 - val_mae: 0.4453\n",
            "Epoch 202/1000\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.3187 - mse: 0.3187 - mae: 0.4390 - val_loss: 0.3382 - val_mse: 0.3382 - val_mae: 0.4397\n",
            "Epoch 203/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.3215 - mse: 0.3215 - mae: 0.4357 - val_loss: 0.3414 - val_mse: 0.3414 - val_mae: 0.4420\n",
            "Epoch 204/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.3130 - mse: 0.3130 - mae: 0.4353 - val_loss: 0.3510 - val_mse: 0.3510 - val_mae: 0.4582\n",
            "Epoch 205/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3198 - mse: 0.3198 - mae: 0.4389 - val_loss: 0.3454 - val_mse: 0.3454 - val_mae: 0.4497\n",
            "Epoch 206/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3327 - mse: 0.3327 - mae: 0.4474 - val_loss: 0.3473 - val_mse: 0.3473 - val_mae: 0.4454\n",
            "Epoch 207/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3386 - mse: 0.3386 - mae: 0.4509 - val_loss: 0.3752 - val_mse: 0.3752 - val_mae: 0.4803\n",
            "Epoch 208/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3230 - mse: 0.3230 - mae: 0.4394 - val_loss: 0.3539 - val_mse: 0.3539 - val_mae: 0.4596\n",
            "Epoch 209/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3208 - mse: 0.3208 - mae: 0.4427 - val_loss: 0.3463 - val_mse: 0.3463 - val_mae: 0.4458\n",
            "Epoch 210/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3134 - mse: 0.3134 - mae: 0.4348 - val_loss: 0.3519 - val_mse: 0.3519 - val_mae: 0.4498\n",
            "Epoch 211/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3227 - mse: 0.3227 - mae: 0.4416 - val_loss: 0.3539 - val_mse: 0.3539 - val_mae: 0.4505\n",
            "Epoch 212/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3198 - mse: 0.3198 - mae: 0.4403 - val_loss: 0.3479 - val_mse: 0.3479 - val_mae: 0.4458\n",
            "Epoch 213/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3128 - mse: 0.3128 - mae: 0.4317 - val_loss: 0.3636 - val_mse: 0.3636 - val_mae: 0.4616\n",
            "Epoch 214/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3421 - mse: 0.3421 - mae: 0.4566 - val_loss: 0.3489 - val_mse: 0.3489 - val_mae: 0.4474\n",
            "Epoch 215/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.3148 - mse: 0.3148 - mae: 0.4377 - val_loss: 0.3542 - val_mse: 0.3542 - val_mae: 0.4558\n",
            "Epoch 216/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3207 - mse: 0.3207 - mae: 0.4439 - val_loss: 0.3507 - val_mse: 0.3507 - val_mae: 0.4514\n",
            "Epoch 217/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3211 - mse: 0.3211 - mae: 0.4396 - val_loss: 0.3557 - val_mse: 0.3557 - val_mae: 0.4557\n",
            "Epoch 218/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3266 - mse: 0.3266 - mae: 0.4430 - val_loss: 0.3655 - val_mse: 0.3655 - val_mae: 0.4617\n",
            "Epoch 219/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3122 - mse: 0.3122 - mae: 0.4319 - val_loss: 0.3563 - val_mse: 0.3563 - val_mae: 0.4561\n",
            "Epoch 220/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3137 - mse: 0.3137 - mae: 0.4337 - val_loss: 0.3521 - val_mse: 0.3521 - val_mae: 0.4463\n",
            "Epoch 221/1000\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.3517 - mse: 0.3517 - mae: 0.4634 - val_loss: 0.3743 - val_mse: 0.3743 - val_mae: 0.4730\n",
            "Epoch 222/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3212 - mse: 0.3212 - mae: 0.4404 - val_loss: 0.3593 - val_mse: 0.3593 - val_mae: 0.4519\n",
            "Epoch 223/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3174 - mse: 0.3174 - mae: 0.4379 - val_loss: 0.3543 - val_mse: 0.3543 - val_mae: 0.4490\n",
            "Epoch 224/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3114 - mse: 0.3114 - mae: 0.4287 - val_loss: 0.3610 - val_mse: 0.3610 - val_mae: 0.4539\n",
            "Epoch 225/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3178 - mse: 0.3178 - mae: 0.4402 - val_loss: 0.3590 - val_mse: 0.3590 - val_mae: 0.4632\n",
            "Epoch 226/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3235 - mse: 0.3235 - mae: 0.4445 - val_loss: 0.3861 - val_mse: 0.3861 - val_mae: 0.4817\n",
            "Epoch 227/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3171 - mse: 0.3171 - mae: 0.4354 - val_loss: 0.3542 - val_mse: 0.3542 - val_mae: 0.4493\n",
            "Epoch 228/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.2958 - mse: 0.2958 - mae: 0.4217 - val_loss: 0.3566 - val_mse: 0.3566 - val_mae: 0.4566\n",
            "Epoch 229/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3041 - mse: 0.3041 - mae: 0.4326 - val_loss: 0.3562 - val_mse: 0.3562 - val_mae: 0.4560\n",
            "Epoch 230/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3136 - mse: 0.3136 - mae: 0.4378 - val_loss: 0.3671 - val_mse: 0.3671 - val_mae: 0.4638\n",
            "Epoch 231/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3212 - mse: 0.3212 - mae: 0.4438 - val_loss: 0.3633 - val_mse: 0.3633 - val_mae: 0.4626\n",
            "Epoch 232/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3057 - mse: 0.3057 - mae: 0.4308 - val_loss: 0.3847 - val_mse: 0.3847 - val_mae: 0.4811\n",
            "Epoch 233/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3044 - mse: 0.3044 - mae: 0.4304 - val_loss: 0.3525 - val_mse: 0.3525 - val_mae: 0.4515\n",
            "Epoch 234/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3006 - mse: 0.3006 - mae: 0.4261 - val_loss: 0.3711 - val_mse: 0.3711 - val_mae: 0.4710\n",
            "Epoch 235/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.2956 - mse: 0.2956 - mae: 0.4241 - val_loss: 0.3560 - val_mse: 0.3560 - val_mae: 0.4536\n",
            "Epoch 236/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3132 - mse: 0.3132 - mae: 0.4355 - val_loss: 0.3491 - val_mse: 0.3491 - val_mae: 0.4409\n",
            "Epoch 237/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3015 - mse: 0.3015 - mae: 0.4258 - val_loss: 0.3718 - val_mse: 0.3718 - val_mae: 0.4698\n",
            "Epoch 238/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3234 - mse: 0.3234 - mae: 0.4436 - val_loss: 0.3520 - val_mse: 0.3520 - val_mae: 0.4480\n",
            "Epoch 239/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3200 - mse: 0.3200 - mae: 0.4370 - val_loss: 0.3601 - val_mse: 0.3601 - val_mae: 0.4548\n",
            "Epoch 240/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3084 - mse: 0.3084 - mae: 0.4311 - val_loss: 0.3605 - val_mse: 0.3605 - val_mae: 0.4564\n",
            "Epoch 241/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3077 - mse: 0.3077 - mae: 0.4331 - val_loss: 0.3567 - val_mse: 0.3567 - val_mae: 0.4488\n",
            "Epoch 242/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.3096 - mse: 0.3096 - mae: 0.4355 - val_loss: 0.3592 - val_mse: 0.3592 - val_mae: 0.4547\n",
            "Epoch 243/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3249 - mse: 0.3249 - mae: 0.4397 - val_loss: 0.3584 - val_mse: 0.3584 - val_mae: 0.4595\n",
            "Epoch 244/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3058 - mse: 0.3058 - mae: 0.4305 - val_loss: 0.3527 - val_mse: 0.3527 - val_mae: 0.4544\n",
            "Epoch 245/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3131 - mse: 0.3131 - mae: 0.4330 - val_loss: 0.3580 - val_mse: 0.3580 - val_mae: 0.4592\n",
            "Epoch 246/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3059 - mse: 0.3059 - mae: 0.4312 - val_loss: 0.3855 - val_mse: 0.3855 - val_mae: 0.4790\n",
            "Epoch 247/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3121 - mse: 0.3121 - mae: 0.4314 - val_loss: 0.3582 - val_mse: 0.3582 - val_mae: 0.4548\n",
            "Epoch 248/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.2995 - mse: 0.2995 - mae: 0.4222 - val_loss: 0.3483 - val_mse: 0.3483 - val_mae: 0.4443\n",
            "Epoch 249/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3045 - mse: 0.3045 - mae: 0.4265 - val_loss: 0.3514 - val_mse: 0.3514 - val_mae: 0.4515\n",
            "Epoch 250/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3258 - mse: 0.3258 - mae: 0.4457 - val_loss: 0.3558 - val_mse: 0.3558 - val_mae: 0.4547\n",
            "Epoch 251/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3006 - mse: 0.3006 - mae: 0.4288 - val_loss: 0.3619 - val_mse: 0.3619 - val_mae: 0.4527\n",
            "Epoch 252/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3018 - mse: 0.3018 - mae: 0.4243 - val_loss: 0.3548 - val_mse: 0.3548 - val_mae: 0.4475\n",
            "Epoch 253/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.2949 - mse: 0.2949 - mae: 0.4238 - val_loss: 0.3595 - val_mse: 0.3595 - val_mae: 0.4564\n",
            "Epoch 254/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.2986 - mse: 0.2986 - mae: 0.4297 - val_loss: 0.3858 - val_mse: 0.3858 - val_mae: 0.4714\n",
            "Epoch 255/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3018 - mse: 0.3018 - mae: 0.4286 - val_loss: 0.3504 - val_mse: 0.3504 - val_mae: 0.4459\n",
            "Epoch 256/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.2972 - mse: 0.2972 - mae: 0.4235 - val_loss: 0.3585 - val_mse: 0.3585 - val_mae: 0.4518\n",
            "Epoch 257/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3082 - mse: 0.3082 - mae: 0.4327 - val_loss: 0.3625 - val_mse: 0.3625 - val_mae: 0.4562\n",
            "Epoch 258/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.2998 - mse: 0.2998 - mae: 0.4272 - val_loss: 0.3525 - val_mse: 0.3525 - val_mae: 0.4536\n",
            "Epoch 259/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.2929 - mse: 0.2929 - mae: 0.4224 - val_loss: 0.3804 - val_mse: 0.3804 - val_mae: 0.4782\n",
            "Epoch 260/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.2982 - mse: 0.2982 - mae: 0.4254 - val_loss: 0.3612 - val_mse: 0.3612 - val_mae: 0.4518\n",
            "Epoch 261/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3235 - mse: 0.3235 - mae: 0.4393 - val_loss: 0.3484 - val_mse: 0.3484 - val_mae: 0.4456\n",
            "Epoch 262/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3029 - mse: 0.3029 - mae: 0.4287 - val_loss: 0.3468 - val_mse: 0.3468 - val_mae: 0.4438\n",
            "Epoch 263/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.2979 - mse: 0.2979 - mae: 0.4241 - val_loss: 0.3474 - val_mse: 0.3474 - val_mae: 0.4464\n",
            "Epoch 264/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.2994 - mse: 0.2994 - mae: 0.4267 - val_loss: 0.3519 - val_mse: 0.3519 - val_mae: 0.4502\n",
            "Epoch 265/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.2946 - mse: 0.2946 - mae: 0.4268 - val_loss: 0.3547 - val_mse: 0.3547 - val_mae: 0.4475\n",
            "Epoch 266/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3136 - mse: 0.3136 - mae: 0.4336 - val_loss: 0.3505 - val_mse: 0.3505 - val_mae: 0.4468\n",
            "Epoch 267/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3045 - mse: 0.3045 - mae: 0.4245 - val_loss: 0.3560 - val_mse: 0.3560 - val_mae: 0.4528\n",
            "Epoch 268/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.3081 - mse: 0.3081 - mae: 0.4319 - val_loss: 0.3537 - val_mse: 0.3537 - val_mae: 0.4489\n",
            "Epoch 269/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.3039 - mse: 0.3039 - mae: 0.4273 - val_loss: 0.3664 - val_mse: 0.3664 - val_mae: 0.4637\n",
            "Epoch 270/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.2941 - mse: 0.2941 - mae: 0.4172 - val_loss: 0.3679 - val_mse: 0.3679 - val_mae: 0.4651\n",
            "Epoch 271/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3070 - mse: 0.3070 - mae: 0.4272 - val_loss: 0.3680 - val_mse: 0.3680 - val_mae: 0.4648\n",
            "Epoch 272/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3070 - mse: 0.3070 - mae: 0.4254 - val_loss: 0.3549 - val_mse: 0.3549 - val_mae: 0.4539\n",
            "Epoch 273/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3001 - mse: 0.3001 - mae: 0.4222 - val_loss: 0.3628 - val_mse: 0.3628 - val_mae: 0.4634\n",
            "Epoch 274/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.2954 - mse: 0.2954 - mae: 0.4226 - val_loss: 0.3590 - val_mse: 0.3590 - val_mae: 0.4545\n",
            "Epoch 275/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3055 - mse: 0.3055 - mae: 0.4309 - val_loss: 0.3569 - val_mse: 0.3569 - val_mae: 0.4500\n",
            "Epoch 276/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3262 - mse: 0.3262 - mae: 0.4402 - val_loss: 0.3538 - val_mse: 0.3538 - val_mae: 0.4467\n",
            "Epoch 277/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3019 - mse: 0.3019 - mae: 0.4228 - val_loss: 0.3575 - val_mse: 0.3575 - val_mae: 0.4605\n",
            "Epoch 278/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.3036 - mse: 0.3036 - mae: 0.4273 - val_loss: 0.3691 - val_mse: 0.3691 - val_mae: 0.4592\n",
            "Epoch 279/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.2963 - mse: 0.2963 - mae: 0.4289 - val_loss: 0.3567 - val_mse: 0.3567 - val_mae: 0.4487\n",
            "Epoch 280/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3055 - mse: 0.3055 - mae: 0.4289 - val_loss: 0.3724 - val_mse: 0.3724 - val_mae: 0.4569\n",
            "Epoch 281/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3102 - mse: 0.3102 - mae: 0.4357 - val_loss: 0.3613 - val_mse: 0.3613 - val_mae: 0.4550\n",
            "Epoch 282/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3228 - mse: 0.3228 - mae: 0.4410 - val_loss: 0.3544 - val_mse: 0.3544 - val_mae: 0.4500\n",
            "Epoch 283/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3016 - mse: 0.3016 - mae: 0.4254 - val_loss: 0.3520 - val_mse: 0.3520 - val_mae: 0.4461\n",
            "Epoch 284/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3023 - mse: 0.3023 - mae: 0.4241 - val_loss: 0.3607 - val_mse: 0.3607 - val_mae: 0.4597\n",
            "Epoch 285/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3010 - mse: 0.3010 - mae: 0.4263 - val_loss: 0.3655 - val_mse: 0.3655 - val_mae: 0.4652\n",
            "Epoch 286/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.2963 - mse: 0.2963 - mae: 0.4235 - val_loss: 0.3428 - val_mse: 0.3428 - val_mae: 0.4409\n",
            "Epoch 287/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.2964 - mse: 0.2964 - mae: 0.4217 - val_loss: 0.3503 - val_mse: 0.3503 - val_mae: 0.4506\n",
            "Epoch 288/1000\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3146 - mse: 0.3146 - mae: 0.4319 - val_loss: 0.3530 - val_mse: 0.3530 - val_mae: 0.4483\n",
            "Epoch 289/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.2940 - mse: 0.2940 - mae: 0.4215 - val_loss: 0.3788 - val_mse: 0.3788 - val_mae: 0.4699\n",
            "Epoch 290/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.3018 - mse: 0.3018 - mae: 0.4355 - val_loss: 0.3500 - val_mse: 0.3500 - val_mae: 0.4433\n",
            "Epoch 291/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.3206 - mse: 0.3206 - mae: 0.4358 - val_loss: 0.3466 - val_mse: 0.3466 - val_mae: 0.4411\n",
            "Epoch 292/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3053 - mse: 0.3053 - mae: 0.4298 - val_loss: 0.3525 - val_mse: 0.3525 - val_mae: 0.4441\n",
            "Epoch 293/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.2962 - mse: 0.2962 - mae: 0.4212 - val_loss: 0.3527 - val_mse: 0.3527 - val_mae: 0.4456\n",
            "Epoch 294/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.2876 - mse: 0.2876 - mae: 0.4144 - val_loss: 0.3559 - val_mse: 0.3559 - val_mae: 0.4487\n",
            "Epoch 295/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.2866 - mse: 0.2866 - mae: 0.4187 - val_loss: 0.3479 - val_mse: 0.3479 - val_mae: 0.4469\n",
            "Epoch 296/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.3285 - mse: 0.3285 - mae: 0.4415 - val_loss: 0.3630 - val_mse: 0.3630 - val_mae: 0.4593\n",
            "Epoch 297/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.2926 - mse: 0.2926 - mae: 0.4241 - val_loss: 0.3448 - val_mse: 0.3448 - val_mae: 0.4410\n",
            "Epoch 298/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.3048 - mse: 0.3048 - mae: 0.4287 - val_loss: 0.3508 - val_mse: 0.3508 - val_mae: 0.4445\n",
            "Epoch 299/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.2811 - mse: 0.2811 - mae: 0.4143 - val_loss: 0.3776 - val_mse: 0.3776 - val_mae: 0.4672\n",
            "Epoch 300/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.2997 - mse: 0.2997 - mae: 0.4224 - val_loss: 0.3600 - val_mse: 0.3600 - val_mae: 0.4485\n",
            "Epoch 301/1000\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.2821 - mse: 0.2821 - mae: 0.4124 - val_loss: 0.3587 - val_mse: 0.3587 - val_mae: 0.4523\n",
            "Epoch 302/1000\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.2821 - mse: 0.2821 - mae: 0.4172 - val_loss: 0.3591 - val_mse: 0.3591 - val_mae: 0.4556\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3,\n",
              "                   estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x7f81445a1d90>,\n",
              "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f8144619700>,\n",
              "                                        'n_hidden': [1, 2],\n",
              "                                        'n_neurons': array([20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
              "       37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
              "       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70,\n",
              "       71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87,\n",
              "       88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])})"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "from scipy.stats import reciprocal\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_distribs = {\n",
        "\"n_hidden\": [1, 2],\n",
        "\"n_neurons\": np.arange(20, 100),\n",
        "\"learning_rate\": reciprocal(3e-4, 3e-2),\n",
        "}\n",
        "\n",
        "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
        "[checkpoint_cb, early_stopping]\n",
        "early_stopping = EarlyStopping(monitor='val_mse', patience=100)\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\",\n",
        "                                           save_best_only=True)\n",
        "rnd_search_cv.fit(x_train, y_train, epochs=1000,\n",
        "                  validation_split=0.2,\n",
        "                  callbacks=[checkpoint_cb, early_stopping])\n",
        "model = keras.models.load_model(\"my_keras_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lB-aIlE-zJ9M",
        "outputId": "f23e064b-90c5-4858-ddef-39cd4f8160ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'learning_rate': 0.00033749292187239906, 'n_hidden': 1, 'n_neurons': 93}\n",
            "-0.4717836876710256\n"
          ]
        }
      ],
      "source": [
        "print(rnd_search_cv.best_params_)\n",
        "print(rnd_search_cv.best_score_)\n",
        "model = rnd_search_cv.best_estimator_.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P05Zcwxe12EF",
        "outputId": "1f69b462-e469-48fb-be4d-7975a70ae338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_123\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " batch_normalization_123 (Ba  (None, 11)               44        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_308 (Dense)           (None, 93)                1116      \n",
            "                                                                 \n",
            " dense_309 (Dense)           (None, 1)                 94        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,254\n",
            "Trainable params: 1,232\n",
            "Non-trainable params: 22\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "XrZbOjDrWiOz"
      },
      "outputs": [],
      "source": [
        "def accuracy(y, y_preds): # acc = Tp / Total\n",
        "    true_counter = 0\n",
        "    for i in range(len(y)):\n",
        "        if y[i] == np.round(y_preds[i]) or abs(y[i] - np.round(y_preds[i]) == 1): # margin 1 is accepted\n",
        "            true_counter += 1\n",
        "    return (true_counter / len(y)) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9VQXH81FeDN",
        "outputId": "d4ec23d4-c952-4025-9220-d226fb439c19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40/40 [==============================] - 0s 2ms/step\n",
            "40/40 [==============================] - 0s 2ms/step - loss: 0.2645 - mse: 0.2645 - mae: 0.3960\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2645290791988373, 0.2645290791988373, 0.39598965644836426]"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "y_trainPreds = model.predict(x_train)\n",
        "model.evaluate(x_train, y_train) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHATBhY7W6qB",
        "outputId": "213e0df6-6061-4e38-dd58-6510d4b2c332"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83.18999218139172"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "y_train = y_train.reset_index(drop = True)\n",
        "accuracy(y_train, y_trainPreds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO3LqafAFpog",
        "outputId": "8a6635b4-2e12-488f-bcba-ab8ff9e08c25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 3ms/step\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 1.7977e-13 - mse: 1.7977e-13 - mae: 2.9951e-07\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.7976730672629448e-13, 1.7976730672629448e-13, 2.995133456806798e-07]"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "y_testPreds = model.predict(x_test)\n",
        "model.evaluate(x_test, y_testPreds) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch1bW3EzXQMQ",
        "outputId": "177fcb06-7722-462c-bdf4-39c35eba5a7d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80.9375"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "source": [
        "y_test = y_test.reset_index(drop = True)\n",
        "accuracy(y_test, y_testPreds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "ewxf_vs5iUrA"
      },
      "outputs": [],
      "source": [
        "# keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "nWA-Arbm2iWy"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n",
        "# checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\",\n",
        "#                                                 save_best_only=True)\n",
        "# history = keras_reg.fit(x_train,y_train, validation_split=0.2, batch_size = 256, epochs = 1000, callbacks=[checkpoint_cb, early_stopping])\n",
        "# model = keras.models.load_model(\"my_keras_model.h5\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}